<?xml version="1.0" encoding="UTF-8"?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
<key id="resume" for="node" attr.name="resume"/>
<key id="date" for="node" attr.name="date"/>
<key id="sub_title" for="node" attr.name="sub_title"/>
<key id="data" for="node" attr.name="data"/>
<key id="location_detail" for="node" attr.name="location_detail"/>
<key id="language" for="node" attr.name="language"/>
<key id="title" for="node" attr.name="title"/>
<key id="constraints" for="node" attr.name="constraints"/>
<key id="url" for="node" attr.name="url"/>
<key id="content" for="node" attr.name="content"/>
<key id="labels" for="node" attr.name="labels"/>
<key id="start_time" for="node" attr.name="start_time"/>
<key id="full_body" for="node" attr.name="full_body"/>
<key id="price" for="node" attr.name="price"/>
<key id="intro" for="node" attr.name="intro"/>
<key id="name" for="node" attr.name="name"/>
<key id="annotated_by" for="node" attr.name="annotated_by"/>
<key id="subscription_limit" for="node" attr.name="subscription_limit"/>
<key id="details" for="node" attr.name="details"/>
<key id="course_info" for="node" attr.name="course_info"/>
<key id="id" for="node" attr.name="id"/>
<key id="target_group" for="node" attr.name="target_group"/>
<key id="category" for="node" attr.name="category"/>
<key id="label" for="edge" attr.name="label"/>
<key id="score" for="edge" attr.name="score"/>
<graph id="G" edgedefault="directed">
<node id="n0" labels=":Skill"><data key="labels">:Skill</data><data key="name">sql server analysis services</data><data key="category">ICT</data></node>
<node id="n1" labels=":Skill"><data key="labels">:Skill</data><data key="category">ICT</data><data key="name">sql server integration services</data></node>
<node id="n2" labels=":Skill"><data key="labels">:Skill</data><data key="name">sql server reporting services</data><data key="category">ICT</data></node>
<node id="n3" labels=":Skill"><data key="labels">:Skill</data><data key="name">critical thinking</data><data key="category">ICT</data></node>
<node id="n4" labels=":Skill"><data key="labels">:Skill</data><data key="name">dashboarding</data><data key="category">ICT</data></node>
<node id="n5" labels=":Skill"><data key="labels">:Skill</data><data key="name">use cases</data><data key="category">ICT</data></node>
<node id="n6" labels=":Skill"><data key="labels">:Skill</data><data key="name">prototyping</data><data key="category">ICT</data></node>
<node id="n7" labels=":Skill"><data key="labels">:Skill</data><data key="name">functiona requirements</data><data key="category">ICT</data></node>
<node id="n8" labels=":Skill"><data key="labels">:Skill</data><data key="name">business requirements</data><data key="category">ICT</data></node>
<node id="n9" labels=":Skill"><data key="labels">:Skill</data><data key="category">ICT</data><data key="name">user acceptance testing</data></node>
<node id="n10" labels=":Skill"><data key="labels">:Skill</data><data key="name">business process modeling</data><data key="category">ICT</data></node>
<node id="n11" labels=":Skill"><data key="labels">:Skill</data><data key="name">quality assurance</data><data key="category">ICT</data></node>
<node id="n12" labels=":Skill"><data key="labels">:Skill</data><data key="name">resource allocation</data><data key="category">ICT</data></node>
<node id="n13" labels=":Skill"><data key="labels">:Skill</data><data key="name">graphic design</data><data key="category">ICT</data></node>
<node id="n14" labels=":Skill"><data key="labels">:Skill</data><data key="name">data-driven decision-making</data><data key="category">ICT</data></node>
<node id="n15" labels=":Skill"><data key="labels">:Skill</data><data key="name">social media marketing</data><data key="category">ICT</data></node>
<node id="n16" labels=":Skill"><data key="labels">:Skill</data><data key="name">risk assessment and mitigation</data><data key="category">ICT</data></node>
<node id="n17" labels=":Skill"><data key="labels">:Skill</data><data key="name">cybersecurity</data><data key="category">ICT</data></node>
<node id="n18" labels=":Skill"><data key="labels">:Skill</data><data key="name">threat detection measures</data><data key="category">ICT</data></node>
<node id="n19" labels=":Skill"><data key="labels">:Skill</data><data key="name">adobe creative suite</data><data key="category">ICT</data></node>
<node id="n20" labels=":Skill"><data key="labels">:Skill</data><data key="name">project planning</data><data key="category">ICT</data></node>
<node id="n21" labels=":Skill"><data key="labels">:Skill</data><data key="name">research</data><data key="category">ICT</data></node>
<node id="n22" labels=":Skill"><data key="labels">:Skill</data><data key="name">restful apis</data><data key="category">ICT</data></node>
<node id="n23" labels=":Skill"><data key="labels">:Skill</data><data key="name">ux/ui</data><data key="category">ICT</data></node>
<node id="n24" labels=":Skill"><data key="labels">:Skill</data><data key="name">data governance</data><data key="category">ICT</data></node>
<node id="n25" labels=":Skill"><data key="labels">:Skill</data><data key="name">data warehousing</data><data key="category">ICT</data></node>
<node id="n26" labels=":Skill"><data key="labels">:Skill</data><data key="name">time series forecasting</data><data key="category">ICT</data></node>
<node id="n27" labels=":Skill"><data key="labels">:Skill</data><data key="name">sentiment analysis</data><data key="category">ICT</data></node>
<node id="n28" labels=":Skill"><data key="labels">:Skill</data><data key="name">data storytelling</data><data key="category">ICT</data></node>
<node id="n29" labels=":Skill"><data key="labels">:Skill</data><data key="name">data blending</data><data key="category">ICT</data></node>
<node id="n30" labels=":Skill"><data key="labels">:Skill</data><data key="name">data querying</data><data key="category">ICT</data></node>
<node id="n31" labels=":Skill"><data key="labels">:Skill</data><data key="name">data cleaning</data><data key="category">ICT</data></node>
<node id="n32" labels=":Skill"><data key="labels">:Skill</data><data key="name">kubernetes</data><data key="category">ICT</data></node>
<node id="n33" labels=":Skill"><data key="labels">:Skill</data><data key="name">shell</data><data key="category">ICT</data></node>
<node id="n34" labels=":Skill"><data key="labels">:Skill</data><data key="name">big data</data><data key="category">ICT</data></node>
<node id="n35" labels=":Skill"><data key="labels">:Skill</data><data key="name">business intelegence</data><data key="category">ICT</data></node>
<node id="n36" labels=":Skill"><data key="labels">:Skill</data><data key="name">latex</data><data key="category">ICT</data></node>
<node id="n37" labels=":Skill"><data key="labels">:Skill</data><data key="name">algorithms</data><data key="category">ICT</data></node>
<node id="n38" labels=":Skill"><data key="labels">:Skill</data><data key="name">reasearch analytics</data><data key="category">ICT</data></node>
<node id="n39" labels=":Skill"><data key="labels">:Skill</data><data key="name">data mining</data><data key="category">ICT</data></node>
<node id="n40" labels=":Skill"><data key="labels">:Skill</data><data key="name">regression analysis</data><data key="category">ICT</data></node>
<node id="n41" labels=":Skill"><data key="labels">:Skill</data><data key="name">statistical analysis</data><data key="category">ICT</data></node>
<node id="n42" labels=":Skill"><data key="labels">:Skill</data><data key="name">data transformation techniques</data><data key="category">ICT</data></node>
<node id="n43" labels=":Skill"><data key="labels">:Skill</data><data key="name">machine learning</data><data key="category">ICT</data></node>
<node id="n44" labels=":Skill"><data key="labels">:Skill</data><data key="name">deep learning</data><data key="category">ICT</data></node>
<node id="n45" labels=":Skill"><data key="labels">:Skill</data><data key="name">computer vision</data><data key="category">ICT</data></node>
<node id="n46" labels=":Skill"><data key="labels">:Skill</data><data key="name">natural language processing</data><data key="category">ICT</data></node>
<node id="n47" labels=":Skill"><data key="labels">:Skill</data><data key="name">optimization</data><data key="category">ICT</data></node>
<node id="n48" labels=":Skill"><data key="labels">:Skill</data><data key="name">linear algebra</data><data key="category">ICT</data></node>
<node id="n49" labels=":Skill"><data key="labels">:Skill</data><data key="name">calculus</data><data key="category">ICT</data></node>
<node id="n50" labels=":Skill"><data key="labels">:Skill</data><data key="name">statistics</data><data key="category">ICT</data></node>
<node id="n51" labels=":Skill"><data key="labels">:Skill</data><data key="name">probability</data><data key="category">ICT</data></node>
<node id="n52" labels=":Skill"><data key="labels">:Skill</data><data key="name">mathematics</data><data key="category">ICT</data></node>
<node id="n53" labels=":Skill"><data key="labels">:Skill</data><data key="name">data analysis</data><data key="category">ICT</data></node>
<node id="n54" labels=":Skill"><data key="labels">:Skill</data><data key="name">data visualization</data><data key="category">ICT</data></node>
<node id="n55" labels=":Skill"><data key="labels">:Skill</data><data key="name">modeling</data><data key="category">ICT</data></node>
<node id="n56" labels=":Skill"><data key="labels">:Skill</data><data key="name">data structures</data><data key="category">ICT</data></node>
<node id="n57" labels=":Skill"><data key="labels">:Skill</data><data key="name">blockchain</data><data key="category">ICT</data></node>
<node id="n58" labels=":Skill"><data key="labels">:Skill</data><data key="name">cryptography</data><data key="category">ICT</data></node>
<node id="n59" labels=":Skill"><data key="labels">:Skill</data><data key="name">smart contracts</data><data key="category">ICT</data></node>
<node id="n60" labels=":Skill"><data key="labels">:Skill</data><data key="name">networking</data><data key="category">ICT</data></node>
<node id="n61" labels=":Skill"><data key="labels">:Skill</data><data key="name">dml</data><data key="category">ICT</data></node>
<node id="n62" labels=":Skill"><data key="labels">:Skill</data><data key="name">rest web services</data><data key="category">ICT</data></node>
<node id="n63" labels=":Skill"><data key="labels">:Skill</data><data key="name">windows</data><data key="category">ICT</data></node>
<node id="n64" labels=":Skill"><data key="labels">:Skill</data><data key="name">technical assistance</data><data key="category">ICT</data></node>
<node id="n65" labels=":Skill"><data key="labels">:Skill</data><data key="name">unix</data><data key="category">ICT</data></node>
<node id="n66" labels=":Skill"><data key="labels">:Skill</data><data key="name">citrix xen server</data><data key="category">ICT</data></node>
<node id="n67" labels=":Skill"><data key="labels">:Skill</data><data key="name">web application testing</data><data key="category">ICT</data></node>
<node id="n68" labels=":Skill"><data key="labels">:Skill</data><data key="name">cyara</data><data key="category">ICT</data></node>
<node id="n69" labels=":Skill"><data key="labels">:Skill</data><data key="name">linux</data><data key="category">ICT</data></node>
<node id="n70" labels=":Skill"><data key="labels">:Skill</data><data key="name">nagios</data><data key="category">ICT</data></node>
<node id="n71" labels=":Skill"><data key="labels">:Skill</data><data key="name">software testing</data><data key="category">ICT</data></node>
<node id="n72" labels=":Skill"><data key="labels">:Skill</data><data key="name">eclipse</data><data key="category">ICT</data></node>
<node id="n73" labels=":Skill"><data key="labels">:Skill</data><data key="name">sonar qube</data><data key="category">ICT</data></node>
<node id="n74" labels=":Skill"><data key="labels">:Skill</data><data key="name">c#.net</data><data key="category">ICT</data></node>
<node id="n75" labels=":Skill"><data key="labels">:Skill</data><data key="name">apache</data><data key="category">ICT</data></node>
<node id="n76" labels=":Skill"><data key="labels">:Skill</data><data key="name">data modeling</data><data key="category">ICT</data></node>
<node id="n77" labels=":Skill"><data key="labels">:Skill</data><data key="name">java servlet</data><data key="category">ICT</data></node>
<node id="n78" labels=":Skill"><data key="labels">:Skill</data><data key="name">selenium webdriver</data><data key="category">ICT</data></node>
<node id="n79" labels=":Skill"><data key="labels">:Skill</data><data key="name">web development</data><data key="category">ICT</data></node>
<node id="n80" labels=":Skill"><data key="labels">:Skill</data><data key="name">internet of things</data><data key="category">ICT</data></node>
<node id="n81" labels=":Skill"><data key="labels">:Skill</data><data key="name">r studio</data><data key="category">ICT</data></node>
<node id="n82" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms project</data><data key="category">ICT</data></node>
<node id="n83" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms outlook</data><data key="category">ICT</data></node>
<node id="n84" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms office</data><data key="category">ICT</data></node>
<node id="n85" labels=":Skill"><data key="labels">:Skill</data><data key="name">iceberg</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n86" labels=":Skill"><data key="labels">:Skill</data><data key="name">ccna</data><data key="category">ICT</data></node>
<node id="n87" labels=":Skill"><data key="labels">:Skill</data><data key="name">ipsec</data><data key="category">ICT</data></node>
<node id="n88" labels=":Skill"><data key="labels">:Skill</data><data key="name">android</data><data key="category">ICT</data></node>
<node id="n89" labels=":Skill"><data key="labels">:Skill</data><data key="name">hsrp</data><data key="category">ICT</data></node>
<node id="n90" labels=":Skill"><data key="labels">:Skill</data><data key="name">cpp</data><data key="category">ICT</data></node>
<node id="n91" labels=":Skill"><data key="labels">:Skill</data><data key="name">application designer</data><data key="category">ICT</data></node>
<node id="n92" labels=":Skill"><data key="labels">:Skill</data><data key="name">netbeans</data><data key="category">ICT</data></node>
<node id="n93" labels=":Skill"><data key="labels">:Skill</data><data key="name">cloud computing</data><data key="category">ICT</data></node>
<node id="n94" labels=":Skill"><data key="labels">:Skill</data><data key="name">openshit</data><data key="category">ICT</data></node>
<node id="n95" labels=":Skill"><data key="labels">:Skill</data><data key="name">web designing</data><data key="category">ICT</data></node>
<node id="n96" labels=":Skill"><data key="labels">:Skill</data><data key="name">jdeveloper</data><data key="category">ICT</data></node>
<node id="n97" labels=":Skill"><data key="labels">:Skill</data><data key="name">computer and firesafety</data><data key="category">ICT</data></node>
<node id="n98" labels=":Skill"><data key="labels">:Skill</data><data key="name">cisco</data><data key="category">ICT</data></node>
<node id="n99" labels=":Skill"><data key="labels">:Skill</data><data key="name">sql developer</data><data key="category">ICT</data></node>
<node id="n100" labels=":Skill"><data key="labels">:Skill</data><data key="name">computer networks</data><data key="category">ICT</data></node>
<node id="n101" labels=":Skill"><data key="labels">:Skill</data><data key="name">ansible</data><data key="category">ICT</data></node>
<node id="n102" labels=":Skill"><data key="labels">:Skill</data><data key="name">bootstrap</data><data key="category">ICT</data></node>
<node id="n103" labels=":Skill"><data key="labels">:Skill</data><data key="name">application software</data><data key="category">ICT</data></node>
<node id="n104" labels=":Skill"><data key="labels">:Skill</data><data key="name">micro services</data><data key="category">ICT</data></node>
<node id="n105" labels=":Skill"><data key="labels">:Skill</data><data key="name">putty</data><data key="category">ICT</data></node>
<node id="n106" labels=":Skill"><data key="labels">:Skill</data><data key="name">spufi</data><data key="category">ICT</data></node>
<node id="n107" labels=":Skill"><data key="labels">:Skill</data><data key="name">agile</data><data key="category">ICT</data></node>
<node id="n108" labels=":Skill"><data key="labels">:Skill</data><data key="name">hacking</data><data key="category">ICT</data></node>
<node id="n109" labels=":Skill"><data key="labels">:Skill</data><data key="name">bgp</data><data key="category">ICT</data></node>
<node id="n110" labels=":Skill"><data key="labels">:Skill</data><data key="name">amdp</data><data key="category">ICT</data></node>
<node id="n111" labels=":Skill"><data key="labels">:Skill</data><data key="name">artificial intelligence</data><data key="category">ICT</data></node>
<node id="n112" labels=":Skill"><data key="labels">:Skill</data><data key="category">ICT</data><data key="name">application / software development</data></node>
<node id="n113" labels=":Skill"><data key="labels">:Skill</data><data key="name">mpls</data><data key="category">ICT</data></node>
<node id="n114" labels=":Skill"><data key="labels">:Skill</data><data key="name">program management</data><data key="category">ICT</data></node>
<node id="n115" labels=":Skill"><data key="labels">:Skill</data><data key="name">kibana</data><data key="category">ICT</data></node>
<node id="n116" labels=":Skill"><data key="labels">:Skill</data><data key="name">maven</data><data key="category">ICT</data></node>
<node id="n117" labels=":Skill"><data key="labels">:Skill</data><data key="name">open vz</data><data key="category">ICT</data></node>
<node id="n118" labels=":Skill"><data key="labels">:Skill</data><data key="name">hibernate</data><data key="category">ICT</data></node>
<node id="n119" labels=":Skill"><data key="labels">:Skill</data><data key="name">routing protocols.</data><data key="category">ICT</data></node>
<node id="n120" labels=":Skill"><data key="labels">:Skill</data><data key="name">cricket</data><data key="category">ICT</data></node>
<node id="n121" labels=":Skill"><data key="labels">:Skill</data><data key="name">automation</data><data key="category">ICT</data></node>
<node id="n122" labels=":Skill"><data key="labels">:Skill</data><data key="name">lithium</data><data key="category">ICT</data></node>
<node id="n123" labels=":Skill"><data key="labels">:Skill</data><data key="name">json</data><data key="category">ICT</data></node>
<node id="n124" labels=":Skill"><data key="labels">:Skill</data><data key="name">socket programming</data><data key="category">ICT</data></node>
<node id="n125" labels=":Skill"><data key="labels">:Skill</data><data key="name">ejb</data><data key="category">ICT</data></node>
<node id="n126" labels=":Skill"><data key="labels">:Skill</data><data key="name">nexus</data><data key="category">ICT</data></node>
<node id="n127" labels=":Skill"><data key="labels">:Skill</data><data key="name">html5</data><data key="category">ICT</data></node>
<node id="n128" labels=":Skill"><data key="labels">:Skill</data><data key="name">hyper-</data><data key="category">ICT</data></node>
<node id="n129" labels=":Skill"><data key="labels">:Skill</data><data key="name">servlet</data><data key="category">ICT</data></node>
<node id="n130" labels=":Skill"><data key="labels">:Skill</data><data key="name">jms</data><data key="category">ICT</data></node>
<node id="n131" labels=":Skill"><data key="labels">:Skill</data><data key="name">multicast</data><data key="category">ICT</data></node>
<node id="n132" labels=":Skill"><data key="labels">:Skill</data><data key="name">windows services</data><data key="category">ICT</data></node>
<node id="n133" labels=":Skill"><data key="labels">:Skill</data><data key="name">hardware &amp; networking</data><data key="category">ICT</data></node>
<node id="n134" labels=":Skill"><data key="labels">:Skill</data><data key="name">sapui5</data><data key="category">ICT</data></node>
<node id="n135" labels=":Skill"><data key="labels">:Skill</data><data key="name">gephi</data><data key="category">ICT</data></node>
<node id="n136" labels=":Skill"><data key="labels">:Skill</data><data key="name">object oriented programming</data><data key="category">ICT</data></node>
<node id="n137" labels=":Skill"><data key="labels">:Skill</data><data key="name">struts</data><data key="category">ICT</data></node>
<node id="n138" labels=":Skill"><data key="labels">:Skill</data><data key="name">active directory</data><data key="category">ICT</data></node>
<node id="n139" labels=":Skill"><data key="labels">:Skill</data><data key="name">postman</data><data key="category">ICT</data></node>
<node id="n140" labels=":Skill"><data key="labels">:Skill</data><data key="name">vb</data><data key="category">ICT</data></node>
<node id="n141" labels=":Skill"><data key="labels">:Skill</data><data key="name"> sapui5 (primary skill)</data><data key="category">ICT</data></node>
<node id="n142" labels=":Skill"><data key="labels">:Skill</data><data key="name">market basket analysis</data><data key="category">ICT</data></node>
<node id="n143" labels=":Skill"><data key="labels">:Skill</data><data key="category">ICT</data><data key="name">web authoring tools html 5</data></node>
<node id="n144" labels=":Skill"><data key="labels">:Skill</data><data key="name">vportal</data><data key="category">ICT</data></node>
<node id="n145" labels=":Skill"><data key="labels">:Skill</data><data key="name">network management</data><data key="category">ICT</data></node>
<node id="n146" labels=":Skill"><data key="labels">:Skill</data><data key="name">inversion of control</data><data key="category">ICT</data></node>
<node id="n147" labels=":Skill"><data key="labels">:Skill</data><data key="name">ajax &amp; jquery</data><data key="category">ICT</data></node>
<node id="n148" labels=":Skill"><data key="labels">:Skill</data><data key="name">programming</data><data key="category">ICT</data></node>
<node id="n149" labels=":Skill"><data key="labels">:Skill</data><data key="name">problem solving</data><data key="category">ICT</data></node>
<node id="n150" labels=":Skill"><data key="labels">:Skill</data><data key="name">functional testing</data><data key="category">ICT</data></node>
<node id="n151" labels=":Skill"><data key="labels">:Skill</data><data key="name">octopus</data><data key="category">ICT</data></node>
<node id="n152" labels=":Skill"><data key="labels">:Skill</data><data key="name">pl/sql developer</data><data key="category">ICT</data></node>
<node id="n153" labels=":Skill"><data key="labels">:Skill</data><data key="name">windows xp</data><data key="category">ICT</data></node>
<node id="n154" labels=":Skill"><data key="labels">:Skill</data><data key="name">ospf</data><data key="category">ICT</data></node>
<node id="n155" labels=":Skill"><data key="labels">:Skill</data><data key="name">work devotee</data><data key="category">ICT</data></node>
<node id="n156" labels=":Skill"><data key="labels">:Skill</data><data key="name">android studio</data><data key="category">ICT</data></node>
<node id="n157" labels=":Skill"><data key="labels">:Skill</data><data key="name">exchange</data><data key="category">ICT</data></node>
<node id="n158" labels=":Skill"><data key="labels">:Skill</data><data key="name">frontend html and .net</data><data key="category">ICT</data></node>
<node id="n159" labels=":Skill"><data key="labels">:Skill</data><data key="name">predictive modelling</data><data key="category">ICT</data></node>
<node id="n160" labels=":Skill"><data key="labels">:Skill</data><data key="name">kvm</data><data key="category">ICT</data></node>
<node id="n161" labels=":Skill"><data key="labels">:Skill</data><data key="name">crm</data><data key="category">ICT</data></node>
<node id="n162" labels=":Skill"><data key="labels">:Skill</data><data key="name">resource planning</data><data key="category">ICT</data></node>
<node id="n163" labels=":Skill"><data key="labels">:Skill</data><data key="name">stl</data><data key="category">ICT</data></node>
<node id="n164" labels=":Skill"><data key="labels">:Skill</data><data key="name">domain name system</data><data key="category">ICT</data></node>
<node id="n165" labels=":Skill"><data key="labels">:Skill</data><data key="name">accounts payable-fi-a/p</data><data key="category">ICT</data></node>
<node id="n166" labels=":Skill"><data key="labels">:Skill</data><data key="name">mac</data><data key="category">ICT</data></node>
<node id="n167" labels=":Skill"><data key="labels">:Skill</data><data key="name">kannada</data><data key="category">ICT</data></node>
<node id="n168" labels=":Skill"><data key="labels">:Skill</data><data key="name">it literacy</data><data key="category">ICT</data></node>
<node id="n169" labels=":Skill"><data key="labels">:Skill</data><data key="name">siem</data><data key="category">ICT</data></node>
<node id="n170" labels=":Skill"><data key="labels">:Skill</data><data key="name">highly dedicated towards work</data><data key="category">ICT</data></node>
<node id="n171" labels=":Skill"><data key="labels">:Skill</data><data key="name">ansys</data><data key="category">ICT</data></node>
<node id="n172" labels=":Skill"><data key="labels">:Skill</data><data key="name">bid management</data><data key="category">ICT</data></node>
<node id="n173" labels=":Skill"><data key="labels">:Skill</data><data key="name">wireshark</data><data key="category">ICT</data></node>
<node id="n174" labels=":Skill"><data key="labels">:Skill</data><data key="name">adobe flex</data><data key="category">ICT</data></node>
<node id="n175" labels=":Skill"><data key="labels">:Skill</data><data key="name">xml</data><data key="category">ICT</data></node>
<node id="n176" labels=":Skill"><data key="labels">:Skill</data><data key="name">tally</data><data key="category">ICT</data></node>
<node id="n177" labels=":Skill"><data key="labels">:Skill</data><data key="name">testng</data><data key="category">ICT</data></node>
<node id="n178" labels=":Skill"><data key="labels">:Skill</data><data key="name">data driven</data><data key="category">ICT</data></node>
<node id="n179" labels=":Skill"><data key="labels">:Skill</data><data key="name">marathi</data><data key="category">ICT</data></node>
<node id="n180" labels=":Skill"><data key="labels">:Skill</data><data key="name">svn</data><data key="category">ICT</data></node>
<node id="n181" labels=":Skill"><data key="labels">:Skill</data><data key="name">wsus</data><data key="category">ICT</data></node>
<node id="n182" labels=":Skill"><data key="labels">:Skill</data><data key="name">oracle system upgrades</data><data key="category">ICT</data></node>
<node id="n183" labels=":Skill"><data key="labels">:Skill</data><data key="name">mobile applications</data><data key="category">ICT</data></node>
<node id="n184" labels=":Skill"><data key="labels">:Skill</data><data key="name">page object model</data><data key="category">ICT</data></node>
<node id="n185" labels=":Skill"><data key="labels">:Skill</data><data key="name">design patterns</data><data key="category">ICT</data></node>
<node id="n186" labels=":Skill"><data key="labels">:Skill</data><data key="name">typewriting</data><data key="category">ICT</data></node>
<node id="n187" labels=":Skill"><data key="labels">:Skill</data><data key="name">network security</data><data key="category">ICT</data></node>
<node id="n188" labels=":Skill"><data key="labels">:Skill</data><data key="name">project management</data><data key="category">ICT</data></node>
<node id="n189" labels=":Skill"><data key="labels">:Skill</data><data key="name">d3js</data><data key="category">ICT</data></node>
<node id="n190" labels=":Skill"><data key="labels">:Skill</data><data key="name">automation testing</data><data key="category">ICT</data></node>
<node id="n191" labels=":Skill"><data key="labels">:Skill</data><data key="name">bamboo</data><data key="category">ICT</data></node>
<node id="n192" labels=":Skill"><data key="labels">:Skill</data><data key="name">sap ui5/fiori</data><data key="category">ICT</data></node>
<node id="n193" labels=":Skill"><data key="labels">:Skill</data><data key="name">scrum</data><data key="category">ICT</data></node>
<node id="n194" labels=":Skill"><data key="labels">:Skill</data><data key="name">service virtualization</data><data key="category">ICT</data></node>
<node id="n195" labels=":Skill"><data key="labels">:Skill</data><data key="name">waf</data><data key="category">ICT</data></node>
<node id="n196" labels=":Skill"><data key="labels">:Skill</data><data key="name">relay server</data><data key="category">ICT</data></node>
<node id="n197" labels=":Skill"><data key="labels">:Skill</data><data key="name">sauce labs</data><data key="category">ICT</data></node>
<node id="n198" labels=":Skill"><data key="labels">:Skill</data><data key="name">java &amp; j2ee</data><data key="category">ICT</data></node>
<node id="n199" labels=":Skill"><data key="labels">:Skill</data><data key="name">computer hardware</data><data key="category">ICT</data></node>
<node id="n200" labels=":Skill"><data key="labels">:Skill</data><data key="name">catia v6</data><data key="category">ICT</data></node>
<node id="n201" labels=":Skill"><data key="labels">:Skill</data><data key="name">mobile testing</data><data key="category">ICT</data></node>
<node id="n202" labels=":Skill"><data key="labels">:Skill</data><data key="name">mcafee esm</data><data key="category">ICT</data></node>
<node id="n203" labels=":Skill"><data key="labels">:Skill</data><data key="name">microsoft visual studio</data><data key="category">ICT</data></node>
<node id="n204" labels=":Skill"><data key="labels">:Skill</data><data key="name">ubuntu linux</data><data key="category">ICT</data></node>
<node id="n205" labels=":Skill"><data key="labels">:Skill</data><data key="name">ajax</data><data key="category">ICT</data></node>
<node id="n206" labels=":Skill"><data key="labels">:Skill</data><data key="name">adf</data><data key="category">ICT</data></node>
<node id="n207" labels=":Skill"><data key="labels">:Skill</data><data key="name">syslog sender</data><data key="category">ICT</data></node>
<node id="n208" labels=":Skill"><data key="labels">:Skill</data><data key="name">tcl</data><data key="category">ICT</data></node>
<node id="n209" labels=":Skill"><data key="labels">:Skill</data><data key="name">adobe photoshop</data><data key="category">ICT</data></node>
<node id="n210" labels=":Skill"><data key="labels">:Skill</data><data key="name">database management system</data><data key="category">ICT</data></node>
<node id="n211" labels=":Skill"><data key="labels">:Skill</data><data key="name">httpclient</data><data key="category">ICT</data></node>
<node id="n212" labels=":Skill"><data key="labels">:Skill</data><data key="name">radtool</data><data key="category">ICT</data></node>
<node id="n213" labels=":Skill"><data key="labels">:Skill</data><data key="name">creo parametric 2.0</data><data key="category">ICT</data></node>
<node id="n214" labels=":Skill"><data key="labels">:Skill</data><data key="name">jenkins</data><data key="category">ICT</data></node>
<node id="n215" labels=":Skill"><data key="labels">:Skill</data><data key="category">ICT</data><data key="name">cisco monitoring tools: em7</data></node>
<node id="n216" labels=":Skill"><data key="labels">:Skill</data><data key="name">sublime</data><data key="category">ICT</data></node>
<node id="n217" labels=":Skill"><data key="labels">:Skill</data><data key="name">mockito</data><data key="category">ICT</data></node>
<node id="n218" labels=":Skill"><data key="labels">:Skill</data><data key="name">creative team leadership</data><data key="category">ICT</data></node>
<node id="n219" labels=":Skill"><data key="labels">:Skill</data><data key="name">winscp</data><data key="category">ICT</data></node>
<node id="n220" labels=":Skill"><data key="labels">:Skill</data><data key="name">l3vpn</data><data key="category">ICT</data></node>
<node id="n221" labels=":Skill"><data key="labels">:Skill</data><data key="name">clustering</data><data key="category">ICT</data></node>
<node id="n222" labels=":Skill"><data key="labels">:Skill</data><data key="name">metalogix</data><data key="category">ICT</data></node>
<node id="n223" labels=":Skill"><data key="labels">:Skill</data><data key="name">sharegate</data><data key="category">ICT</data></node>
<node id="n224" labels=":Skill"><data key="labels">:Skill</data><data key="name">iterative development</data><data key="category">ICT</data></node>
<node id="n225" labels=":Skill"><data key="labels">:Skill</data><data key="category">ICT</data><data key="name">data structures &amp; algorithms</data></node>
<node id="n226" labels=":Skill"><data key="labels">:Skill</data><data key="name">jackson-2</data><data key="category">ICT</data></node>
<node id="n227" labels=":Skill"><data key="labels">:Skill</data><data key="name">sdet</data><data key="category">ICT</data></node>
<node id="n228" labels=":Skill"><data key="labels">:Skill</data><data key="name">frameworks (c#) 4.0</data><data key="category">ICT</data></node>
<node id="n229" labels=":Skill"><data key="labels">:Skill</data><data key="name">cucumber</data><data key="category">ICT</data></node>
<node id="n230" labels=":Skill"><data key="labels">:Skill</data><data key="name">inside sales</data><data key="category">ICT</data></node>
<node id="n231" labels=":Skill"><data key="labels">:Skill</data><data key="name">jdbc</data><data key="category">ICT</data></node>
<node id="n232" labels=":Skill"><data key="labels">:Skill</data><data key="name">oracle peoplesoft</data><data key="category">ICT</data></node>
<node id="n233" labels=":Skill"><data key="labels">:Skill</data><data key="name">dhcp</data><data key="category">ICT</data></node>
<node id="n234" labels=":Skill"><data key="labels">:Skill</data><data key="name">e-commerce</data><data key="category">ICT</data></node>
<node id="n235" labels=":Skill"><data key="labels">:Skill</data><data key="name">soap web services</data><data key="category">ICT</data></node>
<node id="n236" labels=":Skill"><data key="labels">:Skill</data><data key="name">soap ui</data><data key="category">ICT</data></node>
<node id="n237" labels=":Skill"><data key="labels">:Skill</data><data key="name">tomcat</data><data key="category">ICT</data></node>
<node id="n238" labels=":Skill"><data key="labels">:Skill</data><data key="name">esxi</data><data key="category">ICT</data></node>
<node id="n239" labels=":Skill"><data key="labels">:Skill</data><data key="name">gitlab</data><data key="category">ICT</data></node>
<node id="n240" labels=":Skill"><data key="labels">:Skill</data><data key="name">windows 7</data><data key="category">ICT</data></node>
<node id="n241" labels=":Skill"><data key="labels">:Skill</data><data key="name">salesforce</data><data key="category">ICT</data></node>
<node id="n242" labels=":Skill"><data key="labels">:Skill</data><data key="name">multimedia</data><data key="category">ICT</data></node>
<node id="n243" labels=":Skill"><data key="labels">:Skill</data><data key="name">analytical and logical skills</data><data key="category">ICT</data></node>
<node id="n244" labels=":Skill"><data key="labels">:Skill</data><data key="name">windows 8</data><data key="category">ICT</data></node>
<node id="n245" labels=":Skill"><data key="labels">:Skill</data><data key="category">ICT</data><data key="name">software development life cycle</data></node>
<node id="n246" labels=":Skill"><data key="labels">:Skill</data><data key="name">jsp</data><data key="category">ICT</data></node>
<node id="n247" labels=":Skill"><data key="labels">:Skill</data><data key="name">rest</data><data key="category">ICT</data></node>
<node id="n248" labels=":Skill"><data key="labels">:Skill</data><data key="name">junit</data><data key="category">ICT</data></node>
<node id="n249" labels=":Skill"><data key="labels">:Skill</data><data key="name">power center</data><data key="category">ICT</data></node>
<node id="n250" labels=":Skill"><data key="labels">:Skill</data><data key="name">spring mvc</data><data key="category">ICT</data></node>
<node id="n251" labels=":Skill"><data key="labels">:Skill</data><data key="name">sentimental analysis</data><data key="category">ICT</data></node>
<node id="n252" labels=":Skill"><data key="labels">:Skill</data><data key="name">lan/wan</data><data key="category">ICT</data></node>
<node id="n253" labels=":Skill"><data key="labels">:Skill</data><data key="name">flexbuilder</data><data key="category">ICT</data></node>
<node id="n254" labels=":Skill"><data key="labels">:Skill</data><data key="name">middleware mvc and wcf</data><data key="category">ICT</data></node>
<node id="n255" labels=":Skill"><data key="labels">:Skill</data><data key="name">tortoise svn</data><data key="category">ICT</data></node>
<node id="n256" labels=":Skill"><data key="labels">:Skill</data><data key="name">waterfall</data><data key="category">ICT</data></node>
<node id="n257" labels=":Skill"><data key="labels">:Skill</data><data key="name">oracle sql developer</data><data key="category">ICT</data></node>
<node id="n258" labels=":Skill"><data key="labels">:Skill</data><data key="name">kabbadi</data><data key="category">ICT</data></node>
<node id="n259" labels=":Skill"><data key="labels">:Skill</data><data key="name">soap</data><data key="category">ICT</data></node>
<node id="n260" labels=":Skill"><data key="labels">:Skill</data><data key="name">qmf</data><data key="category">ICT</data></node>
<node id="n261" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms visio</data><data key="category">ICT</data></node>
<node id="n262" labels=":Skill"><data key="labels">:Skill</data><data key="name">abap/4</data><data key="category">ICT</data></node>
<node id="n263" labels=":Skill"><data key="labels">:Skill</data><data key="name">o365</data><data key="category">ICT</data></node>
<node id="n264" labels=":Skill"><data key="labels">:Skill</data><data key="name">cloustack</data><data key="category">ICT</data></node>
<node id="n265" labels=":Skill"><data key="labels">:Skill</data><data key="name">jira</data><data key="category">ICT</data></node>
<node id="n266" labels=":Skill"><data key="labels">:Skill</data><data key="name">orm eclipse link</data><data key="category">ICT</data></node>
<node id="n267" labels=":Skill"><data key="labels">:Skill</data><data key="name">xpeditor</data><data key="category">ICT</data></node>
<node id="n268" labels=":Skill"><data key="labels">:Skill</data><data key="name">mainview</data><data key="category">ICT</data></node>
<node id="n269" labels=":Skill"><data key="labels">:Skill</data><data key="name">software integration</data><data key="category">ICT</data></node>
<node id="n270" labels=":Skill"><data key="labels">:Skill</data><data key="name">apache nifi</data><data key="category">ICT</data></node>
<node id="n271" labels=":Skill"><data key="labels">:Skill</data><data key="name">sap abap</data><data key="category">ICT</data></node>
<node id="n272" labels=":Skill"><data key="labels">:Skill</data><data key="name">sap hana</data><data key="category">ICT</data></node>
<node id="n273" labels=":Skill"><data key="labels">:Skill</data><data key="name">qtp</data><data key="category">ICT</data></node>
<node id="n274" labels=":Skill"><data key="labels">:Skill</data><data key="name">copilot</data><data key="category">ICT</data></node>
<node id="n275" labels=":Skill"><data key="labels">:Skill</data><data key="name">large language model</data><data key="category">ICT</data></node>
<node id="n276" labels=":Skill"><data key="labels">:Skill</data><data key="name">grafical neural network</data><data key="category">ICT</data></node>
<node id="n277" labels=":Skill"><data key="labels">:Skill</data><data key="name">sql</data><data key="category">Languages</data></node>
<node id="n278" labels=":Skill"><data key="labels">:Skill</data><data key="name">python</data><data key="category">Languages</data></node>
<node id="n279" labels=":Skill"><data key="labels">:Skill</data><data key="name">r</data><data key="category">Languages</data></node>
<node id="n280" labels=":Skill"><data key="labels">:Skill</data><data key="name">c</data><data key="category">Languages</data></node>
<node id="n281" labels=":Skill"><data key="labels">:Skill</data><data key="name">c#</data><data key="category">Languages</data></node>
<node id="n282" labels=":Skill"><data key="labels">:Skill</data><data key="name">javascript</data><data key="category">Languages</data></node>
<node id="n283" labels=":Skill"><data key="labels">:Skill</data><data key="name">js</data><data key="category">Languages</data></node>
<node id="n284" labels=":Skill"><data key="labels">:Skill</data><data key="name">java</data><data key="category">Languages</data></node>
<node id="n285" labels=":Skill"><data key="labels">:Skill</data><data key="name">scala</data><data key="category">Languages</data></node>
<node id="n286" labels=":Skill"><data key="labels">:Skill</data><data key="name">sas</data><data key="category">Languages</data></node>
<node id="n287" labels=":Skill"><data key="labels">:Skill</data><data key="name">matlab</data><data key="category">Languages</data></node>
<node id="n288" labels=":Skill"><data key="labels">:Skill</data><data key="name">c++</data><data key="category">Languages</data></node>
<node id="n289" labels=":Skill"><data key="labels">:Skill</data><data key="name">c / c++</data><data key="category">Languages</data></node>
<node id="n290" labels=":Skill"><data key="labels">:Skill</data><data key="name">perl</data><data key="category">Languages</data></node>
<node id="n291" labels=":Skill"><data key="labels">:Skill</data><data key="name">typescript</data><data key="category">Languages</data></node>
<node id="n292" labels=":Skill"><data key="labels">:Skill</data><data key="name">bash</data><data key="category">Languages</data></node>
<node id="n293" labels=":Skill"><data key="labels">:Skill</data><data key="name">html</data><data key="category">Languages</data></node>
<node id="n294" labels=":Skill"><data key="labels">:Skill</data><data key="name">css</data><data key="category">Languages</data></node>
<node id="n295" labels=":Skill"><data key="labels">:Skill</data><data key="name">php</data><data key="category">Languages</data></node>
<node id="n296" labels=":Skill"><data key="labels">:Skill</data><data key="name">powershell</data><data key="category">Languages</data></node>
<node id="n297" labels=":Skill"><data key="labels">:Skill</data><data key="name">rust</data><data key="category">Languages</data></node>
<node id="n298" labels=":Skill"><data key="labels">:Skill</data><data key="name">kotlin</data><data key="category">Languages</data></node>
<node id="n299" labels=":Skill"><data key="labels">:Skill</data><data key="name">ruby</data><data key="category">Languages</data></node>
<node id="n300" labels=":Skill"><data key="labels">:Skill</data><data key="name">dart</data><data key="category">Languages</data></node>
<node id="n301" labels=":Skill"><data key="labels">:Skill</data><data key="name">assembly</data><data key="category">Languages</data></node>
<node id="n302" labels=":Skill"><data key="labels">:Skill</data><data key="name">swift</data><data key="category">Languages</data></node>
<node id="n303" labels=":Skill"><data key="labels">:Skill</data><data key="name">vba</data><data key="category">Languages</data></node>
<node id="n304" labels=":Skill"><data key="labels">:Skill</data><data key="name">lua</data><data key="category">Languages</data></node>
<node id="n305" labels=":Skill"><data key="labels">:Skill</data><data key="name">groovy</data><data key="category">Languages</data></node>
<node id="n306" labels=":Skill"><data key="labels">:Skill</data><data key="name">delphi</data><data key="category">Languages</data></node>
<node id="n307" labels=":Skill"><data key="labels">:Skill</data><data key="name">objective-c</data><data key="category">Languages</data></node>
<node id="n308" labels=":Skill"><data key="labels">:Skill</data><data key="name">haskell</data><data key="category">Languages</data></node>
<node id="n309" labels=":Skill"><data key="labels">:Skill</data><data key="name">elixir</data><data key="category">Languages</data></node>
<node id="n310" labels=":Skill"><data key="labels">:Skill</data><data key="name">julia</data><data key="category">Languages</data></node>
<node id="n311" labels=":Skill"><data key="labels">:Skill</data><data key="name">clojure</data><data key="category">Languages</data></node>
<node id="n312" labels=":Skill"><data key="labels">:Skill</data><data key="name">solidity</data><data key="category">Languages</data></node>
<node id="n313" labels=":Skill"><data key="labels">:Skill</data><data key="name">lisp</data><data key="category">Languages</data></node>
<node id="n314" labels=":Skill"><data key="labels">:Skill</data><data key="name">f#</data><data key="category">Languages</data></node>
<node id="n315" labels=":Skill"><data key="labels">:Skill</data><data key="name">fortran</data><data key="category">Languages</data></node>
<node id="n316" labels=":Skill"><data key="labels">:Skill</data><data key="name">erlang</data><data key="category">Languages</data></node>
<node id="n317" labels=":Skill"><data key="labels">:Skill</data><data key="name">apl</data><data key="category">Languages</data></node>
<node id="n318" labels=":Skill"><data key="labels">:Skill</data><data key="name">cobol</data><data key="category">Languages</data></node>
<node id="n319" labels=":Skill"><data key="labels">:Skill</data><data key="name">ocaml</data><data key="category">Languages</data></node>
<node id="n320" labels=":Skill"><data key="labels">:Skill</data><data key="name">crystal</data><data key="category">Languages</data></node>
<node id="n321" labels=":Skill"><data key="labels">:Skill</data><data key="category">Languages</data><data key="name">javascript / typescript</data></node>
<node id="n322" labels=":Skill"><data key="labels">:Skill</data><data key="name">golang</data><data key="category">Languages</data></node>
<node id="n323" labels=":Skill"><data key="labels">:Skill</data><data key="name">nosql</data><data key="category">Languages</data></node>
<node id="n324" labels=":Skill"><data key="labels">:Skill</data><data key="name">transact-sql</data><data key="category">Languages</data></node>
<node id="n325" labels=":Skill"><data key="labels">:Skill</data><data key="name">no-sql</data><data key="category">Languages</data></node>
<node id="n326" labels=":Skill"><data key="labels">:Skill</data><data key="name">visual basic</data><data key="category">Languages</data></node>
<node id="n327" labels=":Skill"><data key="labels">:Skill</data><data key="name">pascal</data><data key="category">Languages</data></node>
<node id="n328" labels=":Skill"><data key="labels">:Skill</data><data key="name">mongo</data><data key="category">Languages</data></node>
<node id="n329" labels=":Skill"><data key="labels">:Skill</data><data key="name">pl/sql</data><data key="category">Languages</data></node>
<node id="n330" labels=":Skill"><data key="labels">:Skill</data><data key="name">sass</data><data key="category">Languages</data></node>
<node id="n331" labels=":Skill"><data key="labels">:Skill</data><data key="name">vb.net</data><data key="category">Languages</data></node>
<node id="n332" labels=":Skill"><data key="labels">:Skill</data><data key="name">mssql</data><data key="category">Languages</data></node>
<node id="n333" labels=":Skill"><data key="labels">:Skill</data><data key="name">etl</data><data key="category">Tools</data></node>
<node id="n334" labels=":Skill"><data key="labels">:Skill</data><data key="name">git</data><data key="category">Tools</data></node>
<node id="n335" labels=":Skill"><data key="labels">:Skill</data><data key="name">docker</data><data key="category">Tools</data></node>
<node id="n336" labels=":Skill"><data key="labels">:Skill</data><data key="name">tableau</data><data key="category">Tools</data></node>
<node id="n337" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms excel</data><data key="category">Tools</data></node>
<node id="n338" labels=":Skill"><data key="labels">:Skill</data><data key="name">power bi</data><data key="category">Tools</data></node>
<node id="n339" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms word</data><data key="category">Tools</data></node>
<node id="n340" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms powerpoint</data><data key="category">Tools</data></node>
<node id="n341" labels=":Skill"><data key="labels">:Skill</data><data key="name">sap</data><data key="category">Tools</data></node>
<node id="n342" labels=":Skill"><data key="labels">:Skill</data><data key="name">ssis</data><data key="category">Tools</data></node>
<node id="n343" labels=":Skill"><data key="labels">:Skill</data><data key="name">looker</data><data key="category">Tools</data></node>
<node id="n344" labels=":Skill"><data key="labels">:Skill</data><data key="name">qlik</data><data key="category">Tools</data></node>
<node id="n345" labels=":Skill"><data key="labels">:Skill</data><data key="name">alteryx</data><data key="category">Tools</data></node>
<node id="n346" labels=":Skill"><data key="labels">:Skill</data><data key="name">spss</data><data key="category">Tools</data></node>
<node id="n347" labels=":Skill"><data key="labels">:Skill</data><data key="name">ssrs</data><data key="category">Tools</data></node>
<node id="n348" labels=":Skill"><data key="labels">:Skill</data><data key="name">dax</data><data key="category">Tools</data></node>
<node id="n349" labels=":Skill"><data key="labels">:Skill</data><data key="name">sharepoint</data><data key="category">Tools</data></node>
<node id="n350" labels=":Skill"><data key="labels">:Skill</data><data key="name">splunk</data><data key="category">Tools</data></node>
<node id="n351" labels=":Skill"><data key="labels">:Skill</data><data key="name">cognos</data><data key="category">Tools</data></node>
<node id="n352" labels=":Skill"><data key="labels">:Skill</data><data key="name">visio</data><data key="category">Tools</data></node>
<node id="n353" labels=":Skill"><data key="labels">:Skill</data><data key="name">google sheets</data><data key="category">Tools</data></node>
<node id="n354" labels=":Skill"><data key="labels">:Skill</data><data key="name">spreadsheets</data><data key="category">Tools</data></node>
<node id="n355" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms access</data><data key="category">Tools</data></node>
<node id="n356" labels=":Skill"><data key="labels">:Skill</data><data key="name">datarobot</data><data key="category">Tools</data></node>
<node id="n357" labels=":Skill"><data key="labels">:Skill</data><data key="name">nuix</data><data key="category">Tools</data></node>
<node id="n358" labels=":Skill"><data key="labels">:Skill</data><data key="name">esquisse</data><data key="category">Tools</data></node>
<node id="n359" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms sql server</data><data key="category">Databases</data></node>
<node id="n360" labels=":Skill"><data key="labels">:Skill</data><data key="name">mysql</data><data key="category">Databases</data></node>
<node id="n361" labels=":Skill"><data key="labels">:Skill</data><data key="name">cassandra</data><data key="category">Databases</data></node>
<node id="n362" labels=":Skill"><data key="labels">:Skill</data><data key="name">postgresql</data><data key="category">Databases</data></node>
<node id="n363" labels=":Skill"><data key="labels">:Skill</data><data key="name">mongodb</data><data key="category">Databases</data></node>
<node id="n364" labels=":Skill"><data key="labels">:Skill</data><data key="name">elasticsearch</data><data key="category">Databases</data></node>
<node id="n365" labels=":Skill"><data key="labels">:Skill</data><data key="name">dynamodb</data><data key="category">Databases</data></node>
<node id="n366" labels=":Skill"><data key="labels">:Skill</data><data key="name">redis</data><data key="category">Databases</data></node>
<node id="n367" labels=":Skill"><data key="labels">:Skill</data><data key="name">db2</data><data key="category">Databases</data></node>
<node id="n368" labels=":Skill"><data key="labels">:Skill</data><data key="name">neo4j</data><data key="category">Databases</data></node>
<node id="n369" labels=":Skill"><data key="labels">:Skill</data><data key="name">mariadb</data><data key="category">Databases</data></node>
<node id="n370" labels=":Skill"><data key="labels">:Skill</data><data key="name">firebase</data><data key="category">Databases</data></node>
<node id="n371" labels=":Skill"><data key="labels">:Skill</data><data key="name">firestore</data><data key="category">Databases</data></node>
<node id="n372" labels=":Skill"><data key="labels">:Skill</data><data key="name">couchdb</data><data key="category">Databases</data></node>
<node id="n373" labels=":Skill"><data key="labels">:Skill</data><data key="name">sqlite</data><data key="category">Databases</data></node>
<node id="n374" labels=":Skill"><data key="labels">:Skill</data><data key="name">hbase</data><data key="category">Databases</data></node>
<node id="n375" labels=":Skill"><data key="labels">:Skill</data><data key="name">teradata</data><data key="category">Databases</data></node>
<node id="n376" labels=":Skill"><data key="labels">:Skill</data><data key="name">oracle</data><data key="category">Databases</data></node>
<node id="n377" labels=":Skill"><data key="labels">:Skill</data><data key="name">hive</data><data key="category">Databases</data></node>
<node id="n378" labels=":Skill"><data key="labels">:Skill</data><data key="name">redshift</data><data key="category">Databases</data></node>
<node id="n379" labels=":Skill"><data key="labels">:Skill</data><data key="name">snowflake</data><data key="category">Databases</data></node>
<node id="n380" labels=":Skill"><data key="labels">:Skill</data><data key="name">rdbms</data><data key="category">Databases</data></node>
<node id="n381" labels=":Skill"><data key="labels">:Skill</data><data key="name">azure</data><data key="category">Cloud</data></node>
<node id="n382" labels=":Skill"><data key="labels">:Skill</data><data key="category">Cloud</data><data key="name">google cloud platform</data></node>
<node id="n383" labels=":Skill"><data key="labels">:Skill</data><data key="name">aws cloud</data><data key="category">Cloud</data></node>
<node id="n384" labels=":Skill"><data key="labels">:Skill</data><data key="category">Cloud</data><data key="name">amazon web services</data></node>
<node id="n385" labels=":Skill"><data key="labels">:Skill</data><data key="name">databricks</data><data key="category">Cloud</data></node>
<node id="n386" labels=":Skill"><data key="labels">:Skill</data><data key="name">bigquery</data><data key="category">Cloud</data></node>
<node id="n387" labels=":Skill"><data key="labels">:Skill</data><data key="name">aurora</data><data key="category">Cloud</data></node>
<node id="n388" labels=":Skill"><data key="labels">:Skill</data><data key="name">vmware</data><data key="category">Cloud</data></node>
<node id="n389" labels=":Skill"><data key="labels">:Skill</data><data key="name">ibm cloud</data><data key="category">Cloud</data></node>
<node id="n390" labels=":Skill"><data key="labels">:Skill</data><data key="name">watson</data><data key="category">Cloud</data></node>
<node id="n391" labels=":Skill"><data key="labels">:Skill</data><data key="name">openstack</data><data key="category">Cloud</data></node>
<node id="n392" labels=":Skill"><data key="labels">:Skill</data><data key="name">heroku</data><data key="category">Cloud</data></node>
<node id="n393" labels=":Skill"><data key="labels">:Skill</data><data key="name">colocation</data><data key="category">Cloud</data></node>
<node id="n394" labels=":Skill"><data key="labels">:Skill</data><data key="name">ovh</data><data key="category">Cloud</data></node>
<node id="n395" labels=":Skill"><data key="labels">:Skill</data><data key="name">linode</data><data key="category">Cloud</data></node>
<node id="n396" labels=":Skill"><data key="labels">:Skill</data><data key="name">spark</data><data key="category">Libraries</data></node>
<node id="n397" labels=":Skill"><data key="labels">:Skill</data><data key="name">hadoop</data><data key="category">Libraries</data></node>
<node id="n398" labels=":Skill"><data key="labels">:Skill</data><data key="name">kafka</data><data key="category">Libraries</data></node>
<node id="n399" labels=":Skill"><data key="labels">:Skill</data><data key="name">airflow</data><data key="category">Libraries</data></node>
<node id="n400" labels=":Skill"><data key="labels">:Skill</data><data key="name">pyspark</data><data key="category">Libraries</data></node>
<node id="n401" labels=":Skill"><data key="labels">:Skill</data><data key="name">pandas</data><data key="category">Libraries</data></node>
<node id="n402" labels=":Skill"><data key="labels">:Skill</data><data key="name">tensorflow</data><data key="category">Libraries</data></node>
<node id="n403" labels=":Skill"><data key="labels">:Skill</data><data key="name">pytorch</data><data key="category">Libraries</data></node>
<node id="n404" labels=":Skill"><data key="labels">:Skill</data><data key="name">numpy</data><data key="category">Libraries</data></node>
<node id="n405" labels=":Skill"><data key="labels">:Skill</data><data key="name">scikit-learn</data><data key="category">Libraries</data></node>
<node id="n406" labels=":Skill"><data key="labels">:Skill</data><data key="name">keras</data><data key="category">Libraries</data></node>
<node id="n407" labels=":Skill"><data key="labels">:Skill</data><data key="name">jupyter</data><data key="category">Libraries</data></node>
<node id="n408" labels=":Skill"><data key="labels">:Skill</data><data key="name">react</data><data key="category">Libraries</data></node>
<node id="n409" labels=":Skill"><data key="labels">:Skill</data><data key="name">matpltlib</data><data key="category">Libraries</data></node>
<node id="n410" labels=":Skill"><data key="labels">:Skill</data><data key="name">spring</data><data key="category">Libraries</data></node>
<node id="n411" labels=":Skill"><data key="labels">:Skill</data><data key="name">gdpr</data><data key="category">Libraries</data></node>
<node id="n412" labels=":Skill"><data key="labels">:Skill</data><data key="name">plotly</data><data key="category">Libraries</data></node>
<node id="n413" labels=":Skill"><data key="labels">:Skill</data><data key="name">seaborn</data><data key="category">Libraries</data></node>
<node id="n414" labels=":Skill"><data key="labels">:Skill</data><data key="name">graphql</data><data key="category">Libraries</data></node>
<node id="n415" labels=":Skill"><data key="labels">:Skill</data><data key="name">nltk</data><data key="category">Libraries</data></node>
<node id="n416" labels=":Skill"><data key="labels">:Skill</data><data key="name">opencv</data><data key="category">Libraries</data></node>
<node id="n417" labels=":Skill"><data key="labels">:Skill</data><data key="name">ggplot2</data><data key="category">Libraries</data></node>
<node id="n418" labels=":Skill"><data key="labels">:Skill</data><data key="name">selenium</data><data key="category">Libraries</data></node>
<node id="n419" labels=":Skill"><data key="labels">:Skill</data><data key="name">mxnet</data><data key="category">Libraries</data></node>
<node id="n420" labels=":Skill"><data key="labels">:Skill</data><data key="name">tidyverse</data><data key="category">Libraries</data></node>
<node id="n421" labels=":Skill"><data key="labels">:Skill</data><data key="name">rshiny</data><data key="category">Libraries</data></node>
<node id="n422" labels=":Skill"><data key="labels">:Skill</data><data key="name">hugging face</data><data key="category">Libraries</data></node>
<node id="n423" labels=":Skill"><data key="labels">:Skill</data><data key="name">spacy</data><data key="category">Libraries</data></node>
<node id="n424" labels=":Skill"><data key="labels">:Skill</data><data key="name">networkx</data><data key="category">Libraries</data></node>
<node id="n425" labels=":Skill"><data key="labels">:Skill</data><data key="name">scrum framework</data><data key="category">Frameworks</data></node>
<node id="n426" labels=":Skill"><data key="labels">:Skill</data><data key="name">express.js</data><data key="category">Frameworks</data></node>
<node id="n427" labels=":Skill"><data key="labels">:Skill</data><data key="name">node.js</data><data key="category">Frameworks</data></node>
<node id="n428" labels=":Skill"><data key="labels">:Skill</data><data key="name">angular</data><data key="category">Frameworks</data></node>
<node id="n429" labels=":Skill"><data key="labels">:Skill</data><data key="name">flask</data><data key="category">Frameworks</data></node>
<node id="n430" labels=":Skill"><data key="labels">:Skill</data><data key="name">django</data><data key="category">Frameworks</data></node>
<node id="n431" labels=":Skill"><data key="labels">:Skill</data><data key="name">vue.js</data><data key="category">Frameworks</data></node>
<node id="n432" labels=":Skill"><data key="labels">:Skill</data><data key="name">phoenix</data><data key="category">Frameworks</data></node>
<node id="n433" labels=":Skill"><data key="labels">:Skill</data><data key="name">fastapi</data><data key="category">Frameworks</data></node>
<node id="n434" labels=":Skill"><data key="labels">:Skill</data><data key="name">jquery</data><data key="category">Frameworks</data></node>
<node id="n435" labels=":Skill"><data key="labels">:Skill</data><data key="name">asp.net</data><data key="category">Frameworks</data></node>
<node id="n436" labels=":Skill"><data key="labels">:Skill</data><data key="name">ruby on rails</data><data key="category">Frameworks</data></node>
<node id="n437" labels=":Skill"><data key="labels">:Skill</data><data key="name">react.js</data><data key="category">Frameworks</data></node>
<node id="n438" labels=":Skill"><data key="labels">:Skill</data><data key="name">laravel</data><data key="category">Frameworks</data></node>
<node id="n439" labels=":Skill"><data key="labels">:Skill</data><data key="name">next.js</data><data key="category">Frameworks</data></node>
<node id="n440" labels=":Skill"><data key="labels">:Skill</data><data key="name">angular.js</data><data key="category">Frameworks</data></node>
<node id="n441" labels=":Skill"><data key="labels">:Skill</data><data key="name">drupal</data><data key="category">Frameworks</data></node>
<node id="n442" labels=":Skill"><data key="labels">:Skill</data><data key="name">svelte</data><data key="category">Frameworks</data></node>
<node id="n443" labels=":Skill"><data key="labels">:Skill</data><data key="name">symfony</data><data key="category">Frameworks</data></node>
<node id="n444" labels=":Skill"><data key="labels">:Skill</data><data key="name">blazor</data><data key="category">Frameworks</data></node>
<node id="n445" labels=":Skill"><data key="labels">:Skill</data><data key="name">gatsby</data><data key="category">Frameworks</data></node>
<node id="n446" labels=":Skill"><data key="labels">:Skill</data><data key="name">fastify</data><data key="category">Frameworks</data></node>
<node id="n447" labels=":Skill"><data key="labels">:Skill</data><data key="name">ember.js</data><data key="category">Frameworks</data></node>
<node id="n448" labels=":Skill"><data key="labels">:Skill</data><data key="name">nuxt.js</data><data key="category">Frameworks</data></node>
<node id="n449" labels=":Skill"><data key="labels">:Skill</data><data key="name">deno</data><data key="category">Frameworks</data></node>
<node id="n450" labels=":Profile"><data key="labels">:Profile</data><data key="name">Data Engineer</data></node>
<node id="n451" labels=":Profile"><data key="labels">:Profile</data><data key="name">Data Scientist</data></node>
<node id="n452" labels=":Profile"><data key="labels">:Profile</data><data key="name">Data Analyst</data></node>
<node id="n453" labels=":Profile"><data key="labels">:Profile</data><data key="name">Business Analyst</data></node>
<node id="n454" labels=":Profile"><data key="labels">:Profile</data><data key="name">Software Engineer</data></node>
<node id="n455" labels=":Profile"><data key="labels">:Profile</data><data key="name">Machine Learning Engineer</data></node>
<node id="n456" labels=":Profile"><data key="labels">:Profile</data><data key="name">Cloud Engineer</data></node>
<node id="n457" labels=":Skill"><data key="labels">:Skill</data><data key="name">identity provider</data></node>
<node id="n458" labels=":Skill"><data key="labels">:Skill</data><data key="name">security information management</data></node>
<node id="n459" labels=":Skill"><data key="labels">:Skill</data><data key="name">security event management</data></node>
<node id="n460" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=40da239bb7af8d45&amp;bb=5_Vz4GcWrbycs2moqfpeoolQXIH2zV2Mc64f2_yM_ax0V7mx23thqoXB_Y9EuJcWj59UNLUl81N0qAT8ZFBHNAxQUKI807SzNRz5Hos_oxctmAjGQsrlgw%3D%3D&amp;xkcb=SoCJ67M3CNuuXyw3hB0KbzkdCdPP&amp;fccid=d2536e400fa1f147&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in RYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay98613  126000 a yearJob typeFulltime LocationChampaign IL 61820 BenefitsPulled from the full job descriptionHealth insurancePaid time offParental leaveRetirement planTuition reimbursement Full job description Description  Corteva Agriscience LLC seeks a fulltime Data Analyst based in Champaign IL Position includes a telecommute benefit within commuting distance to a Champaign IL Corteva office as directed This position will be responsible for the deployment of operational methods for the integration and organic technical improvement of the Pasture  Land Management PLM Digital Tools decision support platform   Qualifications  Requirements Masters degree or equiv in Geographic Information Science Data Science or a related field  3 years related exp Must also have 12 months of exp with 1 perform Quality Assurance checks on incoming data feature sets using ArcGIS and connect data sets into enterprise geodata systems 2 work with complex and multifaceted data sets in conjunction with commercial biology and GIS specialists to support timeseries analysis of crops and grasses and related management decisions 3 expertise to create new tools and technology in Data Engineering including skills in R Python and development for cloud Linux and Windows platforms 4 Crop and Land Use classification involving the separation of areas covered by different plant species and types of land and 5 utilize the follow toolstechnologies RShiny fullstack development solution ArcPy Time series predictive modeling ObjectBased Image Analysis using ECognition software Please apply online at httpscareerscortevacomenus Salary 98613 to 126000year  Benefits – How We’ll Support You    Numerous development opportunities offered to build your skills  Be part of a company with a higher purpose and contribute to making the world a better place  Health benefits for you and your family on your first day of employment  Four weeks of paid time off and two weeks of wellbeing pay per year plus paid holidays  Excellent parental leave which includes a minimum of 16 weeks for mother and father  Future planning with our competitive retirement savings plan and tuition reimbursement program  Learn more about our total rewards package here – Corteva Benefits  Check out life at Corteva wwwlinkedincomcompanycortevalife    Are you a good match Apply today We seek applicants from all backgrounds to ensure we get the best most creative talent on our team  </data></node>
<node id="n461" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=59bd344a6bc2a8bb&amp;bb=5_Vz4GcWrbycs2moqfpeone1jO-vYngvohSM3MKqR1WMFWjcVwRl_w_rNCSflsUqVMwjbQNxMIDs17_rLnVc8L6JCGGLmGeTXvSv9ZlIV5DyHRjF4HBWHw%3D%3D&amp;xkcb=SoAU67M3CNuuXyw3hB0JbzkdCdPP&amp;fccid=e5f741bf43df9079&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Supervising experienceYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay90000  100000 a yearJob typeFulltime LocationArlington VA 22209 BenefitsPulled from the full job description401k401k matchingADD insuranceCell phone reimbursementDental insuranceFlexible spending accountHealth insuranceShow morechevron down Full job description      Trilogy Federal provides financial management information technology IT consulting program management services and strategic consulting to federal agencies Trilogy has an extensive history helping federal clients achieve their most ambitious business modernization and optimization goals with the ability to deliver targeted subject matter expertise and full life cycle support       Trilogy Federal is looking for a strategic and thoughtful Sr Data Analyst to consult and develop datacentered projects In keeping with this overarching aim the Sr Data Analyst will be required to outline work requirements quickly develop an understanding of our customer’s datarelated challenges provide solutions that consider the customer’s specific requirements and constraints and work with a crossfunctional team to deliver timely and tangible results You should also harness your mastery of data analysis to consult and directly participate in various aspects of these and other projects To be successful as a Sr Data Analyst you should use data to ultimately inform sound decisionmaking in support of automation efforts and efficiency The ideal candidate will also assist in the development of junior staff       Primary Responsibilities    Formulating suggesting and managing datadriven projects which are geared at furthering the businesss interests  Collating and cleaning data from various entities for later use by junior data scientists  Delegating tasks to Junior Data Scientists to realize the successful completion of projects  Monitoring the performance of Junior Data Scientists and providing them with practical guidance as needed  Selecting and employing advanced statistical procedures to obtain actionable insights  Crossvalidating models to ensure their generalizability  Producing and disseminating nontechnical reports that detail the successes and limitations of each project  Suggesting ways in which insights obtained might be used to inform business strategies  Staying informed about developments in Data Science and adjacent fields to ensure that outputs are always relevant  Assist clients on functional and data requirements to enhance reporting effectiveness  Develop Microsoft Forms to assist clients with gathering information Use analytics to evaluate and summarize responses  Provide subject matter experience for clients seeking to improve content management and versioning  Adhere to established methodologies while analyzing processes for improved performance and adaptability       Minimum Requirements    Able to obtain a Public Trust Clearance  Bachelor’s degree data science statistics computer science or similar preferred  10 years’ experience  Consultative mindset and demonstrated work experience providing solutions for clients and proactively collaborating with Stakeholders  Deep knowledge of statistics and linear algebra concepts ANOVA distributions PCA  Proficiency in R or Python  Proficiency in SQL  Familiar with machine learning principles and techniques  Demonstrable history of devising and overseeing datacentered projects  Ability to relay insights in laymans terms such that these can be used to inform business decisions  Capacity to work independently or collaboratively with a crossfunctional team       Preferred Qualifications    VA experience preferred  Advanced degree in data science statistics computer science or similar  Detailoriented with the ability to manage multiple tasksrequests  Strong written and verbal communications skills  Supervision and mentorship skills       Benefits including but not limited to    Health dental and vision plans  Optional FSA  Paid parental leave  Safe Harbor 401k with employer contributions 100 vested from day 1  Paid time off and 11 paid holidays  No cost group term lifeADD plan and optional supplemental coverage  Pet insurance  Monthly phone and internet stipend  Tuition and training reimbursement          90000  100000 a year         This range is not a guarantee of compensation or salary as Trilogy Federal conducts an individual equity review for every candidate based on experience location education industry experience and comparisons to internal pay bands In addition to salary Trilogy offers robust benefits including medicaldentalvision insurance coverage 401k match paid holidays paid time off tuition reimbursement and a very supportive worklife balance       Regarding remote positions Trilogy Federal is only able to offer virtual employment in the following states Colorado Connecticut Delaware DC Florida Illinois Indiana Maine Maryland Massachusetts New York South Carolina Texas and Virginia     Trilogy Federal is an Equal Employment Opportunity employer We do not discriminate based upon race religion color national origin gender including pregnancy childbirth or related medical conditions sexual orientation gender identity gender expression age status as a protected veteran status as an individual with a disability or other applicable legally protected characteristics    </data></node>
<node id="n462" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e12d8bb2c6a0f34f&amp;bb=5_Vz4GcWrbycs2moqfpeoucwXJhrmuzkrNSGPujtOGTHyJCG4h9GSoMbOjsgdHUbb-xt_FpaOeEtfAeTSXiAwXndpKgHdr387eJzs4jGpZuKZvoM0-MjxQ%3D%3D&amp;xkcb=SoCg67M3CNuuXyw3hB0IbzkdCdPP&amp;fccid=a5b926a01ca57f85&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid Professional In Human Resources certificationYesNoSkillsDo you have experience in SQLYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location7501 West Memorial Road Oklahoma City OK 73142 Full job description This position works with HR and the business to develop reports and provide analysis to inform and support decisionmaking This role will utilize data analysis methodologies reporting and data visualization tools requires an understanding of data miningmashing and data visualization with an analytical mindset an understanding of data quality auditing and proactively improve the quality of reporting and data and the ability to frequently interface with executive and senior leaders  RESPONSIBILITIES   Conducts analysis of HR andor business data to identify actionable insights Merges data from multiple sourcessystems and provides adhoc reporting when needed  Leverages internal HR andor business data using internal tools systems and data sources  Collaborates with crossfunctional teams HR IT Product Management and Development to develop progressive analytics use cases for Product Management  Proactively partners with key stakeholders to address business gaps or opportunities identified through analytics  Develops reports dashboards and models to explore data and explain opportunities  Merges structured semistructured and unstructured data from several systems to highlight company trends related to recruiting and other business areas  Performs moderately complex data analysis and generates insights to support the HR workforce analytics needs of the business to drive informed business decisions  Delivers high quality analysis reports presentations using simple and effective visualizations that can be scaled for consumption by a larger audience  Creates and maintains monthly and quarterly workforce metrics and analysis  Builds strong relationships and collaborates with teammates business leaders and other internal clients  Automates data and analytics solutions builds and prototypes dashboards to provide insights  Continuously monitors data quality and integrity  Performs statistical tests on large datasets to determine data quality and integrity  Evaluate system performance and design as well as its effect on data quality  Collaborate with database developers to improve data collection and storage processes  Run data queries to identify coding issues and data exceptions as well as cleaning data  Documents processes and maintains data records  Adhere to best practices in data analysis and collection  Keep abreast of developments and trends in data quality analysis  Manipulates and analyzes large data sets to gain insights and provide recommendations  Documents and maintains procedures for reports and records of information management and storage protocols  Develops and delivers weekly monthly and quarterly client group metrics  Leverages advanced technical knowledge of excel and other data visualization tools  Utilize project management tools to manage data development projects   EducationCertification   Bachelors degree   Experience   2 years of experience in Data Analytics  Experience with data visualization and advanced analytics tools  Human Resources experience   PREFERRED QUALIFICATIONS  EducationCertification   Bachelors degree in Business Finance Human Resources or Quantitative Analytics  PMI and PHR certification   Experience   2 years of experience in HR Data Analytics  Human ResourcesWorkforce Analytics experience  Experience with Power BI  Experience in coding languages including DAX SQL and Python   SkillsAbilities   Critical analytical thinker with strong communication skills  Strong analytical skills and the ability to collect organize analyze and disseminate significant amounts of information with attention to detail and accuracy  Information chain analysis and management  Root cause analysis  Technical expertise regarding data models database design development data mining and segmentation techniques  Adept at queries report writing and presenting findings  Selfstarter who thrives in a fastpaced environment  Strong understanding of HR processes and procedures  Collaborative partner builds and maintains professional relationships  Project management skills    Paycom is an equal opportunity employer and prohibits discrimination and harassment of any kind Paycom makes employment decisions on the basis of business needs job requirements individual qualifications and merit Paycom wants to have the best available people in every job Therefore Paycom does not permit its employees to harass discriminate or retaliate against other employees or applicants because of race color religion sex sexual orientation gender identity pregnancy national origin military and veteran status age physical or mental disability genetic characteristic reproductive health decisions family or parental status or any other consideration made unlawful by applicable laws Equal employment opportunity will be extended to all persons in all aspects of the employeremployee relationship This policy applies to all terms and conditions of employment including but not limited to hiring training promotion discipline compensation benefits and separation of employment The Human Resources Department has overall responsibility for this policy and maintains reporting and monitoring procedures Any questions or concerns should be referred to the Human Resources Department To learn more about Paycoms affirmative action policy equal employment opportunity or to request an accommodation  Click on the link to find more information paycomcomcareerseeoc  </data></node>
<node id="n463" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=4c7cccccab488b05&amp;bb=5_Vz4GcWrbycs2moqfpeoj0WPOUYKNebkDvEThw9K-NIhq1tlOPAotqoJx_CaDs_Gc1Jzlc0K4CF54nN6U8K_ZWTsyZCd-AV2I9X1oZ919YK0vSteDpkCw%3D%3D&amp;xkcb=SoAu67M3CNuuXyw3hB0PbzkdCdPP&amp;fccid=3504833e2f12cab9&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo LocationPortland ME 04101 Full job description JOIN OUR TEAM  Onpoint Health Data is a dynamic fastgrowing nonprofit company located in Portland Maine committed to delivering independent reliable and insightful data solutions to clients nationwide If you are a motivated selfstarter looking for an opportunity to work with emerging technologies and a collaborative energetic team Onpoint would be a perfect fit We offer a competitive benefits package and a great office space conveniently located in Portlands East End We work a hybrid schedule with two inoffice days each week Tuesdays and Thursdays  DEPARTMENT  Data Analytics and Operations  DESCRIPTIVE SUMMARY  The Health Data Analyst HDA works closely with the Analytics team to review analyze and provide graphical and verbal presentation of health care data The HDA learns to run queries and interprets outcomes to provide a key quality assurance role for our clients data The HDA will also run reports that inform our clients on health care quality access and cost Under direction of senior staff the HDA also investigates unusual findings in the data and helps build new reports and products The HDA must be detail oriented capable of learning independently and able to work on multiple projects at a time in a fastpaced teamoriented environment  RESPONSIBILITIES  Analysis and Reporting  Learn to use software tools such as SQL SAS R to query medical claims data and provide clients reports for multiple projects eg use existing code to conduct linkages run quality measure rates refresh of person level files for analytic use After becoming familiar with coding tools begin to modify code to develop new reports Prepare graphical reports using Tableau Excel and other statistical programs and PowerPoint Work with senior members of the Analytics Team to present health data in an accurate efficient and thoughtful manner May take a lead analyst role on clientproject after becoming familiar with Onpoint processes and databases  Quality Assurance  Support analytic staff by running Onpoints data quality processes for extracts and other Onpoint products and reports This may include running queries using SAS and SQL to identify unusual findings in analyses and reports and reviewing them with the project lead and senior analysts Assist with evaluation and testing of new software applications Help complete review and quality assurance of multiple Onpoint reports and deliverables  General  Work with Health Analytics Manager and project leads regularly to balance multiple projects and priorities Begin to develop expertise in All Payer Claims Data Onpoints data systems and other healthcare topics Begin to develop strong understanding of relational databases Actively seek opportunities to learn and contribute more to the team As a member of a broader team take on and perform duties beyond specific role as assigned  Other  Understand the value that Onpoint places on maintaining the confidentiality and integrity of our corporate and client data and meet applicable privacy and security compliance requirements Ensure that Onpoint and client data is accessed handled processed transmitted disclosed and stored according to operational and IT policies and procedures Immediately report any suspected or actual violation of privacy and security policies or unauthorized access or disclosure of Onpoint or client data Understand that compliance with all privacy and security policies laws and regulations is part of each employees annual performance evaluation Adhere to all policies and procedures as outlined in the Onpoint Health Data Employee Handbook Perform all other duites as assigned  QUALIFICATIONS  Bachelors degree in sciences mathematics or another major with an emphasis on analytical or quantitative skills development or equivalent experience Strong problem solving and critical thinking skills Proficiency utilizing Excel andor other data visualization tools to work with and analyze data sets Excellent verbal and written communication skills An interest in working with clients in a professional environment Detailoriented Experience handling multiple projects while successfully meeting deliverable due dates Experience working collaboratively as well as independently within multidisciplinary teams Experience in health care data and analysis health information technology statistics relational databases andor knowledge of querying and analytical tools eg SAS SQL Tableau R Python Excel preferred  Onpoint Health Data is an equal opportunity employer and prohibits discrimination and harassment of any kind We recruit employ train compensate and promote without regard to race religion creed color national origin age gender identity andor expression sexual orientation marital status disability veteran status or any other basis protected by applicable federal state or local law   </data></node>
<node id="n464" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d3955bfc7bf6d8c9&amp;bb=5_Vz4GcWrbycs2moqfpeoo0NvaY6k_3QwLj4kgfVOpnvTZPntbWi0AVB9zB_eNu2NtfptJXxjwqmsDAYiVZvRq7HMolAysL95n-wWFjY5ni4-_9Zx8iESw%3D%3D&amp;xkcb=SoCa67M3CNuuXyw3hB0ObzkdCdPP&amp;fccid=b7dd518f0a9be79f&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay54974  78343 a yearShift and scheduleWeekends as neededEvenings as needed Location160 Calhoun St Bldg B36 Charleston SC 29424 BenefitsPulled from the full job descriptionDental insuranceDisability insuranceEmployee assistance programHealth insuranceLife insurancePaid time offTuition reimbursementShow morechevron down Full job description        POSTING INFORMATION           Internal Title          Student Success Data Analyst            Position Type          Classified            Faculty  NonFaculty  Administration          NonFaculty            Pay Band          7            Level          1            Department          Institutional Research            Job Purpose          The Student Success Data Analyst plays the lead role in supporting the datadriven decisionmaking functions of the institution directly related to student success and retention This position serves the Offices of Institutional Research and Student Success by providing firstclass reporting solutions and analyses to support improved outcomes for the College’s students and enrollment management activities of academic administration            Minimum Requirements          Bachelor’s degree in business administration computer science mathematicsstatistics engineering management information science data processing or related field and experience with Tableau CognosArgos SQL andor SAS is required Experience with data mining including techniques of data extraction documentation analysis and reporting is required Experience working in a higher education environment is preferred Candidates with an equivalent combination of experience andor education are encouraged to apply            Required Knowledge Skills and Abilities   Required  Experience with Tableau CognosArgos SQL andor SAS Advanced SAS programming experience is preferred         Working knowledge and practitioner of methods and techniques of data extraction documentation analysis and reporting         Experience in analyzing data and presenting findings using a wide variety of descriptive and inferential statistics including customizing the results based on the research needs and audience         Proficiency with MS Office including Outlook Word Excel Teams SharePoint and PowerPoint         Knowledge and understanding of the data and information that is important to an academic institution         Ability to establish and maintain effective working relationships with staff and to communicate effectively         Must be able to work well under strict deadlines and have exceptional attention to detail         Ability to manage multiple projects with shifting priorities in a fastpaced environment         Must understand the significance of maintaining data integrity and confidentially with student records knowledge of and adherence to FERPA regulations         Preferred  Exceptional organizational skills and experience managing and documenting complex projects         Knowledge of higher education information systems such as Ellucian Banner and related data systems and repositories such as Salesforce IPEDS US News and CSRDE              Additional Comments Regarding Position          Limited overnight travel may be required to attend professional development conferences and meetings Some weekend and evening activities may be required            This position may be eligible for limited remote telecommuting   Please visit the College’s Telecommuting page and Academic Affairs divisional guidelines for more information httpshrcofcedutelecommutingindexphp            Special Instructions to Applicants            Please complete the application to include all current and previous work history and education A resume will not be accepted nor reviewed to determine if an applicant has met the qualifications for the position                   Salary is commensurate with educationexperience which exceeds the minimum requirements                   Offers of employment are contingent upon a successful background check            All applications must be submitted online httpsjobscofcedu              Salary          54974  78343            Posting Date          03212024            Closing Date          04192024            Benefits   Insurance HealthDentalVision Life Insurance Paid Leave SickAnnualParental Retirement Long Term Disability Paid Holidays Free CARTA Bus Service Employee Tuition Assistance Program ETAP Employee Assistance Program EAP Full Benefits Package – Click Here       Open Until Filled          No            Posting Number          2024043            EEO Statement          The College of Charleston is an Affirmative ActionEqual Opportunity employer and does not discriminate against any individual or group on the basis of gender sexual orientation gender identity or expression age race color religion national origin veteran status genetic information or disability                        Job Duties            Activity           Under the joint direction of the Associate Provost for Student Success and the Executive Director of Institutional Research directs the development and longitudinal tracking of strategic student success initiatives Participates in Office for Student Success leadership meetings Collaborates with student success departments to manage a comprehensive analyticsbased research program to support improved outcomes for the College’s students Analyzes and communicates findings to the studentfacing and studentsupport units of the institution to educate and inform their interactions assess student support initiatives and enhance the student experience Establishes data tracking and data collection methods that ensure accuracy and validity in the final analyses             Essential or Marginal           Essential             Percent of Time           40                 Activity           Manages the creation of several retention and graduation reports and special studies annually and on an ad hoc basis including but not limited to the annual IR retention packet and analyses of progress and retention of special populations of entering freshmen ie Transfer Students Honors SPECTRA participants CSL visitors Coordinates and directs the development of a common set of student success dashboards to provide distributed data on student success and retention classroom success class enrollment management and major and minor choice and movement Serves as the primary resource for users of the student success dashboards including developing and maintaining training and documentation on their proper use and understanding             Essential or Marginal           Essential             Percent of Time           30                 Activity           Provides data validation and consultatory support to the Student Success Insights CRM Advise implementation and leadership team Uses statistical analysis to identify indicators to predict student success and retention and to identify risk factors that hinder success Monitors the accuracy of such predictions and informs scoring rubrics for the College’s Student Success Insights application             Essential or Marginal           Essential             Percent of Time           15                 Activity           Supports and assists IR colleagues on projects supporting student success and enrollment management such as ad hoc and operational requests using SAS Tableau Cognos or Argos senior leadership and executive requests maintains these components of the OIR website completes external surveys and studies relying on such data including but not limited to the CSRDE and dashboard development and reporting using Tableau focused on these data and metrics             Essential or Marginal           Marginal             Percent of Time           10                 Activity           Develops and maintains detailed documentation metadata on project work and IR data architecture regarding Banner Student data fields incoming requests and documents project tasks Tracks project progress and reports on status and project dependencies             Essential or Marginal           Essential             Percent of Time           5                </data></node>
<node id="n465" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=058d20f5de8e0c46&amp;bb=5_Vz4GcWrbycs2moqfpeoiNGQv5BGkaIWMAAzYtwni2nPizzooXS0-C4lsj_WYo2dQC-Mt2vT8ufoun3ES0wB6j4GgauAtzY4CviI9cmSlmhZLlCaMwquQ%3D%3D&amp;xkcb=SoAH67M3CNuuXyw3hB0NbzkdCdPP&amp;fccid=a5b926a01ca57f85&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeShift and scheduleWeekends as neededEvenings as needed Location7501 West Memorial Road Oklahoma City OK 73142 Full job description This individual uses techniques and tasks to work as a liaison among stakeholders to understand the operations structure and policies of the organization and also recommend IT solutions that enable the organization to successfully complete new business initiatives In addition this individual is highly organized works well in a team environment and partners closely with the IT Systems Analysts role while leading projects   RESPONSIBILITIES   Protects organizations value by keeping information confidential  Works with stakeholders and functional areas to develop business requirements  Identifies and resolves gaps in technology processes and resources  Ensures timely delivery of requirements and design documents in line with project milestones  Regularly provides guidance and direction to junior Business Analysts   Documentation   Statement of Work  Project Plan  Business Process Diagrams  User Training Guides  Meeting Minutes  Project Status Updates   Miscellaneous   Attends meetings and serves on committees as requested  Performs additional duties and assignments as requested   EducationCertification   Bachelor’s degree in Information Science Business Administration or related discipline   Experience   2 years of IT related Business Analysis experience  1 years of relevant technical IT experience  Long term multiphase systems implementation projects  Advanced knowledge and experience with diagraming  Principles and best practices from PMIBOK  User Acceptance Testing  The Systems Development Life Cycle  Project Proposals  Software Testing  Regression Testing  Change Management  Emerging technology trends    PREFERRED QUALIFICATIONS  EducationCertification   Has PMI or vendor specific certifications   SkillsAbilities   Demonstrates interpersonal skills required to successfully work in a team environment and communicates effectively across a variety of stakeholder groups Has excellent written and verbal communication skills  Must adhere to PMI code of ethics and professional conduct  Experience in defining detailed business requirements to support strategic andor tactical projects  Proven ability to document business requirements business processes and project plans  Can evaluate critical systems prioritize workflow and determine solutions  Is confident to have face to face interactions with the business  stakeholders including upper management and chief officers  Can easily adapt and learn new software systems and technology  Must demonstrate a commitment to continuous learning and mentoring  Can Interpret and apply laws regulations policies and procedures  Must possess proficiency in MS Office applications Word Excel PowerPoint Outlook Visio  Can work flexible hours including weekends and evenings    Paycom is an equal opportunity employer and prohibits discrimination and harassment of any kind Paycom makes employment decisions on the basis of business needs job requirements individual qualifications and merit Paycom wants to have the best available people in every job Therefore Paycom does not permit its employees to harass discriminate or retaliate against other employees or applicants because of race color religion sex sexual orientation gender identity pregnancy national origin military and veteran status age physical or mental disability genetic characteristic reproductive health decisions family or parental status or any other consideration made unlawful by applicable laws Equal employment opportunity will be extended to all persons in all aspects of the employeremployee relationship This policy applies to all terms and conditions of employment including but not limited to hiring training promotion discipline compensation benefits and separation of employment The Human Resources Department has overall responsibility for this policy and maintains reporting and monitoring procedures Any questions or concerns should be referred to the Human Resources Department To learn more about Paycoms affirmative action policy equal employment opportunity or to request an accommodation  Click on the link to find more information paycomcomcareerseeoc  </data></node>
<node id="n466" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=8a40fc14efd55ab5&amp;bb=5_Vz4GcWrbycs2moqfpeosQYDdqcUP_oHBGIvzYko64P8Cj8NADZG45bbmxRHAlVaTL34mUUDz7rhU5G7dn-NCWGrieMFPW4_xww-8ymt7Nr7FiGRDjlHQ%3D%3D&amp;xkcb=SoCz67M3CNuuXyw3hB0MbzkdCdPP&amp;fccid=328bc12db37ac8ae&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNo Job detailsHere’s how the job details align with your profileShift and scheduleChoose your own hours LocationLititz PA 17543 BenefitsPulled from the full job description401k matchingDental insuranceFlexible scheduleHealth insurancePaid time offProfit sharing Full job description Join one of Pennsylvania’s fastest growing companies today  WebstaurantStore Inc is looking for an EDI Analyst to join our growing company This role focuses on technical communications with our customers and vendors and is a key part of growing our business and serving the needs of foodservice professionals worldwide   Responsibilities include   Taking starttofinish ownership of onboarding vendors to our B2B platform  Working with trading partners to integrate specific documents partner setups and processing requirements  Scheduling and executing business integration production launches  Troubleshooting document processing errors and working with trading partners directly to resolve them in a timely matter  Supporting internal resources to resolve issues answer questions and retransmitting documents when necessary  Coordinating and testing software upgrades  Documenting business requirements for new integrations  Providing technical support to both internal and external clients    Successful Candidates will have   General computer knowledge and basic skills  Familiarity with FTP SFTP or AS2 connections  Basic SQL knowledge  Aptitude to learn new softwaretechnologies  Strong selfmanagement skills  Effective written and verbal communication skills  Great organizational and project tracking skills  Effective planning and organizational skills    Preferred Experience   Knowledge of key EDI document types 210 214 315 810 846 850 855 and 856  Familiarity with CData’s ArcESB integration software    We’re looking for employees with a strong desire to grow ask questions and challenge themselves Webstaurantstore offers competitive compensation and a comprehensive benefits package including paid time off medicaldental insurance 401k matching flexible work hours and profit sharing Employees can choose to work remote or if they live close to the office and choose to come in enjoy our inoffice gym and casual work environment Webstaurantstore’s Development department offers an optional condensed work week allowing you to choose a schedule that suits your lifestyle  H1B Sponsorship not available W2 only We do not accept unsolicited resumes from thirdparty recruiters In addition we are not seeking to expand our 3rd party partnerships at this time Remote work qualifications   Access to a reliable and secure highspeed internet connection Cable or fiber internet connections at least 75mbps download10mbps upload are preferred as satellite connections often cannot support the technologies used to perform daytoday tasks  Access to a home router and modem  A dedicated home office space that is noise and distractionfree The space should have strong wireless connection or a wired Ethernet connection wired connection is preferred if possible  A valid physical address apartment suite etc PO Boxes are not supported as a physical address is required for you to receive your computer equipment  The desire and ability to work and communicate with other team members via chat webcam etc  Legal residents of one of the following states AK AL AR AZ CT DE FL GA IA ID IN KS KY LA MD ME MI MN MO MS NC ND NH NM NV OH OK PA SC SD TN TX UT VA VT WI WV and WY H1B Visa Sponsorship Not Available W2 only   </data></node>
<node id="n467" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=596b7016093d8a6a&amp;bb=5_Vz4GcWrbycs2moqfpeou9aUbI7eFGVhKTpjvSXH4Gu1yLkxFd_IYc6yVrnfQ8PvIZVZqgBR5auIuHHItDMepJzSXesKxB8Jl4vAKr2NSyNTx9bw5BnBQ%3D%3D&amp;xkcb=SoBa67M3CNuuXyw3hB0DbzkdCdPP&amp;fccid=8d435d1203639850&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in WindowsYesNo Job detailsHere’s how the job details align with your profilePay45000  50000 a yearJob typeFulltime LocationLouisville KY Full job descriptionThe Quality Assurance QA Analyst holds a pivotal position in Alines continuous delivery of innovative and varied products This role entails conducting QA testing on our desktop and web offerings utilizing an array of manual automation and application lifecycle management tools    Responsibilities    Carry out daily Quality Assurance tasks as assigned by the Senior QA Analyst Test Lead and Product Manager ensuring alignment with project objectives Draft manual test cases and integrate them into the master test library utilizing TFS or other relevant technologies contributing to comprehensive test coverage Collaborate with Business Analysts Senior QA Analysts and Product Champions to assist in or create requirements such as use cases user stories and bug reports Regularly communicate testing progress trends and quality insights to the product team fostering transparency and informed decisionmaking Document and report on test executions defects and bugs maintaining accurate records to support quality assurance processes Contribute to the deployment pipeline by aiding in deployment procedures and compiling release notes ensuring smooth and efficient product releases Assist in planning scheduling and measuring results of User Acceptance Testing facilitating effective validation of product functionalities Execute integration and functional testing tasks as assigned verifying the seamless operation of product features Perform automated acceptance tests in accordance with project requirements leveraging automation tools where applicable Evaluate and troubleshoot complex requirements and issues by utilizing diverse information sources ensuring robust problemsolving Analyze data and present findings to the team when necessary enabling informed decisionmaking and continuous improvement May perform other duties as needed andor assigned  Requirements    Possess a degree in a computingrelated discipline or demonstrate an equivalent combination of education and practical experience Hold a minimum of one 1 year of experience in Application Development Quality Assurance exhibiting proficiency in testing methodologies and practices Prior exposure to healthcare or longterm care environments is advantageous though not mandatory Demonstrate proficiency in testing various platforms including Windows ASPNET NET SQL GUI applications web services and web applications encompassing both backend and frontend functionalities Display a solid understanding of software design techniques enabling effective evaluation of application architectures Exhibit experience working within Agile development methodologies illustrating familiarity with iterative development processes and principles Showcase a track record of delivering highquality results within fastpaced and dynamic business environments demonstrating adaptability and efficiency Possess strong communication skills with the ability to clearly document and articulate issues and processes facilitating effective collaboration and problem resolution  </data></node>
<node id="n468" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=9c93f01ad9bf5f75&amp;bb=5_Vz4GcWrbycs2moqfpeon59oPAMunNDKRrP6eR5ACDpRR6ibg5nWGFqrEAhlOSWP1g94NRJIaPnPaAMkLHFL-fU1FD4vYlJgcvTjwrWIG04sXM8VFilCA%3D%3D&amp;xkcb=SoDu67M3CNuuXyw3hB0CbzkdCdPP&amp;fccid=cdf50da77e83deaa&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Internet of thingsYesNo Job detailsHere’s how the job details align with your profilePay49458  63073 a yearJob typeTemporaryFulltime Location301 W High St Jefferson City MO 65101 BenefitsPulled from the full job descriptionHealth insurance Full job description Job Location  The office for this position is located in the Truman Bldg 301 W High Street Jefferson City MO 65101 Candidates who complete a successful onboarding and training period may be eligible to work at an alternate location in compliance with OAITSDs Distributed Workforce Plan  Why you’ll love this position  As a Business Analyst the core responsibilities of this position include the overall direction coordination implementation execution control and completion of project reviews Projects will cover a diverse range of technologies and objectives Determining technical requirements of agency projects and translating them into functional specifications Determines systems scope objectives and functionality based on user input and understanding of business processes Provides the basis for the design or modification of information systems through technical analysis and document requirements  This is a temporary fulltime benefiteligible position based upon federal ARPA funding Position funding will end no later than December 31 2026  This position is with the Office of Administration Information Technology Services Division OAITSD  ITSD Core Values  We Innovate and Partner with Passion Respect and Integrity United as OneTeam   Build a relationship with the customer Process mapping Defines objectives Defining systems scope Embrace ITSD Core Values Documents requirements Estimating cost Provides the basis for the design or modification of information systems Facilitate meetings Defines roles and responsibilities Defines functionality based on user input and understanding   Project Review Team consists of review and analysis of agency projects software software versions databases web interfaces applications connectivity requirements security requirements access requirements hardware and roles and responsibilities coordinate reviews with Engineering and Communications Client Engagement Services and Office of Cyber Security as required Skill in outlining project scope objectives and functionality Skill in translating business processes into information technology requirements Ability to assess situations including risks and benefits and receive feedback from stakeholders Coordinating midlevel work with customers related to technical work Research technologies Providing recommendations to ITSD partners Advanced Knowledge of IOT Internet Of Things Devices Knowledge of IT Business areas create and interpret process diagramsflow charts Ability to translate technical terminology into common terms Assess situations risks  benefits and receiveorganize feedback from stakeholders Successful background check results are required for employment in this position This may include background checks involving a candidate’s name andor fingerprints and other screenings as needed for the specific position  Lack of postsecondary education will not be used as the sole basis denying consideration to any applicant  The classification for this position is Business Analyst click for more information  The State of Missouri offers an excellent benefits package that includes a defined pension plan generous amounts of leave and holiday time and eligibility for health insurance coverage Your total compensation is more than the dollars you receive in your paycheck To help demonstrate the value of working for the State of Missouri we have created an interactive Total Compensation Calculator This tool provides a comprehensive view of benefits and more that are offered to prospective employees The Total Compensation Calculator and other applicant resources can be found here  If you have questions please contact ITSDRecruitingoamogov   </data></node>
<node id="n469" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=7442c0d147a3743a&amp;bb=5_Vz4GcWrbycs2moqfpeohbqLTENsG3AqI5-8ZOQTiw6LXYb4ek8YjYuFtFyEwS-CZm4hyoUK1YbE9U5hOUYNYSKI10u11oi7SwyJtx1G3I%3D&amp;xkcb=SoBz67M3CNuuXyw3hB0BbzkdCdPP&amp;fccid=a3f737e511d9fc8c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SalesYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay113700  160600 a year LocationFoster City CA BenefitsPulled from the full job description401kDental insuranceFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offShow morechevron down Full job description Company Description  Visa is a world leader in payments and technology with over 259 billion payments transactions flowing safely between consumers merchants financial institutions and government entities in more than 200 countries and territories each year Our mission is to connect the world through the most innovative convenient reliable and secure payments network enabling individuals businesses and economies to thrive while driven by a common purpose – to uplift everyone everywhere by being the best way to pay and be paid  Make an impact with a purposedriven industry leader Join us today and experience Life at Visa Job Description  Visa Digital Products team is on the forefront of Visa’s innovation responsible for building digital platforms such as Visa Token Services and Visa Click to Pay aka Secure Remote CommerceSRC This team is also responsible for setting standards in the payment industry level EMVCo and W3C for digital commerce Platform product team is responsible for defining product requirements for Visa Click to Pay and Token products collaborate with cross functional teams including but not limited to Product Technology Commercialization UX DesignResearch Legal Marketing Branding and different Visa Regional teams to build great products at scale and drive adoption  This position will be responsible for shaping and developing the product strategy requirements and delivery of digital and physical token provisioning solutions leveraging Visa Token Service  Job Description  Want to build the future payment experience for Visa cardholders around the world  Imagine being part of an agile team where your ideas transform the payment experience for millions of Visa cardholders globally Visa Digital Platform team is looking for Product Managers to join our diverse team to build new product capabilities and enhancing existing ones Our platform capabilities will be used by merchants PSPs and wallets to enable payment experience in their online and physical checkouts  Essential Functions   Defining product and market requirements by understanding the needs of issuers and merchantstoken requestors during token provisioning process  Establish detailed business requirements and specifications for existing and new services and products  Collaborate deeply within product and other crossfunctional teams such as development architecture testing integration design etc  Analyze data to provide actionable insights and iterate product capabilities  Proactively identify gaps in the current product offering and lead the effort to develop solutions that drive business value  Track payment industry trends standards and competitive offerings in the ecommerce and mobile payments arena Identify opportunities for new valueadded and differentiated features  Coordinate with the regional product teams and internal teams to ensure countrymarket and regulatory requirements are met delivered and tested for a successful launch  Participate in client facing discussions provide feedback learn evaluate and apply in defining product  Maintain indepth knowledge of services APIs offered by Digital Solutions Product team  Managing grooming planning and execution of a steady product backlog in a very fast paced agile environment  Perform triage on critical issues escalating as necessary and communicating consistently and clearly with all concerned parties   This is a hybrid position Hybrid employees can alternate time between both remote and office Employees in hybrid roles are expected to work from the office 23 set days a week determined by leadershipsite with a general guidepost of being in the office 50 or more of the time based on business needs Qualifications  Basic Qualifications   2 or more years of work experience with a Bachelor’s Degree or an Advanced Degree eg Masters MBA JD MD or PhD  Ecommerce and payment industry knowledge  Experience in driving product strategy and go to market  Product experience that enables excellent user experiences especially with services for ecommerce or payment systems or financial systems  Experience demonstrating strong leadership selfmotivation accountability and team player   Preferred Qualifications   3 or more years of work experience with a Bachelor’s Degree or more than 2 years of work experience with an Advanced Degree eg Masters MBA JD MD  Strong collaboration and communication skills with the ability to effectively work crossorganizationally  Demonstrate strong customer centric mindset  Successful demonstration of product delivery in either or both Agile eg scrum and waterfall software development methodologies  Ability to lead drive consensus and deliver in a matrix organization with multiple stakeholders  Creativity and resourcefulness to overcome unexpected roadblocks  Proven track record of taking ownership and driving meaningful results  Ability to deliver initiatives from conception through completion  Superior analytical and problemsolving skills to synthesize and communicate complex information effectively    Additional Information  Work Hours Varies upon the needs of the department  Travel Requirements This position requires travel 510 of the time  MentalPhysical Requirements This position will be performed in an office setting The position will require the incumbent to sit and stand at a desk communicate in person and by telephone frequently operate standard office equipment such as telephones and computers  Visa is an EEO Employer Qualified applicants will receive consideration for employment without regard to race color religion sex national origin sexual orientation gender identity disability or protected veteran status Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law  Visa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law including the requirements of Article 49 of the San Francisco Police Code  US APPLICANTS ONLY The estimated salary range for a new hire into this position is 11370000 to 16060000 USD per year which may include potential sales incentive payments if applicable Salary may vary depending on jobrelated factors which may include knowledge skills experience and location In addition this position may be eligible for bonus and equity Visa has a comprehensive benefits package for which this position may be eligible that includes Medical Dental Vision 401 k FSAHSA Life Insurance Paid Time Off and Wellness Program  </data></node>
<node id="n470" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=22f1dbe077acca2b&amp;bb=5_Vz4GcWrbycs2moqfpeoinCZ37mVbOwCZ2gSO4nYxbXk5N0Ne0BnTKYh029sVe1qOo3lMSJfHeQfijPITncKfTkq_-1qmsHGEsmSjFqQ3_-pKemgXJSkA%3D%3D&amp;xkcb=SoDH67M3CNuuXyw3hB0AbzkdCdPP&amp;fccid=ce4e6cf83cbd6851&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Systems analysisYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay4339  6949 a monthJob typePermanentParttimeFulltime LocationEnterprise AL Full job description   Description   This posting is being used to fill various Data Processing Systems Analyst positions at the specified location The authorized level of the position is Data Processing Systems Analyst V Applications are being accepted down to the Data Processing Systems Analyst II in the event of recruiting difficulties  Salary Range All new external applicants will be placed on the initial step of the salary range However placement above the initial step of the salary range will be based on special characteristics and critical needs of the position an applicants exceptional qualifications the availability of funds and other relevant factors Such appointments require prior approval Hawaii State Department of Education employees will be placed on the salary range in accordance with Department regulations   Data Processing Systems Analyst II SR18 433900  528200 per month           Data Processing Systems Analyst III SR20 469000  571300 per month           Data Processing Systems Analyst IV SR22 507600  617700 per month           Data Processing Systems Analyst V SR24 571300  694900 per month        Examples of Duties   1 Plans and carries out various factfinding tasks to secure pertinent information on the electronic data processing needs and problems of departments 2 Conducts studies related to work flow work measurement time distribution organizational and functional relationships forms design etc 3 Meets with and interviews personnel from various levels of management and operations 4 Analyzes and coordinates subsystems 5 Determines the feasibility of conversion of operations and procedures to electronic data processing 6 Analyzes procedures from the standpoint of feasibility of using different machines their capacities cost etc 7 Prepares comprehensive reports of findings and recommendations 8 Prepares overall plans and develops detailed procedures and processes for conversion to machine processing utilizing current technologies such as teleprocessing data base management systems etc and including the preparation of flow charts and functional block diagrams 9 Integrates the various systems and procedures with the needs and requirements of the various agencies wherever possible 10 In a line department works in close cooperation with representatives of staff agencies 11 In the central agency works in close cooperation with representatives of operating departments 12 Prepares and maintains procedural manuals 13 May conduct management studies associated with other program or management objectives and needs 14 Prepares reports of work activities and correspondence 15 Reviews work results and follows through to see that management and staff officials have a clear understanding of problem areas 16 Provides guidance and assistance in implementing recommendations accepted by administrator of units serviced 17 Supervises a group of analysts andor programmers and support personnel in specific program areas as a unit supervisor or team leader 18 Coordinates work performed with supervisors of other functional areas and 19 Trains lower level analysts as required   Minimum Qualifications   Basic Education Requirement Graduation from an accredited fouryear college or university with a Bachelor’s degree Excess work experience as described under the Specialized or Supervisory Experience below or any other progressively responsible administrative professional or analytical work experience which provided knowledge skills and abilities comparable to those acquired in four years of successful study while completing a college or university curriculum leading to a baccalaureate degree may be substituted on a yearforyear basis To be acceptable the experience must have been of such scope level and quality as to assure the possession of comparable knowledge skills and abilities  The education or experience background must also demonstrate the ability to write clear and comprehensive reports and other documents read and interpret complex written material and solve complex problems logically and systematically  Experience Requirements Except for the substitutions provided below applicants must have had the types of experience described in the statements immediately following and in the amounts shown in the table below or any equivalent combination of training and experience     Class Title               Specialized Experience yrs               Supervisory Experience yrs               Total Experience yrs                  Data Prossg Systs Anal II               05               0               05                  Data Prossg Systs Anal III               15               0               15                  Data Prossg Systs Anal IV               25               0               25                  Data Prossg Systs Anal V               35                              35           For the Data Processing Systems Analysts V level at least one year of the required Specialized Experience must have been at the fully competent level comparable to the class Data Processing Systems Analyst IV in the State service  For the Data Processing Systems Analyst V level supervisory aptitude rather than actual supervisory experience may be accepted           Specialized Experience Progressively responsible work experience in computer systems analysis which involved the analysis and design of systems for electronic processing of data or stored computer programming experience which included participation in systems analysis       Supervisory Experience Experience in computer systems analysis andor computer programming which included 1 planning and directing the work of others 2 assigning and reviewing their work 3 advising them on difficult and complex problem areas and 4 timing and scheduling their work       For the Data Processing Systems Analyst V level supervisory aptitude rather than actual supervisory experience may be accepted Supervisory aptitude is the demonstration of aptitude or potential for the performance of supervisory duties through successful completion of regular or special assignments which involve some supervisory responsibilities or aspects by serving as a group or team leader or in similar work in which opportunities for demonstrating supervisory capabilities exist by completion of training courses in supervision accompanied by application of supervisory skills in work assignments or by favorable appraisals by a supervisor indicating the possession of supervisory potential       Substitutions Allowed          1 A Bachelor’s degree from an accredited college or university in computer science or in another major including completion of course work comparable to a major in computer science may be substituted for six 6 months of Specialized Experience           2 A Master’s degree in information and computer science from an accredited college or university may be substituted for one and onehalf 112 years of Specialized Experience           3 A Doctorate’s degree in information and computer science from an accredited college or university may be substituted for two 2 years of Specialized Experience           4 Excess Supervisory Experience of the type and quality described above may be substituted for Specialized Experience on a yearforyear basis       Quality of Experience Possession of the required number of years of experience will not in itself be accepted as proof of qualification for a position The applicants overall experience must have been of such scope and level of responsibility as to conclusively demonstrate that applicant has the ability to perform the duties of the position for which applicant is being considered      Supplemental Information   Salary The advertised salary is based on fulltime employment and includes shortage and school year differentials if applicable  Requirements Applicants must meet all the requirements for the position they are seeking as of the date of the application unless otherwise specified Unless specifically indicated the required education and experience may not be gained concurrently Calculation of experience is based on fulltime 40hour workweeks Parttime experience is prorated Example Twelve months of experience at 20 hoursweek is equivalent to six months of experience not one year Also hours worked in excess of 40 hoursweek will not be credited Example Twelve months of experience at 60 hoursweek is equivalent to one year of experience not one and a half years  Temporary Assignment Claims of Temporary Assignment TA experience to meet the minimum qualification requirements must be verified and attached to the application using one of the options below           A copy of the applicant’s TA History Report or equivalent systemgenerated report                A signed letter from the applicant’s supervisor that includes the applicant’s name hisher TA job title the TA start and end dates from mmyy to mmyy hisher specific TA duties performed and either the TA hours worked per week or total TA hours worked or                Copies of the applicant’s signed SF10 Forms         Documents Attach all relevant supporting documents to your application Documents that were attached to applications submitted before November 16 2023 do not automatically attach or transfer to applications submitted on and after December 16 2023 All submitted documents become the property of the Hawaii State Department of Education      Information about Temporary Positions Temporary positions may be extended year to year dependent upon funding and departmental needs Making yourself available for temporary positions increases your employment possibilities and may lead to permanent opportunities A person hired for a temporary position may also become a temporary employee upon satisfactory completion of the initial probation period of at least six months Once a temporary employee you would be eligible to apply for promotion and transfer opportunities to permanent as well as other temporary positions You may also enjoy other rights and benefits as afforded to an employee in a permanent position with the exception of return rights and placement rights associated with a reductioninforce        Equal Opportunity The Hawaii State Department of Education does not discriminate in its educational policies programs and activities on the basis of sex race color religion national origin age and disability in accordance with Title IX of the Education Amendments of 1972 Title VI of the Civil Rights Act of 1964 Section 504 of the Rehabilitation Act of 1973 Age Discrimination Act of 1975 and Americans with Disabilities Act of 1991 The Department does not discriminate in its employment policies programs and activities on the basis of sexual orientation arrest and court record and National Guard participation as well as on the basis of sex race color religion national origin age and disability in accordance with Title VII of the Civil Rights Act of 1964 Age Discrimination in Employment Act of 1967 Americans with Disabilities Act of 1991 Equal Pay Act of 1963 and Chapter 378 Part I Hawaii Revised Statutes    </data></node>
<node id="n471" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=ba777c434c7d9514&amp;bb=5_Vz4GcWrbycs2moqfpeoqDeEJQ6Xila-sADgBr4jBdzMzOI8X0N7uLLDEoDc6nQWfUTuXK2cEMyX1_KhIRBnjLFZGC5OUd5lLcGHUuYnvrGQ5kzQMP1dw%3D%3D&amp;xkcb=SoBJ67M3CNuuXyw3hB0HbzkdCdPP&amp;fccid=c77b6bd0f9166ba4&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo LocationNew York NY BenefitsPulled from the full job description401k401k matchingGym membershipHealth insurancePaid time offParental leave Full job description  A market leader in credit intelligence Reorg brings together journalists financial analysts legal analysts technologists and data scientists to collect and synthesize highly complex information into actionable intelligence Since 2013 tens of thousands of professionals across hedge funds investment banks management consulting and law firm verticals have come to rely on Reorg to make better faster and more confident decisions in pace with the fastmoving credit markets For more information visit wwwreorgcom    Working at Reorg       Consistent with our growth Reorg hires innovators and trailblazers across the globe to drive our business and our incredible corporate culture alike Our core values – Action Oriented Customer First Mindset Effective Team Players and Driven to Excel – define an organizational ethos that’s as highperforming as it is human Among other perks Reorg employees enjoy competitive health benefits matched 401k and pension plans Paid time off generous parental leave gym subsidies educational reimbursements for career development recognition programs petfriendly offices and much more      The Role   Reorg’s Digital Marketing team is seeking a Marketing Data Analyst to unlock the power of datadriven decisionmaking within our marketing and commercial teams You will play a pivotal role in extracting valuable insights from complex data sets to shape our marketing strategies enhance customer experience and drive our business forward  Responsibilities  Developing a deep understanding of our products internal tools strategic priorities stakeholder needs as well as key customers segments Data Mining and Analysis Implement advanced data mining techniques to extract valuable insights from large structured and unstructured data sets Insights Generation Analyze data to identify trends patterns and insights that inform marketing strategies and decisionmaking processes Reporting and Visualization Develop and present clear comprehensive reports and visualizations that translate complex data into actionable insights for the marketing team Crossfunctional Collaboration Work closely with marketing sales product and IT teams to ensure alignment of insights and strategies across the organization Customer Segmentation and Targeting Utilize data mining to refine customer segmentation and targeting strategies enhancing personalization and engagement Campaign Analysis Measure and analyze the effectiveness of marketing campaigns providing recommendations for optimization and future strategies  Requirements  Bachelor’s degree or higher 3 years of data science statistics or a related field Proven experience in data mining analysis and reporting within a marketing context Strong analytical skills with the ability to collect organize analyze and disseminate significant amounts of information with attention to detail and accuracy Proficiency in data mining and analytics tools eg SQL R Python and data visualization tools eg Tableau Power BI Knowledge of digital marketing channels and metrics Excellent communication and presentation skills with the ability to translate data insights into actionable marketing strategies Strong problemsolving skills and the ability to work under pressure in a fastpaced environment Team player with the ability to collaborate effectively with crossfunctional teams Ability to work from the New York City office 3 days per week US remote candidates will also be considered   At Reorg we consider a range of factors in connection with compensation decisions including experience skills location and our business needs and limitations As a result compensation may vary within and across similar roles and positions Please note that the salary range information below is a good faith estimate for this position and actual compensation for any individual may fall outside this range if warranted by the circumstances applicable to that individual If we identify a role that would be suitable for a broader range of skills and experience such that we would consider hiring at multiple levels then the range listed below may reflect that breadth  The salary range estimate for this position is 75000  90000  The actual compensation will be at Reorg’s sole discretion and will be determined by the aforementioned and other relevant factors This position is eligible for additional commissionbased compensation  Reorg provides equal employment opportunities EEO to all employees and applicants for employment without regard to race color religion sex national origin age disability or genetics In addition to federal law requirements Reorg complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities This policy applies to all terms and conditions of employment including recruiting hiring placement promotion termination layoff recall transfer leaves of absence compensation and training   </data></node>
<node id="n472" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=c73c7674e8c30528&amp;bb=5_Vz4GcWrbycs2moqfpeoo9yHK4S4a9K40Zz8bDtUgezdIPUrST222wurnjoUL4O3FxOossULaaoZls-PYKj6RIwgwfI5ibzq7UAB-DQ4-zRJmqBljaZw6ZJc7lgtn9n&amp;xkcb=SoD967M3CNuuXyw3hB0GbzkdCdPP&amp;fccid=5f407eee75c82197&amp;cmp=Simpalm&amp;ti=Entry+Level+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Quality assuranceYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay45000  50000 a yearJob typeFulltimeShift and schedule8 hour shiftMonday to Friday Location11821 Parklawn Drive Rockville MD 20852 BenefitsPulled from the full job description401k matchingPaid holidaysPaid sick timePaid time offProfessional development assistanceWork from home Full job descriptionEntry Level Business Analyst This position is a remote position we have offices in Maryland and Chicago no need to come to office Simpalm is a leading Mobile App and Web Development company with offices in Chicago and Maryland Our clients include enterprises nonprofits and startups We are looking to fill a Business Analyst position to engage with our clients on Software Development Projects Applicants from nonIT and noncomputer science background having good communication and analytical skills can also apply We will provide full training to the right candidates We prefer candidates from MarylandDCVAPennsylvania area We are looking for a combination of aptitude and skills Simpalm will provide the right compensation based upon the experience and talent This is an inhouse position where you will work on our existing projects This job is only for US Citizens and Green Card Holders Job responsibilities include following task  Candidate must be able to articulate the ideasbusiness needs or problems to higher management Candidate must be able to analyze current business processes understand current environment identify gapspain points in current business process Candidate must be able to facilitate meetings with cross functional teams  Ability to present and articulate ideas in user friendly language Ability to work in a team oriented collaborative environment Perform User Acceptance Testing Willingness to learn new technologies and methodology  Required Skills  Experience with Microsoft Office including Excel Access PowerPoint and Word Associate Degree or BABS degree orand Master degree Excellent verbal and written skills Ability to learn new technologies  About Simpalm Simpalm is a leading mobile app and web development company in USA Simpalm also provides specialized consultancy in information technology to several clients in North America Simpalm has gained strategic knowledge in Mobile Technologies Business Analysis Quality Assurance and Project Implementation in last few years and deliver successful projects for several industries and solved real time problems at all levels of Product Development Lifecycle We have built strong clientele in Mobile technology Government Insurance Retail Healthcare Pharmaceutical and Financial Industry Job Type Fulltime Pay 4500000  5000000 per year Benefits  401k matching Paid holidays Paid sick time Paid time off Professional development assistance Work from home  Compensation package  Performance bonus  Experience level  No experience needed Under 1 year  Schedule  8 hour shift Monday to Friday  Work Location Hybrid remote in Rockville MD 20852 </data></node>
<node id="n473" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=6e315ee13e882e48&amp;bb=5_Vz4GcWrbycs2moqfpeojF2gmIUERjvkHJpDuUiXlEY6s2-HVZDI9vE4KUK77FJ_Ikgt-TO5_Mziztye2r2luQUGiHVrpFU-HK8Gdj6V0k%3D&amp;xkcb=SoBg67M3CNuuXyw3hB0FbzkdCdPP&amp;fccid=c8b6362835043e9a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay67400  121300 a yearJob typeFulltime LocationDelaware BenefitsPulled from the full job description401kHealth insurancePaid time offTuition reimbursement Full job description You could be the one who changes everything for our 28 million members Centene is transforming the health of our communities one person at a time As a diversified national organization you’ll have access to competitive benefits including a fresh perspective on workplace flexibility  Position Purpose  Perform various analysis and interpretation to link business needs and objectives for assigned function This role will be heavily focused around reporting process analysis and reconciliation between different systems including analyzing enrollment 834 files and identifying data load issues This role will also work with enrollment shared services to determine root cause and follow up on fixes as applicable   Support business initiatives through data analysis identification of implementation barriers and user acceptance testing of various systems  Identify and analyze user requirements procedures and problems to improve existing processes  Perform detailed analysis on multiple projects recommend potential business solutions and ensure successful implementations  Identify ways to enhance performance management and operational reports related to new business implementation processes  Coordinate with various business units and departments in the development and delivery of training programs  Develop share and incorporate organizational best practices into business applications  Diagnose problems and identify opportunities for process redesign and improvement  Formulate and update departmental policies and procedures  Serve as the subject matter expert on the assigned function product to ensure operational performance  Ability to travel   EducationExperience  Bachelor’s degree in related field or equivalent experience 46 years of business process or data analysis experience preferably in healthcare Advanced knowledge of Microsoft Applications including Excel and Access preferred Project management experience preferred  Benefits and Payment Configuration  Bachelor’s degree in related field or equivalent experience 4 years of business process analysis preferably in healthcare ie documenting business process gathering requirements or claims paymentanalysis experience Advanced knowledge of Microsoft Applications including Excel and Access preferred Experience in benefits pricing contracting or claims and knowledge of provider reimbursement methodologies Knowledge of managed care information or claims payment systems preferred Previous structured testing experience preferred  Compliance CodingPrepay Compliance Payment Integrity  Bachelor’s degree in related field or equivalent experience 4 years of business process analysis ie ie documenting business process gathering requirements Experience in healthcare industry preferably with managed care techniques and administrative philosophy Experience in claims coding analysis or medical claim reviewresearch preferred Knowledge of Amisys claims payment system and Business Objects preferred  Encounters  Bachelor’s degree in related field or equivalent experience 4 years of business process analysis ie documenting business process gathering requirements experience in healthcare industry or 3 years of managed care encounters experience Advanced knowledge of Microsoft Applications including Excel and Access preferred Experience with encounters or claims business analysis experience in healthcare preferably managed care or Medicaid Knowledge of Amisys or other claims system and HIPAA transactions ie 837 999 824 277 preferred Experience witSQL Scripting or basic query writing is required  Medicare  Bachelor’s degree in related field or equivalent experience 4 years of business process analysis ie documenting business process gathering requirements Experience in healthcare industry preferably with managed care techniques and administrative philosophy Experience in managed health care experience preferably with Medicare Experience working with and leading diverse teams in matrix managed environments Advanced knowledge of Microsoft applications including Excel and Access preferred Project management experience preferred  Provider Data  Bachelor’s degree in related field or equivalent experience 4 years of business process analysis documenting business process gathering requirements experience in healthcare industry andor working in a data driven environment Advanced knowledge of Microsoft Applications including Excel Project and Visio preferred Knowledge of data migration software enhancementplanning and Agile preferred Experience managing projects with a high reliance on technology  Member  Provider Solutions  Bachelor’s degree in related field or equivalent experience 4 years of business process analysis ie documenting business process gathering requirements experience in healthcare industry andor customer service or enrollment functions Advanced knowledge of Microsoft Applications including Excel and Visio preferred Experience managing projects with a high reliance on technology Knowledge of data integration software enhancementsplanning and Agile preferred Pay Range 6740000  12130000 per year   Centene offers a comprehensive benefits package including competitive pay health insurance 401K and stock purchase plans tuition reimbursement paid time off plus holidays and a flexible approach to work with remote hybrid field or office work schedules Actual pay will be adjusted based on an individuals skills experience education and other jobrelated factors permitted by law Total compensation may also include additional forms of incentives  Centene is an equal opportunity employer that is committed to diversity and values the ways in which we are different All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability veteran status or other characteristic protected by applicable law  </data></node>
<node id="n474" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=24bd65a938d77b9d&amp;bb=opXyxKwanvGY0hw0dAe57CzlzK_pS0ZvWwFrMhi39ywhMaSqkxjE-BfIrcFObe6W-w4XtyY2hW3ArfFMPnK03sDt6JxNfXoAfNZbgi2aUzNEZPHobEFsDA%3D%3D&amp;xkcb=SoAC67M3CNuqr0yUwp0LbzkdCdPP&amp;fccid=d39a1724952b0584&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in jQueryYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationIllinois Full job description      US         ID             Full Time            01042024       Aspire Systems Digital Transformation Inc has an opening for a Computer Programmer Analyst at our offices in Oak Brook Illinois The Computer Programmer Analyst will work with ASDT’s customers to design develop and deliver highly customized software services using development methodologies and processes developed by our company The Computer Programmer Analyst will have the following specific duties  Computer Programmer Analyst  Planning   Analyze and assess existing business systems and procedures  Assist in defining and documenting business requirements  Coordinate with the development team to create solutions  Define software development project plans including scoping and scheduling  Design and develop Proof of Concepts POC for complex requirements   Technical   Understand and adapt existing product architecture and design  Develop projectspecific reusable components and core APIs using ASP Net MVC C XML and Web API  Implement client and serverside form validations using JavaScript jQuery and Angular  Design develop and analyze databases creating reports using MS SQL Server SSIS packages and SSRS  Validate task timelines and analyze data structures for database design  Create data methods for returning and saving data  Develop SQL Server procedurestriggers as per technical specifications  Set up development quality assurance and user acceptance testing environments   Quality Assurance   Collaborate with test automation and performance test engineers to develop test scripts  Develop test and deploy web application interfaces  Continuously improve release processes to enhance product quality   Coordination Tasks   Collect analyze and prioritize customer requirements  Interact with customers to discuss requirements and clarifications  Gather compile and interpret client solution requirements and write requirement specifications  Gather functional requirements and prepare technical specifications   Research and Development   Research identify and fulfill internal and external program user requirements  Analyze user requirements and interpret the existing Physical Data Model  Evaluate Schema Objects including Tables Indexes and Constraints   Minimum Requirements  Master’s Degree or foreign equivalent in Computer Science Electrical Engineering Mathematics or a related field plus 2 years of experience in a related position Experience must include working in the following technologies Java PHPSpring Struts Google Guice Hibernate jBPM JPAMysql Postgres MongoTomcat JettyApache Ant MavenApache KafkaSVN GitJenkins Bamboo Teamcity Junit Mockito Applicants meeting all requirements please submit your CV to careersusaspiresyscom    </data></node>
<node id="n475" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=6ea70b860113feaa&amp;bb=opXyxKwanvGY0hw0dAe57NAWFZadm0w38R2VCUo1_6JUWNxM_Q-NHmXky73LBL62RwbVBiQVqUuw1UKmx5U3TH8jt-jENxM5bEoU69ULdItzYmKLY5M7Qw%3D%3D&amp;xkcb=SoC267M3CNuqr0yUwp0KbzkdCdPP&amp;fccid=8d25321c1defe73f&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in XMLYesNoEducationDo you have a Bachelors degreeYesNo LocationTampa FL BenefitsPulled from the full job descriptionBenefits from day oneOpportunities for advancement Full job description   Tampa Florida  DOP  3061718      Job Description   Rate Renewal Analyst   Job Description  Here we grow again   Wipro is seeking individuals who combine excellent problemsolving skills with the ability to function effectively both as part of a team or on an individual basis to bring their talent to our team   Wipro is a leading global IT solutions and services company with over 240000 dedicated employees serving clients across multiple continents and 66 countries   We offer a strong compensation package that includes competitive pay and day one benefits Wipro also offers many opportunities for career advancement within our engaging and exciting culture   Job Summary  The Business Analyst is a crucial role in creating and maintaining the strategic partnership between Business needs and Technology delivery in a fastpaced development environment This role will be responsible for developing requirements based on Business Group needs The Analyst will participate in meetings with both Technology and Business Partners to facilitate the understanding clarification and implementation of the business requirements   Essential Job FunctionsDuties   Utilize high technical aptitude and skills coupled with business intelligence and a deep understanding of the customers needs so that they can be transformed into specific SDLC requirement artifacts  Act as an interface between business units technology teams and support teams  Serve as the subject matter expert for applicable knowledge areas which may include but not limited to assisting development and QA with data and root cause analysis and consultant to Business andor Clients as needed  Lead and facilitate requirement solicitation sessions for assigned projects Drive collaboration amongst project team members to ensure requirements are delivered on time and are managed through project execution  Collaborate with BA Team and Development teams to ensure baseline requirement artifacts are kept uptodate  Writing descriptions of business needs and business program functions including creating process and data flow diagrams workflow diagrams test scripts training curriculum and quality assuranceaudit procedures  Drafting raw curriculum to support development of training materials relevant to the approved client business processes  Establishing and maintaining positive client business relationships across the organization including guiding others in understanding client businesses processes and priorities   Qualifications   Bachelors degree or equivalent experience preferred  5 plus years in the role of Business Analyst Business System Analyst or System Analyst for technology based projects  5 years of experience as a team member on technologybased projects in highly complex businesstobusiness andor outsourced client environments  Strong knowledge of the benefit enrollment life cycle  Extensive experience with EDI 834 and some experience with EDI 820  Extensive knowledge and experience with data mapping specific to enrollment data  Intermediate to extensive exposure to XML schemas and canonical structures  Strong knowledge of Software Development Lifecycle SDLC methodologies Waterfall Iterative Agile etc  Experience with process and data flow diagramming  Prefer knowledge of enterprise modeling applications such as Enterprise Architect ProVision andor other modeling andor languages including UML  Business process reengineering skills a plus  Ability to travel as needed  Excellent verbal written and interpersonal skills         If you encounter any suspicious mail advertisements or persons who offer jobs at Wipro please email us at helpdeskrecruitmentwiprocom Do not email your resume to this ID as it is not monitored for resumes and career applications      Any complaints or concerns regarding unethicalunfair hiring practices should be directed to our Ombuds Group at ombudspersonwiprocom   We are an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race color caste creed religion gender marital status age ethnic and national origin gender identity gender expression sexual orientation political orientation disability status protected veteran status or any other characteristic protected by law      Wipro is committed to creating an accessible supportive and inclusive workplace Reasonable accommodation will be provided to all applicants including persons with disabilities throughout the recruitment and selection process Accommodations must be communicated in advance of the application where possible and will be reviewed on an individual basis Wipro provides equal opportunities to all and values diversity      </data></node>
<node id="n476" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=295c3f257768f74a&amp;bb=opXyxKwanvGY0hw0dAe57KQwmg_0YmHlOlVPwaItrrsdgEVMuFxzzx3nFnnGPcKdfZ_JayuIwMiFgWWR2dB9deuXGD7wwCiGdfegDp3l1Dw%3D&amp;xkcb=SoAr67M3CNuqr0yUwp0JbzkdCdPP&amp;fccid=a892c8c946e25608&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in User acceptance testingYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay116974  187158 a yearJob typeFulltime LocationIrvine CA BenefitsPulled from the full job descriptionRetirement plan Full job description     “I can succeed as the Senior Digital Analytics Analyst at Capital Group”        As the Senior Digital Analytics Analyst you demonstrate strong working knowledge in the assigned area and work under direction only as needed You develop and deliver complex digital analytics insights and guidance to support business decision making and continuously improve digital experiences        Primary ResponsibilitiesEssential Functions      Independently produce deep analysis for various digital assets in the assigned business area Develop and deliver reports to support the implementation and performance of digital assets     Track measure and analyze website and application trends  Gather organize and analyze data to inform strategy formulate insights and make actionable recommendations  Design and build reports using tools like Adobe Analytics Tableau etc  Automate and deliver reoccurring reports  Provide real time reporting and monitoring of anomalies in web performance  Design and execute analysis of ad hoc requests     Serve as a subject matter expert on analytics tagging perform user acceptance testing implements AB tests supports implementation and execution     Independently write requirements for measurement framework for new digital assets landing pages mobile applications etc  Identify defects in tagging and implementation and work with IT to solution on the problem  Validate all tagging implementations  Maintain an understanding of industry best practices for tagging implementation and execution  Partner with IT to implement and execute all tagging     Support optimization of business processes     Manage and Support marketing technology tools  Collaborate with cross functional teams including IT Marketing Pods and Digital Channel Stakeholders to bring enhancement to current processes  Assist in stakeholder and team trainings on the technologies       “I am the person Capital Group is looking for”        SkillsQualifications      You have a minimum of 3 years’ relevant experience not necessarily within Finance with the ability to motivate inspire and achieve goals  You have demonstrated experience delivering actionable insight for a consumer business  You have excellent problemsolving skills along with the ability to find creative solutions for challenges regarding campaigns and communication of results  You are able to manage multiple projects at the same time in a fastpaced environment You can independently identify and set priorities in rapidly changing and ambiguous environment  You have knowledge of SEO and SEM best practices digital marketing tools Adobe Analytics Adobe Audience Manager other DMPs etc and channels  You have advanced ETL and SQL writing skills  You have knowledge of in at least one statistical analysis tool such as R or Python  You are an expert in dash boarding tools eg Tableau and have advanced experience in Excel  You have a BABSc degree in Business  Mathematics  Statistics  Computer Science  Analytics  Econometrics Master’s degree is preferred      ‎   Southern California Base Salary Range 116974187158      ‎       ‎       ‎       ‎       ‎       ‎   ‎      ‎       ‎       ‎       ‎       ‎       ‎       ‎     In addition to a highly competitive base salary per plan guidelines restrictions and vesting requirements you also will be eligible for an individual annual performance bonus plus Capital’s annual profitability bonus plus a retirement plan where Capital contributes 15 of your eligible earnings     You can learn more about our compensation and benefits   here     Temporary positions in Canada and the United States are excluded from the above mentioned compensation and benefit plans    We are an equal opportunity employer which means we comply with all federal state and local laws that prohibit discrimination when making all decisions about employment As equal opportunity employers our policies prohibit unlawful discrimination on the basis of race religion color national origin ancestry sex including gender and gender identity pregnancy childbirth and related medical conditions age physical or mental disability medical condition genetic information marital status sexual orientation citizenship status AIDSHIV status political activities or affiliations military or veteran status status as a victim of domestic violence assault or stalking or any other characteristic protected by federal state or local law   </data></node>
<node id="n477" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=3c27f0c6c46ce5eb&amp;bb=opXyxKwanvGY0hw0dAe57HIlX25ndIavy6u9nlEq1NrRLBcX-7AXjThNhIWrZaOZpF77Eig5NKQrrQVlD_wRuMCxed_ARp7A7SqVePLtKsQ%3D&amp;xkcb=SoCf67M3CNuqr0yUwp0IbzkdCdPP&amp;fccid=e290923a98afee94&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay92986  158076 a yearJob typeParttimeShift and scheduleOn call LocationReston VA Full job description    ICF is looking for a Senior Business Analyst to support an Enterprise Performance Management EPM project on the Strategy and Business Solutions Support Contract If you are looking for an opportunity to help inform and direct IT investment decisions and develop architectural targets and standards for a client then this is the job for you This position will work remotely for a Federal client in Arlington VA      What you’ll be doing           Leading efforts for the Enterprise Performance Management EPM project            Formulating and defining strategies and business processes for the EPM project through research and factfinding activities            Recommending and applying strategic thinking process improvement and reengineering methodologies and principles to the EPM project by proposing completing and analyzing business data and hybrid enterprise models            Identifying best practices documenting and assessing performance measurements and identifying business and technology solutions to streamline processes            Leading group facilitation sessions interviews and other activities to gather information from stakeholders            Eliciting and managing business and systems level requirements for the EPM project            Employing industry standard tools to manage requirements            Developing medium to complex business process models            Applying portfolio management concepts including businessIT alignment prioritization and market research techniques for the EPM project            Conducting knowledge transfer of strategic plans scope requirements and other analyses to program and project teams to ensure successful implementation of solutions to client areas            Ensuring recommendeddelivered solutions meet business needs for the EPM project         What you will need           7 years of work experience as a business analyst            US Citizenship required by Federal Government            The ability to successfully obtain a Public Trust which involves a thorough background and financial investigation         Preferred SkillsExperience           Bachelors degree in Accounting Finance Business Administration Marketing Information Systems Management Computer Science or an equivalent combination of education andor experience            Excellent written and verbal communication and personal skills with ability to present to groups including Clevel executives            Solid understanding of the commercial IT market for hardware and software            Exhibit strong communication facilitation interview document writing and presentation skills            Mastery in independently applying business analysis on moderately to highly complex projects            Demonstrated ability to elicit requirements document user stories develop business process models and perform feasibility and cost benefit analyses            Advanced knowledge and experience with applying Agile Methodologies concepts and practices            Knowledge and experience using and applying human centric design principles            Advanced proficiency in analytical project planning negotiating and interpersonal skills            Ability to perform costbenefit analysis trending forecasting and financial analysis            Knowledge and understanding of onprem and offprem Cloud based environments            Knowledge and understanding of Enterprisewide platform deployment software            Proficiency in using Microsoft applications Word Excel PowerPoint Project SharePoint Teams and Visio            Experience using Tableau Power Apps Azure DevOps Salesforce Appian and ServiceNow         Additional Qualifications           Ability to express information to individuals or groups effectively taking into account the audience and nature of the information            Ability to work in a fastpaced environment            Ability to effectively manage a team            Ability to understand people processes and technologies to address business needs for process improvements            Ability to lead a highly complex project            Ability to identify problems and use sound judgement to generate and evaluate alternatives to make recommendations            Ability to thoroughly pay attention to details when performing and reviewing work            Ability to work with clients and customers to assess their needs provide information or assistance resolve their problems and satisfy their expectations           Working at ICF    ICF is a global advisory and technology services provider but we’re not your typical consultants We combine unmatched expertise with cuttingedge technology to help clients solve their most complex challenges navigate change and shape the future       We can only solve the worlds toughest challenges by building an inclusive workplace that allows everyone to thrive We are an equal opportunity employer committed to hiring regardless of any protected characteristic such as race ethnicity national origin color sex gender identityexpression sexual orientation religion age disability status or militaryveteran status Together our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals For more information please read our        EEO  AA policy           Reasonable Accommodations are available including but not limited to for disabled veterans individuals with disabilities and individuals with sincerely held religious beliefs in all phases of the application and employment process To request an accommodation please email        icfcareercentericfcom    and we will be happy to assist All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations        Read more about        workplace discrimination rights    the        Pay Transparency Statement    or our benefit offerings which are included in the        Transparency in Benefits Coverage Act        Pay Range  There are multiple factors that are considered in determining final pay for a position including but not limited to relevant work experience skills certifications and competencies that align to the specified role geographic location education and certifications as well as contract provisions regarding labor categories that are specific to the position The pay range for this position is   9298600  15807600   Nationwide Remote Office US99  </data></node>
<node id="n478" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d9a7297f21c8f6ce&amp;bb=opXyxKwanvGY0hw0dAe57GAcfHWYRk9ge17ur7-zYiYoOlZaMI0acSvVfnvvjzImzOzdSroAq8tpieqV1qYzc1_ZSImCGGC_LrpsEc2_sV_i9cCzFXFYRg%3D%3D&amp;xkcb=SoAR67M3CNuqr0yUwp0PbzkdCdPP&amp;fccid=e84543660fa54e90&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Software troubleshootingYesNo Job detailsHere’s how the job details align with your profilePay10000  10833 a monthJob typeFulltime Location200 East 10th Street Austin TX 78701 BenefitsPulled from the full job descriptionOnthejob training Full job description  TTSTC  Business Analyst 00041198   Organization TREASURY SAFEKEEPING TRUST COMPANY        Primary Location TexasAustin     Work Locations Thomas J Rusk Building 200 East 10th Street Austin 78701          Job Computer and Mathematical        Employee Status Regular        Schedule Fulltime     Standard Hours Per Week 4000        Travel Yes 5  of the Time        State Job Code 0225        Salary Admin Plan B        Grade 29        Salary Pay Basis 1000000  1083300 Monthly        Number of Openings 1        Overtime Status Exempt        Job Posting Apr 1 2024 93733 AM        Closing Date Ongoing        Description   Applications must be filed at  httpscappstaleonetcareersectionexjobdetailftljob00041198    About the Comptroller’s Office  Innovation collaboration and a commitment to excellence best describes the culture here at the Comptroller’s Office We take pride in the work we do serving as Texas accountant tax collector treasurer and much more    Culture  Our agency workforce is as diverse as the people of Texas we serve We value our employees and take very seriously our collective commitment to public service Personal development opportunities are strongly encouraged through available workshops teambuilding exercises and on the job training We offer flexible scheduling that helps employees maintain a healthy worklife balance If you are seeking to gain knowledge build your career and network among goaloriented professionals this is the place for you  Click here to see an inside look at the Texas Comptroller’s office Click here to see a video from the Texas Treasury Safekeeping Trust Company    General description  Join the Texas Treasury Safekeeping Trust Company as a Business Analyst Work involves performing advanced seniorlevel business analysis work Work involves overseeing the gathering development and documentation of user requirements the review assessment and development of business processes the creation and validation of user acceptance testing the performance of post implementation support of systems and support of the systems development life cycle May supervise the work of others Works under minimal supervision with considerable latitude for the use of initiative and independent judgment    Minimum Qualifications  Graduation from an accredited fouryear college or university with major coursework in business administration finance accounting computer science management information systems or a related field Seven years of experience in investment management or banking industries Seven years of experience in business analysis methodologies Seven years of experience with investment terminology asset classes and portfolio management practices Experience with Agile Lifecycle methodology Experience with analyzing and interpreting technical information documentationspecifications Experience translating userbusiness needs into technical requirements Experience communicating technical instructions to nontechnical users Experience implementing and supporting FinancialAccountingInvestments systems  Experience facilitating the implementation and testing of systems and business processes    Substitution  One additional year of related work experience may substitute for 30 semester hours of required education with a maximum substitution of 120 semester hours four years    In this role you will  Schedule plan and lead requirements gathering analyze inputs from stakeholders review procedures and pain points to improve existing systems propose plan of action creates process workflows and coordinates andor analyzes computer system capabilities workflows and scheduling limitations within an investment performance environment Performs quality assurance QA testing for both vendorproduced and internally developed GL Accounting Investment Accounting Trading and Cash Management and Investment Performance applications and their interfaces between multiple systems Liaises between business units in regard to the analysis of applications troubleshooting of system problems designing of businesscritical workflows testing and implementation of solutions specific to investment management risks and analytics Performs and coordinates the development of data sourcing standardization validation and assistance in the creation of analytical processes to improve investment risk management and quantitative analytics Assist IT teams with support of applications interfaces issues resolution and business process improvements based on investment accounting trading systems and client facing applications      Qualifications    Have Knowledge of   Microsoft Excel including functions and macros and Structured Query Language SQL Investment terminology and asset classes Finance and accounting terms and processes Reconciliation and analysis of financial information to ensure consistency between systems Implementing troubleshooting and resolving problems with accounting or finance systems Financial analysis and techniques      Skill in   Effectively conveying information and encouraging an exchange of ideas Identifying defining and solving problems      Ability to   Maintain confidentiality and protect the privacy of state employees taxpayers and other members of the public Follow all agency confidentiality privacy and information security policies and procedures Appropriately store secure and protect sensitive records and other confidential documents and data in accordance with the agency’s certified Records Retention Schedule Seek guidance and clarification from relevant agency specialists when potential confidential issues arise Use discretion and appropriate judgement in communicating confidential and sensitive information Work with others to achieve a common goal Adjust to changing workplace demands Meet the needs and expectations of internal and external customers Effectively demonstrate skill and ability to perform the specific job duties and tasks as defined by a job description Be dependable meet deadlines and produce highquality work     Veterans To receive veterans preference a copy of your DD214 Member 4 is required  Veterans Reservist or Guardsmen with an MOS or additional duties that fall within the essential duties of the job position or other related fields pertaining to the minimum experience requirements may meet the minimum qualifications All active duty reservists guardsmen and veterans are encouraged to apply if they meet the qualifications for this position Please call Human Resources CPA Veterans Liaison at 51247535608005315441 for more information or assistance  If selected for the position the following must be provided for proof of veterans’ preference  Veteran must provide form DD 214 Surviving Spouse or Orphan must provide DD 1300 or DD 214     Military Occupational Specialty MOS codes that may correspond to the state classification title for this position are listed on the State Auditor’s Office Job Descriptions click on the occupational category for the position Additional MOS can be found at the State Auditor’s Office Military Crosswalk Guide  The Texas Veterans Commission provides helpful employment information Go to httpwwwtexasskillstoworkcom httpwwwonetonlineorg httpshrsaotexasgovCompensationSystemJobDescriptions or wwwcareeronestoporg for assistance translating your military experience and training courses into civilian job terms qualificationsrequirements and skill sets    Applications submitted through Work In Texas  Work In Texas WIT applicants must complete the supplemental questions to be considered for the posting To complete the supplemental questions please go to CAPPS Recruit to register or login and access your profile  The Comptroller’s Office is proud to be an equal opportunity workplace We are committed to equal employment opportunity regardless of race color ancestry religion sex national origin sexual orientation age citizenship marital status disability gender identity   </data></node>
<node id="n479" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=2bae4a0b31651fd6&amp;bb=opXyxKwanvGY0hw0dAe57N0jIPiVJnvNVYRddEl6KswhlDKewMsjbFiaybVMJ0CJTlufEEaf-cPCbwvE21uTNp2PkRNjc9vPyfeYWgIkGQw%3D&amp;xkcb=SoCl67M3CNuqr0yUwp0ObzkdCdPP&amp;fccid=14771eef3e48e5c2&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in XMLYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay7441800  12757440 a yearJob typeFulltime LocationOne Sw Columbia Street Suite 150 Portland OR 97258 BenefitsPulled from the full job description401k401k matchingDental insuranceDisability insuranceEmployee assistance programEmployee discountHealth insuranceShow morechevron down Full job description    About Us    Umpqua Bank is headquartered in the Pacific Northwest with 5000 employees and offers banking services to customers throughout the nation It’s an especially exciting time to join our team as following the recent merger with Columbia Bank we have grown to become a leading westernbased regional bank with more than 50B in assets under management and an unwavering commitment to our associates our customers and our communities  We create a great place to work by offering a special brand of relationship banking and by providing a culture where associates thrive Associates who embody our core values fit in well here and we are eager to meet candidates who demonstrate behaviors that align with Trust Ownership Growth Empathy Teamwork Heart Enjoyment and Relationships  About the Role      Serves as subject matter expert for descriptivepredictive data analysis and reporting within a division business unit andor department Acquires analyzes interprets and disseminates data sets to optimize or create process program andor fiscal solutions for complex organizational challenges May lead projects to improve data quality andor improve workflow process and procedure Defines or helps define program scope and objectives involving all relevant stakeholders and ensuring technical feasibility May fully oversee lifecycles of programs of varying scope including ideation execution monitoring and continuous improvement      Acquires primary and secondary data from varied sources and analyze data extracts and reports to identify trends and data points  Utilizes databases and information systems to research and identify areas for improvement andor new products Interprets data and makes recommendations  Responsible for utilizing data bases and information systems to research and identify areas for improvement andor new products Interprets data and makes recommendations  Maintains detailed knowledge of department functions and in areas of workflow and system data management  Prepares and may lead timely and accurate progress reports utilizing multiple databases and software applications ensuring consistency standardization and compliance  Maintain detailed knowledge of department functions and in areas of workflow and system data management  May consult with managers and possibly executives in the use of analytical and reporting tools to develop effective cost quality operational and customer satisfaction outcomes May act as a liaison to collaborate with other operating groups to resolve problems or make recommendations for programprocess changes  May partner with Technology Advancement Group TAG external vendors and business partners in the development of advancedcomplex reporting capabilities and resolution of issues relating to reporting applications tools and data  Demonstrates compliance with all bank regulations for assigned job function and applies to designated job responsibilities – knowledge may be gained through coursework and onthejob training Keeps up to date on regulation changes  Follows all Bank policies and procedures compliance regulations and completes all required annual or jobspecific training  Maintain a working knowledge of Banks written policies and procedures regarding Bank Secrecy Act Regulation CC Regulation E Bank Security and other regulations as applicable to this job description  May be asked to coach mentor or train others and teach coursework as subject matter expert  Actively learns demonstrates and fosters the Umpqua corporate culture in all actions and words  Takes personal initiative and is a positive example for others to emulate  May perform other duties as assigned     About You      710 years of of related business experience with emphasis in banking information systems controls andor data analysismanagement required  Bachelor’s Degree in Business economics or finance or the equivalent in eduacatin and experience preferred  Advanced knowledge of business analysis business management data systems IT risk management project management and technical problem resolution  Advanced data mining performance metrics and reporting vendor management change management and report writing Statistical analysis descriptive statistics and exploratory data analysis  Advanced knowledge of statistics and experience using statistical packages for analyzing datasets Excel SPSS SAS etc  Experience with reporting packages Business Objects etc databases SQL etc and programming XML Javascript or ETL frameworks preferred  Superior attention to detail  Ability to translate large and complex data sets into understandable conclusions and where applicable actionable solutions  Demonstrated knowledge of banking products systems procedures regulations business acumen and ability to interpret data to make recommendations for improvements as necessary  Ability to work with individuals and teams at all levels across the organization and with external company representatives and vendors Strong communication interpersonal and influencing skills  Technical expertise with automatic data collection and reporting systems including capacity for program troubleshooting and system controls    Work Style  Umpqua Bank offers a Flexible Workplace Program and this opportunity comes with the Flex Office work style which is working in office from a designated company location three days weekly with flexibility to work remotely up to two days weekly  Our Benefits  We offer a competitive total rewards package including base wages and comprehensive benefits The pay range for this role is 7441800 to 12757440 and the pay rate for the selected candidate is dependent upon a variety of nondiscriminatory factors including but not limited to jobrelated knowledge skills and experience education and geographic location The role may be eligible for performancebased incentive compensation and those details will be provided during the recruitment process  We offer eligible associates comprehensive healthcare coverage medical dental and vision plans a 401kretirement savings plan with employer match for qualifying associate contributions an employee assistance program life insurance disability insurance tuition assistance mental health resources identity theft protection legal support auto and home insurance pet insurance access to an online discount marketplace and paid vacation sick days volunteer days and holidays Benefit eligibility begins the first day of the month following the date of hire for associates who are regularly scheduled to work at least thirty hours weekly  Our Commitment to Diversity  Umpqua Bank is an equal opportunity and affirmative action employer committed to employing engaging and developing a diverse workforce All qualified applicants will receive consideration for employment without regard to race color national origin religion sex age sexual orientation gender identity gender expression protected veteran status disability or any other applicable protected status or characteristics If you require an accommodation to complete the application or interviews please let us know by email careersumpquabankcom  To Staffing and Recruiting Agencies  Our posted job opportunities are only intended for individuals seeking employment at Umpqua Bank Umpqua Bank does not accept unsolicited resumes or applications from agencies and Umpqua Bank will not be responsible for any fees related to unsolicited resume submissions Staffing and recruiting agencies are not authorized to submit profiles applications or resumes to this site or to any Umpqua Bank employee and any such submissions will be considered unsolicited unless requested directly by a member of the Talent Acquisition team    </data></node>
<node id="n480" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=08ead38c6eaa0cb2&amp;bb=opXyxKwanvGY0hw0dAe57Iqo23XMBTCNfErx8MMfI3U6OFGBaGKgqlfYhDGtVnIyIfnPLEYsyBNb7bn_9NUJvRqLTJLSurFXH---7jxc6EA%3D&amp;xkcb=SoA467M3CNuqr0yUwp0NbzkdCdPP&amp;fccid=268912b4b344e163&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location107 Gilbreth Parkway Mullica Hill NJ 08062 Full job description MAJOR FUNCTION The Managing Data Analyst will be a key member of the Advanced Analytics team for developing business intelligence solutions and visualizations of strategic data insights They will organize and manage large scale data projects including business requirement gathering collaborating with business partners and developing data visualizations for operational and strategic improvement projects QUALIFICATIONS  Education  Experience  Bachelor’s or Master’s degree in a quantitative field such as Mathematics Statistics Physics Economics Computer Science Minimum 2 years of relevant experience in data analysis or related field   CertificationLicensure  NA  Knowledge  Skills  Strong analytical and problemsolving skills Advanced proficiency in Excel SQL and basic data visualization tools  Proven working experience in Analytics and Business Intelligence for the purpose of deriving business insights Ability to collect organize analyze and disseminate significant amounts of information with attention to detail and accuracy  Proven experience in data analysis with a focus on project management Mastery of data quality and business intelligence concepts familiar with data science concepts Experience with data models database design development data mining and segmentation techniques Knowledge of statistics and experience using statistical packages for analyzing data sets Excel SPSS SAS etc Excellent leadership and communication skills    Physical Requirements N Never O Occasionally 20 F Frequently 2080 C Constantly 80  Lifting 20lbs O Standing F Sitting C  Lifting 2050lbs N Climbing N Kneeling F  Lifting50lbs N Crouching O Reaching F  Carrying O Hearing C Walking F  Pushing O Talking C Vision C  Environmental Conditions  Noise N Varied Temperatures N Cleaning Agents N  Noxious odors N Patient Exposure N Operative Equipment N       At Inspira Health you’ll join with the area’s most dedicated and distinguished team to bring quality and compassionate care to our communities We focus on clinical excellence providing evidencebased care to help each patient achieve the best possible outcome The scope and depth of our network can open many doors for your learning and career growth Our charitable nonprofit health care organization serves communities across southern New Jersey The network which traces its roots to 1899 comprises three hospitals a comprehensive cancer center sleep medicine cardiac testing digestive health and wound care urgent care imaging and rehabilitation and primary and specialty physician practices in Gloucester Cumberland Salem and Camden counties Inspira is an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race religion creed color national origin ancestry age marital status affectional or sexual orientation familial status disability liability for service in the Armed Forces of the United States nationality sex gender identity or expression  </data></node>
<node id="n481" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=2e384e8154e509c0&amp;bb=opXyxKwanvGY0hw0dAe57HFk-Y9Vl8TUsMJvdc1S_h_dLA37o2NLtmpYXNxA1KZWh5YNtUgCc9wKXrFfWtrThSVft5CJg6UxWNJszV5gwHKOFMJ0XBUwLA%3D%3D&amp;xkcb=SoCM67M3CNuqr0yUwp0MbzkdCdPP&amp;fccid=dd616958bd9ddc12&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo LocationIndiana BenefitsPulled from the full job description401k401k matchingCommuter assistanceFlexible spending accountHealth insuranceHealth savings accountLife insuranceShow morechevron down Full job description Intrepid sets the standard for delivering excellence in the federal marketplace and is known for treating employees like family We provide our employees with a challenging and supportive work environment paired with a competitive salary and an industryleading 401k contribution We are looking for an Army Mass Transit Benefit Program Analyst to join our team in supporting the United States Army Financial Management Command USAFMCOM  Your daytoday work will include  Provide oversight and support for USAFMCOM’s Mass Transit Benefit Program MTBP Responsibilities include assisting with data entry sorting filing and analysis working to resolve issues directly with participants providing detailed status reports to stakeholders and other adhoc reports on the program and providing monthly training to participants Tasks will include  Provide document processing support including but not limited to data gathering compilation analysis filing and report submission Collect review and analyze data received including but not limited to participant applications location submission forms and recertification checklists for validation  Maintain all Installation Manager and Reviewing Official data within the Transportation Incentive Program System TIPS and access database Assist with the resolution of benefit returns and repayments for participants officials and other stakeholders Provide application site technical assistance for Transportation Incentive Program TIP participants officials and other stakeholders Host system training monthly to TIP participants  At a minimum you should have   Ability to obtain and maintain a federal SECRET security clearance Bachelor’s degree 310 years of experience in quality assurance data entry customer service GFEBS 1 years and the Army military structure We are seeking individuals with varying degrees of experience in these categories  Proficiency in Microsoft Word PowerPoint Visio and Excel  Experience with creating or generating detailed status reports and excellent attention to detail  Experience supporting DoD stakeholders and DoDArmy Financial Management   You will be highly desirable if you have   An active and current SECRET federal security clearance from the DoD  Experience working with federal benefit programs   This job description is subject to change at any time Work Type OnSite Indianapolis IN Estimated Salary Range 11000000 13000000 The provided salary range serves as a broad reference However Intrepid takes various factors into account when establishing base salary offers including the positions scope and responsibilities as well as the candidates experience education skills and prevailing market conditions  WorkLife at Intrepid Wondering what its like to work here Let us give you a glimpse of our exceptional workplace culture Our employees have consistently nominated us for the Best Places to Work award and we take pride in our familylike environment remarkable benefits and gotheextramile attitude The Hours We sincerely value worklife balance Our flexiblehours policy allows you to balance extra time during significant projects with days that are lighter Moreover we offer generous accrual of paid personal leave that doesn’t lose its value no use it or lose it here as well as 11 paid holidays per year The Benefits Our benefits are renowned starting with our outstanding 401k program No match required We contribute 14 of your biweekly pay to your account regardless of your contribution With our lowfee index funds from Fidelity your retirement savings will grow substantially Plus your professional financial advisors are already covered Our topnotch health insurance plan through Blue Cross Blue Shield includes low deductibles 200year and is mostly covered by Intrepid or you can choose a highdeductible plan with an eligible HSA the choice is yours We also provide complimentary life insurance affordable dental vision disability critical illness and pet insurance Additionally you can set aside pretax dollars for medical and dependent care expenses through an FSA We even offer a 1000 scholarship for newborn or adopted children as well as those enrolled in higher education The Perks Enjoy typical perks like corporate discounts as well as unique experiences as an Intrepid employee Youll be a VIP at our annual events including the Chili CookOff Thanksgiving Lunch  Lawn Games IceCream Social Intrepig BBQ and the grand endofyear Christmas bash with amazing prizes Remote workers have special virtual engagement opportunities and exclusive events so no one is left out of the fun Give Back Giving back is ingrained in our values Through our employeemanaged charitable fund the Intrepid Ideal Community Fund ICF we contribute tens of thousands of dollars each year to organizations that help people in need Join us in various volunteer opportunities and help us make a difference in our communities Our vision is to one day create ideal communities where every citizens needs are met Join Us Theres something for everyone at Intrepid If our benefits perks values and mission resonate with you were thrilled to meet you Start your journey as an Intrepid employee by applying today We cant wait to hear from you  About Intrepid Intrepid is a VEVRAA Federal Contractor and an Equal Opportunity Employer committed to making employment decisions based on merit and value All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability or status as a protected veteran CJ  </data></node>
<node id="n482" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=521058b3a089ccb1&amp;bb=opXyxKwanvGY0hw0dAe57BAUnFaloNDuUyXvgStiGCz0rgwjIYvSkYk0OAmG-SoyvzibVR2AsxOlQ5M3ijq7RdYI1-6cOBHWMKWIgHUO2rU%3D&amp;xkcb=SoBl67M3CNuqr0yUwp0DbzkdCdPP&amp;fccid=ae5bfc395c530fbc&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SmartsheetYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay102340  132440 a yearJob typeFulltime LocationKnoxville TN 37902 BenefitsPulled from the full job description401k401k matchingAdoption assistanceDental insuranceDisability insuranceFlexible scheduleHealth insuranceShow morechevron down Full job description At US Bank we’re on a journey to do our best Helping the customers and businesses we serve to make better and smarter financial decisions and enabling the communities we support to grow and succeed We believe it takes all of us to bring our shared ambition to life and each person is unique in their potential A career with US Bank gives you a wide evergrowing range of opportunities to discover what makes you thrive at every stage of your career Try new things learn new skills and discover what you excel at—all from Day One   Job Description     The Project Analyst position within our organization holds a crucial role within our Design Program through implementation of continuous improvement initiatives program advancements and comprehensive documentation strategies specifically tailored for PAO Market Activation This role is specifically focused on driving enhancements in Site Selection processes CapEx documentation Smartsheet reporting monthly document generation and overall process refinement within the PAO Market Activation framework Additionally it involves spearheading the advancement and documentation of InStore lease programs alongside rigorous reporting obligations To excel in this capacity we seek an individual with exceptional organizational and analytical abilities underpinned by a robust grasp of project management principles The ideal candidate will possess a talent for conceptualizing and developing the programmatic solutions that yield tangible improvements They will be adept at conducting indepth research analyzing data and crafting recommendations that not only reduce costs but also amplify benefits Effective communication and collaboration skills are imperative as this role requires seamless coordination and partnership across various departments     Moreover this role will entail assisting the Market Activation Director with reporting calendar management scheduling program management and presentation generation This aspect of the role involves providing crucial support to ensure smooth operations and effective communication within the Market Activation team The successful candidate will demonstrate proficiency in managing calendars coordinating schedules and generating insightful reports to aid decisionmaking processes Additionally they will possess strong program management skills enabling them to oversee multiple initiatives simultaneously Presentation generation will be a key responsibility requiring the ability to translate complex data and insights into compelling visual narratives These tasks are integral to the success of the Market Activation Director and the overall efficiency of the Market Activation function        Basic Qualifications     Bachelors or Masters degree or equivalent work experience 10 or more years of experience in project management activities   Preferred SkillsExperience     Expert knowledge of assigned business line or functional area Strong organizational and analytical skills Thorough knowledge of project management Ability to identify and resolve exceptions and to analyze data Demonstrated leadership skills Microsoft Office Suite experience    Data management experience       The role offers a hybridflexible schedule which means theres an inoffice expectation of 3 or more days per week and the flexibility to work outside the office location for the other days     If there’s anything we can do to accommodate a disability during any portion of the application or hiring process please refer to our disability accommodations for applicants   Benefits  Our approach to benefits and total rewards considers our team members’ whole selves and what may be needed to thrive in and outside work Thats why our benefits are designed to help you and your family boost your health protect your financial security and give you peace of mind Our benefits include the following some may vary based on role location or hours   Healthcare medical dental vision  Basic term and optional term life insurance  Shortterm and longterm disability  Pregnancy disability and parental leave  401k and employerfunded retirement plan  Paid vacation from two to five weeks depending on salary grade and tenure  Up to 11 paid holiday opportunities  Adoption assistance  Sick and Safe Leave accruals of one hour for every 30 worked up to 80 hours per calendar year unless otherwise provided by law    EEO is the Law  US Bank is an equal opportunity employer committed to creating a diverse workforce We consider all qualified applicants without regard to race religion color sex national origin age sexual orientation gender identity disability or veteran status among other factors   EVerify  US Bank participates in the US Department of Homeland Security EVerify program in all facilities located in the United States and certain US territories The EVerify program is an Internetbased employment eligibility verification system operated by the US Citizenship and Immigration Services  The salary range reflects figures based on the primary location which is listed first The actual range for the role may differ based on the location of the role In addition to salary US Bank offers a comprehensive benefits package including incentive and recognition programs equity stock purchase 401k contribution and pension all benefits are subject to eligibility requirements Pay Range 10234000  12040000  13244000   Job postings typically remain open for approximately 20 days of the posting date listed above however the job posting may be closed earlier should it be determined the position is no longer required due to business need Job postings in areas with a high volume of applicants such as customer service contact center and Financial Crimes investigations remain open for approximately 5 days of the posting listed date  </data></node>
<node id="n483" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=3bbe03e53d1d4994&amp;bb=opXyxKwanvGY0hw0dAe57ObutinbanqlQaPnCBreu95dJoXuNNuYXNx0om5ocoHPheWPWgIe-mKP6FE78v3U2xan6EyOtTXPJcrgiSfMp5U%3D&amp;xkcb=SoDR67M3CNuqr0yUwp0CbzkdCdPP&amp;fccid=2a341562d64c7cdb&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Statistical analysisYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeShift and schedule8 hour shiftDay shiftMonday to Friday Location500 Parnassus Ave San Francisco CA 94143 Full job description         Uses skills as a seasoned experienced research professional with a full understanding of indepth statistical analyses and  or research software programming techniques Demonstrates good judgment in selecting methods and techniques for obtaining solutions       The Research Data Analyst will work as part of a Research Program team to conduct geographic information system GIS and epidemiologic analyses in support of research projects manuscripts and grant proposals The Research Data Analyst’s primary role will be to work alongside the Principal Investigators and study teams to clean organize and analyze large multilevel datasets including surveillancecancer registry data and longitudinal data The Research Data Analyst will develop and maintain research gathering retrieval and reporting systems prepare reports and documents research methods work independently and with a variety of integrated epidemiologic and geographic datasets and explore and make recommendations for geospatial data resources and analytic approaches The Research Data Analyst will participate in the design of analyses and the tabulation and presentation of research data as well as coauthor and may leadauthor scientific manuscripts Additional tasks may include contributing to the development of grant proposals for the Research Program The Research Data Analyst will be expected to use skills as an experienced research professional with a full understanding of statistical analyses and application of epidemiologic principles communicate and disseminate research findings with a broad set of collaborators provide highlevel support to internal and external investigatorsfaculty in geospatialGIS and epidemiologic research projects and publications       THIS POSITION IS A 1YEAR CONTRACT APPOINTMENT  The final salary and offer components are subject to additional approvals based on UC policy  To see the salary range for this position we recommend that you make a note of the job code and use that to look up TCS NonAcademic Titles Search httpstcsucopedunonacademictitles  Please note An offer will take into consideration the experience of the final candidate AND the current salary level of individuals working at UCSF in a similar role  For roles covered by a bargaining unit agreement there will be specific rules about where a new hire would be placed on the range  To learn more about the benefits of working at UCSF including total compensation please visit httpsucnetuniversityofcaliforniaeducompensationandbenefitsindexhtml    Department Description      The Department of Epidemiology and Biostatistics DEB housed within the UCSF School of Medicine has a vision to advance discoveries and insights into the distribution determinants and outcomes of disease that will drive improvements in population health worldwide The DEB has three missions 1 education 2 science and 3 research Its educational mission is to train students in epidemiologic and biostatistical methods for studying disease etiology and prevention evaluating tests and treatments and using evidencebased approaches in clinical practice and population health The scientific mission of the DEB is to perform outstanding basic clinical and population health research along a spectrum from molecules to society and to develop tools for the translation of knowledge that will improve clinical practice and population health and ensure optimal use of resources The clinical mission of the Department is to support the practice of public health and preventive medicine in local state national and international health agencies       Required Qualifications   Bachelors degree in related area and 5 years of related experience and  or equivalent experience  training  Experience with preparing reports manuscript writing and support grant proposals  Excellent oral and writing communication skills including the ability to analyze and draw conclusions from large datasets  Thorough knowledge of research function  Thorough skills associated with statistical analysis and systems programming database design and data security measures  Thorough skills in analysis and consultation  Skills to communicate complex information in a clear and concise manner both verbally and in writing  Research skills at a level to evaluate alternate solutions and develop recommendations  Outstanding skills in manipulating and analyzing largescale GIS data and programming experiences in ArcGIS R STATA andor SAS  Excellent attention to detail and organizational skills and ability to track multiple projects and responsibilities simultaneously  Selfmanagement skills Ability to work in fast paced environment under changing conditions handle multiple complex tasks and demands and meet deadlines and deliverables with minimal supervision  Ability to work collaboratively and meet deadlines and deliverables with minimal supervision     Preferred Qualifications   Master’s degree in related area and 3 years of relevant experience and  or related field  Academic background and recognized expertise in geography or epidemiology and ability to apply advanced geographyepidemiologic principles and methods in the analysis of health data  Skills in project management     About UCSF      The University of California San Francisco UCSF is a leading university dedicated to promoting health worldwide through advanced biomedical research graduatelevel education in the life sciences and health professions and excellence in patient care It is the only campus in the 10campus UC system dedicated exclusively to the health sciences We bring together the world’s leading experts in nearly every area of health We are home to five Nobel laureates who have advanced the understanding of cancer neurodegenerative diseases aging and stem cells       Pride Values      UCSF is a diverse community made of people with many skills and talents We seek candidates whose work experience or community service has prepared them to contribute to our commitment to professionalism respect integrity diversity and excellence – also known as our PRIDE values        In addition to our PRIDE values UCSF is committed to equity – both in how we deliver care as well as our workforce We are committed to building a broadly diverse community nurturing a culture that is welcoming and supportive and engaging diverse ideas for the provision of culturally competent education discovery and patient care Additional information about UCSF is available at diversityucsfedu        Join us to find a rewarding career contributing to improving healthcare worldwide       Equal Employment Opportunity      The University of California San Francisco is an Equal OpportunityAffirmative Action Employer All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin age protected veteran or disabled status or genetic information       Organization      Campus       Job Code and Payroll Title      006257 RSCH DATA ANL 3        Job Category      Professional NonClinical Research and Scientific       Bargaining Unit      99  PolicyCovered No Bargaining Unit        Employee Class      Contract       Percentage      100       Location      Flexible combination of onsite and remote work San Francisco CA       Shift      Days       Shift Length      8 Hours        Additional Shift Details      MondayFriday 40 hours 830am to 530 pm flexible         </data></node>
<node id="n484" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e72df72245369a86&amp;bb=opXyxKwanvGY0hw0dAe57FYdc_44Fd5lOAvX9CklQ6Qz0AkZaBMNGb4BdKdUUy1GySXMXO-17R-tHW6dByfXky4ClBfVHWaCluNsbTg-tjHA26CmL5xq9A%3D%3D&amp;xkcb=SoBM67M3CNuqr0yUwp0BbzkdCdPP&amp;fccid=2df6a1e69a70a1e7&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay114000  176090 a yearJob typeFulltimeShift and scheduleDay shift LocationSterling VA BenefitsPulled from the full job description401kDental insuranceDisability insuranceEmployee stock purchase planHealth insuranceLife insurancePaid parental leaveShow morechevron down Full job description Sr Business Analyst  Cognizant’s Digital Engineering practice is seeking a highly qualified Sr Business Analyst with 10 years’ experience developing and building highperforming scalable enterprise applications You will be part of a digital software team that works on highdemand applications Our engineers have a passion for highquality reliable and maintainable code You will work side by side with product managers designers and clients making decisions together to quickly deliver valuable working software to clients and their users Our engineers are agile and retrospective and not afraid to identify what we’re doing wrong so we can fix it and what we’re doing right so we can improve on it Above all we judge success by the success of our team and the happiness of our customers  Cognizant Digital Engineering If you’re like us you’ve got big ideas At Cognizant we’re exploring new ideas every day We help industry leading companies reinvent their business models and innovate products that create new value—by connecting people with things insights and experiences Cognizant digital engineering designs engineers and delivers digital products and experiences that drive digitalfirst business models We offer the most comprehensive digital engineering expertise and clientcentric methodology for sustainable innovation  Location Sterling  VA or Remote  You must be legally authorized to work in the USA      Business Analyst Job Responsibilities      the development testing and product managers operations personnel CPOs Project managers etc to ensure accurate development and implementation based on Core customer requirements and needs Warranty on certain models from requirements analysis to detailed characterization     Efficiently lead the conversation and gather requirements from Business understand the concerns identify the risk and communicate it in timely fashion    Ensure document the application system by following multiple cross functional requirements eg Security Quality crossfunctional Business Legal    Perform evaluate and communicate thorough quality assurance at every stage of systems development    Determine and develop user requirements for systems in production to ensure maximum usability    Partner with stakeholders across business units ex sales finance security compliance to develop analyses and documentation in a collaborative way communicating effectively and efficiently with production managerial and executive teams    Evaluate analyze and communicate systems requirements on a continuing basis and maintain systems processes including the delivery of monthly status reports to all appropriate parties    Ability to make quick informed decision and understand the clear priority and escalate as needed    Must be creative to deliver innovative solutions to address critical business needs Highly agile and ability to quickly pivot the plan and execute      Required skills and qualifications    At least 3 years of domain knowledge in Communication and Media processes    Five or more years of experience in analytics and systems development    High proficiency with SQL and database management    Proven analytical abilities    Experience in generating process documentation and reports    Excellent communication skills with an ability to translate data into actionable insights    Preferred skills and qualifications    Bachelor’s degree or equivalent in information technology or computer science    Strong working knowledge of relevant Microsoft applications including Visio    Proven ability to manage projects and user testing    Extensive experience with data visualization    High proficiency in technical writing     Salary and Other Compensation  The annual salary for this position is between 114000 176090 USD depending on experience and other qualifications of the successful candidate  This position is also eligible for Cognizant’s discretionary annual incentive program based on performance and subject to the terms of Cognizant’s applicable plans  Application for this role will be received until 4302024  Benefits Cognizant offers the following benefits for this position subject to applicable eligibility requirements  MedicalDentalVisionLife Insurance Paid holidays plus Paid Time Off 401k plan and contributions LongtermShortterm Disability Paid Parental Leave Employee Stock Purchase Plan  Disclaimer The salary other compensation and benefits information is accurate as of the date of this posting Cognizant reserves the right to modify this information at any time subject to applicable law  Why Choose Cognizant  It takes a lot to succeed in today’s fastpaced market and Cognizant Technology Solutions has become a leader in the industry We love big ideas and even bigger dreams We stand out because we put human experiences at the core Our associates enjoy robust benefits and training opportunities from our industryrecognized awardwinning Academy team You will have access to hundreds of technical trainings to keep your skillsets fresh and have opportunities to acquire certifications on the newest technologies  Everything we do at Cognizant we do with passion—for our clients fortune 100 companies our communities and our organization It’s the defining attribute that we look for in our people  If you love ambiguity excited by change and excel through autonomy we’d love to hear from you  About Cognizant Digital Engineering  Welldesigned software transcends digital technology going beyond the fulfillment of basic requirements to focus instead on human needs Within Cognizant Digital Engineering we help clients develop software products that transform human insights into tangible productionready digital solutions We also work with our clients to scale their native cloud applications Using insights from the lived experiences of our consumers we seamlessly replace traditional service strategies with engaging precise and direct digital applications Designing phenomenal software is vital to success in the digital economy—and we understand that a humancentric approach is key to this design  LIJO1  IND123   Employee Status  Full Time Employee  Shift  Day Job  Travel  No  Job Posting  Apr 01 2024    About Cognizant  Cognizant Nasdaq100 CTSH is one of the worlds leading professional services companies transforming clients business operating and technology models for the digital era Our unique industrybased consultative approach helps clients envision build and run more innovative and efficient businesses Headquartered in the US Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world Learn how Cognizant helps clients lead with digital at wwwcognizantcom or follow us USJobsCognizant    Applicants may be required to attend interviews in person or by video conference In addition candidates may be required to present their current state or government issued ID during each interview   Cognizant is an equal opportunity employer All qualified applicants will receive consideration for employment without regard to sex gender identity sexual orientation race color religion national origin disability protected Veteran status age or any other characteristic protected by law  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application please email CareersNA2cognizantcom with your request and contact information   </data></node>
<node id="n485" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=47bc64f105d6b725&amp;bb=opXyxKwanvGY0hw0dAe57F22CBocPWPSO0N6vz96nt0KE6y7Fj9x0aOW0NE7Aqtv1HSRdUitFvIxng95-KqtRKCRGLOC8RyKoMnrqzBxHCk-FWv1hEPxWg%3D%3D&amp;xkcb=SoD467M3CNuqr0yUwp0AbzkdCdPP&amp;fccid=ad3f825dc242e6fd&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay100000  120000 a yearJob typeFulltime Location4300 Marketpointe Dr  200 Minneapolis MN 55435 BenefitsPulled from the full job description401kDental insuranceEmployee stock ownership planFlexible spending accountHealth insuranceLife insurancePaid time offShow morechevron down Full job description    Job Description            Summary           Barr Engineering Co seeks a data engineering consultant to help us build and expand our newly formed Environmental Management Analytics and Information Systems team Your primary focus will be to support the data engineering and architectural needs of our external client projects When not building endtoend solutions for external projects you will assist Barr’s internal Information Systems team in advancing our rapidly maturing and analytics datarelated capabilities You will use the latest cloudbased tools to design and build solutions for IoT and telemetrybased instrumentation a cloudbased data lake API integration of internal and external platforms and many other tools to support various fascinating engineering efforts              Barr works on thousands of client projects each year The datarelated needs of these projects vary significantly This role is wellsuited for a problemsolver who is passionate about data and wants to work in an environment with significant growth opportunities You will be given the opportunity to learn and develop your technological skills across various platforms and help drive the future of our Environmental Management Analytics and Information Systems team          Responsibilities include                 Support the data engineering management and governance needs of an array of engineering and scientific projects for our external clients                Identify design and implement process improvements redesigning infrastructure for greater scalability optimize data delivery and automate manual processes                Support the acquisition and processing of large datasets generated by IoT and instrumentation                Integrate systems and data sources through APIs and ETL tools                Support data science system implementation and system development efforts                Design data flows and pipelines to optimally extract transform and store data using onprem and cloud technologies                Advance the design and development of Barr’s enterprise data warehouse data lake dashboarding and selfservice reporting ecosystem using the Microsoft Business Intelligence toolset SQL Server SSIS SSAS tabular Power BI Azure                Lead the process of engaging business analysts and business unit staff to elicit and document data requirements                Write complex queries                Design and develop database objects databases tables views stored procedures etc within a normalized and dimensional data warehouse environment                Develop auditing and quality assurance practices to help ensure the accuracy of the data warehouse and reporting                Mentor business unit staff to advance their data capabilities and understanding               Minimum Qualifications                 Bachelor’s degree in computer science information systems a related field or equivalent work experience                Three years of experience in a data engineering database administrator or business intelligence role                Experience developing data warehouse and data lake tools for business purposes                Experience supporting the development of analytical tools for business purposes                High proficiency in SQL queries                Experience with Python developer                Experience developing Rest APIs                Experience with ETL and data modeling technologies eg SSIS Data Factory SSAS  Tabular                Strong understanding of security in an integrated onpremises and cloudbased data environment                Proficient in working with diverse data types including structured semistructured and unstructured formats                Proficiency with Azure technologies such as Data Factory Data Lake and Azure Analysis Services                Experience leading requirementgathering efforts for data engineering projects                Strong organizational skills including the ability to plan monitor and follow through on work commitments and confirm priorities with stakeholders                High level of commitment to delivering exceptional client service                Excellent written and oral communication skills                Strong analytical and problemsolving skills                Possession of a valid driver’s license and acceptable driving record                Legal authorization to work in the United States without the need for sponsorship from Barr now or in the future               Preferred Qualifications                 One or more Microsoft Azure certifications                Experience with databricks                Experience developing in data visualization platforms such as Power BI Tableau or related platforms                Knowledge of data warehouse development processes and techniques including dimensional modeling                Familiarity with AI Machine Learning and Data Science concepts and tools                Experience working with large volumes of streaming data                Knowledge of environmental data eg air quality water quality geological remote monitoring sensors and environmental data systems eg EQuIS GIS EMIS                 A hybrid or remote work arrangement may be considered for this position A hybrid arrangement refers to splitting time worked between a Barr office and a home office a remote arrangement refers working primarily from a home office This position can be based out of Barrs Minneapolis Minnesota office Remote arrangements will be considered based on candidate qualifications location requirements and Barr’s needs              Colorado Applicants Only             The anticipated base salary range for this position is 100000–120000 Compensation will vary based on experience education skill level and other compensable factors Employees in this position may also be eligible for a discretionary cash bonus Our compensation comes in more ways than traditional base salary and we believe that when all those elements are combined our compensation is a competitive part of the total value proposition of working at Barr          Benefits             People report that they stay at Barr because of the camaraderie and career opportunities Another draw is our competitive package of employee benefits which includes professional development funding 401k retirement savings plan employee stock ownership plan ESOP participation medical and dental insurance life insurance disability and accidental death insurance flexible spending accounts for healthcare and dependent care expenses paid holidays paid time off and compensatory time for exemptsalaried staff time off or pay for extra time worked            LIHybrid LIRemote        Open positions at Barr Engineering Co do not have application deadlines Barr Engineering Co is an equal opportunity employer and all applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability or status as a protected veteran  </data></node>
<node id="n486" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0442faa042ce00ed&amp;bb=opXyxKwanvGY0hw0dAe57A2IBhiV8F8OSEOEIdrBNEasvtuieV21yek5CLAahCl2FAzAIfTTsD2HWblABK7ZHbWSvDsWtzQrFF9aD3RE7dtOUwSGlODArA%3D%3D&amp;xkcb=SoB267M3CNuqr0yUwp0HbzkdCdPP&amp;fccid=bb0c672891002d42&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Public healthYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRemote Full job description  Position Revenue Cycle Instructional Designer and Quality Analyst  Epic  Department PFS Customer Service  Schedule Full Time   At this time we are only considering candidates who are Epic Certified in Cadence Prelude or Grand Central   JOB REQUIREMENTS   EDUCATION   Bachelor’s degree preferred with an emphasis in finance public health quality improvement or business Equivalent work experience in Patient Access will also be considered 5 to 7 years of experience    CERTIFICATES LICENSES REGISTRATIONS REQUIRED   Current Certification preferred or must obtain within 12 months of employment    EXPERIENCE   Minimum of 57 years hospital patient access experience or similar registration experience Epic experience preferred    KNOWLEDGE AND SKILLS   Sound judgment and critical thinking Strong analytical verbal and written communication skills as well as interpersonal and presentation skills  Strong PC experience and Microsoft Office experience required including Access Excel and PowerPoint Word skills required Ability to prioritize and handle multiple tasks Strong organization and followthrough skills Accuracy and attention to detail Strong customer service and interpersonal skills Strong communication skills Must be flexible and able to function within a team Ability to maintain composure in stressful circumstances    ESSENTIAL RESPONSIBILITIES  DUTIES   Ensure that datadriven performance management and QI programs are designed and implemented in a manner that aligns with the organization’s strategy and mission Monitors the effectiveness of the performance management and quality improvement programs on a timely and an ongoing basis Reviews and analyzes data to determine issue trends and develops remediation plans andor workflow resolutions Reviews updates and documents policies procedures and subsequent workflowsissue resolution Ensure effective data management for quality improvement projects Has knowledge of measurement framework and is responsible for the design development and implementation of key metrics and data collection tools Oversees and conducts data analytics and data integrity Presents metric driven information and workflow opportunities to Patient Access leadership Produces PowerPoint and other electronic media to inform train educate and engage team members  Develops and maintains positive collaborative supportive working relationships with all members of the Patient Access Team Utilizes hospital’s cultural beliefs as the basis for decision making and to support the hospital’s mission and goals Other duties as assigned by Quality  Patient Safety Manager or Executive leadership of Quality and Patient Safety IND123    Equal Opportunity EmployerDisabledVeterans  </data></node>
<node id="n487" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d98daa02c97ae744&amp;bb=opXyxKwanvGY0hw0dAe57H_UA64rk75CSROn_fdGLzTkPNDKPJf8Xv7I1rN4-WDo1XSvbBSnkMHpSaNL_TLRQaA7pZDT3OH2bMUfILzVNcCw-GH9c70coQ%3D%3D&amp;xkcb=SoDC67M3CNuqr0yUwp0GbzkdCdPP&amp;fccid=6c527578251bc5e1&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo LocationUnited States BenefitsPulled from the full job descriptionGym membershipHealth insurance Full job description    ICON plc is a worldleading healthcare intelligence and clinical research organisation From molecule to medicine we advance clinical research providing outsourced services to pharmaceutical biotechnology medical device and government and public health organisations      With our patients at the centre of all that we do we help to accelerate the development of drugs and devices that save lives and improve quality of life      Our people are our greatest strength are at the core of our culture and the driving force behind our success ICON people have a mission to succeed and a passion that ensures what we do we do well      Hybrid This role will require 3 days a week onsite at our West Point PA 19486 location 2 days remote based     Looking for an individual with ERP system experience SAPAriba preferred    Current or previous Invoicing or Purchase order experience highly preferred    Demonstrable Excel skills needed    Suplpy Chain Logistics Inventory experience preferred but not required    What will you be doing        As a critical requirement in this role the candidate must demonstrate an intermediate to advanced level of proficiency with MS Excel  Experience with data analytics software such as PowerBI Spotfire or Tableau would be nice to have  On a routine basis must be able to create pivot tables and utilize functions vlookup concatenation ifthen within the application  Responsible for building and maintaining MS Excel models using pivot tables to project costs for distribution and warehousing 3rd party packaging 3rd party labeling and drug costs to develop budget forecast  Must be able to fully utilize SAP for financial analysis of RD programs investigational drug and trial costs to accurately maintain and update rolling forecast and profit plan  Must be able to Prepare plan for submission of annual Profit Plan  Develop metrics via Excel models to analyze resource allocations and forecasts for use by managers within the department  Perform KPI analysis and present metric results to global clinical supplies management team  Ability to follow Standard Operating Procedures SOPs  Looking for somoene who can use technology to move the data comparison forward via electronic datasets vs paper comparisons          Education and Work Experience    Bachelor degree Highly Preferred  Looking for about 23 years experience  SAP experience preferred  Intermediate to advanced Excel skills required  Purchase Order experience in SAP a plus  Experience with budgets and forecasting of budgets preferred but not required  Excel SAP Ariba and some type of data analytics software experience would be ideal  Prior supply chain logistics or operations knowledge nice to have     Knowledge Skills and Abilities    Detailoriented  Excellent computer skills including Microsoft applications ERP  CTMS are highly desirable  Proven organization skills  Strong ability to multitask  Understanding of cGMPs      What ICON can offer you   Our success depends on the quality of our people That’s why we’ve made it a priority to build a diverse culture that rewards high performance and nurtures talent      In addition to your competitive salary ICON offers a range of additional benefits Our benefits are designed to be competitive within each country and are focused on wellbeing and work life balance opportunities for you and your family     Our benefits examples include     Various annual leave entitlements  A range of health insurance offerings to suit you and your family’s needs  Competitive retirement planning offerings to maximise savings and plan with confidence for the years ahead  Global Employee Assistance Programme TELUS Health offering 24hour access to a global network of over 80000 independent specialised professionals who are there to support you and your family’s wellbeing  Life assurance  Flexible countryspecific optional benefits including childcare vouchers bike purchase schemes discounted gym memberships subsidised travel passes health assessments among others    Visit our careers website to read more about the benefits of working at ICON httpscareersiconplccombenefits     At ICON diversity inclusion  belonging are fundamental to our culture and values Our rich diversity makes us more innovative which helps us better serve our people patients customers and our communities Were proud of our diverse workforce and the work we’ve done to become a more inclusive organisation We’re dedicated to providing an inclusive and accessible environment for all candidates ICON is committed to providing a workplace free of discrimination and harassment All qualified applicants will receive equal consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability or protected veteran status If because of a medical condition or disability you need a reasonable accommodation for any part of the application process or in order to perform the essential functions of a position please let us know through the form below      httpscareersiconplccomreasonableaccommodations     Interested in the role but unsure if you meet all of the requirements We would encourage you to apply regardless – there’s every chance you’re exactly what we’re looking for here at ICON whether it is for this or other roles    </data></node>
<node id="n488" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=58ba8be124e9c8cc&amp;bb=opXyxKwanvGY0hw0dAe57FVnQzroTinVcU3QR3Mnax5cO14EtQUeGmVCvKwGDf1TRvLBWkYKPgTAKEgC4af8ENGEoQagkz1hgQcgaR0UvI7ADxYA3E1ddA%3D%3D&amp;xkcb=SoBf67M3CNuqr0yUwp0FbzkdCdPP&amp;fccid=f7c3c2d35dd92dfe&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Project managementYesNoEducationDo you have a Bachelors degreeYesNo LocationHouston TX 77043 BenefitsPulled from the full job descriptionDental insuranceEmployee assistance programHealth insurancePaid time offRetirement planVision insurance Full job description    Company Overview    Founded in 1906 over the last 110 years CEMEX has grown into a global building materials company that provides high quality products and reliable service to customers and communities throughout the Americas Europe Africa the Middle East and Asia Here at CEMEX we offer our employees competitive wages career growth excellent benefits including health dental  vision plans vacation or paid time off employee assistance program and retirement plan options along with over a century of stability to build your next career on      Job Summary    The Sales Administration Analyst provides support to the sales team provides metrics analysis and support to the customer base with focus on the MRT and Admixtures business lines for all the US operations and all customer segments      Job Responsibilities    Support on invoicing InternalExternal customers creditdebit memos create update and maintain sales contracts and sales pricing Support during audit inquiries Forecasting Process and control of purchase orders for the department ie confirm no duplications payments are released etc Support during month end  month end reports Support with pricing and volume analysis on all business lines Coordinate as needed creation and setup of additional invoicing of products services and fees Support system customer hierarchy changes  updates Assist with customer reconciliations Provide the sales force with information requests ie volume reports customer pricing PO numbers etc Assist in new customer set ups Process vendor POs for various associations membership dues customer events etc Manage weekly billing process for MRT and Admixtures business lines Request corrections on identified errors prior to invoicing assist sales force troubleshoot pricing errors issues with new contracts etc Coordinate  communicate with terminal managers and regional logistic teams       Qualifications    Bachelor’s degree in business or related field 14 years of experience Proven knowledge and experience with Accounting Invoicing Pricing Customer Care Administration       Knowledge Skills and Abilities    Advanced skills in Microsoft Office Excel including Macro  Pivot Table PowerPoint Word etc SAP skills strongly preferred InvoicingAccounting software experience preferred Excellent analytical and problemsolving skills Ability to track measure and analyze dataresults and key metrics Strong project management organizational skills planning skills Strong interpersonal skills and attention to detail Team player able take a leadership role in dealing effectively with various internal and external teamsdepartments Proven ability to work independently meet multiple deadlines and manage a heavy workload in a fastpaced environment Ability to work independently and with minimal supervision       Working Conditions    Capable of working in an open concept office environment      Physical Requirements    Requires walking sitting lifting pushing pulling and climbing to a significant degree Exerting up to 20 pounds of force occasionally andor a negligible amount of force frequently Job involves sitting most of the time but also involves walking or standing for brief periods of time While performing the duties of this job the employee is regularly required to talk and hear in order to communicate to employeesvisitors      Legal Notices    CEMEX is an EEOAA equal opportunityaffirmative action institution and does not discriminate on the basis of race color religion religious creed sex sexual orientation gender identity or expression national origin ancestry age physical or mental disability medical condition genetic information marital or familial status military or veteran status or any other characteristic protected by under federal state or local law in the programs or activities which it operates CEMEX will consider for employment qualified applicants with criminal histories in a manner consistent with all local state and federal laws CEMEX is an EVerify participating employer Arizona SmokeFree Act CEMEX complies with the State of Arizona’s SmokeFree Act Arizona Revised Statutes § 3660101 Smoking andor the use of tobacco or related products is prohibited in and on CEMEX property as well as any building andor vehicle owned or leased by CEMEX      EEO Statement  En Español    CEMEX es una institución EEOAA igualdad de oportunidadacción afirmativa y no discrimina en base al sexo edad raza color religión discapacidad física o mental credo origen nacional estatus veterano orientación sexual infomación genética identidad de género o expresión de género en los programas o actividades los cuales opera     </data></node>
<node id="n489" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e16d6f5c3ecec838&amp;bb=rUMdW0CXv9MkYKEBBqlMZraTWV5co1lvJH3xsqCVPston4SG909Fo9O2CQL_WBOacO6zv9pj49scJdvhCjYeyb0h8rUTS8Q23TjWnubWnbFE_LEcn3MgRw%3D%3D&amp;xkcb=SoA467M3CNum7-RSRx0LbzkdCdPP&amp;fccid=bb0c672891002d42&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRemote Full job description POSITION SUMMARY   Under the direction of the Senior Director of Finance Pharmacy the Senior Revenue Cycle Business Analyst position is responsible for developing standard reports and overseeing financial activities of BMC Pharmacy They will play a key role in projects across the BMC Pharmacy function with a focus on analysis of data to inform management drive accountability and guide financial decisions This position is very dynamic and given the entrepreneurial and fastpaced atmosphere of the BMC Pharmacy business development office the candidate must be able to work independently and solve problems that have a significant impact to BMC financials and its strategic plan HeShe shares responsibility for the operational effectiveness within the Pharmacy department to ensure that highquality financial services are delivered in accordance with applicable policies procedures and professional standards HeShe analyzes the financialeconomic benefits and cost impact of new and existing business initiatives Prepares recommendations and business models to assist the organization in improving resource allocationutilization Provides analytical support to resolve operating and finance issues Creates a positive constructive and supportive relationship between Finance and Pharmacy   Position Senior Revenue Cycle Business Analyst  Department Pharmacy Finance  Schedule Full Time   ESSENTIAL RESPONSIBILITIES  DUTIES   The Senior Revenue Cycle Business Analyst will play a key role in various aspects of financial planning and evaluation which may include the development of the annual operating budget and multiyear financial plan capital expenditure planning month end close monitoring and new program evaluations   Financial ReportingBudget Tasks   Develops tests and deploys standard reports and dashboards based on business requirement document specifications Complete special projects and other duties as assigned Assists in the development of business plans and sophisticated financial models to adapt to changes in services or long term planning Coleads the annual forecast process including modeling advanced assumptions at a very detailed level    Project ManagementFinancial Planning   Works collaboratively with Business Systems Analysts andor directly with key business users to ensure business requirements and report specifications are documented accurately and completely Conducts market analysis by attending conferences to stay up to date on changes in billingregulatory compliances 340b Incorporates data from multiple sources in evaluations Examples include hospital and physician billing system budget system cost accounting system and general ledger Work on andor lead special projects that require research and analysis May involve partnering with other business areas to identify and resolve issues Conform to hospital standards of performance and conduct including those pertaining to patient rights so that the best possible customer service and patient care may be provided Utilizes hospital’s behavioral standards as the basis for decision making and to support the hospital’s mission and goals Complete special projects and other duties as assigned Develops tests and deploys standard reports and dashboards based on business requirement document specifications Works collaboratively with Business Systems Analysts andor directly with key business users to ensure business requirements and report specifications are documented accurately and completely Incorporates data from multiple sources in evaluations Examples include hospital and physician billing system budget system cost accounting system and general ledger Work on andor lead special projects that require research and analysis May involve partnering with other business areas to identify and resolve issues Conform to hospital standards of performance and conduct including those pertaining to patient rights so that the best possible customer service and patient care may be provided Utilizes hospital’s behavioral standards as the basis for decision making and to support the hospital’s mission and goals Complete special projects and other duties as assigned    JOB REQUIREMENTS   EDUCATION   Bachelors degree required preferably in a finance field or related field of study Masters Degree in Business Public Health or related field preferred    CERTIFICATES LICENSES REGISTRATIONS REQUIRED   Epic Revenue Data Model Clarity Certification    EXPERIENCE   5 years of related managed healthcare or insurance operations experience Expertise with relational database concepts data modeling and OLAP technologies Demonstrated proficiency in Business Objects Crystal Reporting including Universe Designer Web Intelligence Dashboards and Explorer    KNOWLEDGE AND SKILLS   Project management skills including familiarity with project management phases techniques and tools  Ability to overcome obstacles and lead projects to a successful conclusion Ability to translate data into useful information and to communicate key findings and assumptions in concise fashion to senior management for decision making purposes Highly analytical thinking with talent for identifying scrutinizing improving and streamlining work processes Advanced MS Excel Access Word and PowerPoint skills Experience with hospital claims data Tableau andor MS Access or SQL a plus EPIC experience required with EPIC Certifications preferred Strong commitment to excellence and person and professional growth Excellent written and verbal communication skills Strong interpersonal skills to effectively work and communicate with all levels in the organization Knowledge of health care finance and delivery systems preferred but not required    Equal Opportunity EmployerDisabledVeterans  </data></node>
<node id="n490" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d87f3f5ed02483da&amp;bb=rUMdW0CXv9MkYKEBBqlMZmzY5l4z0YFbjjBH_aO_ekh1R01knQsyOdmpAhjuyOFiA0Q48G3eaqNWy7EEstFpt662fHv1QnPlTYLj2OqxfjlHJOW2rdp8Kw%3D%3D&amp;xkcb=SoCM67M3CNum7-RSRx0KbzkdCdPP&amp;fccid=ba380a2cb2aa1ecd&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationBrentwood TN Full job description    Welcome to Ovation Healthcare        At Ovation Healthcare formerly QHR Health we’ve been making local healthcare better for more than 40 years Our mission is to strengthen independent community healthcare We provide independent hospitals and health systems with the support guidance and techenabled shared services needed to remain strong and viable With a strong sense of purpose and commitment to operating excellence we help rural healthcare providers fulfill their missions        The Ovation Healthcare difference is the extraordinary combination of operations experience and consulting guidance that fulfills our mission of creating a sustainable future for healthcare organizations Ovation Healthcare’s vision is to be a dynamic integrated professional services company delivering innovative and executable solutions through experience and thought leadership while valuing trust respect and customer focused behavior        We’re looking for talented motivated professionals with a desire to help independent hospitals thrive Working with Ovation Healthcare you will have the opportunity to collaborate with highly skilled subject matter specialists and operations executives in a collegial atmosphere of professionalism and teamwork        Ovation Healthcare’s corporate headquarters is located in Brentwood TN For more information visit        wwwovationhccom           We are seeking a dynamic and experienced professional to join our team as a Sr Analyst – Contracts and Pricing As a key member in our organization you will be responsible analyzing financial impacts of new and existing agreements as well as completing market baskets for new member hospitals The ideal candidate will possess strong analytical skills a deep understanding of data visualization GPO business practices healthcare and hospital operation knowledge and a proven track record of delivering accurate and timely analysis of large data sets        Duties and Responsibilities           Data Collection and Preparation            Collect and consolidate data from multiple sources including databases and spreadsheets            Cleanse preprocess and validate data to ensure accuracy completeness and consistency            Develop and maintain data pipelines and workflows for efficient data extraction and transformation            Data Analysis and Interpretation            Perform exploratory data analysis to identify trends correlations and outliers            Apply statistical methods and data mining techniques to uncover insights and patterns in the data            Conduct hypothesis testing and regression analysis to validate findings and make datadriven recommendations            Data Visualization and Reporting            Create visualizations dashboards and reports to present data insights in a clear and compelling manner            Use data visualization tools such as Tableau Power BI or matplotlib to communicate complex data concepts effectively            Customize visualizations to meet the needs of different stakeholders and facilitate decisionmaking            Performance Monitoring and Optimization            Monitor key performance indicators KPIs and metrics to track business performance and identify areas for improvement            Collaborate with business units to define performance benchmarks and goals            Analyze data to identify opportunities for process optimization cost reduction and revenue enhancement            Crossfunctional Collaboration            Collaborate with crossfunctional teams to support datadriven decisionmaking            Provide data expertise and insights to support strategic initiatives and business projects            Communicate findings and recommendations to stakeholders through presentations reports and interactive sessions           Knowledge Skills and Abilities           GPO Operations            Healthcare Supply Chain Operations            Detail Oriented            Collaborative            Strategic thinker            Customer Service oriented            Innovative           Work Experience Education and Certifications           Bachelors degree in Computer Science Statistics Mathematics Economics or related field advanced degree is a plus            Proven experience typically 25 years in data analysis business intelligence or related field            Proficiency in SQL for data querying and manipulation            Familiarity with data visualization tools such as Tableau Power BI or matplotlib            Expert user of Microsoft Suite            Strong analytical skills with the ability to interpret complex datasets and derive actionable insights            Excellent communication and presentation skills with the ability to convey technical concepts to nontechnical stakeholders            Detailoriented mindset with a focus on data accuracy quality and integrity            Ability to work independently and collaboratively in a fastpaced environment        </data></node>
<node id="n491" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=4433e2ceeda48f48&amp;bb=rUMdW0CXv9MkYKEBBqlMZo8zAMJBL4aS46_SGGrG76ADAiBNRvyJc67S_GPb4NyY8K0pPppVvnKzV7HZqmm8-FVSbL6OO_nc7_EmSH7kCEnvEFcUoaituQ%3D%3D&amp;xkcb=SoAR67M3CNum7-RSRx0JbzkdCdPP&amp;fccid=d3c6e90e8fb076a1&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Systems engineeringYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRedstone Arsenal AL Full job description  Tactica Solutions LLC is seeking an experienced and qualified Business Analyst for a Systems Engineering  Technical Assistance SETA contract supporting the US Army Space and Missile Defense Command USASMDC Office of the Deputy Chief of Staff Engineer DCSENG for facilities engineering  sustainment and environmental planning  compliance on Redstone Arsenal Alabama  Duties and Responsibilities   Create manage and maintain organizational files folders databases and office correspondence Provide recommendations to optimize daily operations increase efficiency and coordinate corrective action activities to resolve issues Maintain track and report the status of contract actions deliverables training credentials data calls and Rolling Action Item List RAIL items Perform office administrative functions calendar synchronization and deconfliction plan andor participate in inperson and virtual meetings and conference calls prepare and disseminate minutes organize and order supplies and coordinate travel Prepare review and track financial and budgetary reports Military Interdepartmental Purchase Requests DD Form 4484482 MIPRAcceptance Financial Requisition Summary FRS forms Purchase Requisitions PR spend plans and forecasts Interface with USASMDC and other agencies’ financial offices in matters related to funding cost reconciliation obligationsdeobligations and discrepancy resolution Establish collaborate and maintain professional relationships with team members customers vendors suppliers and partners Other duties as assigned  Job Requirements   Requirements – Knowledge Skills and Abilities   Bachelor’s degree Minimum of three 3 years of relevant experience supporting internal and external stakeholders Demonstrated ability to determine differences between contract types Costreimbursable Time and Materials TM Firm Fixed Price FFP etc and experience working within typical CLINSLINACRN structure breakdown ie Labor Travel and ODCs Lead preventative and corrective action efforts conduct root cause and statistical analyses and report findings Proficient working in a flexible environment and demonstrated ability to effectively organize prioritize delegate and multitask Possess excellent time management good judgment conflict resolution and excellent written oral and interpersonal communication skills Must be highly proficient using MS Office software applications Word Excel PowerPoint SharePoint Project Teams Outlook Ability and willingness to occasionally travel in support of customer requirements US citizenship required to obtain and maintain a US government issued security clearance at the appropriate level for the duration of the contract Period of Performance   Preferred Qualifications   Knowledge of US Army rank and grade structure and government civilian pay scale equivalencies Knowledge Management Support Tools KMST Information System familiarity Defense Travel System DTS and Joint Travel Regulations JTR familiarity General Fund Enterprise Business System GFEBS data management tool familiarity UnliquidatedNegative Unliquidated Obligations ULONULO resolution experience Active security clearance with current investigation   Job Location  Redstone Arsenal AL  Physical Requirements Work may involve sitting or standing for extended periods of time Position may require typing and reading from a computer screen Must have sufficient mobility including but not limited to bending reaching and kneeling to complete daily duties in a timely and efficient manner May include lifting weight up to thirty 30 pounds as necessary  Tactica Solutions LLC reserves the right to change or modify job duties and assignments at any time The above job description is not allencompassing Position functions and qualifications may vary depending on business needs  Tactica Solutions LLC is an equal opportunity employer and does not discriminate against applicants based on race color creed religion medical condition legally protected genetic information national origin sex including pregnancy childbirth or related medical condition sexual orientation gender identity and expression age disability or Vietnam era or other eligible veteran status or legally protected characteristics        Get job alerts by email Sign up now Join Our Talent Network      Job Snapshot  Employee Type FullTime      Location Redstone Arsenal AL Onsite      Job Type Finance General Business Accounting      Experience Not Specified      Date Posted 03202024      </data></node>
<node id="n492" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=9823898673bd9775&amp;bb=rUMdW0CXv9MkYKEBBqlMZrCUddfaIYycqf3SYC3jqKlKEHxnUWDzzALHQGCCkMCbWTYgPmX-yCvRo27-ANw1LplJlXzkU16mUjnz1CzeE3_nCEBTldyODA%3D%3D&amp;xkcb=SoCl67M3CNum7-RSRx0IbzkdCdPP&amp;fccid=d3c6e90e8fb076a1&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Systems engineeringYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay200000  215000 a yearJob typeFulltime LocationColorado Springs CO BenefitsPulled from the full job description401kDental insuranceDisability insuranceFlexible spending accountHealth insuranceLife insurancePaid time offShow morechevron down Full job description Job Title Space Training Analyst 4441  Job Location Colorado Springs CO 80919  Job Salary 200000215000  Job Summary  Galapagos Federal Systems LLC is looking for an enthusiastic and highly qualified candidate to assume the role of Space Training Analyst Support and to join our team of qualified diverse individuals  The core responsibility of this position revolves around delivering robust Space Training Analyst Support As part of this role the selected individual will be tasked with executing a spectrum of training operations analyst functions encompassing meticulous planning seamless coordination comprehensive analysis proficient execution and insightful reporting pertaining to space systems training Moreover the incumbent will be expected to provide integrated analysis support across various domains including design reviews the development of training strategies participation in training planning working groups and engagement in analogous trainingcentric forums  Ideal candidates will possess demonstrable analytical prowess across a range of specialized areas including Statics and Experimental Design Database Management and advanced analytical techniques such as Big Data Analytics Additionally candidates should have handson experience in either crafting proprietaryinternal tools or leveraging commercial test and space Model Based System Engineering MBSE Modeling and Simulation MS digital engineering and analysis tools such as Satellite Tool Kit STK or equivalent platforms to dissect space architectures sensors and communication systems  Technical proficiency is paramount with candidates expected to have familiarity in critical domains such as Space System Architectures Satellite and Ground Segment Design EOIRRF Sensors Astrodynamics Propulsion GNC spacerelated data communications and Space Battle Management and Control  Job Requirements   Skills  Experience Required  10 years relevant DoD Department of Defense work experience Perform analysis on pretest predictions models and simulations to design test events and profiles define data collection requirements and establish gonogo criteria to meet test objectives Develop a data management and analysis plan DMAP to ensure proper data collection for Modeling and Simulation MS realtime test execution and postmission data processing evaluation and analysis Create and review test planning documentation including Test Plans Test Information Sheets Safety Packages Data Analysis Management Plans Technical and Safety Review Board Briefs Test Readiness Briefs Test Cards TestAnalysis Reports and Test Results Briefs Monitor the gathering of MS data and realtime test and range parameters evaluating test results to determine if system configurations performance and architecture meet system requirements effectiveness and suitability Set up data requests order and process data conduct quality control reviews resolve data anomalies and analyze data according to the data analysis plan Develop enhance and utilize data analysis tools as needed to support the analysis and reporting of advanced highly complex test simulations and scenarios facilitating the evaluation of system performance effectiveness and suitability Provide operational suitability test support to space system test programs serving as the expert on issues related to reliability availability compatibility transportability interoperability maintainability safety human factors manpower supportability logistics supportability environmental effects system documentation and training requirements Contribute to all aspects of test design and planning using comprehensive knowledge of the system under test test policies and test range capabilities Assist the Test Director in preparing highquality suitability analysis across various test venues during all test phases to inform system development and operational evaluation Develop specific data collection processing evaluation and reporting procedures for space system suitability while proactively identifying suitability gaps risks or other issues across multiple test programs Extensive experience within System Program Offices SPOs andor Program Management Offices PMOs demonstrating a record of accomplishments Exhibits comprehensive knowledge and expertise in Research Development Test and Evaluation RDTE proficiently utilizing test and training materiel systems Possesses a deep understanding of the fielding and sustainment processes associated with space test and training materiel systems Proficient in the application of Model Based Systems Engineering techniques ensuring efficient and effective system development and analysis Wellversed in a diverse array of domains including Space System Architectures Satellite and Ground Segment design electrooptical EO Infrared IR and Radio Frequency RF Sensors Astrodynamics Propulsion Guidance Navigation and Control GNC Spacerelated data communications and Space Battle Management and Control Demonstrates exceptional written and oral communication abilities coupled with strong leadership qualities fostering productive team collaboration and engagement Recognized as an industry leader renowned for exceptional analytical skills and substantial contributions to the field Education  Certifications BSBA required STEM or equivalent experience preferred MSMA preferred STEM or equivalent experience preferred Key SkillsExperience Required for Interview Selection ONLY Research Development Test and Evaluation RDTE of test and training materiel systems Fielding and Sustainment of test and training materiel systems Model Based Systems Engineering Space System Architectures Satellite and Ground Segment design electrooptical EO Infrared IR and Radio Frequency RF Sensors Astrodynamics Propulsion Guidance Navigation and Control GNC Spacerelated data communications or Space Battle Management and Control Recent experience working in a classified environment with classified programs  Benefits  Medical dental vision disability and life insurance Flexible Spending Accounts 401k PTO Tuition reimbursement Paid federal holidays  Security Clearance  Must be a US Citizen A highlevel Department of Defense active security clearance may be required Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to government information  Physical Requirements  Work may involve sitting or standing for extended periods of time and typing and reading from a computer screen The candidate must have enough mobility including bending reaching and kneeling to complete daily duties in a prompt and efficient manner and that may include lifting to thirty pounds as necessary  Company Summary  Headquartered in Hawaii Galapagos Federal Systems LLC is an SBA Certified Native Hawaiian Organization 8a Small Business specializing in global information technology and offering professional solutions in IT Design  Installation Cybersecurity Engineering  Support Application Integration  Development Software  Hardware Engineering Network  Systems Management Information Systems Security and Business Management Services  Leveraging over 30 years of providing IT services to the federal  commercial market with projects found around the world our team has innovative expertise in the development of a wide range of technological solutions Galapagos Federal Systems LLC is an equal opportunity employer  Our service commitment is simple  Quality IT Solutions On Time  On Budget  Company Employment Statement  Galapagos Federal Systems LLC reserves the right to change or modify job duties and assignments at any time The above job description is not all encompassing as positions functions and qualifications may vary depending on business needs Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions  Galapagos Federal Systems LLC is an equal opportunity employer and does not discriminate against applicants based on race color creed religion medical condition legally protected genetic information national origin sex including pregnancy childbirth or related medical condition sexual orientation gender identity and expression age disability or Vietnam era or other eligible veteran status or legally protected characteristics        Get job alerts by email Sign up now Join Our Talent Network      Job Snapshot  Employee Type FullTime      Location Colorado Springs CO Onsite      Job Type Government Strategy  Planning Training      Experience Not Specified      Date Posted 03212024      </data></node>
<node id="n493" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d6c7c2c3fc4429a4&amp;bb=rUMdW0CXv9MkYKEBBqlMZmzY5l4z0YFbq7ISfR2pB9SAglybInhSlmp8etax-Io9O7nba9G3W2a-ymjx5SBrW8RUR1gB8bZ7n-z96-2PnrHCPN9DmJT1oA%3D%3D&amp;xkcb=SoAr67M3CNum7-RSRx0PbzkdCdPP&amp;fccid=0c4c22b64a31ceb4&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SharePointYesNo Job detailsHere’s how the job details align with your profileJob typeContract LocationCamden NJ Full job description  Title Business System AnalystTechnical Writer    Contract Details      Location Camden NJ       Hybrid  Onsite 3 days a week       Duration One year       Start April 22 2024       Candidates Considered US Citizens GC and H1b     Automotive client experience is a very big plus    Business Systems Analyst  Technical Writer  Responsibilities  The Senior Business Analyst is responsible for working with IT team members and business partners to understand and document business systems processes as well as to coordinate IT projects for telematics systems This role will have strong skills in workplan development and progress tracking who will prepare and present updates regularly to relevant management channels ensuring that our goals of transparency and communication are achieved The role includes producing documentation around current and future business and system processes as well as presentations of information presentations and documentation for various levels of the organization from executive leadership to scrum teams   Analyze Current Business Processes Evaluate existing organizational workflows and identify areas for improvement based on industry trends and professional business knowledge  Gather Requirements from Stakeholders Collaborate with various stakeholders including management users and project teams to understand their needs and translate them into functional requirements  Research and Gather Information Collaborate with subject matter experts across various teams to collect relevant information  Write and Edit Content Develop procedure manuals technical specifications and process documentation around current and future processes Ensure accuracy clarity and consistency Manage collection and review of portfolio executive stakeholder updates  Create User Guides and Training Materials Produce materials that help endusers understand software applications systems and processes  Proofread and Edit Review and refine internal and external communication including print and web content to enhance clarity and accuracy  Identify Areas for Improvement Continuously assess business processes and systems Propose enhancements to increase efficiency and productivity  Provide Training and Support Train team on using software effectively in their daily tasks Offer ongoing support as needed  Stay Updated Keep abreast of advancements in technology and industry standards to produce relevant and effective documentation   Business Systems Analyst  Technical Writer  Required Skills  MUST HAVES   6 – 8 years of experience  Analytical Skills Business Systems Analysts must analyze data identify patterns and propose solutions to improve business processes  Business Acumen and Process Understanding Deep understanding of business operations process modeling and translating business requirements into technical specifications  Software Development Familiarity with concepts of databases programming system architectures data modeling and software development life cycles  ProblemSolving Analyzing situations and finding creative solutions Ability to identify issues propose solutions and optimize systems  Tools      Competency in Microsoft applications including Word Excel Outlook PowerPoint and SharePoint  Experience in SharePoint JIRA and Confluence   Research Skills Gathering information from subject matter experts  Writing Skills Creating clear accurate and engaging documentation Experience in communicating to senior leadership Experience in building professional senior  executive leadership presentations  Editing and Proofreading Ensuring quality and consistency  Attention to Detail Ensuring accuracy in technical documentation  Knowledge Acquisition Able to gather and document current and future processes systems and needs  Research Abilities Staying informed about industry advancements and emerging technologies   </data></node>
<node id="n494" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=cbd67268e953c1d8&amp;bb=rUMdW0CXv9MkYKEBBqlMZmGgbRKwes4czC_Pwg0BcM0E9WhxkSzNtYJHqIDL1UK29aNWLfmyzVFueoD-XPiUyTfJn_bschtAbZIHIM9JoIUiwPR-Ak3kLw%3D%3D&amp;xkcb=SoCf67M3CNum7-RSRx0ObzkdCdPP&amp;fccid=d3c6e90e8fb076a1&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationWashington DC Full job description Employment Type Full Time  Location Washington DC 23 days onsiteweek and remote the rest of the time  Supervises No   Must be US Citizen and be able to pass a DHS Full Scope Background Investigation   Description of Work  4SSilversword Software  Services is seeking a SAP BusinessobjectsQlik Sense Developer to fill a Data Reporting Engineer position at Customs and Border Protection The candidate is expected to demonstrate expertise drafting reports using SAP Business Objects Qlik Sense and provide information reporting to the functional team supporting the OCMO The Data Reporting Engineer will support the reporting team by authoring scheduled and adhoc reports according to policy and procedural guidance   Duties  Responsibilities   Design and develop complex reports using SAP Business Object and tools like Web Intelligence Plan and implement automated bursting of reports via publication services and scheduling Perform report requirements analysis provide effort estimation create dashboards and draft scheduled as well as adhoc reports Comprehensive knowledge of Data Reporting methodologies and deliverables  Job Requirements   Requirements  Required Education Skills and Experience   Bachelor’s Degree or higher in a business or technical discipline 5 years of development experience with SAP Business Objects including Web Intelligence and relevant experience as a Data and Report Engineer Excellent communication and consulting skills with ability to work independently Strong Knowledge in Qlik Sense Experience with trouble shooting production issues SAP Business Objects Reporting and Data Visualization experience using Web Intelligence and Dashboards Expertise in dimensional data modeling and knowledgeable in writing SQL Good understanding of User Security in Business Objects Experience with migrating reports and enhancing report performance    Physical Requirements  Work may involve sitting or standing for extended periods of time Position may require typing and reading from a computer screen Must have sufficient mobility including but not limited to bending reaching and kneeling to complete daily duties in a timely and efficient manner There is a possibility that due to parking availability and location of work walking moderate to long distance may be required Possible lifting up to 25 lbs Please note 4SSilversword Software and Services LLC reserves the right to change or modify job duties and assignments at any time The above job description is not all encompassing Positions functions and qualifications may vary depending on business needs 4SSilversword Software and Services LLC is an equal opportunity employer and does not discriminate against applicants based on race color creed religion medical condition legally protected genetic information national origin sex including pregnancy childbirth or related medical condition sexual orientation gender identity and expression age disability or Vietnam era or other eligible veteran status or legally protected characteristics        Get job alerts by email Sign up now Join Our Talent Network      Job Snapshot  Employee Type FullTime      Location Washington DC Onsite      Job Type Admin  Clerical Information Technology      Experience Not Specified      Date Posted 03212024      </data></node>
<node id="n495" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d94a8917c5e7e13f&amp;bb=rUMdW0CXv9MkYKEBBqlMZrJmmP8ZHKC-GBtaYC7NU47BsTlHuLYbDC5dr41nd00DG24IQCGtwJ4ohXQm8VPrHmIzvylZdD9u7QVpp2cv8isUPsYGlj1FpA%3D%3D&amp;xkcb=SoAC67M3CNum7-RSRx0NbzkdCdPP&amp;fccid=7ae183615e6697dd&amp;cmp=Shivansh-Outsourcing&amp;ti=Data+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in S3YesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay55  75 an hourJob typeContractShift and schedule8 hour shift LocationChicago IL BenefitsPulled from the full job description401kDental insuranceHealth insurance Full job descriptionJob Title Data Analyst Location Chicago IL Duration Long Term Role is also available in McLean VA – Richmond VA – Plano TX – New York NY Responsibilities · Utilize AWS cloud infrastructure to design develop and maintain data analytics solutions · Collaborate with crossfunctional teams to gather requirements and translate them into scalable AWSbased solutions · Perform data modeling data transformation and data visualization tasks using AWS services such as EC2 S3 EKS OpenShift Lambda API Gateway RDS CFT etc · Implement best practices for data security data governance and data quality assurance within the AWS environment · Optimize and finetune AWS resources to ensure high performance and cost efficiency · Troubleshoot and resolve issues related to AWS infrastructure and data pipelines · Stay updated on the latest AWS services and features to continuously enhance our data analytics capabilities Requirements · Bachelors degree in Computer Science Information Technology or related field · Minimum of 3 years of handson experience working with AWS cloud services · Strong understanding of AWS architecture principles and best practices · Proficiency in using AWS services such as EC2 S3 EKS OpenShift Lambda API Gateway RDS CFT etc · Experience with data analytics tools and technologies including data modeling ETL processes and data visualization · Familiarity with data security and compliance standards in the AWS cloud environment · Excellent problemsolving skills and ability to troubleshoot complex issues · Strong communication and collaboration skills with the ability to work effectively in a team environment Job Type Contract Salary 5500  7500 per hour Benefits  401k Dental insurance Health insurance  Schedule  8 hour shift  Experience  AWS 2 years Required Data pipelines 2 years Required Data Governance 2 years Required  Work Location On the road </data></node>
<node id="n496" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=1b6d5a60c51bb9b5&amp;bb=rUMdW0CXv9MkYKEBBqlMZh-O6lTTGUcQ72MYObn7v6-1dAzmSHpI8L_pHxT26crfLEDkeDg5iItM-UV4JQXcgnBK_Ndk76AVVzy8sLkwGTTaWgJH-ziAOg%3D%3D&amp;xkcb=SoC267M3CNum7-RSRx0MbzkdCdPP&amp;fccid=1a70f25b2e303d26&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Visual BasicYesNoEducationDo you have a Masters degreeYesNoLanguagesDo you know SpanishYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationPuerto Rico Full job description  Description    V2A Consulting is looking for Data Analyst candidates for our growing Analytics practice Data Analysts are an integral part of our Analytics team They are responsible for managing our knowledge databases and for transforming and visualizing data to facilitate and enhance business decision making as part of our engagements with clients Our Data Analysts usually have a Bachelor’s or Master’s degree plus 03 years of work experience  Key areas of responsibility   Conduct datadriven analyses interpret results and draw conclusions and perform data visualizations through business intelligence tools  Manage databases including data extraction through SQL or Python programming when necessary data transformations data integrations and data pipeline automations Validate data and identify and resolve data quality issues  Present results in a simple and comprehensive way to nontechnical audience  Understand technical requirements and be able to communicate with computer scientists programmers and technology support  Support Data Scientists in the process of building and testing machine learning models particularly during the data preparation phase  Maintain an industry leading knowledge of the tools systems and processes available for bestinclass data wrangling and database management  Train other teams on analytical tools and techniques  Requirements    What we are looking for  Education   Bachelor’s or Masters degree in Data Science Computer Science or Business Analytics  Bachelor’s or Masters degree in Statistics Economics or Mathematics  Bachelor’s or Masters degree in Social Sciences Psychology or any other Bachelor’s or Master’s degree with strong applied statistics or data management and analysis eg Biosciences   Qualifications   03 years of work experience  Problem Solver  Capacity to apply knowledge and skills to solve complex problems  Team Player  Ability to build and manage relationships effectively with technology teams with programmers and with client team members and stakeholders in general  High selfmotivation for learning and setting and achieving challenging goals  Compelled to excel and succeed in every task at hand  Thrives in an entrepreneurial resultsoriented environment  Academic andor professional experience using a programming language to perform data transformations eg Python R SaS C  Academic andor professional experience using statistical methods for analysis  Experience in the use of data transformation and database management tools highly valued eg Microsoft Excel Power Query Data Bricks SAS Enterprise Guide Visual Basic SQL Browser …  Experience in the use of business intelligence tools highly valued eg Power Bi Tableau Business Objects Clik Data Analytics Zoho Analytics SAS Visual Analytics …  Experience in the use of PowerPoint and Word highly valued  Fully Bilingual Spanish and English   </data></node>
<node id="n497" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=b5af5211d46db24d&amp;bb=rUMdW0CXv9MkYKEBBqlMZjVKRIHm-Nn2GPlGbdCiV2yn_0UjsJD32uNnyXLMFu5EynKuiK117Zi1o9pBZg2XM7GieIHbsJhOwMoAemrrbB2wmkOBxfBJ3w%3D%3D&amp;xkcb=SoBf67M3CNum7-RSRx0DbzkdCdPP&amp;fccid=900f958a9853e7ab&amp;cmp=INSPYR-Solutions&amp;ti=Accounting+Clerk&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in NetSuiteYesNo Job detailsHere’s how the job details align with your profilePay50  64 an hourJob typeContractShift and schedule8 hour shiftDay shiftMonday to Friday LocationJacksonville FL BenefitsPulled from the full job description401k401k matchingHealth insuranceLife insuranceRetirement plan Full job descriptionSkillset  Experience  FinanceAccounting MS Excel Advanced Accounting skills Cost Accounting Financial Modeling NetSuiteGeneral Ledger  Our benefits package includes  Comprehensive medical benefits Competitive pay 401k Retirement plan and much more  About INSPYR SolutionsTechnology is our focus and quality is our commitment As a national expert in delivering flexible technology and talent solutions we strategically align industry and technical expertise with our clients business objectives and cultural needs Our solutions are tailored to each client and include a wide variety of professional services project and talent solutions By always striving for excellence and focusing on the human aspect of our business we work seamlessly with our talent and clients to match the right solutions to the right opportunities Learn more about us at inspyrsolutionscom INSPYR Solutions provides Equal Employment Opportunities EEO to all employees and applicants for employment without regard to race color religion sex national origin age disability or genetics In addition to federal law requirements INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities Job Type Contract Salary 5000  6400 per hour Benefits  401k matching Health insurance Life insurance Retirement plan  Experience level  6 years  Schedule  8 hour shift Day shift Monday to Friday  Application Questions  How long youve been working as a FinancialAccounting Data Analyst How many years of experience do you have with General Ledger Are you proficient in MS Excel How far is your location in Jacksonville FL Are you willing to work on a Hybrid Schedule in Jacksonville FL How much is your target pay per hour on W2  Work Location Remote </data></node>
<node id="n498" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=32c9c94184d8e63e&amp;bb=rUMdW0CXv9MkYKEBBqlMZsTf5OpnP5cEV71WK_h6BwxleU6Eu06tFgJbh5vXVXniy9v7GYwFOmHPpYRLEHBbQgFpTm4sazq8gfIXqvwVv2q0TrYBMnyNX8Pbv_qySWrw&amp;xkcb=SoDr67M3CNum7-RSRx0CbzkdCdPP&amp;fccid=eedbea588a224a8b&amp;cmp=Seagull-PME&amp;ti=Data+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay60000  80000 a yearJob typeFulltimeShift and scheduleOvertimeMonday to Friday LocationSeagull PME in San Diego CA 92123 BenefitsPulled from the full job description401kDental insuranceHealth insurancePaid holidaysPaid time off Full job descriptionJob Title Junior to MidLevel Data Analyst  Local Candidates Only Job Description The Data Analyst will work within a highvolume construction and maintenance program and manage dataset develop tools reporting and dashboards to assist management with daytoday operations and decision making Essential Functions  Collect organize and analyze large datasets to identify trends patterns and insights Develop and implement data models databases data collection systems and data analytics strategies Utilize SQL query logic and update backend databases to improve order processing and program processes Utilize python programming to provide data integrity and automate workflow  business task Interpret and present data findings to stakeholders in a clear and concise manner Develop maintain and update PowerBI reporting and dashboards Collaborate with crossfunctional teams to define project requirements and objectives Conduct data quality assessments and ensure data integrity Create visualizations and reports using tools such as PowerBI and Visio Perform adhoc data analysis requests  Skills  Abilities  Strong data analysis skills with the ability to manipulate and interpret complex datasets Proficiency in SQL for querying databases and extracting relevant information Experience with ETL Extract Transform Load processes to transform raw data into usable formats Knowledge of programming languages such as Python for data manipulation and automation tasks Familiarity with project management methodologies to effectively plan and execute data analysis projects Excellent problemsolving skills with the ability to think critically and identify potential issues Strong attention to detail to ensure accuracy in data analysis and reporting Business analysis skills to understand organizational needs and translate them into actionable insights  Minimum Qualifications  Understanding of data structures data integrity and databases architecture Skilled in Python and SQL Skilled in PowerBI and have the ability to create meaningful reports and dashboards Experienced in SharePoint management Must be in San Diego region or willing to relocate to San Diego  Education and Experience  BS in Computer Science or similar degree Relevant experience can fulfill BS requirement  PreScreen   Upon offer employees may be required to complete and pass a preemployment drug screen reference and background check  Work Location this is an inperson position the candidate must be in the Kearney Mesa office Monday through Friday Job Type Fulltime Pay 6000000  8000000 per year Benefits  401k Dental insurance Health insurance Paid holidays Paid time off  Experience level  3 years  Schedule  Monday to Friday Overtime  Application Questions  Do you live in San Diego County  Experience  Data Analyst 3 years Preferred  Ability to Commute  San Diego CA 92123 Required  Work Location In person </data></node>
<node id="n499" labels=":Skill"><data key="labels">:Skill</data><data key="name">arcgis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n500" labels=":Skill"><data key="labels">:Skill</data><data key="name">gis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n501" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">timeseries analysis</data></node>
<node id="n502" labels=":Skill"><data key="labels">:Skill</data><data key="name">arcpy</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n503" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">time series predictive</data></node>
<node id="n504" labels=":Skill"><data key="labels">:Skill</data><data key="name">image analysis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n505" labels=":Skill"><data key="labels">:Skill</data><data key="name">cleaning data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n506" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical procedures</data></node>
<node id="n507" labels=":Skill"><data key="labels">:Skill</data><data key="name">crossvalidating</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n508" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">business strategies</data></node>
<node id="n509" labels=":Skill"><data key="labels">:Skill</data><data key="name">data requirements</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n510" labels=":Skill"><data key="labels">:Skill</data><data key="name">microsoft forms</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n511" labels=":Skill"><data key="labels">:Skill</data><data key="name">anova</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n512" labels=":Skill"><data key="labels">:Skill</data><data key="name">pca</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n513" labels=":Skill"><data key="labels">:Skill</data><data key="name">reporting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n514" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytical mindset</data></node>
<node id="n515" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data quality auditing</data></node>
<node id="n516" labels=":Skill"><data key="labels">:Skill</data><data key="name">data sources</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n517" labels=":Skill"><data key="labels">:Skill</data><data key="name">business gaps</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n518" labels=":Skill"><data key="labels">:Skill</data><data key="name">dashboards</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n519" labels=":Skill"><data key="labels">:Skill</data><data key="name">unstructured data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n520" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">business decisions</data></node>
<node id="n521" labels=":Skill"><data key="labels">:Skill</data><data key="name">analysis reports</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n522" labels=":Skill"><data key="labels">:Skill</data><data key="name">visualizations</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n523" labels=":Skill"><data key="labels">:Skill</data><data key="name">prototypes</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n524" labels=":Skill"><data key="labels">:Skill</data><data key="name">provide insights</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n525" labels=":Skill"><data key="labels">:Skill</data><data key="name">data quality</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n526" labels=":Skill"><data key="labels">:Skill</data><data key="name">quality analysis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n527" labels=":Skill"><data key="labels">:Skill</data><data key="name">excel</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n528" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytical thinker</data></node>
<node id="n529" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">segmentation techniques</data></node>
<node id="n530" labels=":Skill"><data key="labels">:Skill</data><data key="name">powerpoint</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n531" labels=":Skill"><data key="labels">:Skill</data><data key="name">testing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n532" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">relational databases</data></node>
<node id="n533" labels=":Skill"><data key="labels">:Skill</data><data key="name">data processing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n534" labels=":Skill"><data key="labels">:Skill</data><data key="name">cognosargos</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n535" labels=":Skill"><data key="labels">:Skill</data><data key="name">data extraction</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n536" labels=":Skill"><data key="labels">:Skill</data><data key="name">outlook</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n537" labels=":Skill"><data key="labels">:Skill</data><data key="name">word</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n538" labels=":Skill"><data key="labels">:Skill</data><data key="name">data integrity</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n539" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">identify indicators</data></node>
<node id="n540" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">dashboard development</data></node>
<node id="n541" labels=":Skill"><data key="labels">:Skill</data><data key="name">process diagrams</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n542" labels=":Skill"><data key="labels">:Skill</data><data key="name">user training</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n543" labels=":Skill"><data key="labels">:Skill</data><data key="name">diagraming</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n544" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">business processes</data></node>
<node id="n545" labels=":Skill"><data key="labels">:Skill</data><data key="name">project plans</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n546" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">document processing</data></node>
<node id="n547" labels=":Skill"><data key="labels">:Skill</data><data key="name">ftp</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n548" labels=":Skill"><data key="labels">:Skill</data><data key="name">sftp</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n549" labels=":Skill"><data key="labels">:Skill</data><data key="name">as2</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n550" labels=":Skill"><data key="labels">:Skill</data><data key="name">qa testing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n551" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">lifecycle management</data></node>
<node id="n552" labels=":Skill"><data key="labels">:Skill</data><data key="name">test cases</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n553" labels=":Skill"><data key="labels">:Skill</data><data key="name">user stories</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n554" labels=":Skill"><data key="labels">:Skill</data><data key="name">bug reports</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n555" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">deployment procedures</data></node>
<node id="n556" labels=":Skill"><data key="labels">:Skill</data><data key="name">release notes</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n557" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">project requirements</data></node>
<node id="n558" labels=":Skill"><data key="labels">:Skill</data><data key="name">analyze data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n559" labels=":Skill"><data key="labels">:Skill</data><data key="name">web services</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n560" labels=":Skill"><data key="labels">:Skill</data><data key="name">web applications</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n561" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">technical requirements</data></node>
<node id="n562" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">functional specifications</data></node>
<node id="n563" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">systems scope objectives</data></node>
<node id="n564" labels=":Skill"><data key="labels">:Skill</data><data key="name">process mapping</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n565" labels=":Skill"><data key="labels">:Skill</data><data key="name">iot</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n566" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">process diagramsflow charts</data></node>
<node id="n567" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">product requirements</data></node>
<node id="n568" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">strategy requirements</data></node>
<node id="n569" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">problemsolving skills</data></node>
<node id="n570" labels=":Skill"><data key="labels">:Skill</data><data key="name">flow charts</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n571" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">functional block diagrams</data></node>
<node id="n572" labels=":Skill"><data key="labels">:Skill</data><data key="name">data sets</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n573" labels=":Skill"><data key="labels">:Skill</data><data key="name">trends patterns</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n574" labels=":Skill"><data key="labels">:Skill</data><data key="name">insights</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n575" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">reporting and visualization</data></node>
<node id="n576" labels=":Skill"><data key="labels">:Skill</data><data key="name">analytical skills</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n577" labels=":Skill"><data key="labels">:Skill</data><data key="name">microsoft office</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n578" labels=":Skill"><data key="labels">:Skill</data><data key="name">access</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n579" labels=":Skill"><data key="labels">:Skill</data><data key="name">user requirements</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n580" labels=":Skill"><data key="labels">:Skill</data><data key="name">diagnose problems</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n581" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">identify opportunities</data></node>
<node id="n582" labels=":Skill"><data key="labels">:Skill</data><data key="name">process redesign</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n583" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">gathering requirements</data></node>
<node id="n584" labels=":Skill"><data key="labels">:Skill</data><data key="name">process analysis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n585" labels=":Skill"><data key="labels">:Skill</data><data key="name">poc</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n586" labels=":Skill"><data key="labels">:Skill</data><data key="name">mvc</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n587" labels=":Skill"><data key="labels">:Skill</data><data key="name">analyze databases</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n588" labels=":Skill"><data key="labels">:Skill</data><data key="name">database design</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n589" labels=":Skill"><data key="labels">:Skill</data><data key="name">sql server</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n590" labels=":Skill"><data key="labels">:Skill</data><data key="name">test scripts</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n591" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">requirement specifications</data></node>
<node id="n592" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">functional requirements</data></node>
<node id="n593" labels=":Skill"><data key="labels">:Skill</data><data key="name">phpspring</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n594" labels=":Skill"><data key="labels">:Skill</data><data key="name">google guice</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n595" labels=":Skill"><data key="labels">:Skill</data><data key="name">jbpm</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n596" labels=":Skill"><data key="labels">:Skill</data><data key="name">jpamysql</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n597" labels=":Skill"><data key="labels">:Skill</data><data key="name">ant</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n598" labels=":Skill"><data key="labels">:Skill</data><data key="name">business needs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n599" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data flow diagrams</data></node>
<node id="n600" labels=":Skill"><data key="labels">:Skill</data><data key="name">workflow diagrams</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n601" labels=":Skill"><data key="labels">:Skill</data><data key="name">edi 834</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n602" labels=":Skill"><data key="labels">:Skill</data><data key="name">edi 820</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n603" labels=":Skill"><data key="labels">:Skill</data><data key="name">provision</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n604" labels=":Skill"><data key="labels">:Skill</data><data key="name">uml</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n605" labels=":Skill"><data key="labels">:Skill</data><data key="name">adobe analytics</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n606" labels=":Skill"><data key="labels">:Skill</data><data key="name">ab tests</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n607" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">adobe audience manager</data></node>
<node id="n608" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">strategic thinking</data></node>
<node id="n609" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">reengineering methodologies</data></node>
<node id="n610" labels=":Skill"><data key="labels">:Skill</data><data key="name">interviews</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n611" labels=":Skill"><data key="labels">:Skill</data><data key="name">negotiating</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n612" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">costbenefit analysis</data></node>
<node id="n613" labels=":Skill"><data key="labels">:Skill</data><data key="name">trending</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n614" labels=":Skill"><data key="labels">:Skill</data><data key="name">forecasting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n615" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">financial analysis</data></node>
<node id="n616" labels=":Skill"><data key="labels">:Skill</data><data key="name">project</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n617" labels=":Skill"><data key="labels">:Skill</data><data key="name">power apps</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n618" labels=":Skill"><data key="labels">:Skill</data><data key="name">devops</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n619" labels=":Skill"><data key="labels">:Skill</data><data key="name">appian</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n620" labels=":Skill"><data key="labels">:Skill</data><data key="name">servicenow</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n621" labels=":Skill"><data key="labels">:Skill</data><data key="name">data sourcing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n622" labels=":Skill"><data key="labels">:Skill</data><data key="name">microsoft excel</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n623" labels=":Skill"><data key="labels">:Skill</data><data key="name">reporting tools</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n624" labels=":Skill"><data key="labels">:Skill</data><data key="name">business objects</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n625" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">business intelligence</data></node>
<node id="n626" labels=":Skill"><data key="labels">:Skill</data><data key="name">data insights</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n627" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">business requirement</data></node>
<node id="n628" labels=":Skill"><data key="labels">:Skill</data><data key="name">data science</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n629" labels=":Skill"><data key="labels">:Skill</data><data key="name">data models</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n630" labels=":Skill"><data key="labels">:Skill</data><data key="name">data gathering</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n631" labels=":Skill"><data key="labels">:Skill</data><data key="name">microsoft word</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n632" labels=":Skill"><data key="labels">:Skill</data><data key="name">analyzing data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n633" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical analyses</data></node>
<node id="n634" labels=":Skill"><data key="labels">:Skill</data><data key="name">stata</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n635" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">customer requirements</data></node>
<node id="n636" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">gather requirements</data></node>
<node id="n637" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">systems requirements</data></node>
<node id="n638" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">database management</data></node>
<node id="n639" labels=":Skill"><data key="labels">:Skill</data><data key="name">data lake</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n640" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">cloud technologies</data></node>
<node id="n641" labels=":Skill"><data key="labels">:Skill</data><data key="name">data warehouse</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n642" labels=":Skill"><data key="labels">:Skill</data><data key="name">ssas</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n643" labels=":Skill"><data key="labels">:Skill</data><data key="name">tabular</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n644" labels=":Skill"><data key="labels">:Skill</data><data key="name">data factory</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n645" labels=":Skill"><data key="labels">:Skill</data><data key="name">ai</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n646" labels=":Skill"><data key="labels">:Skill</data><data key="name">equis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n647" labels=":Skill"><data key="labels">:Skill</data><data key="name">data collection</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n648" labels=":Skill"><data key="labels">:Skill</data><data key="name">data analytics</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n649" labels=":Skill"><data key="labels">:Skill</data><data key="name">powerbi</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n650" labels=":Skill"><data key="labels">:Skill</data><data key="name">spotfire</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n651" labels=":Skill"><data key="labels">:Skill</data><data key="name">pivot tables</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n652" labels=":Skill"><data key="labels">:Skill</data><data key="name">vlookup</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n653" labels=":Skill"><data key="labels">:Skill</data><data key="name">pivot table</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n654" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">report specifications</data></node>
<node id="n655" labels=":Skill"><data key="labels">:Skill</data><data key="name">decision making</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n656" labels=":Skill"><data key="labels">:Skill</data><data key="name">olap</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n657" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytical thinking</data></node>
<node id="n658" labels=":Skill"><data key="labels">:Skill</data><data key="name">scrutinizing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n659" labels=":Skill"><data key="labels">:Skill</data><data key="name">market baskets</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n660" labels=":Skill"><data key="labels">:Skill</data><data key="name">cleanse</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n661" labels=":Skill"><data key="labels">:Skill</data><data key="name">preprocess</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n662" labels=":Skill"><data key="labels">:Skill</data><data key="name">validate data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n663" labels=":Skill"><data key="labels">:Skill</data><data key="name">data pipelines</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n664" labels=":Skill"><data key="labels">:Skill</data><data key="name">trends</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n665" labels=":Skill"><data key="labels">:Skill</data><data key="name">correlations</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n666" labels=":Skill"><data key="labels">:Skill</data><data key="name">outliers</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n667" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical methods</data></node>
<node id="n668" labels=":Skill"><data key="labels">:Skill</data><data key="name">matplotlib</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n669" labels=":Skill"><data key="labels">:Skill</data><data key="name">report findings</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n670" labels=":Skill"><data key="labels">:Skill</data><data key="name">teams</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n671" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">experimental design</data></node>
<node id="n672" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">predictions models</data></node>
<node id="n673" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data collection requirements</data></node>
<node id="n674" labels=":Skill"><data key="labels">:Skill</data><data key="name">process data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n675" labels=":Skill"><data key="labels">:Skill</data><data key="name">data anomalies</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n676" labels=":Skill"><data key="labels">:Skill</data><data key="name">systems processes</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n677" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">organizational workflows</data></node>
<node id="n678" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">gather information</data></node>
<node id="n679" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">technical specifications</data></node>
<node id="n680" labels=":Skill"><data key="labels">:Skill</data><data key="name">confluence</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n681" labels=":Skill"><data key="labels">:Skill</data><data key="name">business object</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n682" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">web intelligence plan</data></node>
<node id="n683" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data transformation</data></node>
<node id="n684" labels=":Skill"><data key="labels">:Skill</data><data key="name">aws</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n685" labels=":Skill"><data key="labels">:Skill</data><data key="name">openshift</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n686" labels=":Skill"><data key="labels">:Skill</data><data key="name">lambda</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n687" labels=":Skill"><data key="labels">:Skill</data><data key="name">gateway</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n688" labels=":Skill"><data key="labels">:Skill</data><data key="name">rds</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n689" labels=":Skill"><data key="labels">:Skill</data><data key="name">cft</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n690" labels=":Skill"><data key="labels">:Skill</data><data key="name">data security</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n691" labels=":Skill"><data key="labels">:Skill</data><data key="name">visualizing data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n692" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">datadriven analyses</data></node>
<node id="n693" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data transformations</data></node>
<node id="n694" labels=":Skill"><data key="labels">:Skill</data><data key="name">data integrations</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n695" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data pipeline automations</data></node>
<node id="n696" labels=":Skill"><data key="labels">:Skill</data><data key="name">data preparation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n697" labels=":Skill"><data key="labels">:Skill</data><data key="name">power query</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n698" labels=":Skill"><data key="labels">:Skill</data><data key="name">data bricks</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n699" labels=":Skill"><data key="labels">:Skill</data><data key="name">enterprise guide</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n700" labels=":Skill"><data key="labels">:Skill</data><data key="name">zoho</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n701" labels=":Skill"><data key="labels">:Skill</data><data key="name">visual analytics</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n702" labels=":Skill"><data key="labels">:Skill</data><data key="name">patterns</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n703" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">databases architecture</data></node>
<node id="n704" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d4750ea9e73478c1&amp;bb=6um9CuQK5F1Am4GwBb0CRWwz-fJ7GYLQC631A26mWuuSSi3u9rWkAOahJnyvX9rXQsdW8WgVvlum0-Qm_1cldnsrTsNAJi3eJBJqnQ3u4327GqH8MenSag%3D%3D&amp;xkcb=SoCq67M3CNjXLyxSa50LbzkdCdPP&amp;fccid=dd616958bd9ddc12&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in MarketingYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay45  50 an hourJob typeFulltime LocationGrand Blanc MI 48439 BenefitsPulled from the full job description401k matchingADD insuranceDental insuranceDisability insuranceEmployee assistance programFlexible spending accountHealth insuranceShow morechevron down Full job description      Hybrid position based in Grand Blanc MI  Dort Financial Credit Union is looking for team members who will execute our mission Enrich peoples lives members employees communities to help achieve our vision To be the leader in our industry by providing quality financial services developing an empowered and diverse team and making our communities a better place to live Dort Financial Credit Union upholds a culture of excellence with opportunities of engagement and advancement for our team members We abide by our core values of Empowerment Accountability Transparency Collaboration and Volunteerism each day and have a strong focus in community involvement  We offer a competitive benefits package immediately upon hire including medical dental and vision insurance LifeADD and Disability Insurance Supplemental Life insurance for employeespousedependent HSA and FSA plans and tuition reimbursement for fulltime team members LegalShield Pet Benefits Employee Assistance Program Telemedicine We also offer a matching 401 k including a safe harbor a referral bonus program and paid time off including holidays  Come join our team Apply today  Minimum Formal Education Bachelor’s Degree in Computer Science Software Engineering or an equivalent field required Master’s Degree in Computer Science Software Engineering or an equivalent field preferred  Experience Three years of handson data management experience including data warehousing data integration modeling optimization and data quality andor other areas directly relevant to data engineering responsibilities and tasks Experience required specifically focusing on data warehouse engineering and data processes Engineering and data management experience in a financial institution preferred Requires proficient frequent and ongoing use of database software platforms Strong fundamental knowledge of financial institution operation and marketing is required  Other Requirements Position requires bondability of employee Position also requires ability to work independently with minimal supervision ability to communicate in a tactful manner excellent organizational skills diplomacy selfmotivation and discipline flexible hours ability to maintain professional conduct in or out of the office with staff members vendors and volunteers a team player attitude toward Dort Financial Credit Union and the exhibiting of that team spirit to staff       </data></node>
<node id="n705" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d4d6c4c6f37316f1&amp;bb=6um9CuQK5F1Am4GwBb0CRa1AirNJaMwJ4o3mwpIPerDu3sZYc1dTh0Nc_Z9YinHW2XlL-OnDGQldyMKb6T95dom1udnLL0fEz7LJBcq8NWyqMxHkD6uzbw%3D%3D&amp;xkcb=SoAe67M3CNjXLyxSa50KbzkdCdPP&amp;fccid=2ebe881d9f3fab51&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay117000 a yearJob typeFulltime LocationDenver CO BenefitsPulled from the full job description401k 6 Match401k matchingDental insuranceFlexible spending accountHealth insuranceLife insuranceVision insurance Full job description      The Data Solutions Engineer at Redaptive is responsible for leading programmatic meter design based on customer needs and expectations for energy efficiency projects Reporting to the Senior Manager of Delivery Engineering the Data Solutions Engineer will bring advanced technical expertise in electrical engineering and energy efficiency to deliver highimpact solutions This role is crucial in driving the successful implementation of energy efficiency projects within the organization       Redaptive is an EnergyasaService provider that funds and installs energysaving and energygenerating equipment Redaptive’s programs help many of the world’s most sophisticated organizations reduce energy waste save money lower their carbon emissions and meet their sustainability goals across their entire real estate portfolios With Redaptive customers can overcome capital and contractual barriers to achieve energysaving benefits quickly all with realtime data powered by Redaptive’s proprietary Data Solutions metering platform Redaptive was founded in 2015 and is headquartered in Denver CO Redaptive is backed by CarVal ENGIE New Ventures Linse Capital CBRE Evergy Ventures Rabobank CPP Investments and Honeywell       Our company culture is exciting collaborative and fastpaced We are passionate about changing the world and helping our customers become more environmentally sustainable and profitable We are looking for team members who are driven passionate and want to take on a diverse set of challenges to help grow a great company Redaptive Inc is an equal employment opportunity employer and all qualified applicants will receive consideration for employment For more information visit wwwredaptivecom       LITD1       Responsibilities and Duties    Direct the development of programmatic meter designs tailored to meet customer needs and expectations  Design and review the scope of work for electric gas and water meter customer proposals  Oversee the creation of intricate plans and specifications for programmatic panel audits and designs ensuring precision efficiency and compliance with industry standards and codes  Collaborate closely with multidisciplinary teams to integrate sophisticated audit methodologies seamlessly into overall project designs  Provide advanced technical leadership in electrical engineering specializing in programmatic meter designs to guide and support project teams in achieving successful implementations of energy efficiency solutions across direct indirect and distributor models  Lead team of electrical engineers both in the US and in India  Stay at the forefront of industry advancements trends and emerging technologies related to programmatic meter designs  Lead engagements with external vendors and contractors to ensure the seamless execution of advanced programmatic meter design methodologies  Conduct rigorous quality assurance reviews of complex programmatic meter designs ensuring the highest standards of accuracy reliability and adherence to project specifications  Oversee the activity and implement strategic corrective actions to address discrepancies or challenges identified during the review process  Oversee the generation of detailed documentation including intricate drawings schematics and technical specifications to effectively communicate advanced programmatic meter designs to internal teams and external stakeholders  Produce indepth reports to provide comprehensive updates on the progress and status of advanced programmatic meter designs  Other duties as assigned       Required Abilities Skills and Education    Bachelors degree in Electrical Engineering or a related field  Demonstrated experience in designing and implementing programmatic electrical meter solutions       The Perks    Equity plan participation  Companysubsidized benefits medical dental vision life insurance  Flexible Spending Accounts healthcare and dependent care  6 401k match with immediate vesting  Flexible Time Off  Expected annual salary 117K subject to adjustment for relevant experience skills geo location  Annual bonus subject to company and individual performance    The company is an Equal Opportunity Employer drugfree workplace and complies with ADA regulations as applicable All duties and responsibilities are essential functions and requirements and are subject to possible modification to reasonably accommodate individuals with disabilities The requirements listed in this document are the minimum levels of knowledge skills or abilities             About Redaptive Inc       Redaptive is an EnergyasaService and technology provider that funds and installs energysaving and energygenerating equipment Redaptive’s programs help organizations accelerate and scale efforts to reduce energy waste optimize costs increase resiliency and meet sustainability goals across their entire real estate portfolio With Redaptive customers can quickly overcome capital and resource barriers to achieve energyoriented benefits Our proprietary bestinclass meters and data solutions validate and report on critical building information Redaptive is backed by CarVal ENGIE New Ventures Linse Capital CBRE Evergy Ventures Rabobank CPP Investments and Honeywell       Our company culture is fun collaborative and fast paced We are passionate about changing the world and helping our customers to become more environmentally sustainable and profitable We are looking for team members who are driven passionate and want to take on a diverse set of challenges to help grow a great company Redaptive Inc is an equal employment opportunity employer and all qualified applicants will receive consideration for employment For more information visit redaptivecom       This employer participates in EVerify       CCPA Notice for California Job Applicants     Please no thirdparty recruiters     </data></node>
<node id="n706" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=2f06eb85463e473e&amp;bb=6um9CuQK5F1Am4GwBb0CRaJecpjte4Pm8CiEVF9eFFVMTGfvQ8YKzWPZg-MWQhBAETSHGsAY6P-S5FgDF-iFCNX51_MBTp7_MbOMH220Wu0Qz7GqFmukGw%3D%3D&amp;xkcb=SoCD67M3CNjXLyxSa50JbzkdCdPP&amp;fccid=b8b16403ce1d3b7e&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TeachingYesNoEducationDo you have a Bachelors degreeYesNo LocationHoboken NJ BenefitsPulled from the full job description401k matchingDental insuranceGym membershipHealth insurancePaid parental leaveParental leaveUnlimited paid time offShow morechevron down Full job description Company Description  Founded in Europe in 2004 Tipico is now a licensed US Sportsbook operating in New Jersey Iowa Ohio and Colorado Renowned in Germany and globally Tipico offers online betting across 30 sports Guided by values such as innovation and inclusion Tipico focuses on creating topnotch mobile sports betting and casino products Recently recognized as the No 1 rated casino app in the US Tipico is dedicated to enhancing gaming excitement for millions daily Join us in redefining excellence in online entertainment  Please note this role is located in our Hoboken NJ office we work off a hybrid model and come into the office 2 days per week Job Description  The Senior Data Engineer is responsible for developing constructing testing and maintaining Tipico’s data platform and pipelines This is achieved by ensuring the infrastructure is reliable and a high level of data quality is maintained  The Senior Data Engineer will report directly to the VP of data or team lead and work together with rest of the data engineers BI analysts and product teams This person will   Create and maintain both batch ETL and realtime data pipelines and architecture  Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs  Assemble large and complex data sets that meet functional and nonfunctional business requirements  Identify design and implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure for greater scalability etc  Build the infrastructure required for optimal extraction transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies  Collaborate with stakeholders including data architects Executive Product Data and IT team members from the beginning to the delivery of a project  Build a scalable modern and trusted data eco system making sure it is responsive available and in line with business requirements  Design build and scale data pipelines across a variety of source systems and streams internal thirdparty as well as cloudbased distributedelastic environments and downstream applications andor selfservice solutions  Think of new ways to help make our data platform more scalable resilient and reliable and then work across our team to put your ideas into action  Take part in sprints workloads and deliverables through agile methodologies  Manage quality assurance verification of accuracy and consistency of data – specifically KPI measures sent to the business  Provide expert level advice to data engineers to deliver high quality data pipelines and ETL flows Review design and code produced by other engineers    Qualifications  Education   Bachelors degree in Computer Science Computer Engineering relevant technical field or equivalent practical experience   Experience   Experience working in an online gaming or fastmoving industry required  7 years of work experience in data engineers and data platforms  Must have experience in successfully implementing datacentric applications such as data warehouses operational data stores and data integration projects  Experience with SQL ETL data modeling and Python  Experience working with terabyte to petabyte scale data  Designing technical solutions and teaching team members   Skills   Excellent Verbal and written Communication skills  Strong analytical and problemsolving skills that balance with creative approaches  Ability to take ownership of projects with diligence and consistency in carrying out tasks with a high level of attention to detail and priority  Collaborative able to engage in interactive discussions with the rest of the team and able to communicate technical concepts clearly and concisely   Language   Fluent in English Oral and written – essential   FunctionalTechnical Competence   Strong experience with largescale production databases and SQL  Solid understanding of cloud data services AWS services such as S3 Athena EC2 RedShift EMR EKS RDS and Lambda  Proficient with one or more big data  new data technologies such as DBT NIFI Airflow Kafka  Knowledge in working with timeseriesanalytics databases such as Elasticsearch  Understanding of containerization and orchestration technologies like DockerKubernetes  Familiarity with Marketing tooling andor CRM systems is preferred  Familiarity with using business intelligence tools such as Domo Superset or other similar tooling   Personal Characteristics   Exemplifies our values – Trust Passion and Progress  Proactive agile innovative and solution orientated work approach with strong sense of ownership and accountability for his’sher’s work and impact on business  Thrives working in a fastpaced and challenging environment  Ability to think outside of the box be innovative adaptable identify continuous improvement areas and come up with creative solutions to challenges  Detailoriented and organized  Strong work ethic and ability to work in a collaborative environment  Ability to multitask and prioritize  Adaptability and flexibility  Constructively challenges with positive arguments  Comfortable with being challenged  seeking challenge    Additional Information  Whats in it for you   Work in a new and thriving industry with high growth potential  Competitive salary and performance bonus  Medical Dental and Vision Insurance  401k employer matching  Unlimited PTO with 15 paid holidays  100 paid parental leave  Professional training and development opportunities  Gym reimbursement  Free workout classes at Prime Cycle in Hoboken NJ  1year free Apple Fitness subscription  Work in an environment with a startup feeling backedup by a leading European sports betting house We are a highvolume business and are taking off in the US   </data></node>
<node id="n707" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=aa8d48c4acb55fc2&amp;bb=6um9CuQK5F1Am4GwBb0CRYnk9aBWpYcZRDjhGjYx5ErM-Dzez13tfkqt3VC7IXGaaabDHqfSEU0BI5Js3jN9fSZSJcBLJzfNarLxE1nsuCA%3D&amp;xkcb=SoA367M3CNjXLyxSa50IbzkdCdPP&amp;fccid=cbfa56f19cc95796&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Statistical analysisYesNoEducationDo you have a Doctoral degreeYesNo Job detailsHere’s how the job details align with your profilePay9712  14567 an hourJob typeParttime LocationMoffett Field CA 94035 BenefitsPulled from the full job description401kADD insuranceFlexible scheduleHealth insuranceMilitary leavePaid time offParental leaveShow morechevron down Full job description Title RD Data ScientistEngineer    This position is a part time remote opportunity   BELONG CONNECT GROW with KBR   Around here we define the future We are a company of innovators thinkers creators explorers volunteers and dreamers But we all share one goal to improve the world responsibly and safely   KBR is rapidly developing capabilities across a range of engineering RD areas We are looking for candidates who have interest in the emerging field of edge computing and its applications that span UAVs to healthcare applications This position is for an RD data scientistengineer working on collecting analyzing and documenting 5G communication data for edge computing for SLAM and other applications suitable for solution in an edge network Assist in developing data collection process Analyze collected data and recommend modifications to the collection process Write technical documentation regarding SLEDGE edge computing   Required skills and traits   Candidates should have solid understanding of edge computing and previous experience in optimization and statistical analysis  FilteringSignal processingTraining and Validation data set preprocessing  Model Development including noise modeling  Experience performing statistical parameter estimation including familiarity with Maximum Likelihood Estimation MLE and Maximum APosterior Estimation MAP  Experience computing statistical validation metrics on large data sets including chisquare coefficient of determinism R2 and parameter confidence intervals  Experience in developing technical documentation  Track record of successful technical publications  15 years of experience    Other desired skills   Familiarity with models for wireless communication systems  Excellent verbal and written communication    Education Candidate must have a PhD under the discipline of Data ScienceEngineering   Basic Compensation  9712 per hour  14567 per hour  This range is for the California area only   The offered rate will be based on the selected candidate’s knowledge skills abilities andor experience and in consideration of internal parity   Additional Compensation  KBR may offer bonuses commissions or other forms of compensation to certain job titles or levels per internal policy or contractual designation Additional compensation may be in the form of sign on bonus relocation benefits short term incentives long term incentives or discretionary payments for exceptional performance   KBR BENEFITS  KBR offers a wide range of benefits for their employees we offer medical prescription dental vision ADD disability benefits retirement 401k travel benefits PTO holidays flexible work schedules parental leave military leave education assistance and the list goes on and on We also support career advancement through professional training and development   INCLUSION AND DIVERSITY AT KBR  At KBR we are passionate about our people sustainability and our Zero Harm culture These inform all that we do and are at the heart of our commitment to and ongoing journey toward being a more inclusive and diverse company That commitment is central to our team of teams philosophy and fosters an environment of real collaboration across cultures and locations Our individual differences and perspectives bring enhanced value to our teams and help us develop solutions for the most challenging problems We understand that by embracing those differences and working together we are more innovative more resilient and safer   KBR is an equal opportunity employer All qualified applicants will receive consideration for employment without regard to race color religion disability sex sexual orientation gender identity or expression age national origin veteran status genetic information union status andor beliefs or any other characteristic protected by federal state or local law   </data></node>
<node id="n708" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=f5e6e4a3b0592bcf&amp;bb=6um9CuQK5F1Am4GwBb0CRYiB27BWFvmObtvnwmwwOh6DqzcGz58H0IQrnJB5BtB71sJjOEIquXhd1864aeMxwgBFaaHchyV5KdSHOvDgrfJIdFp8fEjtTw%3D%3D&amp;xkcb=SoC567M3CNjXLyxSa50PbzkdCdPP&amp;fccid=a5b926a01ca57f85&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SharePointYesNoEducationDo you have a High school diploma or GEDYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeShift and scheduleWeekends as neededEvenings as neededOn call Location7501 West Memorial Road Oklahoma City OK 73142 Full job description Primary point of contact for Data Center technologies and related mechanical devices Participates in projects to design and implement mechanical and electrical improvements to data centers Develops procedures to outline data center processes and related tasks Participates and leads in the design and engineering data center technologies and future datacenter related initiatives Mentors Data Center Operations team members and helps increase their effectiveness and skillset Serves as a Level I escalation point for Data Center monitoring team to address criticalcomplex Data Center related issues Key contributor and driver of important challenging and highvisibility team projects  RESPONSIBILITIES   Acts as Subject Matter Expert and first level support for the operation and maintenance of the Data Center technologies  Helps lead small projects assigned to Data Center Team  Participates in projects related to data center design construction and certifications  Participates in projects related to data center design and architecture  Participates in projects related to data center construction and design to achieve Uptime Institute Data Center Tier levels  Participates in opportunities for improvement and architecture changes  Develops and implements strategies to ensure proper power cooling and other capacity needs  Implements strategies to ensure proper power cooling and other capacity needs of the Data Center to ensure availability  As required must be available on a 24hour basis for emergency support Only when required and considered abnormal to general operations  Establishes specifications by conferring with users analyzing workflow access information and security requirements  Maintains Data Center performance by performing monitoring and analysis performance tuning troubleshooting problems escalating problems to vendors ect…  Updates job knowledge by participating in educational opportunities reading professional publications maintaining personal networks participating in professional organizations  Protects organizations value by keeping information confidential  Accomplishes organization goals by accepting ownership for accomplishing new and different requests exploring opportunities to add value to job accomplishments  Documents specific duties activities problems solved and issues resolved  Attends meetings and serves on committees as requested  Regular attendance as required  Performs additional duties and assignments as requested   EducationCertification   High School graduate required  Bachelor’s degree required  CDCTP Certified Data Center Technician Professional  DCCA – Schneider Electric Data Center Certified Associate  BICSI – Data Center Design Consultant   Experience   7 years of related experience   Additional Requirements   Due to the nature of this position and the need for employees in this position to either work an oncall schedule or be on site within a short period of time the successful applicant must live within 45miles of the posted office location   PREFERRED QUALIFICATIONS  EducationCertification   Bachelor’s degree in Electrical or Mechanical Engineering Preferred   Experience   Expertise with Microsoft Office Office Word OneNote Excel PowerPoint SharePoint Teams  Advanced Zoom administrative user  Experience planning and managing executive meetings and events   Knowledge of   Datacenter HVAC CRAC CRAH cooling technologies  Datacenter power UPS generator operations etc  Datacenter fire suppression systems FM200 HFC125 etc  General knowledge of datacenter design and construction  LANWAN  Emerging Technology Trends  Datacenter Best Practice and Industry Standard Methodology   SkillsAbilities   Evaluate critical systems prioritize workflow and determine solutions  Excellent written and verbal communication skills  Interpret and apply laws regulations and policies  Read and understand technical manuals  Work for extended time at keyboardterminal  Maintain effective working relationships with supervisor and coworkers  Work flexible hours including weekends and evenings  Value open honest communication  Eager to be transparent and truthful as a primary matter of course    Paycom is an equal opportunity employer and prohibits discrimination and harassment of any kind Paycom makes employment decisions on the basis of business needs job requirements individual qualifications and merit Paycom wants to have the best available people in every job Therefore Paycom does not permit its employees to harass discriminate or retaliate against other employees or applicants because of race color religion sex sexual orientation gender identity pregnancy national origin military and veteran status age physical or mental disability genetic characteristic reproductive health decisions family or parental status or any other consideration made unlawful by applicable laws Equal employment opportunity will be extended to all persons in all aspects of the employeremployee relationship This policy applies to all terms and conditions of employment including but not limited to hiring training promotion discipline compensation benefits and separation of employment The Human Resources Department has overall responsibility for this policy and maintains reporting and monitoring procedures Any questions or concerns should be referred to the Human Resources Department To learn more about Paycoms affirmative action policy equal employment opportunity or to request an accommodation  Click on the link to find more information paycomcomcareerseeoc  </data></node>
<node id="n709" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=87fe6a6dc89ed3c0&amp;bb=6um9CuQK5F1Am4GwBb0CReMIwbbdpkgMio4kclv8tLC38TVsa3BSDPwx32J2PNlZW0TQjE6c46AYe4wsWJ15_NadU_pUl93VATN_91if8Bw%3D&amp;xkcb=SoAN67M3CNjXLyxSa50ObzkdCdPP&amp;fccid=9f38ffb6fcd14039&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid AWS Certified Solutions Architect certificationYesNoSkillsDo you have experience in SnowflakeYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay130000  170000 a yearJob typeFulltime Location25 Lafayette St Newark NJ 07102 BenefitsPulled from the full job description401k401k matchingDental insuranceHealth insuranceLife insurancePaid time offVision insurance Full job description    Job Classification   Technology  Engineering  Cloud       A GLOBAL FIRM WITH A DIVERSE  INCLUSIVE CULTURE        As the Global Asset Management business of Prudential we’re always looking for ways to improve financial services We’re passionate about making a meaningful impact  touching the lives of millions and solving financial challenges in an everchanging world       We also believe talent is key to achieving our vision and are intentional about building a culture on respect and collaboration When you join PGIM you’ll unlock a motivating and impactful career – all while growing your skills and advancing your profession at one of the world’s leading global asset managers       If you’re not afraid to think differently and challenge the status quo come and be a part of a dedicated team that’s investing in your future by shaping tomorrow today       At PGIM You Can        What you will do       In PGIM Fixed Income our technology group is a dynamic fastpaced environment with exciting changes on the horizon under new senior leadership We are in middle of a technology transformation to build a new platform on CloudAWS from ground up We are seeking a highly skilled and experienced handson Information Architect to join our team As Information Architect Data Modeler you will be responsible for designing and developing logical and physical data models that represent an organizations data requirements and business processes You will work closely with stakeholders database administrators developers and data analysts to ensure accurate and efficient data management        This will be a key role in our new technical initiative and will report into Enterprise Architecture If this sounds interesting then PGIM could be the place for you        What you can expect           Requirements Analysis Collaborating with business stakeholders and data users to understand their data needs business processes and reporting requirements This involves gathering requirements conducting interviews and analyzing existing systems               Data Modeling Creating logical and physical data models that represent the structure relationships and constraints of the organizations data This includes designing entityrelationship diagrams data flow diagrams and data dictionaries using industrystandard modeling techniques               Database Design Translating the logical data model into a physical database design that is optimized for performance scalability and data integrity This includes defining table structures indexes and data types while adhering to database management system DBMS constraints               Data Integration Collaborating with data integration teams to ensure seamless integration of data from various sources into the organizations databases This involves mapping data elements and transforming data to conform to the data model               Collaboration and Communication Working closely with crossfunctional teams such as database administrators developers and data analysts to align data model designs with system requirements Effective communication and collaboration are essential to ensure data model implementation aligns with business objectives               Documentation and Standards Documenting data models data definitions and data standards to provide clear guidelines for data management This documentation serves as a reference for developers data analysts and other stakeholders               Data Governance Collaborating with data governance teams to establish and enforce data management policies standards and best practices This involves ensuring compliance with data privacy regulations and promoting data stewardship          What you will bring           7 years of experience of in Data Modeling and Database Designing in Relational and NonRelational Database at least 5 years in Financial Industries focused on Fixed Income Products            Bachelors degree in Computer Science Information Systems or a related field            Proficiency in SQL data querying and performance optimization techniques            Proficiency in using Data Modeling tools like Erwin ER Studio and DBT            Expert experience in building Dimensional as well as highly normalized data models for OLTP            Familiarity with cloud infrastructure technologies like containers Kubernetes and serverless computing            Experience working with Data Governance tools like Collibra Informatica            Experience building catalogues business terms technical metadata and data lineage            Excellent problemsolving and analytical skills with the ability to translate business requirements into technical solutions            Strong experience with data structures data modeling data lineage and data catalogues          What will set you apart           Experience with Snowflake highly desirable            Certifications in cloud platforms or data management eg AWS Certified Solutions Architect etc are highly desirable       PGIM welcomes all applicants even if you dont meet every requirement If your skills align with the role we encourage you to apply      What we offer you           Medical dental vision life insurance and PTO Paid Time Off            Retirement plans            401k plan with generous company match up to 4            Companyfunded pension plan            Wellness Programs to help you achieve your wellbeing goals including up to 1600 a year for reimbursement of items purchased to support personal wellbeing needs            WorkLife Resources to help support topics such as parenting housing senior care finances pets legal matters education emotional health and career development            Tuition Assistance to help finance traditional college enrollment approved degrees many accredited certificate programs and industry designations          To find out more about our Total Reward package visit Work Life Balance  Prudential Careers       Note Prudential is required by state specific laws to include the salary range for this role when hiring a resident in applicable locations The salary range for this role is from 130000 to 170000 Specific pricing for the role may vary within the above range based on many factors including geographic location candidate experience and skills Roles may also be eligible for additional compensation andor benefits Eligibility to participate in a discretionary annual incentive program is subject to the rules governing the program whereby an award if any depends on various factors including without limitation individual and organizational performance       About PGIM Fixed Income Group       PGIM Fixed Income is a global asset manager offering active solutions across all fixed income markets With approximately 1000 employees and 793bn assets under management the company has offices in Newark London Letterkenny Amsterdam Munich Zurich Tokyo Hong Kong and Singapore Our business climate is a safe inclusive environment centered around mutual respect intellectual honesty transparency and teamwork Our leaders are focused on talent  culture dedicated to fostering growth  development at all levels to develop the industry leaders of tomorrow       About PGIM – Global Asset Management       PGIM is the global asset management business of Prudential Financial Inc NYSE PRU a leading global investment manager with nearly US127 trillion in assets under management as of June 30th 2023 With offices in 18 countries PGIM’s businesses offer a range of investment solutions for retail and institutional investors around the world across a broad range of asset classes including public fixed income private fixed income fundamental equity quantitative equity real estate and alternatives       With a history dating back 145 years and experience through more than 30 market cycles PGIM takes a longterm view not only in our investment philosophy but also in how we develop our talent We want to see our employees excel from their first day with the firm and throughout their tenure with PGIM We will inspire you support you and help you reach your greatest personal and professional aspirations If PGIM sounds like the place for you join us For more information about PGIM visit PGIMcom       Prudential Financial Inc of the United States is not affiliated with Prudential plc which is headquartered in the United Kingdom       Our Commitment to Diversity Equity and Inclusion       Prudential Financial Inc is focused on creating a fully inclusive culture where all employees feel comfortable bringing their authentic selves to work We don’t just accept difference—we celebrate it support it and thrive on it At Prudential employees have a unique opportunity to build their career path by owning their development their career and their future We encourage employees to hone their skills and explore continued opportunities within Prudential       LIMM1                                                                       Prudential Financial Inc of the United States is not affiliated with Prudential plc which is headquartered in the United Kingdom                                                                 Prudential is a multinational financial services leader with operations in the United States Asia Europe and Latin America Leveraging its heritage of life insurance and asset management expertise Prudential is focused on helping individual and institutional customers grow and protect their wealth The companys wellknown Rock symbol is an icon of strength stability expertise and innovation that has stood the test of time Prudentials businesses offer a variety of products and services including life insurance annuities retirementrelated services mutual funds asset management and real estate services                                                                 We recognize that our strength and success are directly linked to the quality and skills of our diverse associates We are proud to be a place where talented people who want to make a difference can grow as professionals leaders and as individuals Visit                                                               wwwprudentialcom                                to learn more about our values our history and our brand                                                                 Prudential is an equal opportunity employer All qualified applicants will receive consideration for employment without regard to race color religion national origin ancestry sex sexual orientation gender identity national origin genetics disability marital status age veteran status domestic partner status  medical condition or any other characteristic protected by law                                                                 The Prudential Insurance Company of America Newark NJ and its affiliates                                                                 Note that this posting is intended for individual applicants Search firms or agencies should email Staffing at                                                               staffingagenciesprudentialcom                                for more information about doing business with Prudential                                                                 PEOPLE WITH DISABILITIES                                If you need an accommodation to complete the application process which may include an assessment please email                                                               accommodationshwprudentialcom                                                                                                Please note that the above email is solely for individuals with disabilities requesting an accommodation If you are experiencing a technical issue with your application or an assessment please email                                                               careerstechnicalsupportprudentialcom                                to request assistance                                                                    </data></node>
<node id="n710" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=14c7ced6eb4b5a8d&amp;bb=6um9CuQK5F1Am4GwBb0CRccptyNIUyg7BkbypdyLzWd0B1c66IshCu6im3iFpURM95gEskdN2I1EQABN_TElVXJeDU-om4DEEZH4Det4WbmPEKFyLIOG6A%3D%3D&amp;xkcb=SoCQ67M3CNjXLyxSa50NbzkdCdPP&amp;fccid=be90d89fb010b0bf&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Statistical analysisYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay124000  186000 a yearJob typeFulltime LocationRemote Full job description    Ready for a new challenge Join Guidewire as a Data Engineer We are building a new team to take advantage of our market leading position and access to data to build advanced predictive models and variables to drive the next generation of growth in the PC insurance market This is an opportunity to come on board early and shape our direction and also have a broad impact on the insurance industry   Responsibilities   Participate in multiple simultaneous projects with a focus on data preparation and pipeline development for predictive modeling and GenAI Includes  Data interrogation exploratory data analysis profiling and reconciliation  Data pipeline creation revision and monitoring based on product manager and data scientistactuarial input  Feature creation initial testing and ongoing management  Data reporting  Work collaboratively with other staff including product managers data scientists other data analysts architects to solve problems develop innovative approaches and provide project status  Create and manage data assets datasets features and code with a focus on simplicity scalability reusability and clear documentation  Additional work assignments not related to specific project work such as new feature development external data research providing user feedback on new products and tools under development   Qualifications   Bachelors or Masters degree or equivalent industry experience  Passion for solving complex data problems  710 years of relevant experience  Hands on experience building modeling datasets and features for predictive modeling  Expert SQL skills including advanced functions table design view creation stored procedure and script design and coding general performance tuning  Expert data profiling and analysis skills including statistical analysis  Experience building ML data pipelines and working with feature stores  Indepth knowledge of Python  Experience with AWS data capabilities like Airflow Glue Redshift EMR  Exposure to developing and automating data quality monitoring  Knowledge of USCanadian PC standard lines of business is a must  Knowledge of PC policy administration and claims processing concepts is a must  Exposure to Guidewire ClaimCenter andor PolicyCenter data a plus  Exposure to and understanding of NLP and other ML techniques  Exposure to AWS Sagemaker and Feature Store a bonus  Experience and skill in communicating insights derived from data to both technical and nontechnical personnel  Ability to plan and carry out projects with minimal direction  Selfmotivated and detailoriented with desire to solve problems      The US base salary range for this fulltime position is 124000  186000  bonus  equity  benefits Our salary ranges are determined by role level and location The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations Within the range individual pay is determined by work location and additional factors including jobrelated skills experience and relevant education or training Your recruiter can share more about the specific salary range for your preferred location during the hiring process       LIREMOTE       feature       dataengineer predictivemodeling propertyandcasualtyinsurance      About Guidewire     Guidewire is the platform PC insurers trust to engage innovate and grow efficiently We combine digital core analytics and AI to deliver our platform as a cloud service More than 540 insurers in 40 countries from new ventures to the largest and most complex in the world run on Guidewire      As a partner to our customers we continually evolve to enable their success We are proud of our unparalleled implementation track record with 1600 successful projects supported by the largest RD team and partner ecosystem in the industry Our Marketplace provides hundreds of applications that accelerate integration localization and innovation      For more information please visit wwwguidewirecom and follow us on Twitter GuidewirePandC      Guidewire Software Inc is proud to be an equal opportunity and affirmative action employer We are committed to an inclusive workplace and believe that a diversity of perspectives abilities and cultures is a key to our success Qualified applicants will receive consideration without regard to race color ancestry religion sex national origin citizenship marital status age sexual orientation gender identity gender expression veteran status or disability All offers are contingent upon passing a criminal history and other background checks where its applicable to the position      Disability Accommodations and Guidewire’s Appeals Process Guidewire provides accommodations to the hiring process to create a fair opportunity for candidates with disabilities to contend for open positions Accommodation requests should be directed to 650 3564940 or Accommodationsguidewirecom If things do not go as hoped we invite you to use our appeals process Guidewire promises to independently review any denied accommodation and any decision not to offer you the position The appeals process is the same in either case Within five business days of receiving a notice of denial of an accommodation or receiving a notice of your nonselection for a vacancy call 650 3564940 or email Accommodationsguidewirecom to make an appeal Guidewire will assign a new decisionmaker to review the request andor hiring decision who will then notify you in writing of a decision within 10 business days    </data></node>
<node id="n711" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=cbdb712f306acc0d&amp;bb=6um9CuQK5F1Am4GwBb0CRdQSMSpxjagQ2rFs8Mm1O7hBa6ZgNvySnARj9cWTV0ki6qkujgOvxepQj2OgCXoRWcpsxp-TCWpkX_fA_TBuhmY%3D&amp;xkcb=SoAk67M3CNjXLyxSa50MbzkdCdPP&amp;fccid=c30fa7d623eff08c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid TSSCI with Polygraph licenseYesNoCertificationsDo you have a valid AWS Certification certificationYesNoSkillsDo you have experience in Web servicesYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationAnnapolis Junction MD 20701 BenefitsPulled from the full job description401k matchingHealth insuranceHealth savings accountPaid time offPaid training Full job description  What you will be doing  The Software Engineer shall be responsible for collaborating with system and software engineers to design and develop custom Kubernetes operators to manage the deployment of various SQL databases in support of the Database as a Service DBaaS mission solution for the CASA Compute Environment  What do you need   Experience using the Linux CLI to perform file system operations and package management  Experience writing BashPython scripts to automate administrative tasks and workflows  Experience developing with multiple programming languages such as Java Python and Go in a Linux environment  Experience with containerization technologies such as Docker  Experience with SQL technologies such as MySQL MariaDB and PostgreSQL  Experience with Kubernetes operators to automate the management of complex applications throughout their lifecycle  Experience with creating Helm Charts for Kubernetes  Experience with CICD concepts principles methodologies and tools such as GitLab  Experience with Git Version Control System       Desired Skills    Familiar with container orchestration technologies such as Kubernetes  Experience with the Atlassian Tool Suite eg JIRA Confluence    Clearance Active TSSCI with an appropriate polygraph is required to be considered for this role   LCAT Description The Software Engineer designs develops tests deploys documents maintains and enhances complex and diverse software systems based upon documented requirements These systems might include but are not limited to processing intensive analytics novel algorithm development manipulation of extremely large data sets realtime systems business management information systems and systems which incorporate data repositories data transport services and application and systems development and monitoring Works individually or as part of a team Reviews and tests software components for adherence to the design requirements and documents test results Resolves software problem reports Utilizes software development and software design methodologies appropriate to the development environment Provides specific input to the software components of system design to include hardwaresoftware tradeoffs software reuse use of Open Source Software OSS andor Commercial OffTheShelf COTS Government OffTheShelf GOTS software in place of new development and requirements analysis and synthesis from system level to individual software components Experience developing in UNIX Ability to perform shell scripting Working knowledge of Configuration Management CM tools and Web Services implementation  The Level 2 Software Engineer SWE shall possess the following capabilities   Analyze user requirements to derive software design and performance requirements  Debug existing software and correct defects  Design and code new software or modify existing software to add new features  Write or review software and system documentation  Integrate existing software into new or modified systems or operating environments  Develop simple data queries for existing or proposed databases or data repositories  Software development using languages such as C C Python Ruby Perl JavaScript etc  Has experience with agile development processes  Has experience with source code control systems such as Git  Serve as team lead at the level appropriate to the software development process being used on any particular project  Design and development of relational and nonrelational database applications  Use of orchestration frameworks such as Spring and Kafka  Familiarization with queue management systems  Develop or implement algorithms to meet or exceed system performance and functional standards  Develop and execute test procedures for software components  Develop software solutions by analyzing system performance standards and conferring with users or system engineers analyzing systems flow data usage and work processes and investigating problem areas  Modify existing software to adapt to new hardware or to improve its performance  Design develop and modify software systems using scientific analysis and mathematical models to predict and measure outcomes and consequences of design decisions  Java development using the Eclipse IDE Integrated Development Environment  Development of Java 2 Enterprise Edition J2EE applications  Experience using collaboration and software development tools ie Atlassian  Software development using continuous integration practices  Experience with container technologies ie Docker  Unix shell scripting  Development of event driven or data driven analytics  Development of cloudbased solutions and technologies  Design or implement complex algorithms requiring adherence to strict timing system resource or interface constraints Perform quality control on team products  Recommend and implement suggestions for improving documentation and software development process standards  Oversee one or more software development teams and ensure the work is completed in accordance with the constraints of the software development process being used on any particular project  Confer with system engineers and hardware engineers to derive software requirements and to obtain information on project limitations and capabilities performance requirements and interfaces  Coordinate software installation on a system and monitor performance to ensure operational specifications are met    SWE2 Qualifications Masters degree in Computer Science or related discipline from an accredited college or university plus three 3 years of experience as a SWE in programs and contracts of similar scope type and complexity  OR  Bachelors degree in Computer Science or related discipline from an accredited college or university plus five 5 years of experience as a SWE in programs and contracts of similar scope type and complexity  OR  Seven 7 years of experience as a SWE in programs and contracts of similar scope type and complexity   Clearance Active TSSCI with an appropriate polygraph is required to be considered for this role   Who are we Praxis Engineering was founded in 2002 and is headquartered in Annapolis Junction MD  with growing offices in Chantilly VA and Aberdeen MD  Praxis Engineering is a consulting product and solutions firm dedicated to the practical application of software and system engineering technologies to solve complex problems  With over 300 employees supporting more than 50 contracts Praxis brings together world class engineers with proven engineering best practices domain expertise commercial technologies and proven agile management approaches to create high value solutions aimed at helping our customers meet their most critical business and mission objectives  Praxis Engineering is a wholly owned subsidiary of General Dynamics IT    Why Praxis  We are focused on continual learning and evolution We don’t do things because “that’s the way we’ve always done things” we listen to our employees and adapt to the changing marketplace We look at the big picture and encourage our engineers to get training and certifications in emerging technologies that will help shape our customer’s mission Weve been profitable year after year Were always on the lookout for great engineers to join the team and we recognize that our employees are the heart and soul of what we do We focus on recruiting talented people treating them right and then allowing them to do what they do best No red tape No micromanagement Smart people want to work with smart people and we love people who are passionate about what they do and finding ways to do it better   And then there is the   Benefits   Attractive total compensation package to include competitive salary and medical benefits with an option for FREE employee HSA medical plan   Office perks such as free soft drinks and snacks both healthy and notsohealthy  Praxis swag annual gift certificate to purchase top brand Praxis apparel  401k contributionmatch combination of profit sharecontribution 35 and employer match up to 45 for a total of 8  Annual bonus plan  4 weeks Paid Time Off  10 holidays  comp time eligibility 30 days of leave to start       We reward longevity On your 5th work anniversary – you will receive an additional week of PTO to 5 weeks of PTO Making it 35 days of leave altogether  On your 10th work anniversary – you will receive an additional week of PTO to 6 weeks of PTO Making it 40 days of leave altogether  At any time your unused PTO can be traded in for     Carryover a max of 380hours of leave from year to year You can choose to have a sabbatical one year or trade in your unused PTO for something nice   Training is a priority Take advantage of our endless inhouse training opportunities  or seek out vendor offered paid training opportunities like conferences certification courses and seminars      Conferences recently attended by Praxis employees AWS Summit IoT World Black Hat and DefCon  Training  Certifications Splunk AWS Big DataCloudera VMWare Scrum Masterthe list of certifications goes on and on  Praxis University Cyber Research Data Analytics IoT AWS and RedHat course offerings and handson training   We truly believe the right worklife balance can exist and its here at Praxis Our work is extremely important but your job is just a part of who you are When you enjoy your life outside of our walls youre at your best the next time you walk through our doors We do all we can to assure that happens every day   Praxis Engineering provides equal employment opportunities EEO to all employees and applicants for employment without regard to race color religion sex sexual orientation gender identity national origin disability or veteran status or any other protected class   </data></node>
<node id="n712" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=cfe7dd2ded6a625f&amp;bb=6um9CuQK5F1Am4GwBb0CRZGVlmCkaq0b1orcQMD9zf1xlTIPB9yVgNcHWpAqylVLY8Q3yoMuImeUtQSf28DKOGbcrMFGno86Mswesgth87M%3D&amp;xkcb=SoDN67M3CNjXLyxSa50DbzkdCdPP&amp;fccid=fe2d21eef233e94a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNo Job detailsHere’s how the job details align with your profilePayFrom 79600 a yearJob typeFulltime LocationSeattle WA BenefitsPulled from the full job descriptionHealth insurance Full job description 3 years of analyzing and interpreting data with Redshift Oracle NoSQL etc experience Experience with data visualization using Tableau Quicksight or similar tools Experience with data modeling warehousing and building ETL pipelines Experience in Statistical Analysis packages such as R SAS and Matlab Experience using SQL to pull data from a database or data warehouse and scripting experience Python to process data for modeling Experience developing and presenting recommendations of new metrics allowing better understanding of the performance of the business  At AWS were working to be the most customer centric company on earth To get there we need exceptionally talented smart and driven individual The Data Center Availability team works with the hundreds of AWS data centers around the globe to deliver the highest quality and lowest cost physical security availability capacity and scaling results for our customers Our aim is to standardize operations globally by delivering tools policy processes and procedures to our internal teams The Availability team is seeking a Systems Development Engineer to design develop and implement software solutions for our Global Asset Management systems Our mission is to provide critical data to operators and engineers that enables them to make informed decisions for maintenance of data center facilities Our team of Systems Development Engineers builds webbased tools to provide data collection interfaces as well as data aggregation processes that vend the data to partner teams for further analytics    AWS Infrastructure Services owns the design planning delivery and operation of all AWS global infrastructure In other words we’re the people who keep the cloud running We support all AWS data centers and all of the servers storage networking power and cooling equipment that ensure our customers have continual access to the innovation they rely on We work on the most challenging problems with thousands of variables impacting the supply chain — and we’re looking for talented people who want to help    You’ll join a diverse team of software hardware and network engineers supply chain specialists security experts operations managers and other vital roles You’ll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers And you’ll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion    Key job responsibilities    Lead the design implementation and delivery of BI solutions in complex ambiguous or poorly defined problem spaces Proactively work to improve the consistency and integration between the team’s BI solutions and any related systems or artifacts owned by other teams Influence the team’s technical and business strategy by making insightful contributions to team priorities and lead in identifying and solving BI architecture deficiencies that limit the innovation Able to communicate your ideas effectively to achieve the right outcome for the team and customer and seek diverse perspectives listen to feedback and are willing to change direction if it creates a better outcome Lead design reviews for BI solutions or analyses for the team and actively participate in reviews for partner teams Actively participate in the hiring process as well as mentor others  improving their skills their knowledge of your BI solutions and their ability to get things done Influence technical decisions made by partner teams via collaborative software effort or by driving BI engineering best practices eg analytical rigor code quality data quality data modelling operational excellence automation visualization  We are open to hiring candidates to work out of one of the following locations    Herndon VA USA  Seattle WA USA     Experience with AWS solutions such as EC2 DynamoDB S3 and Redshift Experience in data mining ETL etc and using databases in a business environment with largescale complex datasets Excellent communication verbal and written and interpersonal skills to translate ambiguous business requirements into complex analyses and actionable insights  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus    Our compensation reflects the cost of labor across several US geographic markets The base pay for this position ranges from 79600year in our lowest geographic market up to 185000year in our highest geographic market Pay is based on a number of factors including market location and may vary depending on jobrelated knowledge skills and experience Amazon is a total compensation company Dependent on the position offered equity signon payments and other forms of compensation may be provided as part of a total compensation package in addition to a full range of medical financial andor other benefits For more information please visit httpswwwaboutamazoncomworkplaceemployeebenefits This position will remain posted until filled Applicants should apply via our internal or external career site </data></node>
<node id="n713" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=bc12bcb061a4a1a3&amp;bb=6um9CuQK5F1Am4GwBb0CRa8Mtk7BFMpARdJvYaCROXM28Oflll82Y5mBNGhlcmneshwcArkE0AmL5C1Nn0usiz-PLjmc2bhTKWIBr_dC6-s%3D&amp;xkcb=SoB567M3CNjXLyxSa50CbzkdCdPP&amp;fccid=fe2d21eef233e94a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Vulnerability assessmentYesNo Job detailsHere’s how the job details align with your profilePayFrom 115000 a yearJob typeFulltime LocationNew York NY BenefitsPulled from the full job descriptionHealth insurance Full job description 1 years of test automation frameworks and tools building experience 2 years of noninternship professional software development testing experience Experience programming with at least one modern language such as Java C or C including objectoriented design Experience in penetration testing and exploitabilityfocused vulnerability assessment  We are looking for a Software Dev Engineer in Test for Measurement Ad Tech and Data Science System to join our diverse team at Amazon in USA NY  Growing your career as a Full Time Software Dev Engineer in Test is an incredible opportunity to develop key skills  If you are strong in attention to detail emotional intelligence and have the right drive for the job then apply for the position of Software Dev Engineer in Test Measurement Ad Tech and Data Science at Amazon today    The ideal candidate will be detail oriented have strong organizational skills able to work independently juggle multiple tasks at once and maintain professionalism under pressure all while achieving high quality results You should have deep knowledge of software engineering practices and can drive design and implementation of test strategies in highly complex systems    Key job responsibilities  Convert manual regression tests to automated tests and maintain a high bar on the test quality    Own and improve the automated test coverage metrics    Use defined standards and best practices for testing of applications    A day in the life  Although each day may bring new challenges the majority of your time we will be creating test strategies new automation test tools and infrastructure We collaborate closely with software development teams to achieve a high quality releases that meet tight timelines without introducing new regressions or lowering product quality    We are open to hiring candidates to work out of one of the following locations    New York NY USA     Experience in working with AWS SDK and CDK strong understanding of AWS essentials  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus    Our compensation reflects the cost of labor across several US geographic markets The base pay for this position ranges from 115000year in our lowest geographic market up to 223600year in our highest geographic market Pay is based on a number of factors including market location and may vary depending on jobrelated knowledge skills and experience Amazon is a total compensation company Dependent on the position offered equity signon payments and other forms of compensation may be provided as part of a total compensation package in addition to a full range of medical financial andor other benefits For more information please visit httpswwwaboutamazoncomworkplaceemployeebenefits This position will remain posted until filled Applicants should apply via our internal or external career site </data></node>
<node id="n714" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=5630ec42e5127d07&amp;bb=6um9CuQK5F1Am4GwBb0CRf0Rk7K7KyiSxByhr6Efwpiju8JUrcgSX78Sh6pTpUTyfk20gRkKRSTJUU5hQ00hJegdGXay-ObW9s6T2KRE00Y%3D&amp;xkcb=SoDk67M3CNjXLyxSa50BbzkdCdPP&amp;fccid=5bd99dfa21c8a490&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid CCNP certificationYesNoSkillsDo you have experience in VoIPYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location901 W Trade St Charlotte NC 28202 Full job description Job Description   About us   At Bank of America we are guided by a common purpose to help make financial lives better through the power of every connection Responsible Growth is how we run our company and how we deliver for our clients teammates communities and shareholders every day   One of the keys to driving Responsible Growth is being a great place to work for our teammates around the world We’re devoted to being a diverse and inclusive workplace for everyone We hire individuals with a broad range of backgrounds and experiences and invest heavily in our teammates and their families by offering competitive benefits to support their physical emotional and financial wellbeing   Bank of America believes both in the importance of working together and offering flexibility to our employees We use a multifaceted approach for flexibility depending on the various roles in our organization   Working at Bank of America will give you a great career with opportunities to learn grow and make an impact along with the power to make a difference Join us   Job Description  This job is responsible for performing commoditized activities which may include monitoring managing events servicing requests and engineering Key responsibilities may include assisting with network systems applications access requests production support or security engineering and consistently performing activities independently   Overview   The role of the Data Network Implementation Engineer is to provide technical implementation services to support the evolution and ongoing support of the IP Network in response to projects all types upgrades service and feature enhancements and for remediationbreakfix services The work is always in alignment to the current and approved architectural roadmaps technology standards and templates governance and change management policies set forth by the firm While the role primarily has an implementation and validation engineering focus a strong understanding of design engineering concepts is required   The technology areas of focus for the role includes Data Center and end user networks public and private Network Transport and Optical systems and emerging technologies SwitchedEthernet LTE etc CorporateBranch and Building Networking including Wireless Knowledge in related technology areas such as Voice and Voice over IP VoIP solutions is a plus Network Appliance Video and Unified Communications knowledge are important   This role also provides networking technical support to Network Operations and partners with the Design and Architecture Engineering teams An understanding of the role of Operations troubleshooting practices and the use of proactive and reactive tools is important to the Data Network Implementation Engineer’s role They must be mindful of how their role impacts the firm’s business and reputation Concepts such as driving value always delivering quality and understanding how their work impacts service resiliency CR are important Understanding risk and having strong decisionmaking skills on assessing risk to the production network is essential to the success of the Network Implementation Engineer   This role is part of a global solutions and service delivery organization This position will interface with several collaborators internal and external customerssuppliersproviders architecture product engineering product management finance and business management and operations teams At times they may interface with various levels of senior management Strong written verbal and presentation skills are a must The candidate must be able to work on their own and successfully in team settings in various sizes and locations Adherence and use of standards product sets templates systems and artifacts are important to the success of the engineer the department and the firm at large   Responsibilities   Monitors all installed systems and infrastructure to ensure the highest levels of availability within a technical domain  Opens triage bridge lines and updates bridge boards engaging teams as required  Maintains solutions that align to security redundancy and archiving of blueprints and strategies  Owns event management and fulfilment items such as password resets and reporting requests  Supports client onboarding by familiarizing new clients with technology products and services as quickly and easily as possible  Writes and maintains documentation such as scripts and instructions and supports change activities  Candidate must understand how designs turn into Implementation Implementations that are based upon standards and predefined strategies  Works with Design and Architecture Engineering in a “knowledge sharing” capacity in support of the team’s adoption and successful delivery of technology new systems or process changes  Plays a strong role in project delivery lifecycle management Partners with the design Engineering team to create detailed implementation plans for all design test and Accept criteria BackOut and validation plans and procedures Adheres to project closeout practices such as asset tracking inventories chargeback documentation and the related systems tools and process updates  The Implementation Engineer stands between the project manager design engineer and operations They support initiatives as they transition between design implementation and operations They are active in all phases as an SME to support solve program and facilitates decision making The goal is to help the orderly and timely execution of projects and initiatives in an optimal strategic and low risk way  Role performs QA function prior to every implementation project Reviews a design and understands how it is based upon standards and how it matches to the “commission” details installation turnup Any questions must be review with the Design Engineer  Technical areas of focus include but are not limited to Core WANMAN Technologies – MPLSE MetroE Leased Line Broadband Direct Internet Access Dark Fiber DWDM Carrier Circuits Tunneling protocols – SSLTLS IPDESC GRE DMVPN MACSEC Routing Switching Firewalls Load Balancers LAN TCPIP DNS UDP Latency NAC QoS CAC EIGRP BGP ISIS OSPF Multicast NHRP ATM PPP IPv4 IPv6 MPLS ACL VNP Building WirelessWiFi solutions 802xx and carrier “inbuilding wireless services DAS etc Knowledge of Cable Systems a plus    Required Qualifications   2 years’ experience required in Data Networkingrelated disciplines in design operate and implementation services  Design Implementation Support SDWAN is required preferred if on Palo Alto Cloudgenix  Design Implementation Support SDACCESS is required preferred if on Cisco DNA Center  Design Implementation Support Wireless is required preferred if on HPE Aruba  Experience with CISCO NSO and general experience in Network Automation tools and processes  Strong skills in Scripting required Python Ansible Perl PowerShell  Strong Automation Skills – Expect Rancid RESTSOAP  Software development experience should think like a software developer  Strong knowledge of Network Security  Must adhere to global network design authority processes and procedures  Must demonstrate good computer skills and the use of various applications such as MS Office MS Visio  Ability to recognize opportunities for automation and process improvements  Ability to assemble professional documents and artifacts  Extensive knowledge and experience using both reactively and productive advanced tooling includes but is not limited to Snifferwire Shark Prognosis Scripting SolarWinds HP NAOpsWareOpenView etc  Application experience with an innovation “mindset” and how it impacts how engineering work gets done  VoiceVoIPUC and Video knowledge understanding of VoIPUC systems ie Cisco CUCM AVAYA Communications Manager Microsoft LYNC and Skype for Business etc    Desired Qualifications   Leadership Selfstarter selfdirected and shows initiative  Coding and application design experience  Wireless LAN Aruba  Crisis Management experience  Experience working in an Agile environment  Focused on execution delivery and commitment to dates  Can tie strategy and actions to business impact and results  Demonstrates ownership Is accountable and can hold others accountable professionally  Bachelor’s degree in engineering computer science related field and or technical training  Industry Certifications in CCDP CCNA CCNP and CCIE      Associated certifications JUNOSJNCP Cisco CCVP and CCIPVoice     Skills   Analytical Thinking  Collaboration  Influence  Production Support  Result Orientation  Adaptability  Business Acumen  Innovative Thinking  Solution Delivery Process  Solution Design  Automation  DevOps Practices  Project Management  Risk Management  Stakeholder Management    Shift 1st shift United States of America    Hours Per Week 40  </data></node>
<node id="n715" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=c861387fe1303f35&amp;bb=6um9CuQK5F1Am4GwBb0CRXpGdK_WcI4Piiajbat7m4pKR_Drf1bVFmalUiA7FiwFUfKUoBdyL_YdHyzMu7JANBFDl-rqag3uVG0O-Cu7WVs%3D&amp;xkcb=SoBQ67M3CNjXLyxSa50AbzkdCdPP&amp;fccid=fe2d21eef233e94a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid PMP certificationYesNoSkillsDo you have experience in Data center experienceYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeShift and scheduleOn call LocationHerndon VA Full job description Bachelor’s degree in Mechanical Engineering Electrical Engineering or an equivalent engineering science plus 3 years of relevant controls experience OR 8 years of relevant controls experience in lieu of a degree 3 years of experience with industrial controls in critical environment data center pharmaceutical manufacturing oil  gas petrochemical laboratory power water etc 2 years of general project or vendor management experience request for proposals bidding change orders quality control and RFI and submittal tracking associated with construction and project execution  As part of the global controls team you will work with highly motivated experts and innovators in the data center industry You will be responsible for troubleshooting project management and maintaining the building management system BMS and electrical power monitoring system EPMS Using Amazon leadership principles you will develop new processes and standards while innovating in the controls space    AWS Data centers have multiple components such as generators uninterruptable power sources diesel generators electrical switchgear power distribution units variable frequency drives automaticstatic transfer switches chillers aircooled and watercooled pumps cooling towers heat exchangers CRAHs air economizers etc All these components have local control systems that interact with each other via open andor proprietary communications protocols The BMS is the primary method of control of all mechanical systems within a data center The EPMS is the primary method of monitoring all electrical systems within a data center    Key job responsibilities    As a Data Center Controls Engineer you will   Troubleshoot and perform Root Cause Analysis or Corrective Action for BMS and EPMS related issues in AWS data centers Train and assist internal customers and stakeholders with the creation design configuration validation installation commissioning and operation of BMS and EPMS systems Provide technical assistance and support to operations during life cycle of the data center Review results and action items from the quarterly maintenances for BMS and EPMS and take actions to get them resolved Develop BMS  EPMS projects scope of work schedule budget and level of efforts LOE to projects requested by customers and stakeholders Manage scope schedule finance and execution of BMS and EPMS improvement projects in AWS data centers Assist in procurement related activities including request for quotationproposals responding to request for information review of vendors proposal and issuance of purchase orders Participate in AWS global oncall schedule to provide immediate BMS and EPMS technical support to inservice data centers Attend project related meetings coordinate with project leaders and regularly report status to Controls and stakeholders management Support Controls projects related commissioning activities in the data centers Review implement troubleshoot and iterate on the controls sequence of operation SOO and provide necessary feedback to the design team Develop and modify controls logic programming and graphical user interfaces Manage multiple stakeholder deliverables requirements and navigate challenging situations Financially manage BMS and EPMS service contracts Frequently visit locally assigned inoperation data centers to troubleshoot meet customers supervise vendor’s work to ensure compliance with the scope design SOO and applicable local codes  We are open to hiring candidates to work out of one of the following locations    Herndon VA USA     MS in Engineering Mechanical Electrical Chemical ControlsAutomation Experience designing configuring programming installing troubleshooting or servicing HVAC Controls or Electrical SCADA systems application specific controllers software and networks Experiencing using common industrial communication protocols MQTT BACnet andor MODBUS Demonstrated understanding of engineering documentation electrical and mechanical diagrams and standard operating procedures Ability to manage multiple stakeholder deliverables requirements and navigate difficult situations Experience designing data centers or critical MEP infrastructure Registered as a Professional Engineer PE or certified Project Management Professional PMP Prior AWSAmazon experience 4 Yr Military Service  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus </data></node>
<node id="n716" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=21c4c57f601cf095&amp;bb=6um9CuQK5F1Am4GwBb0CRahu4SXuDvw-jeIKNG-1ncD8ZA-cAauZYS4XqtoYSAF6U3xq4JhM8_61IiNgOoMbB68qiA6k6zYuQzevBOHG1s4%3D&amp;xkcb=SoBq67M3CNjXLyxSa50GbzkdCdPP&amp;fccid=fe2d21eef233e94a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Software deploymentYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationCanton MS Full job description Bachelor’s Degree in Electrical Engineering or equivalent experience 3 cumulative years of experience with industrial or commercial engineering in Mission Critical facilities including but not limited to data centers power generation oilgas facilities Experienced Engineer  As an Amazon Field Engineer you will provide full lifecycle support to AWS Data Centers from design inception through site improvement and maintenance You will be the ‘go to’ engineering resource for your region when technical advice is needed and will use your subject matter expertise and engage with diverse teams to    Perform design and equipment submittal review for new Data Centers in your region Troubleshoot conduct Root Cause Analysis RCA and create Corrective Action CA documentation for siteequipment failures Directly support operational issues with adhoc training complex operating procedure reviews including critical equipment and event support Own the conceptual design for existing data center upgrades and designsolutions which add capacity improve availability increase efficiency and sustainability Interface with internal data center operations team data center design engineering team server hardware team environmental health and safety team to promote standards that maintain consistency and reliability in services delivered Develop innovative solutions for AWS’s data centers Work on concurrent projects sometimes in multiple geographical regions Initiate and lead engineering audits including onsite visits within Amazon’s data centers Produce reports outlining risks with recommended mitigations and remediation Act as resident engineer during new construction projects Support construction commissioning and turnover  AWS Infrastructure Services owns the design planning delivery and operation of all AWS global infrastructure In other words we’re the people who keep the cloud running We support all AWS data centers and all of the servers storage networking power and cooling equipment that ensure our customers have continual access to the innovation they rely on We work on the most challenging problems with thousands of variables impacting the supply chain — and we’re looking for talented people who want to help    You’ll join a diverse team of software hardware and network engineers supply chain specialists security experts operations managers and other vital roles You’ll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers And you’ll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion    Key job responsibilities  Amazons vision is to be the worlds most customercentric company and this role is key to that vision As a Field Engineer you will be leading projects to fit out our data centers to meet everevolving customer needs as we continue expanding our fleet to hyperscale As an ideal candidate you    Possess Strong Engineering Judgement and are able to provide recommendations despite uncertaintyambiguity Are detail and data oriented Have experience solving problems with engineered solutions Have experience managing engineering projects and consultants Build trust and relationships with different stakeholders eg Operations Controls Construction Design Commissioning Product Managers Technical Program Managers  Are adaptable and inclined to get into the field to see things up close Excited about a mix of office and field work  A day in the life  Each day you will interact with different teams responsible for all aspects of the data centers You will prioritize your activities to support data center capacity availability and safety focusing on the actions that are most impactful You will have the opportunity to work on projects locally and globally We have an immediate opening for a Field Engineer in America If you meet these qualifications exude passion and enjoy the challenge of innovative projects at hyperscale this job is for you    About the team  Why AWS  Amazon Web Services AWS is the world’s most comprehensive and broadly adopted cloud platform We pioneered cloud computing and never stopped innovating — that’s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses    Diverse Experiences  Amazon values diverse experiences Even if you do not meet all of the preferred qualifications and skills listed in the job description we encourage candidates to apply If your career is just starting hasn’t followed a traditional path or includes alternative experiences don’t let it stop you from applying    WorkLife Balance  We value worklife harmony Achieving success at work should never come at the expense of sacrifices at home which is why we strive for flexibility as part of our working culture When we feel supported in the workplace and at home there’s nothing we can’t achieve in the cloud    Inclusive Team Culture  Here at AWS it’s in our nature to learn and be curious Our employeeled affinity groups foster a culture of inclusion that empower us to be proud of our differences Ongoing events and learning experiences including our Conversations on Race and Ethnicity CORE and AmazeCon gender diversity conferences inspire us to never stop embracing our uniqueness    Mentorship and Career Growth  We’re continuously raising our performance bar as we strive to become Earth’s Best Employer That’s why you’ll find endless knowledgesharing mentorship and other careeradvancing resources here to help you develop into a betterrounded professional    We are open to hiring candidates to work out of one of the following locations    Canton MS USA     Organized and have the ability to set priorities and meet deadlines and budget Possess leadership and problemsolving skills Experience using a variety of web based and other software tools for calculation and data processing Direct experience with the design construction operation or maintenance of mission critical facilities especially data centers Experience as resident engineer or handson in the field design consultant or owner’s engineer Knowledge of building codes and regulations for your region Experience reading interpreting and creating construction drawings specifications and submittal documents Ability to carry design concepts through exploration development and into deploymentmass production Possess excellent communication and writing skills attention to detail maintain high quality standards Basic understanding of both mechanical andor electrical equipmentdesign related to data centers Including but not limited to uninterruptable power sources diesel generators electrical switchgear power distribution units variable frequency drives automaticstatic transfer switches chillers aircooled and watercooled pumps cooling towers heat exchangers CRAHs fans air economizers water treatment etc EPMSSCADABMS Controls system experience software andor hardware  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus </data></node>
<node id="n717" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=3e55228a6a1266ab&amp;bb=6um9CuQK5F1Am4GwBb0CRUlExxu2RwPSgRI19yfejZGhtHC96-Gdn2oDE0LAG_PONHzFWS1QCDGlBgfh8U9ErrZxo0Kf5Nc7V-I8gvgXyP1i6Hp_cbvhkA%3D%3D&amp;xkcb=SoD367M3CNjXLyxSa50FbzkdCdPP&amp;fccid=e2856b343cf52d7c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationPittsburgh PA 15238 Full job description  SUMMARY  As a Data Engineer for our Digital Services division you will drive the evolution of our next generation data infrastructure which powers a wide range of analytics and software products You will be responsible for designing developing and maintaining advanced data pipelines using the latest technologies in cloud computing and data processing In this pivotal role you will collaborate with crossfunctional teams including data scientists software engineers and business analysts You will also use your excellent soft skills to interface with nontechnical stakeholders and ensure that their needs are met in the form of functional and nonfunctional requirements  RESPONSIBILITIES  Design build and maintain our data infrastructure to support analytics and integration into software products and services Partner with data scientists to build processing pipelines that enable them to rapidly create highquality machine learning models Work with software engineers to develop and maintain pipelines to feed data to our growing software portfolio Manage ETLELT processes as code and foster a codebase that is extensible and maintainable by all team members Manage the transition from quick prototype to full scalable solution that supports production products Identify recommend and implement innovative approaches and technologies that will evolve our existing processes  Qualifications  Education and experience requirement  BSMS Degree in Computer Science Engineering Data Analytics or related field 3 years’ experience in data engineering ETL development or similar role Strong problemsolving skills to discover address and resolve issues Demonstrated ability to translate technical concepts into actionable insights for different stakeholders    Language and technical skills requirement  Proficient in programming languages such as Python and SQL and development tools such as Git Experience with cloud platforms such as Azure AWS or GCP for deploying and managing data infrastructure Skilled with rational database schema design Handson experience with data processing tools and platforms such as Snowflake Azure Synapse Azure Data Factory Data Bricks etc Solid understanding of machine learning models tools libraries and approaches Experience with business intelligence tools such as Power BI Spotfire Cognos Tableau Qlik     Physical demands and work environment  The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job Reasonable accommodation may be made to enable individuals with disabilities to perform the essential functions  Work is usually performed in an office environment with normal noise levels Involves prolonged sitting and computer usage Work may involve limited travel    Disclaimer  This job description is not intended and should not be construed to be an exhaustive list of all responsibilities skills efforts or working conditions associated with the job It is intended however to be an accurate reflection of those principal job elements essential for making decisions related to job performance employee development and compensation As such the incumbent may perform other duties and responsibilities as required Its contents imply no contractual obligation and may be changed by the company at any time   </data></node>
<node id="n718" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=14b834ae07897808&amp;bb=v29_5p9dArsk-aDq-tPpTJjq1qdZBPaRRRnLfmEhgwoylmi3g7PzxgdnJiQXkMvlK0vLnzwPW6YvKVJ4eS7DyHfqNccSMN7jInm_Mlz8VfE%3D&amp;xkcb=SoCK67M3CNjTYeRSRx0LbzkdCdPP&amp;fccid=11caadcdc98800d4&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid TSSCI with Polygraph licenseYesNoSkillsDo you have experience in Systems engineeringYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationBethesda MD BenefitsPulled from the full job description401k matchingInternal mobility programPaid time off Full job description    Type of Requisition   Regular       Clearance Level Must Currently Possess   Top Secret SCI  Polygraph       Clearance Level Must Be Able to Obtain   Top Secret SCI  Polygraph       Suitability        Public TrustOther Required   None       Job Family   Systems Engineering       Job Qualifications       Skills   Data Transformation ETL Geospatial Software      Certifications       Experience   8  years of related experience      US Citizenship Required   Yes       Job Description       Seize your opportunity to make a personal impact as an ETL Developer supporting customer activities GDIT is your place to make meaningful contributions to challenging projects and grow a rewarding career        At GDIT people are our differentiator As an ETL Developer you will help ensure today is safe and tomorrow is smarter Our work depends on a an ETL Developer joining our highly skilled team to be a premier provider of cyber security services to the customer We provide consummate cyber security risk management “as a service” platform across multiple fabrics and centers We have responsibility to ensure operational IT capabilities provide the client with necessary timeliness accuracy and security of information demanded from all our highly professional roles Be the change lead our change – join us        HOW ETL DEVELOPER WILL MAKE AN IMPACT     Perform data processing and normalization extracttransformload on a number of Customerdirect data sources  Develop templates or scripts to automate everyday ETL operation functions  Identify new tools and processes to improve the ETL processing  Developing testing and monitoring connections via REST API to interfacing systems such as data feeds from external organizations  Become an expert on an AWS data workflow that includes Lambda S3 and other such technologies  Development of technical documentation and briefing materials to support program status reviews control gates and other presentations as directed by program management  Development will take place in an iterative fashion using scrum techniques with inputs from multiple stakeholders with adherence to all reporting requirements  Requires exceptional flexibility and technical skills  Work will be done in a dynamic environment with multiple stakeholders and changing requirements  Meeting with stakeholders analyzing requirements user stories and related artifacts to determine technical specifications for the system environments  Coordinating with technical teams responsible for the cloud hosting infrastructures in order to establish and maintain the environments used by the program for the development test and deployment of the demonstration capability and the final production system  Collaborate with team members to build and maintain positive productive team relationships  Coordinate andor participate in system integration andor user acceptance testing  Write unit and integration test  Monitor applications in production  Participate in code reviews       WHAT YOU’LL NEED TO SUCCEED     Education Bachelor’s Degree Computer Engineering Computer Science Electrical Engineering Information Systems Information Technology Cybersecurity or a closely related discipline  Required Experience 8 yrs  Technical skills  Knowledge of standard ETL tools such as Pentaho AWS Glue etc  Understanding of JAVA Microservices  Understanding of REST API  Understanding of AWS data workflow that includes Lambda S3 and other such technologies  Security Clearance Level TSSCI with active polygraph  Location Bethesda MD  On Customer Site       GDIT IS YOUR PLACE     401K with company match  Comprehensive health and wellness packages  Internal mobility team dedicated to helping you own your career  Professional growth opportunities including paid education and certifications  Cuttingedge technology you can learn from  Rest and recharge with paid vacation and holidays       OpportunityOwned       GDITCareers       WeAreGDIT       JET       SWDevpolyVA        Scheduled Weekly Hours   40       Travel Required   Less than 10       Telecommuting Options   Onsite       Work Location   USA MD Bethesda       Additional Work Locations    We are GDIT A global technology and professional services company that delivers consulting technology and mission services to every major agency across the US government defense and intelligence community Our 30000 experts extract the power of technology to create immediate value and deliver solutions at the edge of innovation We operate across 30 countries worldwide offering leading capabilities in digital modernization AIML Cloud Cyber and application development Together with our clients we strive to create a safer smarter world by harnessing the power of deep expertise and advanced technology   We connect people with the most impactful client missions creating an unparalleled work experience that allows them to see their impact every day We create opportunities for our people to lead and learn simultaneously From securing our nation’s most sensitive systems to enabling digital transformation and cloud adoption our people are the ones who make change real   GDIT is an Equal OpportunityAffirmative Action employer All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability or veteran status or any other protected class  </data></node>
<node id="n719" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=5e034cbc49d22524&amp;bb=v29_5p9dArsk-aDq-tPpTOS-ujR3XGcYVxIfV-N9TRopnypcfZL7c77hXnt-wSRT57lwFcaJejuk6qByCZvcgrYDXgn-oSxO53t4F-Fux3U%3D&amp;xkcb=SoA-67M3CNjTYeRSRx0KbzkdCdPP&amp;fccid=91444a28cb181211&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in ScalaYesNoEducationDo you have a Bachelors degreeYesNo LocationRaleigh NC 27607 BenefitsPulled from the full job description401k matchingADD insuranceDental insuranceDisability insuranceHealth insuranceHealth savings accountPaid time offShow morechevron down Full job description  At NTT DATA we know that with the right people on board anything is possible The quality integrity and commitment of our employees have been key factors in our company’s growth and market presence By hiring the best people and helping them grow both professionally and personally we ensure a bright future for NTT DATA and for the people who work here    For more than 25 years NTT DATA Services have focused on impacting the core of your business operations with industryleading outsourcing services and automation With our industryspecific platforms we deliver continuous value addition and innovation that will improve your business outcomes Outsourcing is not just a method of gaining a onetime cost advantage but an effective strategy for gaining and maintaining competitive advantages when executed as part of an overall sourcing strategy    NTT Data is assisting our healthcare client with recruiting efforts for a 12 month remote AWS Data Engineer role     Location Raleigh NC   Job Summary  The AWS Data Engineer is to focus on a Data Modernization Project This role entails designing and building scalable and optimized data pipelines using key AWS components like Glue S3 Lambda and Redshift Serverless The engineer should be comfortable with Python and Scala coding as required The engineer should have a strong understanding of security best practices in relation to developing and maintaining data pipelines Also required is an understanding of data warehouse design best practices using the AWS stack Prior Healthcare experience pertaining to claims eligibility and clinical data sets is highly preferred    Essential Duties   Work collaboratively with senior leadership and other team members to design build and maintain optimized data pipelines using Glue PythonScala S3 Lambda and Redshift Maintain good code documentation and adhere to software lifecycle expectations including applicable configuration control using Gitlab Have a design code unit test document results mentality to all data pipeline code being developed before handing off to Quality control team for further testing Professionally effective within a fast paced and business objective driven environment selfmanaging deliverables and maintaining accountability to self and the team Work collaboratively with stakeholders like Project Management VendorCustomer partners other business function representatives and end users in an AgileWaterfall hybrid delivery environment Work with the Infrastructure team members as required to meet deliverables   Required   5 years experience in developing data pipelines of which utilizing AWS components 3 years of experience with AWS components Python and Scala coding experience Bachelors degree or equivalent work experience Understanding of data warehouse design best practices using the AWS stack   Preferences   Healthcare experience pertaining to claims eligibility and clinical data sets is highly preferred Knowledge of FHIRHL7 preferred   About NTT DATA Services    NTT DATA Services is a recognized leader in IT and business services including cloud data and applications headquartered in Texas As part of NTT DATA a 30 billion trusted global innovator with a combined global reach of over 80 countries we help clients transform through business and technology consulting industry and digital solutions applications development and management managed edgetocloud infrastructure services BPO systems integration and global data centers We are committed to our clients longterm success Visit nttdatacom or LinkedIn to learn more    NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race color religion citizenship national origin ancestry age sex sexual orientation gender identity genetic information physical or mental disability veteran or marital status or any other characteristic protected by law We are committed to creating a diverse and inclusive environment for all employees If you need assistance or an accommodation due to a disability please inform your recruiter so that we may connect you with the appropriate team    Where required by law NTT DATA provides a reasonable range of compensation for specific roles The starting hourly range for this remote role is 3548 Hourly This range reflects the minimum and maximum target compensation for the position across all US locations Actual compensation will depend on several factors including the candidates actual work location relevant experience technical skills and other qualifications    This position is eligible for company benefits that will depend on the nature of the role offered Company benefits may include medical dental and vision insurance flexible spending or health savings account life and ADD insurance shortand longterm disability coverage paid time off employee assistance participation in a 401k program with company match and additional voluntary or legally required benefits  </data></node>
<node id="n720" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=05628b21568732cc&amp;bb=v29_5p9dArsk-aDq-tPpTCX1f4uUpptZi-p4Rm2BmPTt4rcI2F7VuYNhWzTJiu04vm9Hf8_1JaV0Dv5zspKdYe_RA8eJpHTxJF0vFj5UDk6FH3LaYb4Q8w%3D%3D&amp;xkcb=SoCj67M3CNjTYeRSRx0JbzkdCdPP&amp;fccid=6042ff570f7fb459&amp;cmp=E--business-International-Inc&amp;ti=Data+Engineer&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Test automationYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay130000  150000 a yearJob typeFulltimeShift and schedule8 hour shiftDay shiftMonday to Friday LocationEBusiness International in Bakersfield CA 93309 BenefitsPulled from the full job descriptionDental insuranceHealth insurancePaid time offVision insurance Full job descriptionResponsibilities  Collaborate with crossfunctional teams to understand data requirements and design scalable data solutions on the Databricks platform Develop and maintain ETL processes to ingest transform and load data from various sources into Databricks Delta Lake Design and implement data models and schemas to ensure data accuracy consistency and reliability Optimize and tune data pipelines for performance and efficiency Work closely with Business analysts and other stakeholders to understand their data needs and provide timely data support Implement data security and privacy measures to ensure compliance with industry standards and regulations Monitor and troubleshoot data pipeline issues providing timely resolution to minimize downtime Stay abreast of the latest advancements in Data bricks and other relevant technologies to continually enhance data engineering practices  Qualifications  Bachelor’s degree in computer science Information Technology or a related field Proven experience as a Data Engineer with a focus on Databricks Strong proficiency in programming languages such as Python Scala or Java Handson experience with big data technologies including ApacheSpark and Databricks Delta Lake Solid understanding of data modeling ETL processes and data warehousing concepts Familiarity with cloud platforms such as AWS Azure or Google Cloud Excellent problemsolving and communication skills Ability to work collaboratively in a teamoriented environment  Preferred Qualifications  Databricks Certified Developer or Databricks Certified Data Engineer certification Experience with version control systems such as Git Knowledge of DevOps practices and tools for automated testing and deployment Familiarity with data streaming technologies eg Apache Kafka  Job Type Fulltime Pay 13000000  15000000 per year Benefits  Dental insurance Health insurance Paid time off Vision insurance  Compensation package  Yearly bonus  Experience level  10 years  Schedule  8 hour shift Day shift Monday to Friday  Application Questions  Are you a US Citizen Are you a Green Card Holder What is your email addressemail ID What is your Current Location Mention the location with zip code What would be your notice period if offered What is your salary expectation for fulltime per annum What is the reason for a job change from your current employment  Experience  AWS Data Engineer 10 years Preferred Data Engineer 10 years Preferred  Ability to Commute  Bakersfield CA 93309 Preferred  Ability to Relocate  Bakersfield CA 93309 Relocate before starting work Required  Work Location Hybrid remote in Bakersfield CA 93309 </data></node>
<node id="n721" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d29b20a7680e981b&amp;bb=v29_5p9dArsk-aDq-tPpTGteReRwNdPhXex0-SExOeaJvOaZMqb67UFAEeKNuvFQt-YtHB8zY4KBlt768pzfXnFVOwRpLtKttdhUPmoMBNgW8QCpVT0Fuw%3D%3D&amp;xkcb=SoAX67M3CNjTYeRSRx0IbzkdCdPP&amp;fccid=4d8305dc59101179&amp;cmp=hire-IT-people&amp;ti=Data+Engineer&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePayUp to 60 an hourJob typeFulltimeShift and schedule8 hour shift Location103 Pennsylvania Ave Charleroi PA 15022 BenefitsPulled from the full job description401kDental insuranceHealth insurance Full job descriptionRole ML Data Engineer with Vector DB and GenAI Skills – Band 4B  Location Pennsylvania  PA Duration Fulltime Summary We are seeking a passionate and skilled ML Data Engineer Band 4B to join our team in USA You will play a pivotal role in building and maintaining the data infrastructure and pipelines for our cuttingedge Generative AI applications You will collaborate closely with the Generative AI Full Stack Architect and MLOps Engineer to ensure the quality security and accessibility of data for our Generative AI models Responsibilities  Design develop and implement data pipelines for ingesting preprocessing and transforming unstructured data Image pdf Audio video for Generative AI model training and inference Need to have some level of understanding or working experience with Vector DB’s  Like Pinecone  Redis  Chroma Understanding on Large Language Model’s Llama GPT4  Claude 20  to do text summarization  entity extraction and classification Build and maintain efficient data storage solutions including data lakes warehouses and databases appropriate for largescale generative AI datasets Implement data security and governance policies to ensure the privacy and integrity of sensitive data used in Generative AI projects Collaborate with data scientists and engineers to understand data requirements for Generative AI models and translate them into efficient data pipelines Monitor and optimize data pipelines for performance scalability and costeffectiveness Stay uptodate on the latest advancements in data engineering tools and technologies eg Apache Spark Airflow Snowflake Data Bricks  and apply them to our Generative AI platform Document data pipelines and processes for clarity and transparency Communicate effectively with technical and nontechnical stakeholders about data quality and availability for Generative AI projects  Qualifications  Bachelor’s degree in computer science Data Science Statistics or a related field or equivalent experience 6 years of experience in data engineering or related roles such as data pipeline development data storage or ETLELT processes Proven experience building and maintaining data pipelines for machine learning projects Strong understanding of data modeling principles data quality measures and data security best practices Proficient in programming languages like Python SQL and scripting languages eg Bash Shell Familiarity with cloud platforms eg AWS GCP Azure for data storage and processing along with GenAI services like AWS BedRock Excellent communication collaboration and problemsolving skills Ability to work independently and as part of a team Passion for Generative AI and its potential to solve realworld challenges  Band 4B  Senior individual contributor with substantial data engineering expertise and leadership experience Manages complex data projects and initiatives with independent decisionmaking authority Provides technical guidance and mentorship to junior team members Has a demonstrated track record of success in delivering impactful data solutions  Job Type Fulltime Salary Up to 6000 per hour Expected hours No more than 4000 per week Benefits  401k Dental insurance Health insurance  Compensation package  Yearly pay  Experience level  10 years  Schedule  8 hour shift  Experience  Informatica 1 year Preferred SQL 8 years Preferred  Ability to Commute  Charleroi PA 15022 Required  Ability to Relocate  Charleroi PA 15022 Relocate before starting work Required  Work Location In person </data></node>
<node id="n722" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=1720c72967ffb8ca&amp;bb=v29_5p9dArsk-aDq-tPpTOdHqALYnVC2H56zYQFcIoIdxhtovIUVHiDtgrYAcFWlJc9TZDtxbByd_SVidceXZ-rZS3LGc09fr8jWcW6I9PUAaPnoMvCX3A%3D%3D&amp;xkcb=SoCZ67M3CNjTYeRSRx0PbzkdCdPP&amp;fccid=be4c5737f0408279&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in System designYesNoEducationDo you have a Bachelors degreeYesNo LocationArlington VA 22201 BenefitsPulled from the full job descriptionParental leaveTuition reimbursement Full job description Company Description  Publicis Sapient is a digital transformation partner helping established organizations get to their future digitallyenabled state both in the way they work and the way they serve their customers We help unlock value through a startup mindset and modern methods fusing strategy consulting and customer experience with agile engineering and problemsolving creativity United by our core values and our purpose of helping people thrive in the brave pursuit of next our 20000 people in 53 offices around the world combine experience across truly value Job Description  Publicis Sapient is looking for a Senior Associate Data Engineer Azure to be part of our team of topnotch technologists You will lead and deliver technical solutions for largescale digital transformation projects Working with the latest data technologies in the industry you will be instrumental in helping our clients evolve for a more digital future  Your Impact   Combine your technical expertise and problemsolving passion to work closely with clients turning complex ideas into endtoend solutions that transform our clients business  Translate clients requirements to system design and develop a solution that delivers business value  Lead designed develop and deliver largescale data systems data processing and data transformation projects  Automate data platform operations and manage the postproduction system and processes  Conduct technical feasibility assessments and provide project estimates for the design and development of the solution  Mentor help and grow junior team members   Set Yourself Apart With   Developer certifications in Azure cloud services  Understanding of development and project methodologies  Willingness to travel    Qualifications  Your Technical Skills  Experience   Demonstrable experience in data platforms involving implementation of end to end data pipelines  Handson experience with at least one of the leading public cloud data platforms Azure AWS or Google Cloud  Implementation experience with columnoriented database technologies ie Big Query Redshift Vertica NoSQL database technologies ie DynamoDB BigTable Cosmos DB etc and traditional database systems ie SQL Server Oracle MySQL  Experience in implementing data pipelines for both streaming and batch integrations using toolsframeworks like Azure Data Factory Glue ETL Lambda Spark Spark Streaming etc  Ability to handle module or track level responsibilities and contributing to tasks “handson”  Experience in data modeling warehouse design and factdimension implementations  Experience working with code repositories and continuous integration  Data modeling querying and optimization for relational NoSQL timeseries and graph databases and data warehouses and data lakes  Data processing programming using SQL DBT Python and similar tools  Logical programming in Python Spark PySpark Java Javascript andor Scala  Data ingest validation and enrichment pipeline design and implementation  Cloudnative data platform design with a focus on streaming and eventdriven architectures  Test programming using automated testing frameworks data validation and quality frameworks and data lineage frameworks  Metadata definition and management via data catalogs service catalogs and stewardship tools such as OpenMetadata DataHub Alation AWS Glue Catalog Google Data Catalog and similar  Code review and mentorship  Bachelor’s degree in Computer Science Engineering or related field    Additional Information  Pay Range103000 154000  The range shown represents a grouping of relevant ranges currently in use at Publicis Sapient Actual range for this position may differ depending on location and the specific skillset required for the work itself  Benefits of Working Here   Flexible vacation policy time is not limited allocated or accrued  16 paid holidays throughout the year  Generous parental leave and new parent transition program  Tuition reimbursement  Corporate gift matching program   As part of our dedication to an inclusive and diverse workforce Publicis Sapient is committed to Equal Employment Opportunity without regard for race color national origin ethnicity gender protected veteran status disability sexual orientation gender identity or religion We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures If you need assistance or an accommodation due to a disability you may contact us at hiringpublicissapientcom or you may call us at 16176210200  </data></node>
<node id="n723" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=335b30717e670197&amp;bb=v29_5p9dArsk-aDq-tPpTPOL2xKIPZLGnETamkNRePSpPMGw5FKaX9FhrxgyS9Ipva3Q5CQdH15ImAgMqnpvpjRAcCEV5G4vtSiZ078OiBE%3D&amp;xkcb=SoAt67M3CNjTYeRSRx0ObzkdCdPP&amp;fccid=c1099851e9794854&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay131500  243300 a yearJob typeFulltime LocationSeattle WA BenefitsPulled from the full job descriptionDental insuranceEmployee stock purchase planHealth insuranceRSURetirement plan Full job description Summary   Posted Jan 30 2024    Weekly Hours 40   Role Number200536085   Apple’s App Store is the world’s largest and most innovative app marketplace home to over 15 million apps and serving more than half a billion customers every week across all the Apple devices Since the App Store launched in 2008 it has changed how we all live it has enabled countless new companies spawned new industries and built millions of jobs But we believe we are just getting started We’re seeking a software data engineer to join the App Store Data Engineering team In this role you will help deliver this experience and improve the store every day for both users and developers by generating insights from data in a privacyfriendly manner We enable datadriven innovation by building solutions services and analytical workloads for a variety of internal collaborators and external partners In a world where apps have become essential in people’s daily lives the App Store team has become essential to Apple’s business    Key Qualifications    5 years of handson experience building distributed data processing applications using Apache Spark or Apache Flink  5 years of programming experience in Scala preferred or Java  Experience with Big data Hadoop HDFS Spark SQL Kafka  Proven skills in designing scalable highly available distributed systems using technologies like Kafka Iceberg Kubernetes Airflow and Cassandra  Good understanding of software engineering principles and fundamentals including algorithms and data structures  Selfdirected selfmotivated and ability to create architecture and design documents  Ability to capture crossfunctional requirements and translate them into practical engineering tasks  Excellent communication skills and proven ability to work in a crossfunctional environment Understanding of functional programming ideas and principles      Description   As a member of the App Store Data Engineering team you will have significant responsibility and influence in shaping its strategic direction This is a software engineering position We write robust code not just adhoc scripts Our software process dozens of terabytes of data on daily basis Our volumes are on a petabytes scale Our jobs and applications must be efficient scalable and stable Although we write software data is our main product and firstclass citizen We care about accurate and qualitative data as much as we care about fine clean and manageable code The data we produce power Apple leadership and partners about new innovations and the next big things To succeed here you’ll need to be a proponent of building worldclass analytical solutions To be a part of the team means we will want your ideas concerns and opinions in our discussions We are highly collaborative To join us in our next industryleading software project you will be expected to be part of our very impactful multifunctional team Thanks to Apple’s unique integration of hardware software and services engineers here partner to get behind a single unified vision That vision always includes a deep commitment to strengthening Apple’s privacy policy one of Apple’s core values Although services are a bigger part of Apple’s business than ever before these teams remain small forwardthinking and crossfunctional offering greater exposure to the array of opportunities here    Education  Experience   BS or MS in Computer Science At least 5 years professional software engineering experience preferred    Additional Requirements   Pay  Benefits      At Apple base pay is one part of our total compensation package and is determined within a range This provides the opportunity to progress as you grow and develop within a role The base pay range for this role is between 13150000 and 24330000 and your base pay will depend on your skills qualifications experience and location  Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs Apple employees are eligible for discretionary restricted stock unit awards and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan You’ll also receive benefits including Comprehensive medical and dental coverage retirement benefits a range of discounted products and free services and for formal education related to advancing your career at Apple reimbursement for certain educational expenses  including tuition Additionally this role might be eligible for discretionary bonuses or commission payments as well as relocation Learn more about Apple Benefits  Note Apple benefit compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program  Apple is an equal opportunity employer that is committed to inclusion and diversity We take affirmative action to ensure equal opportunity for all applicants without regard to race color religion sex sexual orientation gender identity national origin disability Veteran status or other legally protected characteristics    </data></node>
<node id="n724" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=5622768f698a234d&amp;bb=v29_5p9dArsk-aDq-tPpTKv9TEgSvKQ0dcrAtloMOzX9bRzNY6aKX10uJX2JEh6Z9MYrlrTkxvHXUTw7oAiYzbhpkJg5gBng-LNQFPaaSQoS9ymPxazHgg%3D%3D&amp;xkcb=SoCw67M3CNjTYeRSRx0NbzkdCdPP&amp;fccid=dd616958bd9ddc12&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SnowflakeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationHouston TX 77056 Full job description SUMMARY  The Data Quality Engineer is a critical role within our Data Governance team you will work closely with functional Subject Matter Experts SMEs and report to the Manager of Data Stewards You will implement data quality capabilities and processes to mature the data quality practice at Westlake The ideal candidate will have a proven track record of implementing a firstgeneration data quality approach across master data domains using both third party and core application tool stack You will be expected to lead conversations on how to improve the data lifecycle for Master Data Objects as well as facilitate the resolution of data quality issues through a Data Stewardship Council model You will be viewed as the go to for the organization on data quality best practices This role requires a high level of technical expertise great attention to detail and strong communication skills to work together with all global business stakeholders   DUTIES AND RESPONSIBILITIES  May include but are not limited to the following   Create a Data Quality Playbook to standardize the roles  responsibilities of data stakeholders for executing source system remediation to drive data quality improvements  Collaborates with Business Leads IT and IS to ensure effective data management throughout the organization  Partner with Data Stewardship Manager and business SME’s to identify and iterate metrics to measure data quality and roll up to Executive Business Leaders  Focus on six areas of master data domains Finance Product Item Customer Account and Vendor  Build cleansing standardization enrichment and validation approaches for both functional and domainspecific data quality dimensions on master data  Complete analysis on adherence to the defined schema data volume data anomalies expected distribution of datapoints in a dataset  Monitor data quality metrics and perform root cause analysis of data quality issues that lead to data downtime and take appropriate steps to address these issues with stakeholders    EDUCATION EXPERIENCE AND QUALIFICATIONS   Degree in Computer Science Statistics Business Administration Computer Information Data Science or related area  8 years of related and progressive experience in Data Quality Management Business Intelligence BI Data Analytics and related fields  Thorough knowledge of data governance framework including data warehouse metadata management data quality reference datamaster data data quality and data modeling  Experience in managing master data domains customer supplier item finance product privacy etc and their integration in enterprise systems ie SAP SFDC JDE Snowflake Oracle etc  Excellent oral and written communication skills including the ability to effectively present technical topics to individuals and groups with potentially varied levels of technical sophistication  Proficiency programming and leveraging data quality tools ie SAP Information Steward BODS USPS Dunn and Bradstreet to support enterprise initiatives  Proficiency data profiling data mapping and with data integration to establish data quality checks and resolving data quality issues ie inconsistencies inaccuracies incompleteness in data products etc  Strong analytical skills to identify patterns trends in data and complete rootcause analysis to resolve data quality issues  Proficiency in programming languages ie SQL Python to help write and automate common functional DQ checks  Excellent communication skills are required to collaborate with stakeholders such as data analysts and data scientists  A good understanding of data governance concepts such as data ownership data privacy and data security is required to ensure compliance with regulations and standards  A mindset of continuous improvement    PHYSICAL DEMANDS  While performing the duties of this job the employee is frequently required to sit stand walk use hands to touch handle or feel reach with hands and arms and talk or hear The employee is occasionally required to stoop kneel or crouch The employee must regularly lift andor move up to 5 pounds frequently lift andor move up to 10 pounds and occasionally lift andor move up to 15 pounds Specific vision abilities required by this job include close vision distance vision color vision peripheral vision depth perception and ability to adjust focus Significant digital dexterity eg using computer keyboard is required Use of oral communication to perform work is required   WORK ENVIRONMENT  The noise level in the work environment is usually moderate as normally based in an open office concept Some of the work may be required in the operating units which can require usage of required PPE including safety glasses hearing protection etc May also result in exposure to outside elements and may require usage of stairs and elevators   </data></node>
<node id="n725" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=715c0d5a7ac990db&amp;bb=v29_5p9dArsk-aDq-tPpTF33Sc5DbV2V1p0RmX1bxfyNlzdVXzTKsTqJSDBSlOGBwbAAjKQHGNXAtb7Xf1dR5ZtQ1iqHG_AKLnI-Om0lYYYsM-6DOcj3sg%3D%3D&amp;xkcb=SoAE67M3CNjTYeRSRx0MbzkdCdPP&amp;fccid=591d10479ecb8117&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNo Job detailsHere’s how the job details align with your profilePay500000 a yearJob typeFulltime LocationDublin CA 94568 Full job description  Job Information   Industry       IT Services         Work Experience       5 years         Salary       500000         City       Dublin         StateProvince       California         Country       United States         ZipPostal Code       94568            Solution Architect  Data Engineering and Analytics  Technical Expertise   Data Architecture  Design    Proven experience designing scalable secure and performant data architectures for big data and analytics workloads   Expertise in data modeling techniques dimensional entityrelationship etc   Knowledge of data governance frameworks and data quality best practices    Cloud Platform Proficiency GCP Focus    Extensive experience with GCP services for data engineering and analytics BigQuery Dataflow Dataproc PubSub etc   Ability to design and implement cloud architecture for data processing and analytics at scale   Understanding of cloud security best practices and compliance requirements    Data Engineering  Integration    Strong understanding of data pipelines ETLELT processes and data transformation techniques   Proficiency in programming languages like Python Java or Scala   Experience with data orchestration tools Airflow Luigi and containerization technologies Docker Kubernetes    Programming and Scripting    Strong skills in programming languages like Python Java or Scala   Ability to write analyze and debug SQL queries    Data Analytics and Visualization    Working knowledge of data analysis tools and platforms   Proficiency in data visualization tools and techniques to present data insights effectively    Machine Learning and AI Optional    Knowledge of machine learning algorithms and their application in data analytics   Familiarity with AI and ML services on GCP AI Platform AutoML etc    Analytical Skills   Data Strategy and Business Intelligence    Ability to develop strategies for data collection analysis and dissemination   Experience in delivering business intelligence and datadriven insights to stakeholders    ProblemSolving and Performance Optimization    Strong analytical and problemsolving skills to address complex datarelated issues   Experience in optimizing data workflows queries and algorithms for performance and costefficiency    Managerial and Soft Skills   Project Management    Experience in leading and managing largescale data projects   Familiarity with project management tools and methodologies eg Agile Scrum    Communication and Leadership    Excellent communication skills to articulate technical concepts to nontechnical stakeholders   Leadership skills to guide and mentor teams    Collaboration and Teamwork    Ability to work collaboratively with crossfunctional teams   Experience in working in a global multicultural environment    Continuous Learning and Adaptation    Commitment to continuous learning and staying updated with the latest trends in technology data engineering and analytics   Ability to adapt to evolving business and technology landscapes     Requirements   Additional Considerations Optional    Experience with data visualization tools Tableau Power BI   Familiarity with data security and privacy regulations GDPR HIPAA   Experience in a specific industry vertical relevant to your organization      </data></node>
<node id="n726" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=41a5c177b94ae9fd&amp;bb=v29_5p9dArsk-aDq-tPpTGaqXreou4bbDN0hjaCj_rjBSb8Sm7Yk_DRnpcJWQLKHXImYGaczI4kbD7LEzTCu75X2UhUGH3le5rduVL9MCik-gnFwHz6qoA%3D%3D&amp;xkcb=SoDt67M3CNjTYeRSRx0DbzkdCdPP&amp;fccid=18a767da5af06fb4&amp;cmp=Founding-Ventures&amp;ti=Ai%2Fml+Engineer&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay150000  300000 a yearJob typeParttimeFulltimeShift and scheduleMonday to Friday LocationRemote BenefitsPulled from the full job descriptionFlexible schedule Full job descriptionFounding Teams is a new AI Tech Incubator and talent platform We are supporting the next generation of AI startup founders with the resources they need including engineering product sales marketing and operations staff to create and launch their products The ideal candidate will have a passion for nextgeneration AI tech startups and working with great global startup talent Job Title  Lead Engineer Company Stealth AI Startup Remote 1620 hours per week Flexible hours  Yes Job Description  Build prototype AI ML models and tools to help us understand our customers and create personalised customer recommendations across multiple use cases and productize solutions to scale  Deeply understand customers their behaviours and pain points and develop a diversity of AI models addressing an array of customers’ needs  Translate business needs into AIML problems and create innovative solutions to advance our business goals  Determine the types and amount of data needed and work with data engineer to identify data sources and ingest into data lake  Structure standardise and annotate data into processable formats for ML enrich data with necessary attributes to allow sophisticated personalization  Help shape the way our data science team does work  researching and making key decisions about what we build how we build it and which tools are best for solving our problems  Work alongside software and data engineers to implement data processing and visualisation systems that make data readily available and simplify how insights are communicated  Evaluate the performance of AI models and make tradeoffs against quality metrics Investigate and resolve performance issues in a timely manner Requirements  Bachelor’s or Master’s degree in Mathematics ML statistics Computer Science SoftwareData Engineering or a related field  Strong mathematical background in probability statistics and optimization algorithms  Experience in building machine learning models and deploying them to production to make real decisions then measuring the impact of these decisions  Deep understanding of and have applied various machine learning techniques for solving realworld problems  Expertise with advanced programming skills in Python Java or any of the major languages to build robust algorithms  Proficient with SQL and can work “full stack” to integrate solutions with our data ecosystem  Confident in taking ownership of projects from start to finish and enjoy the process of turning nebulous ideas into reality  Excellent communication skills  A selfstarter who drives projects and builds strong relationships with stakeholders and teams to tackle large crossfunctional efforts  Thrive with minimal guidance and process  Worked in both small teamsincubators and large corporations Job Types Fulltime Parttime Salary 15000000  30000000 per year Schedule  Monday to Friday  Work Location Remote </data></node>
<node id="n727" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=1bce79ad132b11f7&amp;bb=v29_5p9dArsk-aDq-tPpTLmpMFzHxlnuwfUvVaxtVdo9-QTibMMGEOsQOsRLT8sThR8GC3lB9S1QZwlCHcdguqkIwD5jicjiIpZ_HUiXEG6Rg0zpR2_GvA%3D%3D&amp;xkcb=SoBZ67M3CNjTYeRSRx0CbzkdCdPP&amp;fccid=aa2fd58d0ae65364&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SnowflakeYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay65000  83000 a yearJob typeFulltime LocationEdgewater Park NJ 08010 BenefitsPulled from the full job description401kDental insuranceDisability insuranceFlexible scheduleHealth insurancePaid time offVision insurance Full job description LOCATION 4287 Route 130 S Edgewater Park NJ US 08010  Overview Come join our growing team of data practitioners and be on the leading edge of Burlington’s digital transformation Burlington is seeking a selfdriven and highly motivated individual to join a dynamic team with a passion for data software and engineering At Burlington you will have the opportunity to work with the latest technologies in a goaloriented environment As a Data Engineer I you will be a member of the Enterprise Data and Analytics team supporting business areas including Merchandising Allocations Marketing IT and Supply Chain Analytics teams with insights gained from analyzing Burlington and external data To be successful in this position you will have strong experience pulling data from various internal and external data sources and preparing it for advanced analytics segmentation and modeling Additionally you shoudl have strong interpersonal and relationship building skills as well as written and verbal communication skills Experience  35 years of experience in designing and implementing large scale data loading manipulation processing analysis and exploration solutions Experience developing SQL based data processing 3 years of experience with Data Architecture Data Warehouse Data Lake Data Marts and Data Stores with focus on AIML techniques Experience with Snowflake Oracle Databases AzureAWS and ADLS  Skills and Abilities  Technical expertise with pulling and massaging data Great understanding of firstthird party data Agile Development methodology Database Normalization Advanced SQL skills Understanding of data management principles and processes Passion for data analytics and pushing business innovation  Education  Bachelor’s or master’s degree in Computer Science  Engineering Informatics or related areas  Come join our team You’re going to like it here You will enjoy a competitive wage flexible hours and an associate discount Burlington’s benefits package includes medical dental and vision coverage including life and disability insurance Full time associates are also eligible for paid time off paid holidays and a 401k plan We are a rapidly growing brand and provide a variety of training and development opportunities so our associates can grow with us Our teams work hard and have fun together Burlington associates make a difference in the lives of customers colleagues and the communities where we live and work every day Burlington Stores Inc is an equal opportunity employer committed to workplace diversity LITG1 Posting Number 2024218320  Location USNJEdgewater Park  Address 4287 Route 130 S  Zip Code 08010  Workplace Type Remote  Position Type Regular FullTime  Career Site Category Corporate  Position Category Information Technology  Evergreen Yes  Min USD 6500000Annual  Mid USD 8300000Annual   </data></node>
<node id="n728" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e7bb222220c14106&amp;bb=v29_5p9dArsk-aDq-tPpTKSCcsqsr9m8UJ8tXc5hovEuug5nWHoc2lHaOZkzoqAMyhW2MaJSAUwN5MWvdEV4-pEKwfp8e3ymLcGdz8ltfrJQZ_wltdgkFA%3D%3D&amp;xkcb=SoDE67M3CNjTYeRSRx0BbzkdCdPP&amp;fccid=b5c03f369c8d2b82&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SoCYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay101000  190000 a yearJob typeFulltime LocationHillsboro OR BenefitsPulled from the full job descriptionContinuing education credits Full job description     Your tasks   As part of this team you will have an opportunity to architect and work on many different modules within our devices and to work with leaders in the design of world class data converters These devices integrate large high frequency  30 GHz analog blocks along with complex digital blocks to create our custom System on Chip SoC solutions Your tasks will cover the entire spectrum of mixed signal development starting from conceptual work through the design layout and characterization of the devices Responsible to develop cutting edge SoCs for communication systems according to customer specifications finding optimum tradeoffs between system performance power dissipation and cost Drive block specifications for design team Develop run and maintain data converter system MATLAB and Verilog models for an SoC across various usage modes Work with analog and digital implementation teams to ensure proper development of system designs Ability to simulate entire signal path in a mixed‒signal flow Create and own chip level simulations that verify data converter operating modes are implemented correctly We work with other ASIC design groups based in Germany You will work with a highly qualified team of ASIC engineers located both locally and abroad          Application tips  Watch the video and learn all about our application process  Application Interview Contact persons           Your qualifications   MS degree with 5 years of experience or a PhD degree with 3 years of experience Significant experience modeling data converters or radio systems in MATLAB Good knowledge of communication systemsstandards and their application to different wireless and wire‒line applications Deep understanding of design simulation and measurements of high‒speed ICs using CMOS and CML circuits in at least 3 of these areas Digital‒Analog Converters Serializers and de‒serializers Wideband Output Drivers High Performance Phase Locked Loops Low Noise clock designs Operational Amplifiers and Variable Gain Amplifiers and DSP Equalization and compensation techniques Self‒driven ability to work independently while coordinating with IC designers You will need to be able to travel occasionally to European and US locations Candidates must be current US residents with valid work authorization Work effectively in a group setting share expertise provide and receive feedback communicate technical issues work in a team environment to resolve technical issues Additional desirable but not required qualifications include Experience performing mixed signal circuit design across analog and digital domains BiCMOS IC design experience Experience with an Analog Design Flow eg Cadence Virtuoso Xcelium AMS‒Designer Verilog‒AMS and Spectre SERDES transmitter and receiver design eg CDR DFE CTLE jitter modelling        Interested We are looking forward to receiving your application        This is a fulltime position with a total compensation target salary range of 101k190K plus benefits The range is determined by the position geographic location and level Individual pay within the range is determined by several factors including location education or training relevant work history and jobrelated skills       We are committed to hiring and retaining a diverse workforce We are proud to be an Equal Opportunity Employer making decisions without regard to race color religion creed sexual orientation gender identity marital status national origin age veteran status disability or any other protected class                 The Rohde  Schwarz technology group is among the trailblazers when it comes to paving the way for a safer and connected world with its leading solutions in test  measurement technology systems and networks  cybersecurity Founded more than 85 years ago the group is a reliable partner for industry and government customers around the world         Our offer        Flexible working hour models       Training  continuing education       Privately owned company       Promoting innovation       Longterm  attractive work environment         Show more             Cityregion        Hillsboro Oregon USA        Entry level        Professionals        Employment Type        Fulltime unlimited        Ref Number        3160      </data></node>
<node id="n729" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0be7465eaba47360&amp;bb=v29_5p9dArsk-aDq-tPpTGteReRwNdPhEeRBuApkjM_zesOTGlHM-A01yQR2kXUJOEt4nQf9uiqCKHMlF4xX1yRlufDkM89lxp4xGuGVZFD9UJanbtk-OQ%3D%3D&amp;xkcb=SoBw67M3CNjTYeRSRx0AbzkdCdPP&amp;fccid=11078451211bf0c6&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Power BIYesNoEducationDo you have a Bachelors degreeYesNo LocationChesterfield MO 63017 BenefitsPulled from the full job descriptionDental insuranceHealth insurancePaid time offTuition reimbursementVision insurance Full job description Were a Little Different   Our mission is clear We bring to life a healing ministry through our compassionate care and exceptional service     At Mercy we believe in careers that match the unique gifts of unique individuals  careers that not only make the most of your skills and talents but also your heart Join us and discover why Modern Healthcare Magazine named us in its Top 100 Places to Work    Overview Profisee Azure  Master Data Management Engineer MDM   Hybrid Position can be partially remote work from home Onsite 2 days a week in St Louis    Please note that as of the posting date of this job announcement Mercy is unable to offer immigration sponsorship or visa assistance for this position We encourage all eligible candidates including US citizens permanent residents and those with existing work authorization to apply   A Master Data Management Engineer is responsible for the technical implementation and maintenance of an enterprises master data management systems This role typically involves working closely with data architects data governance teams and IT teams to develop test and deploy master data management solutions that meet the needs of the ministry    Implement and maintain master data management system Profisee MDM Ensure that master data is accurate consistent and accessible to all stakeholders Analyze business requirements for survivorship rules and deploy into Profisee MDM Integrate master data with other enterprise systems and data sources Incorporate new system models into an Enterprise Data Model for MDM Utilize Azure tools including ADF Databricks PowerApps PowerBI and other Microsoft cloud tools to build MDM solutions Write complex system queries into operational systems to extract datasets that meet master data source ingestion requirements Build Profisee Fastapp visualizations to display source system values and golden record contributors Collaborate with active development teams to assist them in utilizing MDM resources for inflight projects Develop and maintain data quality reference data management and data validation processes    Qualifications    Experience Several years of experience in data engineering data integration and master data management Preferably with Profisee MDM  Required Education Bachelors degree or higher in Computer Science Information Systems or a related field  Other Strong technical skills including proficiency in data management technologies and tools Excellent problemsolving and collaboration skills  Preferred Profissee and Azure experience    We Offer Great Benefits   Dayone comprehensive health vision and dental coverage PTO tuition reimbursement and employermatched retirement funds are just a few of the great benefits offered to eligible coworkers including those working 32 hours or more per pay period    Were bringing to life a healing ministry through compassionate care   At Mercy our supportive community will be behind you every step of your day especially the tough ones You will have opportunities to pioneer new models of care and transform the health care experience through advanced technology and innovative procedures Were expanding to help our communities grow Join us and be a part of it all    What Makes You a Good Match for Mercy    Compassion and professionalism go handinhand with us Having a positive outlook and a strong sense of advocacy is in perfect step with our mission and vision Were also collaborative and unafraid to do a little extra to deliver excellent care  thats just part of our commitment If that sounds like a good fit for you we encourage you to apply  </data></node>
<node id="n730" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=99257fcbd78a4d16&amp;bb=v29_5p9dArsk-aDq-tPpTOdHqALYnVC2k3JrfSynAPNuTHI0pxP8ueXchRIfRGdwNLEjsXd58_Iz-zai2z-5bMf_rP_1-wY-Z7OS5wRCS18%3D&amp;xkcb=SoD-67M3CNjTYeRSRx0HbzkdCdPP&amp;fccid=c1099851e9794854&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay131500  243300 a yearJob typeFulltime LocationSeattle WA BenefitsPulled from the full job descriptionDental insuranceEmployee stock purchase planHealth insuranceRSURetirement plan Full job description Summary   Posted Feb 29 2024    Weekly Hours 40   Role Number200525913   Imagine what you could do here At Apple new ideas have a way of becoming extraordinary products services and customer experiences very quickly Bring passion and dedication to your job and theres no telling what you can accomplish Would you like the stimulation and challenges of building worldclass extraordinary scalable systems that power App Store Music TV Fitness Arcade and many more Then this is the opportunity for you at Apple We strive to provide a flawless experience for millions of customers and developers We are seeking a Senior Software Engineer to join Apple Services Engineering ASE who brings deep passion for building distributed large scale data processing systems frameworks and platforms using big data technologies You will partner with Apple TV Search Recommendation  Quality of Service teams and work collaboratively to make a significant impact on Apple TV customers    Key Qualifications    7 years of programming experience in Java or Scala  Experience building distributed data processing applications using Apache Spark or Apache Flink  Proven skills in designing scalable highly available distributed systems using technologies like Kafka Iceberg Kubernetes Airflow and Cassandra  Good understanding of software engineering principles and fundamentals including algorithms and data structures  Selfdirected selfmotivated and ability to create architecture and design documents Ability to capture crossfunctional requirements and translate them into practical engineering tasks      Description   Data is our first class citizen The volume of the data is on petabytes scale And we are software engineers who build distributed systems to generate data analytics The systems should scale well be highly available and provide consistent results The systems we build have significant impact on our stakeholders who develop search and recommendation algorithms for Apple TV customers Our team also serves Quality of Service stakeholders to whom we provide near real time streaming data And most importantly we are deeply committed to Apple’s privacy policy which forms Apple’s core values You will have important responsibility and influence in designing and building the data platform to empower data analytics and delivering the solutions that have significant impact on Apple TV and Sports products  An ideal candidate will lead and drive partner teams towards successful implementation of the full solution We are looking for someone with a love for data and ability to iterate quickly on all stages of data pipelines  This position involves working on a small team to develop large scale data pipelines and analytical solutions using big data technologies  Successful candidates will have strong engineering skills and communication  The ideal candidate has a real passion for quality and an ability to understand complex systems If this sounds like you then we would love to hear from you    Education  Experience   BS or MS in Computer Science At least 5 years professional software engineering experience preferred    Additional Requirements   Pay  Benefits      At Apple base pay is one part of our total compensation package and is determined within a range This provides the opportunity to progress as you grow and develop within a role The base pay range for this role is between 13150000 and 24330000 and your base pay will depend on your skills qualifications experience and location  Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs Apple employees are eligible for discretionary restricted stock unit awards and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan You’ll also receive benefits including Comprehensive medical and dental coverage retirement benefits a range of discounted products and free services and for formal education related to advancing your career at Apple reimbursement for certain educational expenses  including tuition Additionally this role might be eligible for discretionary bonuses or commission payments as well as relocation Learn more about Apple Benefits  Note Apple benefit compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program  Apple is an equal opportunity employer that is committed to inclusion and diversity We take affirmative action to ensure equal opportunity for all applicants without regard to race color religion sex sexual orientation gender identity national origin disability Veteran status or other legally protected characteristics    </data></node>
<node id="n731" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=6bfd23932b581b33&amp;bb=v29_5p9dArsk-aDq-tPpTPVX53GBz4JyJEHKTmQrXPR55TGkqy4plsN2kf0cpVUimSAkfMqwFLoqkJU3qsnEz1Lxfy5ofTwtqhEKXJVn52E%3D&amp;xkcb=SoBK67M3CNjTYeRSRx0GbzkdCdPP&amp;fccid=c1099851e9794854&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNo LocationSan Diego CA BenefitsPulled from the full job descriptionDental insuranceEmployee stock purchase planHealth insuranceRSURetirement plan Full job description Summary   Posted Jan 29 2024    Role Number200536690   Do you want to help reduce carbon emissions worldwide If so we are looking for a Grid Innovation ML Engineer  Data Scientist to help us build customer facing features to achieve this goal Some of the features we’ve already released include Clean Energy Charging and the Grid Forecast Widget We have many exciting features on our roadmap and we are looking for someone who can help us investigate emerging technologies and develop nextgeneration products The ideal candidate thrives in ambiguity and is passionate about creating new products with meaningful and disruptive impact to the world    Key Qualifications    Experience with electricity grid emissions and renewable energy  Strong programming background preferably in Python  Experience with data analysis data science tools and data visualization  Experience with time series forecasting  evaluation clustering methodologies and classification models Can drive cross functional consensus through outstanding communication and presentation skills      Description   As part of the Energy Services Team you will be responsible for researching new ways to measure emissions and evaluate existing methodologies Additionally you will work with data from grid operators and other energy data sources to power new features across Apple’s ecosystem Your role will require you to explore different ways to evaluate our shortterm and longterm impact on the environment We will help you build consensus for new ideas based on a data driven story Apple is committed to creating a diverse working environment for everyone and is proud to be an equal opportunity employer All applicants will receive consideration for employment without regard to race color religion gender sexual orientation national origin disability age veteran or immigrant status    Education  Experience   5 years relevant industry experience with Machine Learning Statistics Data Engineering or similar    Additional Requirements   Pay  Benefits      At Apple base pay is one part of our total compensation package and is determined within a range This provides the opportunity to progress as you grow and develop within a role The base pay range for this role is between 16170000 and 28490000 and your base pay will depend on your skills qualifications experience and location  Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs Apple employees are eligible for discretionary restricted stock unit awards and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan You’ll also receive benefits including Comprehensive medical and dental coverage retirement benefits a range of discounted products and free services and for formal education related to advancing your career at Apple reimbursement for certain educational expenses  including tuition Additionally this role might be eligible for discretionary bonuses or commission payments as well as relocation Learn more about Apple Benefits  Note Apple benefit compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program  Apple is an equal opportunity employer that is committed to inclusion and diversity We take affirmative action to ensure equal opportunity for all applicants without regard to race color religion sex sexual orientation gender identity national origin disability Veteran status or other legally protected characteristics    </data></node>
<node id="n732" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=db8bdcdfb91a7cd2&amp;bb=v29_5p9dArsk-aDq-tPpTJetlI9KDmTb6h3LYM14rTcR3kKkqD2E6xP_Ut5dCSRDysU24o4TehwbpEoDZLY9JyorTD7Hfopjdq1xVT5f0XM%3D&amp;xkcb=SoDX67M3CNjTYeRSRx0FbzkdCdPP&amp;fccid=b189ac19be7e836f&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Wine knowledgeYesNoEducationDo you have a Bachelors degreeYesNo LocationAtlanta GA 30339 BenefitsPulled from the full job description401k matchingCaregiver leaveDental insuranceDisability insuranceHappy hourHealth insuranceLife insuranceShow morechevron down Full job description Republic National Distributing Company RNDC is a familyowned business with roots extending before Prohibition that has evolved into one of the nation’s largest wine and spirits wholesalers Our success is grounded in our core values of Family Service Accountability Honesty and Professionalism We offer a vibrant inclusive culture and workplace experience for individuals who want a career that makes them feel accomplished and engaged RNDC values the health and wellbeing of our associates inside and outside the office offering dynamic health and wellness benefits that supply exceptional care and value RNDC is geared toward growing our footprint and our people Join our team of energetic professionals who believe in many happy hours and are experts in our craft     Summary    The Senior Data Engineer manages and organises RNDCs enterprise data They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of the information Senior Data Engineers will combine raw information from different sources to create consistent and machinereadable datasets that are easy to analyze and support company initiatives They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects They will also implement methods to improve data reliability and quality improve data visibility and reduce effort through automation      In this role you will     Contribute on a team of data engineers through design demand delivery code reviews release management implementation presentations and meetings  Mentor fellow data engineers and contribute to ongoing process improvements for the team  Evaluate business needs and objectives and align architecturedesigns with business requirements  Build the data pipelines required for the optimal extraction transformation integration and loading of raw data from a wide variety of data sources  Assemble large complex data sets and model our data in a way that meets functional  nonfunctional business requirements  Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage  Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization  Build processes to help identify and improve data quality consistency and effectiveness  Ensure our data is managed in a way that it conforms to all information privacy and protection policies  Use agile software development processes to iteratively make improvements to our data management systems        What you bring to RNDC    BachelorsTech School degree in Computer Science Information Systems Engineering or equivalent andor commensurate years of realworld experience in software engineering 4 years of relevant experience in data management 3 years in data engineering with detailed knowledge of data warehouse technical architectures infrastructure components ETL ELT Experience with performance analysis and optimization Experience in data acquisition transformation and storage design using design principles patterns and best practices      Whats in it for you     401k with company matching  Medical dental and vision benefits  Generous paid time off program – work your way up to 5 weeks of PTO a year with the ability to carryover unused PTO  Paid volunteer time  Paid parental leave  Paid caregiver leave  Fertility benefits  Paid training  Company paid life insurance shortterm disability and companypaid holidays  Associate resource groups and diversity equity and inclusion programs available for all associates   Participation in these programs are subject to applicable wait periods and all plan and program terms and eligibility  COVID19 considerations      We follow CDC Guidelines and have a fun and safe environment for our teams         Bonus if you bring     Data engineering certification is a plus  Previous experience in the Wine and Spirits industry      Republic National Distributing Company and National Distributing Company are Equal OpportunityAffirmative Action employers It is our policy not to discriminate against any Employee or Applicant All qualified applicants will receive consideration for employment without regard to race religion color national origin sex age status as a protected veteran among other things or status as a qualified individual with disability This policy of nondiscrimination in employment includes but is not limited to recruitment hiring placement promotion transfer employment advertising or solicitations compensation layoff or termination of employment  RNDC is committed to providing reasonable accommodation to people with disabilities throughout the job application and interview process to the point of undue hardship  </data></node>
<node id="n733" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=f54de58b65553a9f&amp;bb=HKH-ysDsRD4PV8MpMJn7w2eDmkowNZcTUInfdiAaHMHV2koIotLy0T-L-THaOdYv1MKYvuFWfJu7ZDaYL7sS0k_A5VQQxO4AZObu8N4gawDat3NUlddDQw%3D%3D&amp;xkcb=SoCV67M3CNjvTlWbHp0IbzkdCdPP&amp;fccid=9e327220ae07d63f&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid Secret Clearance licenseYesNoSkillsDo you have experience in WeblogicYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location500 12th Street NE Washington DC 20002 Full job description  Overview       CommIT is seeking a Junior Data Engineer with demonstrated expertise supporting projects for the Department of Defense DoD in Washington DC this position can also be hybrid      Established in 2001 CommIT is a Certified VeteranOwned Small Business CVOSB providing innovative technical engineering and data science services Our enterprise systems support includes the Department of Defense’s DoD GCSSMC CAC2S TBMCSMC and the Department of Veteran’s Affairs’ VA telehealth communications We offer acquisition management systems engineering Agile software development cloud management IT modernization data analytics cybersecurity and training including leadingedge DevSecOps automated testing and mobile application development   Responsibilities     Your essential job functions will include but may not be limited to     Provides valuable support to the design team while continually improving their coding and design skills  Assists senior programmers and analysts with all aspects of software design and coding  Builds interactive and complex data visualization and analysis tools such as dashboards  Attends and contributes to project development meetings  Applies codes and improves coding skills  Writes and maintains programming codes for the development of automation or interactive data analytics tools  Works on minor bug fixes of automation or interactive data analytics tools  Monitors the technical performance of data systems  Gathers information from consumers about program functionality  Writes data analytics reports  Archives IHSC datainformation to existing repository platform andor data warehouse or other specified data storage areas  Maintains a repository or database system and SharePoint pages of unstructured and opensource data  Extracts data from various data sources into one for analytic and reporting purposes  Conducts analytic toolsdata systems development tests  Learns the codebase gathers user data and responds to request from senior programmers  Qualifications     Required Experience and Education     Education Required Bachelor of Arts or Bachelor of Science BABS degree required in Computer Science Statistics Data Analytics Data Science Information Management Engineering or a closely related field  Experience Recommended At least three 3 years of data analytics and programmingrelated experience or relevantsimilar experience which includes experienceknowledgeskillsabilities with all of the following      Practical experience in at least two programming applications software packages eg Java CC NET WebLogic HTML ServiceNow SharePoint Power Apps etc preferably including knowledge of relational database design the ability to read and understand data dictionaries data coding and the ability to infer the relationship of the tables to the application and the process  Has an initial level of understanding about maintaining and complying with requirements for the following task areas collecting compiling processing normalizing analyzing and interpreting data using systems support tools such as SQL Python Oracle Tableau Power BI on Microsoft Azure Quicksight on Amazon Web Services Access and Excel  Has experience with documenting and reviewing data sharing agreements outstanding customer service and communication skills relaying technical concepts  Has experience with managing data quality cleansing tagging and metadata management  Has experience with testing and implementing application software with regards to collecting process and storing data  Has experience with project management tools such as MS Project     Security Requirements    Secret Clearance  US Citizenship    Equal Opportunity Employer      CommIT Enterprises Inc is an Equal Opportunity Employer Employment decisions are made without regard to race color religion national origin gender sexual orientation gender identity age physical or mental disability genetic factors militaryveteran status or other characteristics protected by law    </data></node>
<node id="n734" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=5263fa6ea6c8e455&amp;bb=HKH-ysDsRD4PV8MpMJn7w2eDmkowNZcTPcBco0l60TNLz0dGGGFoOChXhcsrpd4xOmujK8LBm4RNmCnMIKUTcU50qYcdK7pU5E7wA2ji6a4%3D&amp;xkcb=SoAb67M3CNjvTlWbHp0PbzkdCdPP&amp;fccid=55a2bdb0a91b873d&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in UnityYesNo Job detailsHere’s how the job details align with your profilePay83000  138200 a yearJob typeFulltimeShift and scheduleOn call LocationRemote Full job description At DICK’S Sporting Goods we believe in how positively sports can change lives On our team everyone plays a critical role in creating confidence and excitement by personally equipping all athletes to achieve their dreams We are committed to creating an inclusive and diverse workforce reflecting the communities we serve  If you are ready to make a difference as part of the world’s greatest sports team apply to join our team today   OVERVIEW   At DICK’S Sporting Goods we believe in how positively sports can change lives On our team everyone plays a critical role in creating confidence and excitement by personally equipping all athletes to achieve their dreams We are committed to creating an inclusive and diverse workforce reflecting the communities we serve   We are creating the future of sports driven by powerful data products and platforms that serve our Athletes and Teammates   We are looking for a Senior Data Engineer to join our passionate team adding your background and experience to make us even stronger In this role you will build dataset and make it accessible to our partner teams by writing great code to simplify the complexity and ensure quality Your work will enable product teams data scientists and decisionmakers across the company to bring together insights and inform our business   We believe that trusted easy to consume data is critical and as a Senior Data Engineer your work will help to build that foundation   You will also be responsible for the daily operations inclusive of troubleshooting and job monitoring You will be a part of the growing Data team reporting to the Sr Director Data Analytics   The impact you will have   DesignStrategy You will design and support the business’s database and table schemas for new and existing data sources for the data warehouse Creates and supports the ETL to facilitate data accommodation into the warehouse In this capacity the Data Engineer designs and develops systems for the maintenance of the business’s data warehouse ETL processes and business intelligence   Collaboration You will be collaborative  working closely with analysts data scientists and other data consumers within the business to gather and deliver high quality data for business cases The Data Engineer also works closely with other disciplinesdepartments and teams across the business in coming up with simple functional and elegant solutions that balance data needs across the business   Analytics You will play an analytical role in quickly and thoroughly analyzing business requirements and subsequently translating the emanating results into good technical data designs In this capacity the Data Engineer establishes the documentation of the data solutions develops and maintains technical specification documentation for all reports and processes   What You Will Do    You’ll be working with a variety of internal teams  Engineering Business  to help them solve their data needs  Your work will provide teams with visibility into how DICKs products are being used and how we can better serve our customers  Identify data needs for business and product teams understand their specific requirements for metrics and analysis and build efficient and scalable data pipelines to enable datadriven decisions across DICKs  Experience in one or more of the following Python Preferred Scala C or Java  Design develop reliable data models and extremely efficient pipelines to build quality data and provide intuitive analytics to our partner teams  Help the Data Analytics  Data Science team apply and generalize statistical and econometric models on large datasets  Drive the collection of new data and the refinement of existing data sources develop relationships with production engineering teams to manage our data structures as the DICKs product evolves  Develop strong subject matter expertise and manage the SLAs for those data pipelines  Participate in design sessions and code reviews to elevate the quality of data engineering across the organization  Participate in an oncall rotation for support during and after business hours  Lead design sessions and code reviews to elevate the quality of data engineering across the organization    Technical Skills    Expert in SQL andor SQL based languages and performance tuning of SQL queries  Strong understanding of NormalizedDimensional model disciplines and similar data warehousing techniques  Experience in one or more of the programming languages are required Python Preferred Scala C or Java Go Kotlin  Strong Experience with cloudbased data warehouses – eg Snowflake Big Query Synapse RedShift etc  Experienced with ETLELT in Databricks with Medallion architecture and with Delta Lake Unity Catalog Delta Sharing Delta Live Tables DLT  Experience with CICD on Databricks using tools such as GitHub Actions and Databricks CLI  Strong Grasp of data management principles Data Lake Data Mesh Data Catalog Data Quality etc    QUALIFICATIONS      5 years of experience in Data Warehousing and development using data technologies such as Relational  NoSQL databases open data formats building data pipelines ETL and ELT with batch or streaming ingestion loading and transforming data  Expert in SQL andor SQL based languages and performance tuning of SQL queries  Strong understanding of NormalizedDimensional model disciplines and similar data warehousing techniques  Experience in one or more of the programming languages are required Python Preferred Scala C or Java Go Kotlin  Strong experience working with ETLELT concepts of data integration consolidation enrichment and aggregation in petabyte scale data sets  Experience with at least one of the following cloud platforms Microsoft Azure Preferred Amazon Web Services AWS or Google Cloud Platform GCP  Strong Experience with cloudbased data warehouses – eg Snowflake Big Query Synapse RedShift etc  Experience with message queuing stream processing Kafka Flink Spark Streams  Strong Grasp of data management principles Data Lake Data Mesh Data Catalog Master Data Data Quality etc  Experience in BI tooling such as Qlik MicroStrategy Tableau PowerBI or Looker  Experience with orchestration tools ControlM Airflow etc  Strong communication skills across different mediums to craft compelling messages to drive action and alignment  Comfort with agile delivery methodologies in a fastpaced complex environment – Scrum SAFe utilizing tools such as Jira Confluence and GitHub  Ideal candidates will have experience working with one of the following industries Retail Supply Chain Logistics Manufacturing or Marketing  Proficient in LinuxUnix environments      LIJN1  Targeted Pay Range 83000  138200 This is part of a competitive total rewards package that could include other components such as incentive equity and benefits Individual pay is determined by a number of factors including experience location internal pay equity and other relevant business considerations We review all teammate pay regularly to ensure competitive and equitable pay We also offer a generous suite of benefits To learn more visit wwwbenefityourliferesourcescom  </data></node>
<node id="n735" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=6152c3cd15008d43&amp;bb=HKH-ysDsRD4PV8MpMJn7w06i79cS0GmtWsI0XZiVZzG7c66KQpH_3SGhBrSUlpb4Nn72_hXfe3vQuTrY8zNworJY64ha0RMhnP3fXBeVfE_WFrbXhhmKjA%3D%3D&amp;xkcb=SoCv67M3CNjvTlWbHp0ObzkdCdPP&amp;fccid=578fa8376f4eec04&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid Certified Information Systems Auditor certificationYesNoSkillsDo you have experience in YAMLYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRemote BenefitsPulled from the full job descriptionOpportunities for advancement Full job description Secure our Nation Ignite your Future   Become an integral part of a diverse team while working at an industry leading organization where our employees come first At ManTech International Corporation you’ll help protect our national security while working on innovative projects that offer opportunities for advancement   We are seeking a motivated career and teamoriented cybersecurity Integration Layer Data Engineer in support of the US Department of Homeland Security DHS Cybersecurity and Infrastructure Security Agency CISA Continuous Diagnostic  Mitigation CDM Data Services Program The CDM Data Services Program is a critical component of CISA’s national effort to ensure the defense and resilience of cyberspace This is a remote position where the candidate can work from any location within the United States provided they are able to work on an eastern time zone schedule   The CDM Data Services Program mission is to provide a standardized platform to collect transform and integrate cybersecurity data from relevant authoritative data sources into a coherent data delivering actionable information into Agency and Federal Dashboards to identify risk areas in support of mitigation as well as to facilitate coordinated agency and national response to cyberthreats   Our Integration Layer Data Engineering and Quality team guides and verifies data transformations so that our automated solution delivers endtoend results with confidence The solution is managed in cloud environments and the team works closely with developers to create an automated continuous integration CI and continuous delivery CD solutions using Agile delivery methodologies   Responsibilities    Work with internal and external stakeholders to examine contractual data requirements in order to drive data modeling pipelines transformation normalization and quality for each solution release through a SAFe Agile Release Train ART to achieve business goals  Act as subject matter expert regarding data requirements formats types lineage and quality to brief internal and external stakeholders  Analyze raw data from different sources and define consistent and machinereadable formats for the data store  Work with stakeholders to track and obtain nonautomated data sources to maintain freshness  Develop document and communicate processes for seamless data ETL extraction transformation and loading through Cloud based data services  In charge of directing developers on how to join and convert raw data from multiple sources into usable information for analytics and reporting  Work collaboratively with crossfunctional teams to design implement and maintain a scalable and secure data repositorylake that can support analyzing trends and patterns  Prepare options levels of effort and estimates when data requirements change  Develop database objects and schemas that support extracting transforming loading and storage of data based on a logical data model LDM  Participate in Agile ceremonies and track and document work in Jira and Confluence  Ensure data integrity quality and accessibility within the repositorylake  Conduct complex data analysis using SQL based searches and instruct developers on how to handle data quality issues  Explore and implement ways to enhance data quality and reliability and use tools to develop analytical dashboards  Work with Data Scientists to improve the quality and accuracy of the information enabling stakeholders to make more responsible cyber risk decisions  Prepare and maintain datasets for testing and modeling  Develop and maintain the solution’s data dictionary and data lineage  Define data retentions and governance for the solution    Position Requirements    Degree in Computer Science IT or similar field  A minimum of 5 years of proven experience as a Data Engineer or similar role  Solid understanding of relational databases and ETL processes  Proficiency in data transformation normalization and configuration  Technical expertise in data ingestion and manipulation  Knowledge of big data platforms and data source formats from APIs JSON csv yaml  Familiarity with API integration and data pipelines  Experience in creating dashboards eg Tableau PowerBI or similar for data visualization  Experience with data abstraction various data conditions including blank and NULL data and detecting and handling data collisions and filtering logic syntax  Experience with SQLTSQL NoSQL and data visualization tools design  Familiarity with data segmentation cleansing enrichment and indexing  Familiarity with application administration configuration and integration  Experience with data security and segregation physically or logically Know the use of rolebased access and attributebased access when limiting data  Ability to independently perform research on industry standards regulatory requirements and cuttingedge technological trends Have passion for new technologies software and processes  Familiarity with agile development methodologies expertise in the Microsoft Office  Google suite of software    Desired Qualifications    Data engineering or data analyst certification  Scaled Agile Framework SAFe certification  Experience with Data Lakes Data Warehouses or Data Lakehouse  Experience with data governance tools  Experience with cloud services such as Azure AWS or GCP  Understanding of cybersecurity tools such as vulnerability CVE scanners software scanners mobile and network host discovery scanners and other tools in order to understand source data  Familiarity with federal cybersecurity concepts such as Vulnerabilities DISA STIGs NIST FISMA Risk Management Framework and MITRE ATTCK Framework  Experience working in government contracting  Knowledge of programming languages eg Python PowerShell  Experience with data compression data deduplication  Experience with Elasticsearch with Kibana Dashboards404    SecurityClearance Requirements    Must be a US citizen and pass a background investigation  Able to obtain and maintain a DHS SuitabilityEntry on Duty EOD    Physical Requirements    Must be able to remain in a stationary position 50  Constantly operates a computer and other office productivity machinery such as a calculator copy machine and computer printer  The person in this position frequently communicates with coworkers management and customers which may involve delivering presentations Must be able to exchange accurate information in these situations    For all positions requiring access to technologysoftware source code that is subject to export control laws employment with the company is contingent on either verifying USperson status or obtaining any necessary license The applicant will be required to answer certain questions for export control purposes and that information will be reviewed by compliance personnel to ensure compliance with federal law ManTech may choose not to apply for a license for such individuals whose access to exportcontrolled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone                         ManTech International Corporation as well as its subsidiaries proactively fulfills its role as an equal opportunity employer We do not discriminate against any employee or applicant for employment because of race color sex religion age sexual orientation gender identity and expression national origin marital status physical or mental disability status as a Disabled Veteran Recently Separated Veteran Active Duty Wartime or Campaign Badge Veteran Armed Forces Services Medal or any other characteristic protected by law   If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system please contact ManTechs Corporate EEO Department at 703 2186000 ManTech is an affirmative actionequal opportunity employer  minorities females disabled and protected veterans are urged to apply ManTechs utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunityaffirmative action policies ManTech does not accept resumes from unsolicited recruiting firms We pay no fees for unsolicited services   If you are a qualified individual with a disability or a disabled veteran you have the right to request an accommodation if you are unable or limited in your ability to use or access httpwwwmantechcomcareersPagescareersaspx as a result of your disability To request an accommodation please click careersmantechcom and provide your name and contact information                        </data></node>
<node id="n736" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=a1b3afc5c50b19a5&amp;bb=HKH-ysDsRD4PV8MpMJn7w0hkBGGS5K8FSSIpoa1Gg6SkjJSuLhyHSqeDS6ff_r6jN9t4JclrTzHRHAoGOG6dcXxp2qOwwMyRRyD1mcIUl8w%3D&amp;xkcb=SoAy67M3CNjvTlWbHp0NbzkdCdPP&amp;fccid=5cc0cdc6dbb121cc&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNo Job detailsHere’s how the job details align with your profilePayFrom 105700 a yearJob typeFulltime LocationSeattle WA BenefitsPulled from the full job descriptionHealth insurance Full job description 3 years of data engineering experience Experience with data modeling warehousing and building ETL pipelines Experience with SQL Experience in at least one modern scripting or programming language such as Python Java Scala or NodeJS  Sales Marketing and Global Services SMGS  AWS Sales Marketing and Global Services SMGS is responsible for driving revenue adoption and growth from the largest and fastest growing small and midmarket accounts to enterpriselevel customers including public sector The AWS Global Support team interacts with leading companies and believes that worldclass support is critical to customer success AWS Support also partners with a global list of customers that are building missioncritical applications on top of AWS services    The AWS Marketing Data Science and Engineering team is seeking an experienced Data Engineer to join our dynamic and innovative group In this role you will be responsible for designing building and optimizing data pipelines that power our cuttingedge marketing analytics and attribution capabilities You will work with complex data sets leveraging your expertise in data engineering to ensure highquality reliable and scalable data solutions    Key job responsibilities  Your primary responsibilities will include ingesting transforming and modeling sales and marketing data to enable multitouch attribution MTA analysis You will collaborate closely with crossfunctional teams including sales marketing and analytics to continuously enhance our MTA measurement and insights Additionally you will play a crucial role in building structured data pipelines conducting data analysis to improve MTA input ensuring data quality for attribution models and optimizing data systems for efficient MTA reporting    A day in the life  To be successful in this role you should possess strong technical skills in data engineering with proficiency in AWS technologies and experience working with largescale data sets Excellent communication and collaboration abilities are essential as you will work closely with various stakeholders to understand their requirements and deliver impactful solutions You should be a selfmotivated individual with a passion for problemsolving and a drive to stay uptodate with the latest data engineering trends and best practices    About the team    ABOUT AWS  Diverse Experiences  Amazon values diverse experiences Even if you do not meet all of the preferred qualifications and skills listed in the job description we encourage candidates to apply If your career is just starting hasn’t followed a traditional path or includes alternative experiences don’t let it stop you from applying    Why AWS  Amazon Web Services AWS is the world’s most comprehensive and broadly adopted cloud platform We pioneered cloud computing and never stopped innovating — that’s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses  WorkLife Balance  We value worklife harmony Achieving success at work should never come at the expense of sacrifices at home which is why we strive for flexibility as part of our working culture When we feel supported in the workplace and at home there’s nothing we can’t achieve in the cloud  Inclusive Team Culture  Here at AWS it’s in our nature to learn and be curious Our employeeled affinity groups foster a culture of inclusion that empower us to be proud of our differences Ongoing events and learning experiences including our Conversations on Race and Ethnicity CORE and AmazeCon gender diversity conferences inspire us to never stop embracing our uniqueness  Mentorship and Career Growth  We’re continuously raising our performance bar as we strive to become Earth’s Best Employer That’s why you’ll find endless knowledgesharing mentorship and other careeradvancing resources here to help you develop into a betterrounded professional    We are open to hiring candidates to work out of one of the following locations    Austin TX USA  Irvine CA USA  Seattle WA USA     Experience with AWS technologies like Redshift S3 AWS Glue EMR Kinesis FireHose Lambda and IAM roles and permissions Experience with big data technologies such as Hadoop Hive Spark EMR  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus    Our compensation reflects the cost of labor across several US geographic markets The base pay for this position ranges from 105700year in our lowest geographic market up to 205600year in our highest geographic market Pay is based on a number of factors including market location and may vary depending on jobrelated knowledge skills and experience Amazon is a total compensation company Dependent on the position offered equity signon payments and other forms of compensation may be provided as part of a total compensation package in addition to a full range of medical financial andor other benefits For more information please visit httpswwwaboutamazoncomworkplaceemployeebenefits This position will remain posted until filled Applicants should apply via our internal or external career site </data></node>
<node id="n737" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0f6a38f0300a2823&amp;bb=HKH-ysDsRD4PV8MpMJn7w4TbaxzNMoh1LZaBIqrFU3zCGknLWCZzo7w4HQXIfTzhT1NT9qNKKdaKM_08jVNJTUUvIfpCj5Bnwpu81V1DXcI59qhx9AG_Gw%3D%3D&amp;xkcb=SoCG67M3CNjvTlWbHp0MbzkdCdPP&amp;fccid=9dd30dd046d9ac7a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Unit testingYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay139800  207325 a yearShift and scheduleOn call Location999 3rd Avenue Seattle WA 98104 BenefitsPulled from the full job descriptionHealth insurancePaid parental leavePaid time offParental leave Full job description Company Overview   DocuSign helps organizations connect and automate how they agree Our flagship product eSignature is the world’s 1 way to sign electronically on practically any device from virtually anywhere at any time Today more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives      What youll do   The Common Data platform team is chartered to build the high availability data backend for AI analytics and reporting needs for DocuSign customers We create maintain and operate scalable technology and data solutions that deliver an exceptional experience for our customers We embrace Agile principles and values favor DevOps practices and view infrastructure as code all while we create an infrastructure that scales and supports our growth and an ambitious vision This requires a hardworking highly collaborative team who can identify investigate and implement new technologies to continue securely scaling our global business     The team is looking for Software Engineers with proven track records to join our development team in Seattle or San Francisco This position will be part of a team working on an architectural shift that will allow us to scale to meet the next 10x of growth We need engineers who are ready to grapple with unique requirements of partner teams and build a cloud development and execution environment We need expertise to configure deploy and execute Azure cloud solutions to deliver bigdata processing as well as distributed REST API services for customer facing products This position will demand critical thinking skills ability to learn and understand complex cloud platforms and the ability to work in agile environments     This position is an individual contributor role reporting to the Director of Engineering      Responsibility    Drive design implementation testing and release of products  Build big data pipelines and analytics infrastructure on Azure with Data Factory Databricks Event Hub Data Explorer Cosmos DB and Azure RDBMS platforms  Build secure networking and reliable infrastructure for High Availability and Disaster Recovery  Build big data streaming solutions with 100s of concurrent publishers and subscribers  Collaborate closely with Product Design and Engineering teams to build new features  Participate in an Agile environment using Scrum software development practices code review automated unit testing endtoend testing continuous integration and deployment  Think about how to solve problems at scale and build faulttolerant systems that leverage telemetry and metrics  Investigate fix and maintain code as needed for production issues  Operate high reliability high availability service and participate in oncall rotation     Job Designation   Hybrid Employee divides their time between inoffice and remote work Access to an office location is required Frequency Minimum 2 days per week may vary by team but will be weekly inoffice expectation     Positions at DocuSign are assigned a job designation of either In Office Hybrid or Remote and are specific to the rolejob Preferred job designations are not guaranteed when changing positions within DocuSign DocuSign reserves the right to change a positions job designation depending on business needs and as permitted by local law     What you bring   Basic    BS degree in Computer Science Engineering or equivalent  8 years of experience within a software engineering related field  Experience with data modeling with NoSQL andor SQL  Experience in cloud platforms  Experience in modern server side development using modern programming languages like C or others  Experience using Git or other version control systems and CICD systems  Experience in writing high quality code that is easy to be maintained by others  Experience in agile methodologies     Preferred    Experience with building cloud solutions on Azure  Strong interest or documented experience in large scale microservice architectures on Kubernetes  Experience building large data lakes and data warehouses  Proficiency in big data processing in Apache Spark with Python or Scala  Proficiency in data streaming applications with Event HubKafka  Spark streaming  Proficiency in data pipeline orchestration with Data Factory or similar  A track record of being a selfstarter  Individualteam responsibility is our main driver in the development work      Wage Transparency   Based on applicable legislation the below details pay ranges in the following locations      California 146800  235025 base salary      Washington and New York including NYC metro area 139800  207325 base salary      This role is also eligible for bonus equity and    benefits     Global benefits provide options for the following       Paid Time Off earned time off as well as paid company holidays based on region  Paid Parental Leave take up to six months off with your child after birth adoption or foster care placement  Full Health Benefits Plans options for 100 employer paid and minimum employee contribution health plans from day one of employment  Retirement Plans select retirement and pension programs with potential for employer contributions  Learning and Development options for coaching online courses and education reimbursements  Compassionate Care Leave paid time off following the loss of a loved one and other lifechanging events     Life at DocuSign   Working here      DocuSign is committed to building trust and making the world more agreeable for our employees customers and the communities in which we live and work You can count on us to listen be honest and try our best to do what’s right every day At DocuSign everything is equal      We each have a responsibility to ensure every team member has an equal opportunity to succeed to be heard to exchange ideas openly to build lasting relationships and to do the work of their life Best of all you will be able to feel deep pride in the work you do because your contribution helps us make the world better than we found it And for that you’ll be loved by us our customers and the world in which we live      Accommodation      DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures If you need such an accommodation including an accommodation to properly use our online system you may contact us at accommodationsdocusigncom      If you experience any technical difficulties or issues during the application process or with our interview tools please get in touch with us at taopsdocusigncom for assistance       Our global benefits     Paid time off       Take time to unwind with earned days off plus paid company holidays based on your region           Paid parental leave       Take up to six months off with your child after birth adoption or foster care placement           Full health benefits       Options for 100 employerpaid health plans from day one of employment           Retirement plans       Select retirement and pension programs with potential for employer contributions           Learning  development       Grow your career with coaching online courses and education reimbursements           Compassionate care leave       Paid time off following the loss of a loved one and other lifechanging events          </data></node>
<node id="n738" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=c757e7531a810657&amp;bb=HKH-ysDsRD4PV8MpMJn7w73Gax-yWY4V-PHmsWAkA0pxpigVdDp_ELv6ngiS4lQ-SzMS8iXXi7VFJF1ONG9rlkfne9h-gaJa6C8qXIgtZMKTlA-3tGjJnQ%3D%3D&amp;xkcb=SoBv67M3CNjvTlWbHp0DbzkdCdPP&amp;fccid=ae3ba26fed950f3e&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SpringYesNoEducationDo you have a Bachelors degreeYesNo LocationAsheville NC Full job description Role Data Engineer Software Developer   Asheville NC Hybrid   Long term role until 2026 end    Description Join the forefront of scientific innovation at Clients National Centers for Environmental Information NCEI as a Scientific Research and Data Analyst within the Climate Science and Services Division CSSD Climate Information Services Branch In this role you will play a key part in assembling maintaining and developing highquality instrumental datasets for targeted stakeholder communities and commercial sectors This task involves developing new products and services that are grounded in documented user requirements These products can have a regional national or global focus and they can employ a variety of NCEI NOAA and other data Development activities will employ an approach known as the coproduction of knowledge” wherein developers continuously communicate with practitioners to ensure that products and services are useful and used    Responsibilities   Design develop test and deploy operational software that generates new datasets products services and reports Tasks include but are not limited to code development database development web development integration testing readiness review and operational release Incorporate NOAA and NCEI IT requirements eg security protocols ad hoc changes into operational software and new releases Manage software using NCEI source code tools and maintain requisite documentation eg business rules operational procedures   Required Skills   Experience with the full software development lifecycle SDLC in an Agile environment Experience in the development and maintenance of scientific operational software Proficiency in a diverse range of programming languages including ArcGIS Fortran and Python Experience producing data visualizations from a variety of datasets and file formats Ability to communicate effectively with a geographically dispersed team Familiarity with using a code repository such as GIT Excellent documentation skills to maintain business rules and operational procedures   Preferred Skills   Familiarity with additional programming languages and development environments including OracleAPEX Groovy JavaJavascript React and Spring Familiarity with NOAA and NCEI IT requirements including security protocols Familiarity with climate science and the development of datasets products services and reports derived from climatological data Knowledge of data access and retrieval processes Familiarity with developing deploying and maintaining software in an AWS environment Proven experience in leading migration projects to cloudbased platforms   Minimum QualificationRequirements   Bachelors degree in computer science or a relevant field 5 years of relevant work experience   </data></node>
<node id="n739" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=b5e24534f5574ed3&amp;bb=HKH-ysDsRD4PV8MpMJn7w8OSs9SN48_rrqI-fMIcMamrHMDqblBT7dKWDgs-45cbzhVQQ-ZV3ppFPd8NS3uRpDyXvvaX6N7XJLr0h8VSPwI%3D&amp;xkcb=SoDb67M3CNjvTlWbHp0CbzkdCdPP&amp;fccid=66403b30a2c0d89c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Spring BootYesNoEducationDo you have a Bachelors degreeYesNo LocationRemote Full job description             Remote United States                      Data Platform                     At Netflix we want to entertain the world and are constantly innovating on how entertainment is imagined created and delivered to a global audience We currently stream content in more than 30 languages in 190 countries topping over 220 million paid subscribers and are expanding into new forms of entertainment such as gaming        The data infrastructure teams at Netflix enable us to leverage data to bring joy to our members in many different ways We provide centralized data platforms and tools for various business functions at Netflix so they can utilize our data to make critical datadriven decisions We do all the heavy lifting to make it easy for our business partners to work with data efficiently securely and responsibly We aspire to lead the industry standard in building a worldclass data infrastructure as Netflix leads the way to be the most popular and pervasive destination for global internet entertainment        We are looking for distributed systems engineers to help evolve and innovate our infrastructure We are committed to building a diverse and inclusive team to bring new perspectives as we solve the next set of challenges In addition we are open to remote candidates We value what you can do from anywhere in the US        Spotlight on Data Infrastructure Teams        Database Access Platform Learn More           Our team champions advancements in data abstractions building streamlined abstractions atop distributed data stores like Apache Cassandra Elasticsearch Memcached and S3 We cater to diverse Netflix use cases including counters flexible keyvalues and time series Beyond this were the custodians of Hollow Netflixs robust memory colocated dataset library for efficient publishing and consumption With Hollow we’re setting new standards in reducing the footprint of inmemory datasets while allowing rapid access to data benefiting hundreds of Netflix applications across all business verticals Our mandate is clear empower Netflixs microservices to meet their increasing and dynamic data requirements        Your Role           Join us as a Senior Software Engineer on the Data Access Platform team Your main task will be developing and advancing the opensource Hollow library and its use at Netflix via abstractions Hollow is used broadly at Netflix in its mission to entertain the world Youll collaborate closely with various teams lead crossfunctional projects and share our experiences with the opensource community        Check out the Netflix OSS Hollow and hear more about our team on the CDE Channel        Data Platform Infrastructure Learn More           The Data Platform Infrastructure team acts as a platform for our own data platforms Our shared infrastructure and tooling enable Netflix to quickly innovate on providing stateoftheart data and analytics systems to the rest of the company without building bespoke scaffolding for each new system To do this we create highleverage infrastructure control and deployment systems that are finetuned for the needs of running our data systems at scale uniquely many of our tools and systems are written in Python and Go so this is a great team to consider if you enjoy working in a variety of languages        Your Role           As a Senior Software Engineer on the Data Infrastructure team you will play an essential role in designing developing and maintaining our data infrastructure Your work will help Netflix to innovate quickly by providing the company with stateoftheart analytics platforms and data stores You will help to provide the fundamental infrastructure that highscale and highcritical datastore teams at Netflix hosting 1000s of clusters of Cassandra EVCache Elasticsearch and RDS build their control planes on top of You will work closely with various teams and lead crossfunctional initiatives to ensure our infrastructure is efficient scalable and reliable        This would be your dream job if you enjoy   Solving real business needs at large scale by applying your software engineering and analytical problem solving skills  Architecting and building a robust scalable and highly available distributed infrastructure  Leading crossfunctional initiatives and collaborating with engineers product managers and TPM across teams  Sharing our experiences with the open source communities and contributing to Netflix OSS     About you   7 years experience in crafting complex scalable distributed data infrastructure  Proficiency in Java C Golang or Python with a solid understanding of multithreading and memory management  Proven track record of developing and maintaining highimpact systems  Experience building and operating scalable faulttolerant distributed systems  You have a BS in Computer Science or a related field  Familiarity with library development DI frameworks preferably SpringBoot and container technologies  Previous exposure to inmemory dataset systems and Hollow experience is a plus for the Database Access team         A few more things about us        As a team we come from many different countries and our fields of education range from the humanities to engineering to computer science Our team includes product managers program managers designers fullstack developers distributed systems engineers and data scientists Folks have the opportunity to wear different hats should they choose to We strongly believe this diversity has helped us build an inclusive and empathetic environment and look forward to adding your perspective to the mix        At Netflix we carefully consider a wide range of compensation factors to determine your personal top of market We rely on market indicators to determine compensation and consider your specific job family background skills and experience to get it right These considerations can cause your compensation to vary and will also be dependent on your location        The overall market range for roles in this area of Netflix is typically 100000  700000        This market range is based on total compensation vs only base salary which is in line with our compensation philosophy Our culture is unique and we tend to live by our values so it’s worth learning more about Netflix here         </data></node>
<node id="n740" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=db2f482c19f350c2&amp;bb=HKH-ysDsRD4PV8MpMJn7w4NuPaTjy9QPKmmFi-J1Pk5yT3cXsd7L5eoGXChyx193vdD3FGEOcvqGgy4f_r70ygllOMRn4T8Kj6eED0PG-MmBZgzxI8G7Tw%3D%3D&amp;xkcb=SoBG67M3CNjvTlWbHp0BbzkdCdPP&amp;fccid=ea0c1f50cbe7289b&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in PythonYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationLas Vegas NV BenefitsPulled from the full job description401k 6 Match401k matchingDisability insuranceEmployee stock ownership planFlexible scheduleHealth insurancePaid time offShow morechevron down Full job description    Own Your Future         Modern Technology Solutions Inc MTSI is seeking a Electronic Warfare Data Analyst  Engineer to join our growing team in Las Vegas NV This position will require working at a remote work location      Why is MTSI known as a Great Place to Work    Interesting Work Our coworkers support some of the most important and critical programs to our national defense and security  Values Our first core value is that employees come first We challenge our coworkers to provide the highest level of support and service and reward them with some of the best benefits in the industry  100 Employee Ownership we have a stake in each others success and the success of our customers Its also nice to know whats going on across the company we have company wide townhall meetings three times a year  Great Benefits  Most FullTime Staff Are Eligible for   Starting PTO accrual of 20 days PTOyear  10 holidaysyear  Flexible schedules  6 401k match with immediate vesting  Semiannual bonus eligibility July and December  Company funded Employee Stock Ownership Plan ESOP  a separate qualified retirement account  Up to 10000 in annual tuition reimbursement  Other company funded benefits like life and disability insurance  Optional zero deductible Blue CrossBlue Shield health insurance plan   Track Record of Success We have grown every year since our founding in 1993      Modern Technology Solutions Inc MTSI is a 100 employeeowned engineering services and solutions company that provides highdemand technical expertise in Digital Transformation Modeling and Simulation Rapid Capability Development Test and Evaluation Artificial Intelligence Autonomy Cybersecurity and Mission Assurance         MTSI delivers capabilities to solve problems of global importance Founded in 1993 MTSI today has employees at over 20 offices and field sites worldwide      For more information about MTSI please visit wwwmtsivacom    Responsibilities     This individual will possess operational and test experience with Electronic Warfare systems They will be tasked to support analysis of electronic warfare systems This individual will participate and monitor the execution of DTOT missions and processes and analyze and evaluate mission data and report results providing feedback to the customer      ROLE AND RESPONSIBILITIES     Electronic Warfare Systems Analysis Develop test scenarios write test plans execute tests and analyze the data then write the report of your findings Air Operation Analysis Provide expert guidance and oversight on Air Defense Systems ADS analysis both in a model and simulation environment and in an open air test environment Flight Test  o Perform mechanicalelectrical review of EW systems subsystems and components to ensure proper development of test documentation and data collection requirements    o Participate in development of test documentation such as the Test Verification plans test information sheets test matrices test cards data analysis plans etc    o Provide EW support for design reviews test planning working groups test card prep execution and post flight reports    Qualifications     EXPERIENCE     Minimum of 2 years of related experience    Experience with Python MatLab or similar analytical tools experience is desired Knowledge of and an understanding of concepts principles and practices of electronic warfare and survivability testing and analysis Experience with ground radar systems and testing with a clear understanding of flight test principles and discipline Excellent communication and analytical skills Working knowledge of computer systems software and current modeling tools Experience using data analysis tools Previous experience as aircrew member Pilot EWO NAV etc as plus     EDUCATION     Bachelor’s degree or Masters degree in engineering or other mathscience      CLEARANCE     Current TopSecret security clearance required      Travel    Approximately 10 travel may be required      US Citizenship is required      LIRR1       MTSIjobs       mtsi    </data></node>
<node id="n741" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=b1535cd9918180a5&amp;bb=HKH-ysDsRD4PV8MpMJn7wxZVQBABGDOZlI08Xyk4sgKpzbLwKJgREO--3EwOTMQDD_LERd2C4SF-F0h1DIcPWM7YHrZbY6xo7H8zudIFSLJVxy6cCs0hUA%3D%3D&amp;xkcb=SoDy67M3CNjvTlWbHp0AbzkdCdPP&amp;fccid=7d7b563c6a3a9653&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TSQLYesNoEducationDo you have a High school diploma or GEDYesNo Job detailsHere’s how the job details align with your profilePay85000 a yearJob typeFulltime Location255 Greenwich Street Manhattan NY 10007 BenefitsPulled from the full job descriptionLoan forgiveness Full job description     Data Analytics and Reporting Engineer           Apply                 Data Analytics and Reporting Engineer      Agency MAYORS OFFICE OF CONTRACT SVCS              Posted on 03302024             Job type Fulltime       Location  MANHATTAN      Title Classification No exam required        Department Chief Technology Office            Salary range 8500000 – 8500000               Job Description           The Mayors Office of Contract Services MOCS is a New York City oversight and service agency that manages procurement citywide from planning and release of agency solicitations to payment of vendors Annually agencies procure billions in products and services from a diverse pool of vendors that represent various industries MOCS therefore aims to ensure that the procurement process remains fair transparent efficient and costeffective         MOCS makes it easier to do business through use of endtoend technology tools increases transparency by publication of enriched data and hosting public hearings and strengthens procurement operations by providing direct assistance and resources to all stakeholders MOCS also partners with agencies and vendors to identify areas for policy reform resulting in ongoing process improvement to reduce administrative burdens and increase the positive impact of services on communities The MOCS Director serves as the City Chief Procurement Officer         MOCS team members operate in a collaborative serviceoriented environment where flexibility and ability to achieve results are valued Staff must conduct all duties relevant to their position in their assigned division and demonstrate an advanced level of expertise This position requires a focus on aligning daily operations to the agencys strategic priorities engage stakeholders in planning and ensure performance using welldefined success metrics and project management strategies All staff are expected to deliver timely and quality work products and services participate in ongoing improvement activities proactively deepen their knowledge of procurement and government operations and will use modern technology software and hardware to complete daily duties This position must collaborate with other team members to implement projects help to maintain andor analyze operational data and interact with external stakeholders         The Data Analytics and Reporting Engineer will report to the Senior Associate Director of Reporting and Analytics at MOCS The Data Analytics and Reporting Engineer will focus on ensuring the effective design and implementation of data products and reporting solutions This position requires a detailedoriented handson contributor who will work closely with MOCS business and BI teams on the analysis and design of these solutions The position will perform the following job responsibilities which include and are not limited to          Responsibilities          Manage reporting needs to include generating and validating routine scheduled unique and ad hoc reports and data extracts especially as regards MOCS Power BI technical resource Collect requirements design build and test reports and dashboards across applications and programs Perform ad hoc data analysis as required Recommend process procedural reporting and system changes that will improve and enhance the Digital Platform Solutions data processes and pipelines Tracking and reporting to manage progress of report completion Ensure reports are filed timely and accurately and are in agreement with other externally reporting information Determine ELTETL requirements and assist with production setup and execution of migrations Identify and resolve data technical issues and mediate business impact Engage in professional development to strengthen skills and increase knowledge in relevant areas of procurement technology government operations public policy and people and change management Understand issues affecting relevant stakeholder groups including but not limited to covered city agencies Minority and Womenowned Business Enterprises MWBEs nonprofits etc Special projects as assigned     Minimum Qualifications        1 Do you have a baccalaureate degree from an accredited college or university  2 Do you have an associate degree from an accredited college or university along with two 2 years of experience with administrative analytic coordinative supervisory or liaison responsibilities  3 Do you have a fouryear high school diploma or its educational equivalent approved by a states Department of Education or a recognized accrediting organization along with four 4 years of experience as described in question 2 above  4 Do you have a satisfactory combination of education andor experience equivalent to that described in questions 1 2 andor 3 above   Preferred Skills        Demonstrate competency andor history of consistent quality performance in the following areas of responsibility  Excellent writing and communication skills  Experienced delivering solutions with Business Intelligence tools such as Power BI AWS QuickSight OBIEE Knowledge of DAX is a plus  Ability to write complex procedures using SQL TSQL Postgres mySQL etc  Understanding of data governance frameworks  Excellent analytical organizational and presentation skills ability to handle multiple tasks under deadline  Qualitative and quantitative data analysis skills statistical analysis a plus  Knowledge of New York City’s data share platforms or procurement systems is a plus Open Data PASSPort FMS Checkbook etc  Working knowledge of database backend systems and data warehouse con               Public Service Loan Forgiveness        As a prospective employee of the City of New York you may be eligible for federal loan forgiveness programs and state repayment assistance programs For more information please visit the US Department of Education’s website at httpsstudentaidgovpslf               Residency Requirement        New York City residency is generally required within 90 days of appointment However City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau Suffolk Putnam Westchester Rockland or Orange County To determine if the residency requirement applies to you please discuss with the agency representative at the time of interview               Additional Information         The City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic including but not limited to an individuals sex race color ethnicity national origin age religion disability sexual orientation veteran status gender identity or pregnancy     Minimum Qualifications       1 Do you have a baccalaureate degree from an accredited college or university  2 Do you have an associate degree from an accredited college or university along with two 2 years of experience with administrative analytic coordinative supervisory or liaison responsibilities  3 Do you have a fouryear high school diploma or its educational equivalent approved by a states Department of Education or a recognized accrediting organization along with four 4 years of experience as described in question 2 above  4 Do you have a satisfactory combination of education andor experience equivalent to that described in questions 1 2 andor 3 above   Preferred Skills       Demonstrate competency andor history of consistent quality performance in the following areas of responsibilitynn Excellent writing and communication skillsn Experienced delivering solutions with Business Intelligence tools such as Power BI AWS QuickSight OBIEE Knowledge of DAX is a plus n Ability to write complex procedures using SQL TSQL Postgres mySQL etcn Understanding of data governance frameworksn Excellent analytical organizational and presentation skills ability to handle multiple tasks under deadlinen Qualitative and quantitative data analysis skills statistical analysis a plus n Knowledge of New York City’s data share platforms or procurement systems is a plus Open Data PASSPort FMS Checkbook etcn Working knowledge of database backend systems and data warehouse con             Public Service Loan Forgiveness       As a prospective employee of the City of New York you may be eligible for federal loan forgiveness programs and state repayment assistance programs For more information please visit the US Department of Education’s website at httpsstudentaidgovpslf             Residency Requirement       New York City residency is generally required within 90 days of appointment However City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau Suffolk Putnam Westchester Rockland or Orange County To determine if the residency requirement applies to you please discuss with the agency representative at the time of interview             Additional Information        The City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic including but not limited to an individuals sex race color ethnicity national origin age religion disability sexual orientation veteran status gender identity or pregnancy                Job ID  631992      Title code 0527A       Civil service title RESEARCH PROJECTS COORMAMGR       Title classification Pending Classification2       Business title Data Analytics and Reporting Engineer       Posted until 20240411       Experience level Manager        Number of positions 1       Work location 255 Greenwich Street       Category Technology Data  Innovation          </data></node>
<node id="n742" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=26719e37964f41b7&amp;bb=HKH-ysDsRD4PV8MpMJn7w4e3Xv0RU8ZnR3wlzpI_Ugl-YDUHmN90daICNEouYr9nSAuMfIsrcKvulNGu17t5W9a8DvJhnOTkOZnE1i0NpWs%3D&amp;xkcb=SoB867M3CNjvTlWbHp0HbzkdCdPP&amp;fccid=cd22d01053af7669&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in OracleYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationUnited States Full job description In response to increased concurrent projects for the design and delivery of our data centers Oracle is recruiting a Senior Data Center Design Electrical Engineer The role is a senior multidisciplinary data center design lead with an electrical background charged with the direct interface between Oracle and our data center partners in the Electrical design field including but not limited to power generation storage and delivery grounding lightening protection fire alarm etc systems for both new site builds and expansions or refurbishments of existing facilities With extensive experience designing and delivering mission critical facilities and professional qualifications in electrical engineering the role is responsible for all design decisions related to assigned data center projects site selection new construction phased expansion retrofits and upgrades and acquisition conversions  Career Level  IC5                                             1 Leads and manages a project or other design and engineering initiatives Provides guidance and engineering leadership in ensuring project or other design and engineering initiatives are meeting or exceeding company expectations                        2Expert operating knowledge of engineering systems to include advanced diagnostics and repairs Ability to apply knowledge of Oracle processes and procedures and industry standards to resolve nonroutine issues Utilizes safe working practices at an EXPERT level eg can apply procedure for lockouttag out can explain MSDS etc Evaluates and assures the safe working practices of others Interacts with other engineering disciplines Works with the colocation providers engineering teams to ensure electrical systems are adequately designed specified and installed to deliver robust operation for Oracle                        3 Contributes to identifying and developing training programs for newer members of the team as it grows Acquires knowledge by expanding experience with systems vendor training participation in industry groups or meetings and shadowing others Is SME with many systems and trains others inside and outside of the group                        4 Provides expert input for effective contract administration including generation and review of contracts change orders cost forecasts and other pertinent documents and documentation                        5 Contributes to mentoring junior team members Directs all internal and external project team members delivering data centers or partdata centers for Oracle Expert level communication to include cross functional SOP development and relevant technical writing Interactions are primarily to exchange information between departments within the organization                        6 Contributes to creating and maintaining best in class policies and procedures Participates in review of policy and procedure documents Develops policy documents with the input of others Assures adherence to developed standards and policy through review of documentation participation in commissioning activities design summits and datacenter commissioning or reviews                        7 Consistently solves complex crossfunctional issues requiring independent action and a high degree of initiative to resolve Makes recommendations for system level enhancements to eliminate potential problems Tests suitability of components or action plans for deployments                        8 Has an expert level system knowledge Is a subject matter expert to others in and outside of the group Extensive knowledge of multiple facets of the relevant engineering discipline Develops and executes projects Leads internal andor external engineering or construction related seminarsconferences1 Leads and manages a project or other design and engineering initiatives Provides guidance and engineering leadership in ensuring project or other design and engineering initiatives are meeting or exceeding company expectations                        2Expert operating knowledge of engineering systems to include advanced diagnostics and repairs Ability to apply knowledge of Oracle processes and procedures and industry standards to resolve nonroutine issues Utilizes safe working practices at an EXPERT level eg can apply procedure for lockouttag out can explain MSDS etc Evaluates and assures the safe working practices of others Interacts with other engineering disciplines Works with the colocation providers engineering teams to ensure electrical systems are adequately designed specified and installed to deliver robust operation for Oracle                        3 Contributes to identifying and developing training programs for newer members of the team as it grows Acquires knowledge by expanding experience with systems vendor training participation in industry groups or meetings and shadowing others Is SME with many systems and trains others inside and outside of the group                        4 Provides expert input for effective contract administration including generation and review of contracts change orders cost forecasts and other pertinent documents and documentation                        5 Contributes to mentoring junior team members Directs all internal and external project team members delivering data centers or partdata centers for Oracle Expert level communication to include cross functional SOP development and relevant technical writing Interactions are primarily to exchange information between departments within the organization                        6 Contributes to creating and maintaining best in class policies and procedures Participates in review of policy and procedure documents Develops policy documents with the input of others Assures adherence to developed standards and policy through review of documentation participation in commissioning activities design summits and datacenter commissioning or reviews                        7 Consistently solves complex crossfunctional issues requiring independent action and a high degree of initiative to resolve Makes recommendations for system level enhancements to eliminate potential problems Tests suitability of components or action plans for deployments                        8 Has an expert level system knowledge Is a subject matter expert to others in and outside of the group Extensive knowledge of multiple facets of the relevant engineering discipline Develops and executes projects Leads internal andor external engineering or construction related seminarsconferences                                           </data></node>
<node id="n743" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=b0df6e0f4f43997c&amp;bb=HKH-ysDsRD4PV8MpMJn7w-ND0crI72GB89nLP2Vbx7Pvyez7sJy9R5d3Ap6nqCohCkGAxq4ibpTmudLg6931hD6GGAAB54Rc9OpE7Hs-1faBrYOBT9rQ0A%3D%3D&amp;xkcb=SoDI67M3CNjvTlWbHp0GbzkdCdPP&amp;fccid=dd7de5cf522ca5fc&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VirtualizationYesNo LocationDurham NC BenefitsPulled from the full job description401kFlexible scheduleHealth insurancePaid time offParental leaveRSU Full job description  Hungry Humble Honest with Heart     The Opportunity      Are you a proactive and detailoriented QA Engineer with experience of testing enterpriselevel software If so join our team at Nutanix and work alongside brilliant software engineers in a fastpaced environment where youll have the autonomy to build automation and break cuttingedge data protection solutions for hybrid cloud infrastructure       Engineering  RD at Nutanix      The QA Engineer Data Protection role would be part of Nutanixs Disaster Recovery and Backup BCDR team DR QA is a team of 50 engineers across Bangalore Sanjose Pune and Durham This team is highly collaborative skilled and friendly fostering a learning and knowledge sharing culture The team members work together to solve problems efficiently and effectively They closely collaborate with other teams across the organization providing opportunities for professional development and growth The team follows a hybrid working style coming to the office twothree days a week and valuing flexible schedules and transparent knowledge sharing through written documentation Overall its a dynamic and supportive team to be a part of       Your Role    End to end System Testing of product to mimic customer scenarios to qualify a product for a release  Design complex test beds and tools to maintain them  Build and automate detailed System Test Plans Execute tests to identify bugs Analyze and find root causes of failures      What You Will Bring    68 years of software engineering experience  Experience in PythonPerlCCJava  Ability to debug and extend automated tests  Skills in debugging complex distributed applications  Experience in test plan design and development  Experience testing a systems product  Strong fundamentals in OOP algorithms data structures and problemsolving Experience in enterprise networking and virtualization concepts      About the Team    Meeting the hiring manager      Hello this is Narayan I joined Nutanix 10 years ago and have worked in the Disaster Recovery and Backup BCDR team during that time I started as a System Test Engineer and built the Durham QA team Over the last few years I have taken over the responsibility of leading all the QA teams for our BCDR group which is spread across multiple cities in India and US     What the Team Says      “Our team is a highly collaborative skilled and friendly group of engineers who foster a culture of learning and knowledge sharing This creates an environment where any problem can be solved efficiently and effectively Our work usually requires close collaboration with other teams across the organization providing a valuable opportunity for professional development and growth”     How we work      We work in a hybrid style and all of our team members would come to the office 2 days a week We are trying to minimize the number of meetings and nurture the culture of writing and sharing documents which helps us achieve a more flexible schedule and transparent knowledge sharing We also have twice a week Nutanix lunch days that are a great opportunity to connect in person with the team and other colleagues         The pay range for this position at commencement of employment is expected to be between USD  128000 and USD  258000 per year       However base pay offered may vary depending on multiple individualized factors including market location jobrelated knowledge skills and experience The total compensation package for this position may also include other elements including a signon bonus restricted stock units and discretionary awards in addition to a full range of medical financial andor other benefits including 401k eligibility and various paid time off benefits such as vacation sick time and parental leave dependent on the position offered Details of participation in these benefit plans will be provided if an employee receives an offer of employment       If hired employee will be in an “atwill position” and the Company reserves the right to modify base salary as well as any other discretionary payment or compensation program at any time including for reasons related to individual performance Company or individual departmentteam performance and market factors    </data></node>
<node id="n744" labels=":Skill"><data key="labels">:Skill</data><data key="name">office</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n745" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data solutions engineer</data></node>
<node id="n746" labels=":Skill"><data key="labels">:Skill</data><data key="name">customer needs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n747" labels=":Skill"><data key="labels">:Skill</data><data key="name">realtime data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n748" labels=":Skill"><data key="labels">:Skill</data><data key="name">project designs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n749" labels=":Skill"><data key="labels">:Skill</data><data key="name">geo location</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n750" labels=":Skill"><data key="labels">:Skill</data><data key="name">data solutions</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n751" labels=":Skill"><data key="labels">:Skill</data><data key="name">data engineer</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n752" labels=":Skill"><data key="labels">:Skill</data><data key="name">developing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n753" labels=":Skill"><data key="labels">:Skill</data><data key="name">constructing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n754" labels=":Skill"><data key="labels">:Skill</data><data key="name">maintaining</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n755" labels=":Skill"><data key="labels">:Skill</data><data key="name">data platform</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n756" labels=":Skill"><data key="labels">:Skill</data><data key="name">pipelines</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n757" labels=":Skill"><data key="labels">:Skill</data><data key="name">bi</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n758" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">automating manual processes</data></node>
<node id="n759" labels=":Skill"><data key="labels">:Skill</data><data key="name">optimizing data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n760" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">redesigning infrastructure</data></node>
<node id="n761" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">‘big data’</data></node>
<node id="n762" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">downstream applications</data></node>
<node id="n763" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">selfservice solutions</data></node>
<node id="n764" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">consistency of data</data></node>
<node id="n765" labels=":Skill"><data key="labels">:Skill</data><data key="name">data warehouses</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n766" labels=":Skill"><data key="labels">:Skill</data><data key="name">data stores</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n767" labels=":Skill"><data key="labels">:Skill</data><data key="name">s3</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n768" labels=":Skill"><data key="labels">:Skill</data><data key="name">athena</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n769" labels=":Skill"><data key="labels">:Skill</data><data key="name">ec2</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n770" labels=":Skill"><data key="labels">:Skill</data><data key="name">emr</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n771" labels=":Skill"><data key="labels">:Skill</data><data key="name">eks</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n772" labels=":Skill"><data key="labels">:Skill</data><data key="name">dbt</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n773" labels=":Skill"><data key="labels">:Skill</data><data key="name">nifi</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n774" labels=":Skill"><data key="labels">:Skill</data><data key="name">dockerkubernetes</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n775" labels=":Skill"><data key="labels">:Skill</data><data key="name">edge computing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n776" labels=":Skill"><data key="labels">:Skill</data><data key="name">validation data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n777" labels=":Skill"><data key="labels">:Skill</data><data key="name">preprocessing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n778" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical parameter estimation</data></node>
<node id="n779" labels=":Skill"><data key="labels">:Skill</data><data key="name">maximum</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n780" labels=":Skill"><data key="labels">:Skill</data><data key="name">likelihood</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n781" labels=":Skill"><data key="labels">:Skill</data><data key="name">estimation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n782" labels=":Skill"><data key="labels">:Skill</data><data key="name">mle</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n783" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">maximum aposterior estimation</data></node>
<node id="n784" labels=":Skill"><data key="labels">:Skill</data><data key="name">map</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n785" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">chisquare coefficient</data></node>
<node id="n786" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">confidence intervals</data></node>
<node id="n787" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">technical documentation</data></node>
<node id="n788" labels=":Skill"><data key="labels">:Skill</data><data key="name">onenote</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n789" labels=":Skill"><data key="labels">:Skill</data><data key="name">hvac</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n790" labels=":Skill"><data key="labels">:Skill</data><data key="name">crac</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n791" labels=":Skill"><data key="labels">:Skill</data><data key="name">crah</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n792" labels=":Skill"><data key="labels">:Skill</data><data key="name">designing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n793" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">physical data models</data></node>
<node id="n794" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">reporting requirements</data></node>
<node id="n795" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">conducting interviews</data></node>
<node id="n796" labels=":Skill"><data key="labels">:Skill</data><data key="name">data dictionaries</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n797" labels=":Skill"><data key="labels">:Skill</data><data key="name">dbms</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n798" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data privacy regulations</data></node>
<node id="n799" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">database designing</data></node>
<node id="n800" labels=":Skill"><data key="labels">:Skill</data><data key="name">erwin</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n801" labels=":Skill"><data key="labels">:Skill</data><data key="name">er studio</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n802" labels=":Skill"><data key="labels">:Skill</data><data key="name">oltp</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n803" labels=":Skill"><data key="labels">:Skill</data><data key="name">predictive models</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n804" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">pipeline development</data></node>
<node id="n805" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">predictive modeling</data></node>
<node id="n806" labels=":Skill"><data key="labels">:Skill</data><data key="name">interrogation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n807" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">exploratory data analysis</data></node>
<node id="n808" labels=":Skill"><data key="labels">:Skill</data><data key="name">profiling</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n809" labels=":Skill"><data key="labels">:Skill</data><data key="name">reconciliation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n810" labels=":Skill"><data key="labels">:Skill</data><data key="name">data profiling</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n811" labels=":Skill"><data key="labels">:Skill</data><data key="name">analysis skills</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n812" labels=":Skill"><data key="labels">:Skill</data><data key="name">ml data pipelines</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n813" labels=":Skill"><data key="labels">:Skill</data><data key="name">glue</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n814" labels=":Skill"><data key="labels">:Skill</data><data key="name">nlp</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n815" labels=":Skill"><data key="labels">:Skill</data><data key="name">ml techniques</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n816" labels=":Skill"><data key="labels">:Skill</data><data key="name">sagemaker</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n817" labels=":Skill"><data key="labels">:Skill</data><data key="name">feature store</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n818" labels=":Skill"><data key="labels">:Skill</data><data key="name">dbaas</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n819" labels=":Skill"><data key="labels">:Skill</data><data key="name">cli</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n820" labels=":Skill"><data key="labels">:Skill</data><data key="name">helm charts</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n821" labels=":Skill"><data key="labels">:Skill</data><data key="name">software design</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n822" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">scientific analysis</data></node>
<node id="n823" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">mathematical models</data></node>
<node id="n824" labels=":Skill"><data key="labels">:Skill</data><data key="name">predict</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n825" labels=":Skill"><data key="labels">:Skill</data><data key="name">quicksight</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n826" labels=":Skill"><data key="labels">:Skill</data><data key="name">data aggregation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n827" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">software development testing</data></node>
<node id="n828" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">penetration testing</data></node>
<node id="n829" labels=":Skill"><data key="labels">:Skill</data><data key="name">regression tests</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n830" labels=":Skill"><data key="labels">:Skill</data><data key="name">automated tests</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n831" labels=":Skill"><data key="labels">:Skill</data><data key="name">test quality</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n832" labels=":Skill"><data key="labels">:Skill</data><data key="name">dns</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n833" labels=":Skill"><data key="labels">:Skill</data><data key="name">network design</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n834" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">develop new processes</data></node>
<node id="n835" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data infrastructure</data></node>
<node id="n836" labels=":Skill"><data key="labels">:Skill</data><data key="name">software products</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n837" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">processing pipelines</data></node>
<node id="n838" labels=":Skill"><data key="labels">:Skill</data><data key="name">feed data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n839" labels=":Skill"><data key="labels">:Skill</data><data key="name">gcp</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n840" labels=":Skill"><data key="labels">:Skill</data><data key="name">synapse</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n841" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">geospatial software</data></node>
<node id="n842" labels=":Skill"><data key="labels">:Skill</data><data key="name">cyber security</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n843" labels=":Skill"><data key="labels">:Skill</data><data key="name">monitoring</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n844" labels=":Skill"><data key="labels">:Skill</data><data key="name">data feeds</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n845" labels=":Skill"><data key="labels">:Skill</data><data key="name">integration test</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n846" labels=":Skill"><data key="labels">:Skill</data><data key="name">code reviews</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n847" labels=":Skill"><data key="labels">:Skill</data><data key="name">pentaho</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n848" labels=":Skill"><data key="labels">:Skill</data><data key="name">microservices</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n849" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data warehouse design</data></node>
<node id="n850" labels=":Skill"><data key="labels">:Skill</data><data key="name">pythonscala</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n851" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">software lifecycle</data></node>
<node id="n852" labels=":Skill"><data key="labels">:Skill</data><data key="name">unit test</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n853" labels=":Skill"><data key="labels">:Skill</data><data key="name">ingest</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n854" labels=":Skill"><data key="labels">:Skill</data><data key="name">transform</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n855" labels=":Skill"><data key="labels">:Skill</data><data key="name">load</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n856" labels=":Skill"><data key="labels">:Skill</data><data key="name">data accuracy</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n857" labels=":Skill"><data key="labels">:Skill</data><data key="name">consistency</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n858" labels=":Skill"><data key="labels">:Skill</data><data key="name">reliability</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n859" labels=":Skill"><data key="labels">:Skill</data><data key="name">delta lake</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n860" labels=":Skill"><data key="labels">:Skill</data><data key="name">google cloud</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n861" labels=":Skill"><data key="labels">:Skill</data><data key="name">automated testing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n862" labels=":Skill"><data key="labels">:Skill</data><data key="name">data streaming</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n863" labels=":Skill"><data key="labels">:Skill</data><data key="name">mlops</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n864" labels=":Skill"><data key="labels">:Skill</data><data key="name">design</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n865" labels=":Skill"><data key="labels">:Skill</data><data key="name">develop</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n866" labels=":Skill"><data key="labels">:Skill</data><data key="name">implement</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n867" labels=":Skill"><data key="labels">:Skill</data><data key="name">ingesting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n868" labels=":Skill"><data key="labels">:Skill</data><data key="name">transforming</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n869" labels=":Skill"><data key="labels">:Skill</data><data key="name">pinecone</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n870" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">large language model’s</data></node>
<node id="n871" labels=":Skill"><data key="labels">:Skill</data><data key="name">llama</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n872" labels=":Skill"><data key="labels">:Skill</data><data key="name">gpt4</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n873" labels=":Skill"><data key="labels">:Skill</data><data key="name">claude 20</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n874" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">text summarization</data></node>
<node id="n875" labels=":Skill"><data key="labels">:Skill</data><data key="name">entity extraction</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n876" labels=":Skill"><data key="labels">:Skill</data><data key="name">classification</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n877" labels=":Skill"><data key="labels">:Skill</data><data key="name">data lakes</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n878" labels=":Skill"><data key="labels">:Skill</data><data key="name">warehouses</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n879" labels=":Skill"><data key="labels">:Skill</data><data key="name">monitor</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n880" labels=":Skill"><data key="labels">:Skill</data><data key="name">optimize</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n881" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">clients requirements</data></node>
<node id="n882" labels=":Skill"><data key="labels">:Skill</data><data key="name">system design</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n883" labels=":Skill"><data key="labels">:Skill</data><data key="name">big query</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n884" labels=":Skill"><data key="labels">:Skill</data><data key="name">vertica</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n885" labels=":Skill"><data key="labels">:Skill</data><data key="name">bigtable</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n886" labels=":Skill"><data key="labels">:Skill</data><data key="name">cosmos db</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n887" labels=":Skill"><data key="labels">:Skill</data><data key="name">data factory glue</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n888" labels=":Skill"><data key="labels">:Skill</data><data key="name">streaming</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n889" labels=":Skill"><data key="labels">:Skill</data><data key="name">data catalogs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n890" labels=":Skill"><data key="labels">:Skill</data><data key="name">service catalogs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n891" labels=":Skill"><data key="labels">:Skill</data><data key="name">openmetadata</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n892" labels=":Skill"><data key="labels">:Skill</data><data key="name">datahub</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n893" labels=":Skill"><data key="labels">:Skill</data><data key="name">alation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n894" labels=":Skill"><data key="labels">:Skill</data><data key="name">glue catalog</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n895" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">google data catalog</data></node>
<node id="n896" labels=":Skill"><data key="labels">:Skill</data><data key="name">hdfs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n897" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">technical expertise</data></node>
<node id="n898" labels=":Skill"><data key="labels">:Skill</data><data key="name">sfdc</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n899" labels=":Skill"><data key="labels">:Skill</data><data key="name">jde</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n900" labels=":Skill"><data key="labels">:Skill</data><data key="name">data mapping</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n901" labels=":Skill"><data key="labels">:Skill</data><data key="name">data integration</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n902" labels=":Skill"><data key="labels">:Skill</data><data key="name">data privacy</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n903" labels=":Skill"><data key="labels">:Skill</data><data key="name">dataflow</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n904" labels=":Skill"><data key="labels">:Skill</data><data key="name">dataproc</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n905" labels=":Skill"><data key="labels">:Skill</data><data key="name">pubsub</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n906" labels=":Skill"><data key="labels">:Skill</data><data key="name">luigi</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n907" labels=":Skill"><data key="labels">:Skill</data><data key="name">ai platform</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n908" labels=":Skill"><data key="labels">:Skill</data><data key="name">automl</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n909" labels=":Skill"><data key="labels">:Skill</data><data key="name">hipaa</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n910" labels=":Skill"><data key="labels">:Skill</data><data key="name">ml models</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n911" labels=":Skill"><data key="labels">:Skill</data><data key="name">annotate data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n912" labels=":Skill"><data key="labels">:Skill</data><data key="name">enrich data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n913" labels=":Skill"><data key="labels">:Skill</data><data key="name">data loading</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n914" labels=":Skill"><data key="labels">:Skill</data><data key="name">manipulation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n915" labels=":Skill"><data key="labels">:Skill</data><data key="name">processing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n916" labels=":Skill"><data key="labels">:Skill</data><data key="name">analysis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n917" labels=":Skill"><data key="labels">:Skill</data><data key="name">exploration</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n918" labels=":Skill"><data key="labels">:Skill</data><data key="name">adls</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n919" labels=":Skill"><data key="labels">:Skill</data><data key="name">accurate</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n920" labels=":Skill"><data key="labels">:Skill</data><data key="name">consistent</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n921" labels=":Skill"><data key="labels">:Skill</data><data key="name">accessible</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n922" labels=":Skill"><data key="labels">:Skill</data><data key="name">powerapps</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n923" labels=":Skill"><data key="labels">:Skill</data><data key="name">extract datasets</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n924" labels=":Skill"><data key="labels">:Skill</data><data key="name">golden record</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n925" labels=":Skill"><data key="labels">:Skill</data><data key="name">data validation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n926" labels=":Skill"><data key="labels">:Skill</data><data key="name">apache flink</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n927" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">create architecture</data></node>
<node id="n928" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">crossfunctional requirements</data></node>
<node id="n929" labels=":Skill"><data key="labels">:Skill</data><data key="name">recommendation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n930" labels=":Skill"><data key="labels">:Skill</data><data key="name">streaming data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n931" labels=":Skill"><data key="labels">:Skill</data><data key="name">evaluation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n932" labels=":Skill"><data key="labels">:Skill</data><data key="name">datasets</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n933" labels=":Skill"><data key="labels">:Skill</data><data key="name">data reliability</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n934" labels=":Skill"><data key="labels">:Skill</data><data key="name">data visibility</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n935" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">loading of raw data</data></node>
<node id="n936" labels=":Skill"><data key="labels">:Skill</data><data key="name">model our data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n937" labels=":Skill"><data key="labels">:Skill</data><data key="name">data tagging</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n938" labels=":Skill"><data key="labels">:Skill</data><data key="name">devsecops</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n939" labels=":Skill"><data key="labels">:Skill</data><data key="name">net</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n940" labels=":Skill"><data key="labels">:Skill</data><data key="name">weblogic</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n941" labels=":Skill"><data key="labels">:Skill</data><data key="name">go</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n942" labels=":Skill"><data key="labels">:Skill</data><data key="name">data mesh</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n943" labels=":Skill"><data key="labels">:Skill</data><data key="name">data catalog</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n944" labels=":Skill"><data key="labels">:Skill</data><data key="name">flink</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n945" labels=":Skill"><data key="labels">:Skill</data><data key="name">microstrategy</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n946" labels=":Skill"><data key="labels">:Skill</data><data key="name">controlm</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n947" labels=":Skill"><data key="labels">:Skill</data><data key="name">github</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n948" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">automated continuous integration</data></node>
<node id="n949" labels=":Skill"><data key="labels">:Skill</data><data key="name">transformation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n950" labels=":Skill"><data key="labels">:Skill</data><data key="name">normalization</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n951" labels=":Skill"><data key="labels">:Skill</data><data key="name">extracting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n952" labels=":Skill"><data key="labels">:Skill</data><data key="name">loading</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n953" labels=":Skill"><data key="labels">:Skill</data><data key="name">storage</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n954" labels=":Skill"><data key="labels">:Skill</data><data key="name">nodejs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n955" labels=":Skill"><data key="labels">:Skill</data><data key="name">kinesis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n956" labels=":Skill"><data key="labels">:Skill</data><data key="name">firehose</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n957" labels=":Skill"><data key="labels">:Skill</data><data key="name">iam</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n958" labels=":Skill"><data key="labels">:Skill</data><data key="name">cloud development</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n959" labels=":Skill"><data key="labels">:Skill</data><data key="name">code development</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n960" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">database development</data></node>
<node id="n961" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">integration testing</data></node>
<node id="n962" labels=":Skill"><data key="labels">:Skill</data><data key="name">sdlc</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n963" labels=":Skill"><data key="labels">:Skill</data><data key="name">memcached</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n964" labels=":Skill"><data key="labels">:Skill</data><data key="name">time series</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n965" labels=":Skill"><data key="labels">:Skill</data><data key="name">inmemory datasets</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n966" labels=":Skill"><data key="labels">:Skill</data><data key="name">evcache</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n967" labels=":Skill"><data key="labels">:Skill</data><data key="name">multithreading</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n968" labels=":Skill"><data key="labels">:Skill</data><data key="name">memory management</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n969" labels=":Skill"><data key="labels">:Skill</data><data key="name">test reports</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n970" labels=":Skill"><data key="labels">:Skill</data><data key="name">obiee</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n971" labels=":Skill"><data key="labels">:Skill</data><data key="name">tsql</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n972" labels=":Skill"><data key="labels">:Skill</data><data key="name">postgres</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n973" labels=":Skill"><data key="labels">:Skill</data><data key="name">data protection</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n974" labels=":Skill"><data key="labels">:Skill</data><data key="name">system testing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n975" labels=":Skill"><data key="labels">:Skill</data><data key="name">execute tests</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n976" labels=":Skill"><data key="labels">:Skill</data><data key="name">oop</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n977" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=341c46f09f4e096c&amp;bb=9GK_E0ECeRmyEyJEGZ3o0TGqQr4cMgu_u42GaV2KqKSEImfcXy_QUhIaOCjDz5oFo6uzCIv7-gS0yfUUvhPajqbHBQshjr1hBWDPALux9so4uoyp3JPOvA%3D%3D&amp;xkcb=SoA267M3CNkF_ewiIp0LbzkdCdPP&amp;fccid=3e2905acd366ad0e&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Statistical analysisYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay92000  110000 a yearJob typeFulltime Location1830 West 38th Avenue Denver CO 80211 BenefitsPulled from the full job description401kDental insuranceEmployee assistance programHealth insurancePaid parental leaveParental leaveProfit sharingShow morechevron down Full job description Within our Corporate Technical Services  MES team located in Denver – Leprino has created a new opening for a Data Scientist I to elevate our manufacturing operations to new heights of manufacturing efficiencies through the use of IoT machine learning and artificial intelligence We take pride in our commitment to not only produce but to innovate for the future   At Leprino Foods starting compensation for this role typically ranges between 92000 and 110000 This position has an annual target bonus of 5    Develop predictive models that forecast production outcomes and pinpoint quality enhancements Engage in data collection employing advanced analytics to uncover optimization opportunities Lead indepth analyses to craft datadriven solutions for continuous process improvement Foster strong partnerships across functions to spearhead innovations in our manufacturing processes Stay at the forefront of data science continuously refining our practices with the latest methodologies Advocate for data literacy sharing insights that drive informed decisionmaking throughout Leprino Design experiments and interpret the results to draw detailed and actionable conclusions Implement advanced algorithms that enhance realtime decisionmaking capabilities Enhance data collection procedures to include information relevant to building analytic systems Apply quality control data validation and cleansing processes to ensure accuracy and integrity of data used for analysis Communicate complex data in a clear and articulate manner to crossfunctional teams and department leaders Proactively identify patterns and anomalies in large data sets to uncover valuable insights Innovate and refine machine learning infrastructure that supports complex model development Drive the adoption of data science methodologies in broader business practices Initiate and lead projects to advance the companys data science capabilities and vision Translate business objectives into actionable data projects and milestones     You Have At Least Required Qualifications  Bachelors in Data Science Computer Science Industrial Engineering or a related field Two years of applicable fulltime experience not internship or academia as a data scientist in a manufacturing environment Advanced programming experience using Python A strong foundation in statistical analysis machine learning and artificial intelligence as it applies to manufacturing operations The ability to work a weekly 32 officehome hybrid schedule     We Hope You Also Have Preferred Qualifications  A Master’s degree in one of the fields mentioned above R Programming experience Experience working with databases and SQL Demonstrated proficiency in collaborating across diverse teams to meet shared objectives A history of enhancing operations with actionable insights from complex data sets Strong communication skills to translate technical findings into understandable insights     Leprino Foods celebrates and supports diversity We believe in equal opportunity and do not discriminate on the basis of race religion ethnicity national origin gender sexual orientation age marital status veteran status or disability We know we are better together and are committed to creating an inclusive and supportive culture that uses the unique talents experiences background and perspectives of each individual employee   Offering You In Return A chance to be part of a global team of individuals passionate about producing and delivering highquality products that help feed and nourish families around the world Leprino Foods could not be where it is today without our incredible employees That is why we share in our success together by rewarding you for your hard work Hiring great people who are in it for the long run is our goal Through competitive salaries and bonuses life medicaldentalvision coverage voluntary benefits employee assistance programs wellness incentives tuition assistance vacation ten paid holidays sick time paid parental leave annual merit increases as well as the LFC ProfitSharing  401k plan Your impact will be noticed and rewarded as you seek to further our company our customers and one another   Our Story Leprino Foods’ history dates back over 70 years when Jim Leprino first started making small batches of mozzarella for local markets and eateries in the Little Italy neighborhood of Denver We’ve grown a bit since then Today Leprino Foods is the world’s largest manufacturer of mozzarella and lactose and a leading producer of whey protein Still owned by Jim and the Leprino family our sights are set to be the “World’s Best Dairy Food and Ingredient Company” To help us achieve that bold vision we’re looking for our secret ingredient You A motivated team member who is the best at what you do Three passionate individuals in a small corner grocery store in the early 1950s have now grown to well over 5000 employees throughout the globe Will you join us on our journey  </data></node>
<node id="n978" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=f4fb0fb20e590a16&amp;bb=9GK_E0ECeRmyEyJEGZ3o0eeOrGRPr0DIAoAMK-6xp6VqU2xIC1OI7SaECEUJODYUhAeu1acj5LmNfSu6i8KRXUvGVPfeJPRBK-OFJxfU4jg%3D&amp;xkcb=SoCC67M3CNkF_ewiIp0KbzkdCdPP&amp;fccid=c2d1b0769a534b4d&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Statistical softwareYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationStamford CT 06902 BenefitsPulled from the full job descriptionOpportunities for advancement Full job description          Responsible for assisting in the development of datadriven solutions to Charter’s business problems In partnership with seasoned Data Scientist utilize analytical statistical and programming skills to clean aggregate and analyze large data sets and interpret results Develop a strong command of statistical techniques and machine learning algorithms as well as a demonstrated practical ability to determine where to invest time synthesize actionable findings across diverse assignments and present findings to audiences with diverse agendas and varying levels of technical expertise             MAJOR DUTIES AND RESPONSIBILITIES        Actively and consistently supports all efforts to simplify and enhance the customer experience  Participate in the execution of the complete analytics lifecycle for problem solving including requirements gathering problem formulation data grooming data exploration model prototyping model validation and algorithm productionalization  Survey varied data sources for analytic relevance  Leverage knowledge in analytical and statistical algorithms to assist in helping stakeholders explore methods to improve their business  Assist in the design and implementation of statistical data quality procedures for existing and new data sources  Observe and assist in the communication of complex data science solutions concepts and analyses in a clear and effective manner to team members and business leaders  Establish links across existing data sources and find new interesting data correlations  Assist in the presentation of data insights and recommendations to key stakeholders  Achieve defined project goals within deadline proactively communicate status and escalate issues as needed  Helps to ensure testing and validation are components of all analytics solutions          REQUIRED QUALIFICATIONS        Required SkillsAbilities and Knowledge       Ability to read write speak and understand English  Ability to develop interpersonal communication verbal and written relationship management and customer service skills with a focus on working effectively in a team environment  Minimal experience with analytical and statistical software  Minimal experience using a data science toolkit such as Python or R  Basic level SQL skills  Effective analytical and problem solving skills with attention to detail and data accuracy  Ability to perform indepth research and analysis  Ability to maintain confidentiality and appropriately handle sensitive information  Knowledge of core computer science principles including algorithms data structures and design methodologies  Prior exposure to cloudbased infrastructures  Prior exposure to big data tools such as Spark and Hive           Required Education       Bachelors degree in computer science statistics operations research andor equivalent combination of education and experience                 Required Related Work Experience and Number of Years Number of Years       Data Analytics 03         Programming experience 02                  PREFERRED QUALIFICATIONS        Preferred SkillsAbilities and Knowledge           Preferred Education           Preferred Related Work Experience and Number of Years       Internship in a role as a data scientist                  WORKING CONDITIONS        Office environment            BDA302 202431383 2024     Here employees don’t just have jobs they build careers That’s why we believe in offering a comprehensive pay and benefits package that rewards employees for their contributions to our success supports all aspects of their wellbeing and delivers real value at every stage of life    A qualified applicant’s criminal history if any will be considered in a manner consistent with applicable laws including local ordinances    Get to Know Us Charter Communications is known in the United States by our Spectrum brands including Spectrum Internet® TV Mobile and Voice Spectrum Networks Spectrum Enterprise and Spectrum Reach When you join us you’re joining a strong community of more than 101000 individuals working together to serve more than 32 million customers in 41 states and keep them connected to what matters most Watch this video to learn more    Who You Are Matters Here We’re committed to growing a workforce that reflects our communities and providing equal opportunities for employment and advancement EOE including disabilityvets Learn about our inclusive culture  </data></node>
<node id="n979" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=c87099f23f14f756&amp;bb=9GK_E0ECeRmyEyJEGZ3o0SYIvnTl8qAqWNVFaMhClJYIUnhZkmc4mA0q3GYR6xDwW_MTJ_n-XzE8lbbubJkM1qbRjrPfzvdX22poD3zKt8uLqSpGdrvAPQ%3D%3D&amp;xkcb=SoAf67M3CNkF_ewiIp0JbzkdCdPP&amp;fccid=43014b1412e0a7b6&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNo LocationSan Francisco CA Full job description  About Pinterest  Millions of people across the world come to Pinterest to find new ideas every day Its where they get inspiration dream about new possibilities and plan for what matters most Our mission is to help those people find their inspiration and create a life they love In your role youll be challenged to take on work that upholds this mission and pushes Pinterest forward Youll grow as a person and leader in your field all the while helping Pinners make their lives better in the positive corner of the internet  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring As you read through the expectations of the position consider how your skills and experiences may complement the responsibilities of the role We encourage you to think through your relevant and transferable skills from prior experiences  Our new progressive work model is called PinFlex a term thats uniquely Pinterest to describe our flexible approach to living and working Visit our PinFlex landing page to learn more   As a Data Scientist on the Sales Analytics team you will serve as a goto expert on data and analytics within the SMB Sales team and with key XFN partners Team members are trusted to drive efficiency and revenue growth by enabling datadriven decisions for deepcomplex problems developing accurate reporting and forecasting models and building scalable data infrastructure  What youll do  Serve as an analytics expert for your partners using data to help them make better decisions Work directly with sales leaders and XFN partners on critical strategic and analytical projects designed to increase revenue sales productivity andor operational efficiency Own the endtoend process of gathering and synthesizing data using relevant tools eg SQL Python testing hypotheses and developing final recommendations Translate analysis results into actionable insights or improvement opportunities to influence decisions made my sales leadership and the broader organization Work with large complex data sets to solve complex problems applying advanced analytical methods as needed Develop and automate reporting and forecasting models iteratively build and prototype dashboards as needed to provide insights at scale Build and maintain business critical data pipelines and workflows develop comprehensive knowledge of internal data structures and metrics advise on changes where needed  What were looking for  3 years of professional experience analyzing data in a fastpaced datadriven environment Extensive experience solving analytical problems using quantitative approaches including in the fields of Statistical Modelling Forecasting Econometrics or other related fields Expertlevel knowledge of SQL with strong data exploration and manipulation skills Experience with one or more programming languages such as Python or R Experience with Tableau or similar data visualization tools Excellent communication skills and ability to explain learnings to both technical and nontechnical partners A scientifically rigorous approach to analysis and data and a welltuned sense of skepticism attention to detail and commitment to highquality resultsoriented output  Relocation Statement   This position is not eligible for relocation assistance Visit our PinFlex page to learn more about our working model  LIHYBRID  LIAT6   Our Commitment to Diversity  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit We want to have the best qualified people in every job All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability protected veteran status or any other characteristic under federal state or local law We also consider qualified applicants regardless of criminal histories consistent with legal requirements If you require an accommodation during the job application process please notify accessibilitypinterestcom for support   </data></node>
<node id="n980" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=206ed2b816e0899f&amp;bb=9GK_E0ECeRmyEyJEGZ3o0QkdTSe7_IxDZ6u08CTgSEdXXuKN146avQHGakdi6wLhGzyecYBRjRhN7A6irMNRaXT16oeGhcqhE-NAOhYmEYDbOWxRrlKTyw%3D%3D&amp;xkcb=SoCr67M3CNkF_ewiIp0IbzkdCdPP&amp;fccid=a5b926a01ca57f85&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in XGBoostYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location7501 West Memorial Road Oklahoma City OK 73142 Full job description The Staff Data Scientist will translate business problems and opportunities into data science projects Conduct data analysis and build machine learning models for internal and clientfacing applications Present findings to key stakeholders Deploy production models This position will work closely with software development to provide valuable insight to improve business operations   RESPONSIBILITIES   Meet with business leaders to understand business challenges and opportunities and then frame these as data science problems  Identify data sources and evaluate suitability for use in analysis and modeling  Manage complex projects by demonstrating scope of data science projects by defining and planning steps  Extract data from enterprise systems and clean the data for analysis and modeling  Conduct analysis to derive insights from data  Develop visualizations of analytics and models for communicating findings  Build test and validate predictive models  Put models into production  Present insights to key business stakeholders  Mentor junior data scientists  Identify opportunities to collect data from new sources or improve existing processes to allow for better analysis and modeling  Implements new or enhanced software designed to access and handle data more efficiently  Identifies creative solutions to challenging data science problems determines project requirements and model requirements and delivers a solution through the lifecycle to the production environment   EducationCertification   Bachelor’s Degree in Computer Science Data Science Business Analytics or similar quantitative field   Experience   5 years in a data scientist role   SkillsAbilities   R andor Python SQL statistical analysis and modeling including linear and generalized linear models time series analysis and forecasting machine learning models such as random forests XGBoost and SVM Spark in Scala or from R or Python  Natural language processing including topic modeling intent  entity extraction andor building domainspecific language models  Demonstrate a high level of autonomy and ability to produce high quality work with minimal guidance  Demonstrated ability to communicate data science findings and recommendations to technical and nontechnical audiences including senior and executive leadership  Experience monitoring models in production  Experience in data mining  Understanding of machine learning  Strong analytical skills and business acumen    PREFERRED QUALIFICATIONS  EducationCertification   Master’s degree in Computer Science Data Science Business Analytics or similar quantitative field   Experience   7 years in a data scientist role   SkillsAbilities   Neural networks including CNN and RNN architectures  Building production APIs in Flask FastAPI or similar frameworks  Experience deploying models in Docker containers on Kubernetes  Experience deploying models ondevice for iOS and Android    Paycom is an equal opportunity employer and prohibits discrimination and harassment of any kind Paycom makes employment decisions on the basis of business needs job requirements individual qualifications and merit Paycom wants to have the best available people in every job Therefore Paycom does not permit its employees to harass discriminate or retaliate against other employees or applicants because of race color religion sex sexual orientation gender identity pregnancy national origin military and veteran status age physical or mental disability genetic characteristic reproductive health decisions family or parental status or any other consideration made unlawful by applicable laws Equal employment opportunity will be extended to all persons in all aspects of the employeremployee relationship This policy applies to all terms and conditions of employment including but not limited to hiring training promotion discipline compensation benefits and separation of employment The Human Resources Department has overall responsibility for this policy and maintains reporting and monitoring procedures Any questions or concerns should be referred to the Human Resources Department To learn more about Paycoms affirmative action policy equal employment opportunity or to request an accommodation  Click on the link to find more information paycomcomcareerseeoc   LIHybrid  </data></node>
<node id="n981" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=aa6d0ebd40b305b0&amp;bb=9GK_E0ECeRmyEyJEGZ3o0YHfcT8AvhZR02s4U5w5Pty9YEG_3gqNREcFI4M7bmogG8LgsPaOsWJpwBTgY7upTICVNZlmWDcj7NPdJzi79GA%3D&amp;xkcb=SoCR67M3CNkF_ewiIp0ObzkdCdPP&amp;fccid=aef928e89977f7f0&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay119200  163900 a year LocationCalifornia BenefitsPulled from the full job description401k matchingDental insuranceHealth insurancePaid time offVision insurance Full job description    Splunk is here to build a safer and more resilient digital world The worlds leading enterprises use our unified security and observability platform to keep their digital systems secure and reliable While customers love our technology its our people that make Splunk stand out as an amazing career destination and why weve won so many awards as a best place to work If you become a Splunker we want your whole authentic self what we call your million data points So bring your work experience problemsolving skills and talent of course but also bring your joy your passion and all the things that make you you    Role Summary     As an Applied Scientist in the Artificial Intelligence group you will be responsible for developing the core AIML capabilities to power the entire Splunk product portfolio and help our customers to drive their journey to digital resiliency You will work on diverse projects that span Cybersecurity and Observability domains applying advanced machine learning techniques to solve complex problems and drive innovation    What youll get to do   Develop core AIML models and algorithms that drive our product’s key use cases in the cybersecurity and observability domains  Collaborate closely with software engineers data scientists and product managers to integrate generative AI solutions into our products and services  Stay up to date with the latest research and developments in the field of AIML and ensure that these advancements are properly incorporated into our technology roadmap   Musthave Qualifications   Bachelors or Masters degree in Computer Science Machine Learning Statistics or a related field   Nicetohave Qualifications     We’ve taken special care to separate the musthave qualifications from the nicetohaves “Nicetohave” means just that Nice To Have So don’t worry if you can’t check off every box We’re not hiring a list of bullet points–we’re interested in the whole you     Strong understanding of machine learning fundamentals and experience with popular frameworks eg TensorFlow PyTorch  Solid programming skills in languages such as Python  Experience working with Generative AI technology and Large Language Models LLM or time series analysis and anomaly detection in projects or internships  Strong problemsolving skills and the ability to think critically and creatively  Strong communication skills with the ability to articulate complex technical concepts to both technical and nontechnical audiences  Demonstrated ability to work effectively in a collaborative team environment    Splunk is an Equal Opportunity Employer      At Splunk we believe creating a culture of belonging isn’t just the right thing to do it’s also the smart thing We prioritize diversity equity inclusion and belonging to ensure our employees are supported to bring their best most authentic selves to work where they can thrive Qualified applicants receive consideration for employment without regard to race religion color national origin ancestry sex gender gender identity gender expression sexual orientation marital status age physical or mental disability or medical condition genetic information veteran status or any other consideration made unlawful by federal state or local laws We consider qualified applicants with criminal histories consistent with legal requirements     Note  Base Pay Range  SF Bay Area Seattle Metro and New York City Metro Area  Base Pay Range 11920000  16390000 per year  California excludes SF Bay Area Washington excludes Seattle Metro Washington DC Metro and Massachusetts  Base Pay Range 10728000  14751000 per year  All other cities and states excluding California Washington Massachusetts New York City Metro Area and Washington DC Metro Area  Base Pay Range 9536000  13112000 per year  Splunk provides flexibility and choice in the working arrangement for most roles including remote andor inoffice roles We have a marketbased pay structure which varies by location Please note that the base pay range is a guideline and for candidates who receive an offer the base pay will vary based on factors such as work location as set out above as well as the knowledge skills and experience of the candidate In addition to base pay this role is eligible for incentive compensation and may be eligible for equity or longterm cash awards     Benefits are an important part of Splunks Total Rewards package This role is eligible for a competitive benefits package which includes medical dental vision a 401k plan and match paid time off and much more Learn more about our comprehensive benefits and wellbeing offering at httpssplunkbenefitscom    </data></node>
<node id="n982" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=8a21d6dc61f9d872&amp;bb=9GK_E0ECeRmyEyJEGZ3o0d1SrIUlPFd-aOcf4U_9CytjKUpBGueljN0jddZMPML7_GC36sXnpySD84VOls-Xr3EqtGX43qdOIzIPkUIUnsB6RYWShdC3gg%3D%3D&amp;xkcb=SoAM67M3CNkF_ewiIp0NbzkdCdPP&amp;fccid=51dc2e5aecda9e29&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TypeScriptYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRemote Full job description    Job Description       Pluralsight is hiring a Senior Machine Learning Engineer in our Draper UT office The role can also be USbased remote        The Opportunity        In this role you will be a part of a product development team that delivers personalized experiences to Pluralsights learners Youll help our learners discover content that is relevant to their interests and goals provide them with a homepage tailored just to their needs and ensure they are receiving the right communication and assistance with their learning journey at the right time You’ll be a part of a team that is user focused has a mentality for experimentation and iterates quickly As an experienced machine learning engineer your expertise will be vital to solving sophisticated problems and building the algorithms and frameworks that accelerate our personalization capabilities        Who you are committed to being            Some of the technologies that we love are Python 3 Tensorflow Airflow Kafka PostgreSQL Redis AWS            Attention to detail  we want to be proud of our work            Test Driven Development  We maintain a suite of good tests for all production code            Pair programming as well as individual with code reviews  we value collaborative development            Continuous Delivery  teams independently ship code to prod every day            Agile  we reduce the time to learn by having short feedback loops            Continual improvement  we take time to sharpen the saw and adjust how we work            Autonomous  responsible teams  we’re empowered to make our own product and development decisions to do the job            Crossfunctional teams  collaborating through all phases of the product dev process            Customer research  we build what our customers actually want            Leaders who trust teams build without topdown feature requirements         What you’ll own            Develop robust scalable production ML systems            Evaluate tradeoffs and do performance tuning for production traffic            Work closely with Data Scientists to take prototype algorithms and models and turn them into customerfacing solutions            Use your engineering expertise to help build solutions to novel problems in software development data engineering and machine learning            Provide technical leadership and mentoring to more junior MLEs knowing both when to step back and when to step in            Build data pipelines Transform and convert data streams into structures needed for algorithm input            Collaborate with Product Managers and UX Designers to better understand the customer provide valuable input into functional design and usability            Evaluate the efficiency of user experiences and ML algorithms determining what data is needed and how to collect it with an understanding of how these metrics are connected the desired outcomes            Apply your experience to make intelligent forwardthinking technical decisions to our development process including implementing new standards tools APIs and workflows           Experience youll need            You have a passion to use machine learning to deliver product personalization at scale and the skills to go with it            You have several years of experience building production machine learning systems services as part of a product development team ideally in the context of recommendations andor personalization            You have worked in a collaborative development environment and have experience with continuous integration and delivery            You are a strong Python developer know your way around TensorFlow and have a proficiency in data structures and database fundamentals Experience with Node and Typescript are helpful            You have a solid base in Computer Science and Math and an understanding of the fundamentals of Machine Learning Regardless of your formal training you get excited about reading up on modern machine learning techniques and applications            You care about writing good code and building great software You understand the tradeoffs when we have to move faster but you know what quality means and how to get there when we need to            You are comfortable moving up and down the stack It matters less that you know the exact frameworks and tools that we use but you must be willing and able to learn very quickly We also mean full stack across other functions  you should be excited to understand the entire business and learn from customers            You are good at breaking down sophisticated features into smaller more manageable tasks You have the ability to explain machine learning solutions to developers and other team members regardless of their technical background         Who you’re committed to being            You love exploring new technologies and keeping your own technical skills sharp            You are attentive to quality code and processes            You have a passion for collaboration innovation learning and excellence            You learn from the technical abilities of those around you            You are an amazing communicator and effective influencer            You use words like analytical conscientious and qualitydriven to describe yourself         Bring yourself Pluralsight is an equal opportunity employer All qualified applicants will receive consideration for employment without regard to race color religion sex national origin sexual orientation gender identity disability age or protected veteran status Pluralsight will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law    We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process to perform essential job functions and to receive other benefits and privileges of employment Please visit the   bottom of our website  to learn how to request an accommodation    For more information on Pluralsight’s commitment to building a more diverse and inclusive workforce please review our most recent Diversity Equity Inclusion and Belonging report   here    LIRemote    LIEB1    </data></node>
<node id="n983" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=1badc4232210fe9a&amp;bb=9GK_E0ECeRmyEyJEGZ3o0TCOMa8mbFlfTR0vlBs9NZCkDqLA9cI1MgCK--pO2EF8rpFkC_nVqxnPT2OJo3fHjxt8BkTEURRDfZ9R_DgNc6Q%3D&amp;xkcb=SoC467M3CNkF_ewiIp0MbzkdCdPP&amp;fccid=d4dd418dfdbf731e&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid Secret Clearance licenseYesNoSkillsDo you have experience in Statistical analysisYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay150000  180000 a yearJob typeFulltime LocationSan Diego CA 92101 Full job description ActioNet has an opportunity for a Data Scientist requiring a Secret clearance located in San Diego County CA ActioNet is an IT service provider and solutions integrator headquartered in Vienna VA that works with the Federal Government and Department of Defense In this role you will  Salary Range 150K180K  As the Data Scientist your tasks will include   Work with stakeholders throughout the organization to identify opportunities for leveraging MCTSSA data to drive business solutions  Mine and analyze data from MCTSSA databases to drive optimization and improvement of product development marketing techniques and business strategies  Assess the effectiveness and accuracy of new data sources and data gathering techniques  Develop custom data models and algorithms to apply to data sets  Use predictive modeling to increase and optimize customer experiences revenue generation ad targeting and other business outcomes  Develop data testing framework and test model quality  Coordinate with different functional teams to implement models and monitor outcomes  Develop processes and tools to monitor and analyze model performance and data accuracy  Conduct data transformation   Basic Qualifications   Ability to apply Data Scientist expertise through technology processes methodologies standards or frameworks  Expertise in applying Data Scientist tools and developing methodologies in an integrated test andor garrison environment  Experience using statistical computer languages R Python SLQ etc to manipulate data and draw insights from large data sets  Experience with programming languages such as C C Java JavaScript etc  Experience working with and creating data architectures  Experience using machine learning techniques clustering decision tree learning artificial neural networks etc and identifying realworld advantages and drawbacks  Experience using advanced statistical techniques and concepts regression properties of distributions statistical tests and proper usage etc      Desired Position Qualifications     Bachelor’s degree in computer science Statistics Mathematics Computer Engineering or related field  Knowledge and experience in statistical and data mining techniques  Experience with distributed data and computing tools  Experience creating and using advanced machine learning algorithms and statistics regression simulation scenario analysis modeling clustering decision trees neural networks etc  Experience visualizing and presenting data for stakeholders   </data></node>
<node id="n984" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=26e0987b7c9b88c2&amp;bb=9GK_E0ECeRmyEyJEGZ3o0RbMnXQapU5yELhfYkg6n_h91hqk7bjMjmPCnVG7WC4WHV10ykiSwWJWEzYFuYIc8eao69a1g0nmADKsWUvZ1cA%3D&amp;xkcb=SoBR67M3CNkF_ewiIp0DbzkdCdPP&amp;fccid=761c44c17d636bfe&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location21931 Michigan Ave Dearborn MI 48124 BenefitsPulled from the full job descriptionDental insuranceEmployee discountHealth insurancePaid time offPrescription drug insuranceTuition reimbursement Full job description Do you believe data tell the real story We do   Global Data Insight  Analytics GDIA aspire to navigate Ford Motor Company through the disruptiveness of the information age harnessing the power of data and artificial intelligence to realize the enterprise’s known goals reveal hidden opportunities and achieve data superiority   Product Engineering and Management PEM is GDIA’s organization for creating analyticalinspired products and services at the intersection of the commercial customer service and connected vehicle domains Leveraging multidisciplinary product teams agile processes and methodologies technologies in the big data space and AIML   GDIA DataTech is looking for a GCP Cloud Data Engineer focused on deep diving into using stateoftheart development tools to understand ingest and present complex data from a series of sources internal and external to the enterprise   You should have a knack for implementing and learning required toolsets working with connected vehicle and legacy datasets to build and publish logical representations of data These wellmanaged and organized data platforms will empower the building of new technologies new applications or industrydisrupting new insights from visually appealing dashboards   Come and join this exciting and fastpaced role which requires exceptional technical data and analytics skills with the opportunity to create a huge impact on Ford’s business    The minimum requirements we seek   Bachelor’s Degree in Data Science Data Analytics Computer Science Computer Engineering Applied Mathematics Statistics Business Analytics Operations Research or related quantitative field  3 years’ experience in handling data and data processes  1 years of experience utilizing Python SQL and Google Cloud Ops  Experience with GCP native BigQuery andor Postgres  Experience building out GCP data pipelines of Data fusion from scratch in a highly distributed and faulttolerant manner  Experience with any of the following Astronomer  Airflow Dataflow Cloud Run DBT andor PubSub  Experience developing data standards  Experience creating Data Models and Data products Experience with unstructured data and streaming data     Our preferred requirements   Advanced Degree in Data Science Data Analytics Computer Science Computer Engineering Applied Mathematics Statistics Business Analytics Operations Research or related quantitative field  Demonstrated experience building visualizations using Looker  Ability to write complex SQL queries needed to query  analyze data  Experience with unstructured data and streaming data using Mongo DBAtlas andor Firestore  Knowledge of data management standards data governance practices and data quality  Demonstrated problem formulation with the ability to take complex problems and break them down to create and implement an action plan  Ability to communicate findings to make data analysis actionable and understandable by Data Operations team members and business partners including IT and analytic teams  Ability to effectively communicate information and ideas in written and verbal formats including process documentation  Highly effective in working with other technical experts Product Managers and business stakeholders  Excellent verbal and written communication skills with the ability to communicate effectively with all levels of management in varying areas of business and enterprise technology Strong collaboration and influencing skills and the ability to energize a crossfunctional team     What you’ll receive in return  As an established global company we offer the benefit of choice You can choose what your Ford future will look like will your story span the globe or keep you close to home Will your career be a deep dive into what you love or a series of new teams and new skills Will you be a leader a changemaker a technical expert a culture builder…or all the above No matter what you choose we offer a work life that works for you including Immediate medical dental and prescription drug coverage generous PTO retirement and savings plans incentive compensation tuition assistance a vehicle discount program and much more   For information on Fords salary and benefits please visit httpscorporatefordcomcontentdamcorporateusenusdocumentscareers2024benefitsandcompGSRsalplan2pdf   Candidates with Ford Motor Company positions must be legally authorized to work in the United States Verification of employment eligibility will be required at the time of hire Visa sponsorship is “not” available for this position   We are an Equal Opportunity Employer committed to a culturally diverse workforce All qualified applicants will receive consideration for employment without regard to race religion color age sex national origin sexual orientation gender identity disability status or protected veteran status In the United States if you need a reasonable accommodation for the online application process due to a disability please call 18883360660    As a GDIA GCP Cloud Data Engineer you will work in a Product Management Delivery model by collaborating directly and continuously with product managers product owners and software engineers and will follow industry best practices to perform and automate complex ETL functions   What you’ll be able to do   As a Data Steward consult with analytics teams IT partners and business partners to provide data domain expertise  Design develop create and production of Data Products for customers  Develop ELELTETL pipelines to make data available in BigQuery analytical data store from disparate batch streaming data sources  Develop and document the process to operationalize data management at Ford by collaborating with Data Governance and Data Acquisition teams  Work with Product Owners Product Managers and Product Teams to prioritize and land data sources in conjunction with Analytics Customer needs  Participate in information governance processes  Establish data standards and quality benchmarks and drive improvements to data quality to improve analytic outcomes  Provide expertise in the business domain required to deliver business objectives  Manage business information issues and problemresolution processes  Drive and be a catalyst for innovation  Persevere and persist in situations with multiple objectives and multivariable problems  Think flexibly in a complex highly interdependent environment with challenging financial technical and project management constraints  Drive longterm enterprise view against tactical requirements Tradeoffs between tactical expediency and longterm business improvements are difficult to negotiate and sustain  Support alignment between business IT and GDIA stakeholders to drive results issue resolution risk mitigation issue avoidance  Provide departmental feedback to other GDIA groups regarding processes and standards  Collaborate and motivate within the department including readily sharing best practices with other Data Operations and GDIA groups   </data></node>
<node id="n985" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=59bd344a6bc2a8bb&amp;bb=9GK_E0ECeRmyEyJEGZ3o0dTGz_ovUZw-ZImvPIdpZ9zKNSoqm-m1CoBRr9TKCZ1DxcoDCtCPWlMdEZ9Zj-IYhoirxyUZW3YKY8Use7x9SmP-UQTd5SyrEw%3D%3D&amp;xkcb=SoDl67M3CNkF_ewiIp0CbzkdCdPP&amp;fccid=e5f741bf43df9079&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Supervising experienceYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay90000  100000 a yearJob typeFulltime LocationArlington VA 22209 BenefitsPulled from the full job description401k401k matchingADD insuranceCell phone reimbursementDental insuranceFlexible spending accountHealth insuranceShow morechevron down Full job description      Trilogy Federal provides financial management information technology IT consulting program management services and strategic consulting to federal agencies Trilogy has an extensive history helping federal clients achieve their most ambitious business modernization and optimization goals with the ability to deliver targeted subject matter expertise and full life cycle support       Trilogy Federal is looking for a strategic and thoughtful Sr Data Analyst to consult and develop datacentered projects In keeping with this overarching aim the Sr Data Analyst will be required to outline work requirements quickly develop an understanding of our customer’s datarelated challenges provide solutions that consider the customer’s specific requirements and constraints and work with a crossfunctional team to deliver timely and tangible results You should also harness your mastery of data analysis to consult and directly participate in various aspects of these and other projects To be successful as a Sr Data Analyst you should use data to ultimately inform sound decisionmaking in support of automation efforts and efficiency The ideal candidate will also assist in the development of junior staff       Primary Responsibilities    Formulating suggesting and managing datadriven projects which are geared at furthering the businesss interests  Collating and cleaning data from various entities for later use by junior data scientists  Delegating tasks to Junior Data Scientists to realize the successful completion of projects  Monitoring the performance of Junior Data Scientists and providing them with practical guidance as needed  Selecting and employing advanced statistical procedures to obtain actionable insights  Crossvalidating models to ensure their generalizability  Producing and disseminating nontechnical reports that detail the successes and limitations of each project  Suggesting ways in which insights obtained might be used to inform business strategies  Staying informed about developments in Data Science and adjacent fields to ensure that outputs are always relevant  Assist clients on functional and data requirements to enhance reporting effectiveness  Develop Microsoft Forms to assist clients with gathering information Use analytics to evaluate and summarize responses  Provide subject matter experience for clients seeking to improve content management and versioning  Adhere to established methodologies while analyzing processes for improved performance and adaptability       Minimum Requirements    Able to obtain a Public Trust Clearance  Bachelor’s degree data science statistics computer science or similar preferred  10 years’ experience  Consultative mindset and demonstrated work experience providing solutions for clients and proactively collaborating with Stakeholders  Deep knowledge of statistics and linear algebra concepts ANOVA distributions PCA  Proficiency in R or Python  Proficiency in SQL  Familiar with machine learning principles and techniques  Demonstrable history of devising and overseeing datacentered projects  Ability to relay insights in laymans terms such that these can be used to inform business decisions  Capacity to work independently or collaboratively with a crossfunctional team       Preferred Qualifications    VA experience preferred  Advanced degree in data science statistics computer science or similar  Detailoriented with the ability to manage multiple tasksrequests  Strong written and verbal communications skills  Supervision and mentorship skills       Benefits including but not limited to    Health dental and vision plans  Optional FSA  Paid parental leave  Safe Harbor 401k with employer contributions 100 vested from day 1  Paid time off and 11 paid holidays  No cost group term lifeADD plan and optional supplemental coverage  Pet insurance  Monthly phone and internet stipend  Tuition and training reimbursement          90000  100000 a year         This range is not a guarantee of compensation or salary as Trilogy Federal conducts an individual equity review for every candidate based on experience location education industry experience and comparisons to internal pay bands In addition to salary Trilogy offers robust benefits including medicaldentalvision insurance coverage 401k match paid holidays paid time off tuition reimbursement and a very supportive worklife balance       Regarding remote positions Trilogy Federal is only able to offer virtual employment in the following states Colorado Connecticut Delaware DC Florida Illinois Indiana Maine Maryland Massachusetts New York South Carolina Texas and Virginia     Trilogy Federal is an Equal Employment Opportunity employer We do not discriminate based upon race religion color national origin gender including pregnancy childbirth or related medical conditions sexual orientation gender identity gender expression age status as a protected veteran status as an individual with a disability or other applicable legally protected characteristics    </data></node>
<node id="n986" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=09df997afe583462&amp;bb=9GK_E0ECeRmyEyJEGZ3o0X6_czBKiYVI12SJ6WKq7ZcwDBWpBQeVBwjx6DO7kXfJcPQjEe49c5Cs_BuUMK6A6X-RPUFoKzXQGGB3CfzuIfo9eJ_LGVJ1oA%3D%3D&amp;xkcb=SoB467M3CNkF_ewiIp0BbzkdCdPP&amp;fccid=fdb210f6a49b1c13&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in PythonYesNoEducationDo you have a Masters degreeYesNo LocationNew York NY 10001 BenefitsPulled from the full job descriptionDisability insuranceHealth insuranceRetirement planWellness program Full job description Every great story has a new beginning and yours starts here  Welcome to Warner Bros Discovery… the stuff dreams are made of   Who We Are…  When we say “the stuff dreams are made of” we’re not just referring to the world of wizards dragons and superheroes or even to the wonders of Planet Earth Behind WBD’s vast portfolio of iconic content and beloved brands are the storytellers bringing our characters to life the creators bringing them to your living rooms and the dreamers creating what’s next…   From brilliant creatives to technology trailblazers across the globe WBD offers career defining opportunities thoughtfully curated benefits and the tools to explore and grow into your best selves Here you are supported here you are celebrated here you can thrive   Every great story has a new beginning   Were excited to announce that Discovery and WarnerMedia have combined to become Warner Bros Discovery Were a premier global media and entertainment company offering audiences the worlds most differentiated and complete portfolio of content brands and franchises across television film sports news streaming and gaming Our mission is simple To be the worlds best storytellers with worldclass products for consumers  From brilliant creatives to technology trailblazers and beyond join us as we step into the next chapter   Warner Bros Discoverys DTC technology and product organization sits at the intersection of tech entertainment and everyday utility We are continuously leveraging new technology to build immersive and interactive viewing experiences Our platform covers everything from search content catalog and video transcoding to personalization global subscriptions and more We are committed to delivering unique and quality user experiences ranging from video streaming to applications across connected TV mobile web and consoles As a pure tech organization we are essential to Warner Bros Discovery’s continued growth building worldclass streaming products from the groundup for our iconic brands like HBO Max Discovery Channel CNN Food Network HGTV Eurosport MotorTrend and many more   About You  We are looking for a passionate Machine Learning Engineer to build and scale the DTC personalization systems and services for our new global streaming app Max as well as any future DTC streaming apps You will be in the unique role of acting as a leader who can work seamlessly with modeling and engineering teams and can build and contribute to architecting a system that serves millions of users worldwide As an engineering leader you are excited about working in an environment that fosters innovation via prototyping development experimentation and productionalization You will bring the right balance between rapid feature iteration and building a common set of platforms and tools to move quickly in the future   Responsibilities     Architect build and scale a recommendation system that powers a state of the art personalization experience to users across Max HBO Discovery and other WBD offerings  Collaborate with other MLOps engineers to develop and improve core components infrastructure and architecture to train deploy and serve models at scale  Lead architecture improvements for our personalization services and infrastructure  Collaborate with data scientists engineers product teams and other key stakeholders and drive ML projects from conception to completion  Author test review and optimize productionlevel code in Python Go and Java while executing best practices in version control and code integration  Use and build upon opensource cloud computing technologies  Participate and support engineering leaders in strategic planning and demonstrate good judgment in setting and delivering against strategic goals for the team  Mentor influence engineers across organizations and lead by example with high quality examples of your work at the organization level  Motivate inspire and create a culture of experimentation and datadriven innovation while constantly striving to be an advocate for doing what is right for our customers    Requirements   6 years of industry experience with 3 years as tech lead experience preferred  Deep practical knowledge of large scale recommender systems or large scale ML rankingretrievaltargeting systems  Knowledge of largescale distributed application architecture design implementation and performance tuning     Good practical knowledge in modern machine learning lifecycle  5 years of programming experience using PythonJava with ability to rapidly prototype ideas and refine towards production  Ability to drive roadmap and directions of scalable production quality systems endtoend  Excellent written and verbal communications skills be comfortable presenting to large audiences     Advanced degree MS or PhD or equivalent industry experience in software engineering computer science machine learning or related fields    How We Get Things Done…   This last bit is probably the most important Here at WBD our guiding principles are the core values by which we operate and are central to how we get things done You can find them at wwwwbdcomguidingprinciples along with some insights from the team on what they mean and how they show up in their day to day We hope they resonate with you and look forward to discussing them during your interview   The Legal Bits… In compliance with local law we are disclosing the compensation or a range thereof for roles in locations where legally required Actual salaries will vary based on several factors including but not limited to external market data internal equity location skill set experience andor performance Base pay is just one component of Warner Bros Discovery’s total compensation package for employees Pay Range 14560000  27040000 salary per year Other rewards may include annual bonuses short and longterm incentives and programspecific awards In addition Warner Bros Discovery provides a variety of benefits to employees including health insurance coverage an employee wellness program life and disability insurance a retirement savings plan paid holidays and sick time and vacation   Warner Bros Discovery embraces the opportunity to build a workforce that reflects the diversity of our society and the world around us Being an equal opportunity employer means that we take seriously our responsibility to consider qualified candidates on the basis of merit without regard to race color religion national origin gender sexual orientation gender identity or expression age mental or physical disability and genetic information marital status citizenship status military status protected veteran status or any other category protected by law   If you’re a qualified candidate and you require adjustments or accommodations to search for a job opening or apply for a position please contact us at recruitadminwbdcom  </data></node>
<node id="n987" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d68459a27c758bac&amp;bb=9GK_E0ECeRmyEyJEGZ3o0dQ4iX5n0ZDeXoUZ0Ak2wv3bNcWI7luR9wFsO22Dr_K5z-pXKiDZhqVkZ9jMmGNX6gCqpPsJPtWa7XetzE_ygrxZqz8ucGdZtg%3D%3D&amp;xkcb=SoDM67M3CNkF_ewiIp0AbzkdCdPP&amp;fccid=dd616958bd9ddc12&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in XGBoostYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay11440  17160 a monthJob typeFulltime LocationUnited States BenefitsPulled from the full job description401k matchingADD insuranceDental insuranceHealth insuranceMilitary leavePaid military leavePaid parental leaveShow morechevron down Full job description Job Description Summary Takes ownership and responsibility over major data science and operations research initiatives Directs a team responsible for modeling and development to support operations initiatives strategic programs and new products and solutions Strong background and experience in the use of sophisticated descriptive diagnostic predictive prescriptive and ensemble modeling advanced statistical techniques and sophisticated mathematical tools Uses expertise to manage a team that initiates and delivers projects and develops endtoend solutions that drive business results including proofsofvalue for new business problems and productionready solutions for operations and customerfacing products Understands and ensures development of solutions supporting the movement of data and information assets following APIFirst  Architecture principles Advances Dataworks’ broad capabilities to use and deploy innovative tools in Dataworks projects platforms and products Applies knowledge of ML Ops CICD processes and data engineering practices to ensure sustainable model development and provide recommendations on complex problems Provides consultation and thought leadership to senior management Mentors other team members to get results and effectively collaborates with multifunctional teams to deliver business goals  Job Description The Data Scientist Manager will supply to the creation and alignment of technical standards for data science and machine learning including the design and construction of reusable data assets They will work with large data sets and solve difficult analytical problems applying advanced methods They will lead implementation of solutions from concept to production using current and emerging technologies to evaluate trends and develop practical insights and recommendations Daytoday they will be deeply involved in code reviews and largescale deployments  Essential Job Duties  Responsibilities    We are seeking someone who has understanding in depth both the business and technical problems Dataworks aims to solve   Exploring data and crafting models to answer core business problems that may lack blueprints   Leading the invention of new approaches and algorithms for taking on data intensive problems   Pioneering RD efforts to rapidly understand and assimilate innovative methods   Scaling up from “laptopscale” to “cluster scale” problems by leading efforts to standardize and modernize solutions   Interacting with senior technologists from the broader enterprise and outside of FedEx partner ecosystems and customers to create room to collaborate and find opportunities for improvement   Codifying standard processes for future reuse in the form of accessible reusable patterns templates and code bases    SkillsKnowledge Considered a Plus    Technical background in computer science operations research data science machine learning artificial intelligence statistics or other quantitative and computational science   A compelling track record of Data Science  Engineer expertise designing and deploying large scale technical solutions which deliver tangible ongoing value   Direct experience having built and deployed robust complex production systems through Cloud technologies that implement modern data scientific methods at scale   Ability to contextswitch to provide support to dispersed teams which may need an “expert hacker” to unblock an especially significant technical obstacle   Demonstrated ability to deliver technical projects with a team often working under tight time constraints to deliver value   An ‘engineering’ mentality willing to make rapid pragmatic decisions to improve performance accelerate progress or magnify impact   Ability to work with distributed teams on codebased results using version control   Solid theoretical grounding in the mathematical core of the major ideas in data science   Sophisticated level of understanding of a class of modelling or analytical techniques often supported by Masters or Doctorallevel research in the subject   Deep fluency in the mathematical ‘primitives’ and generalizations of data science – eg expertise in Linear Algebra and Vector Calculus   Use of agile and devops practices for project and software management including CICD process improvement and quality management experience eg Lean Six Sigma QDM authority   We are looking for demonstrated expertise in working with some of the following common languages and tools   SKLearn XGBoost Tensorflow Pytorch MLlib and other core ML frameworks   Python Scala R C Java and other modern programming languages   MLFlow Databricks Spark Kafka Hive Hadoop and other data tools and frameworks   CPLEX Gurobi and other similar optimization modeling packages    Minimum Qualifications  Master’s Degree or equivalent in computer science operations research statistics applied mathematics or related quantitative subject area Directly related PhD preferred Five to eight 58 years’ work experience in applying data science machine learning artificial intelligence statistical analysis operations research optimization algorithms mathematical modeling and data analytics modeling to decrease cost increase profitability and improve customer experience Extensive knowledge in advanced data science statistical analysis and machine learning methods including the iterative development of analysis pipelines to provide insights at scale Extensive experience conducting endtoend analyses including data gathering and requirements specification processing analysis and presentation Strong familiarity with the transportation industry competitors and evolving technologies Experience providing leadership in a general planning or consulting setting Experience as a leader or a senior member of multifunctional project teams Strong human relations organizational  time management project management and software development skills Excellent interpersonal skills and the ability to present and communicate with impact to executive audiences    Domicile Information  This position can be domiciled anywhere in the United States The ability to work remotely within the United States may be available based on business need    Application Criteria  Upload current copy of Resume Microsoft Word or PDF format only and answer job screening questionnaire by April 8 2024   Additional Information  Colorado Nevada Connecticut New York California Rhode Island Washington Hawaii Illinois and New Jersey Residents Only  Compensation Monthly Salary 1144000  1716000 This compensation range is provided as a reasonable estimate of the current starting salary range for this role Factors that may be used to determine your actual salary may include but are not limited to your specific skills your work location how many years of experience you have and comparison to other employees already in this role   Born out of FedEx a pioneer that ships nearly 20 million packages a day and manages endless threads of information FedEx Dataworks is an organization rooted in connecting the physical and digital sides of our network to meet todays needs and address tomorrows challenges  We are creating opportunities for FedEx our customers and the world at large by   Exploring and harnessing data to define and solve true problems  Removing barriers between data sets to create new avenues of insight  Building and iterating on solutions that generate value  Acting as a change agent to advance curiosity and performance   At FedEx Dataworks we are making supply chains work smarter for everyone  Employee Benefits medical dental and vision insurance paid Life and ADD insurance tuition reimbursement paid sick leave paid parental leave paid vacation paid military leave and additional paid time off geographic pay ranges 401k with Company match and incentive bonus potential sales Incentive compensation for selling roles  Dataworks does not discriminate against qualified individuals with disabilities in regard to job application procedures hiring and other terms and conditions of employment Further Dataworks is prepared to make reasonable accommodations for the known physical or mental limitations of an otherwise qualified applicant or employee to enable the applicant or employee to be considered for the desired position to perform the essential functions of the position in question or to enjoy equal benefits and privileges of employment as are enjoyed by other similarly situated employees without disabilities unless the accommodation will impose an undue hardship If a reasonable accommodation is needed please contact DataworksTalentAcquisitioncorpdsfedexcom  </data></node>
<node id="n988" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=53f9ae1812c4d8d3&amp;bb=9GK_E0ECeRmyEyJEGZ3o0X1iGzgaDJ_cLCT4bc4wUGWx9sRx9q0qpndRdyaaW-KYBKpHIhU-r5T9MlNz9H4264bF7qaTE34ZtbUXr92GoCDJs95QE6ZvSw%3D%3D&amp;xkcb=SoBC67M3CNkF_ewiIp0HbzkdCdPP&amp;fccid=88182458189b2ea5&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Masters degreeYesNo LocationWashington DC BenefitsPulled from the full job descriptionDental insuranceDisability insuranceHealth insurancePaid time offParental leaveTuition reimbursementVision insurance Full job description    The Sr Data Scientist should have a passion for data science and healthcare to contribute to the National Cardiovascular Data Registry’s NCDR data science strategy The role will be add to the advancement of NCDR data analytics through the application of novel approaches using data science tools and techniques The role will collaborate across groups to achieve NCDR goals    This position is based in Washington DC where we have a hybrid work environment roughly 40 of the time in the office    Major Duties and Responsibilities   Build data science products and share analytic insights using machine learning statistical modelling and NLP techniques for NCDR internal and external stakeholders    Leverage Exploratory Data Analysis to analyze and gain insights from clinical registry data to provide critical results enable datadriven decisions enhance product offerings and reduce cost and complexity    Work collaboratively across groups NCDR business lines IT ACC members and other stakeholders to facilitate data science initiatives    Develop data visualization and dashboard solutions using standard tools such as Power BI   Utilize external data sources to discover relevant and informative trends External data sources are including but not limited to clinical claims registry and Patient Reported Outcomes PRO data   Adhere to the data science team documentation and coding best practices to facilitate collaboration and effective communication   Understand data science and analytic methodologies applicable to the realworld evidence RWE team   Complete all mandatory privacy and security training requirements and comply with ACC privacy and security policies     Required Qualifications       Bachelor’s degree preferably a related field   Masters with 3 years relevant experience or Bachelors with 4 years relevant experience    Experience in R or Python and SQL   Knowledge of statistical modeling machine learning building and deploying models andor leveraging unstructured data    Experience with PowerBI or other data visualization tools Tableau ggplot plotly   Ability to communicate technical concepts and implications with nontechnical stakeholders   Proficient with Microsoft Office familiarity with Microsoft Access Visio SharePoint software     Desired Qualifications     Proactively identify and mitigate roadblocks and troubleshoot solutions   Experience with healthrelated data and a strong drive to apply data science to business problems   Experience using GitHub        About Us   At the American College of Cardiology we bring our hearts to work   We are a 500person organization dedicated and committed to our mission to transform cardiovascular care and improve heart health for the past 70 years When you join our team you become part of a passionate culture that envisions a world where innovation and knowledge optimize cardiovascular care and outcomes   Every day we are committed to supporting our more than 56000 members and their patients around the globe and in doing so ensure our staff have a positive environment of teamwork collaboration professionalism and excellence To learn more about why ACC has been recognized as one of Modern Healthcares Best Places to Work in Healthcare please visit our site at wwwaccorgjobs  What We Offer   ACC values all members of our College family including ACC staff As the foundation of the organization ACC staff enjoy worldclass benefits and a culture of worklife balance Our benefit offerings include insurance medical dental vision basic life and short and longterm disability and supplemental options generous paid time off preloaded vacation and sick 12 holidays and an organizational shutdown during the last week of the year parental leave 2 community service days and halfday summer Fridays tuition assistance and a very competitive 10 retirement contribution after a year of service and much more You can visit our careers site for an overview of our full offerings httpswwwaccorgaboutaccjobsattheacc Please note that these offerings may change at any time    COVID Considerations   As an employer in the public health space and an organization that serves members who are essential medical personnel ACC requires all staff to be fully vaccinated against COVID19 upon hire Proof of vaccination will be required Individuals can request an exemption from this requirement due to a medical condition or sincerely held religious belief and those requests for reasonable accommodations will be evaluated individually   ACC is proud to be an equal opportunity and affirmative action employer We celebrate diversity and are committed to creating an inclusive environment for all candidates and employees All employment is decided on the basis of qualifications merit and business need Equal Opportunity Employer including individuals with disabilities and veterans   ACC is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures If you need assistance or an accommodation due to a disability you may contact Crystal Nott Sr Director People Resources  Engagement at cnottaccorg or 2023756423        </data></node>
<node id="n989" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=980d8b9c42495eb8&amp;bb=9GK_E0ECeRmyEyJEGZ3o0TGqQr4cMgu_zVfJNORgw_RDdoBpQsVN2-2kI5q_f-I-N4nTICJD8JE6HMQAoZ3xvPT87DkNBBXUV7pdY6tB4uMDB303eTAJHw%3D%3D&amp;xkcb=SoD267M3CNkF_ewiIp0GbzkdCdPP&amp;fccid=88182458189b2ea5&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Masters degreeYesNo LocationWashington DC BenefitsPulled from the full job descriptionDental insuranceDisability insuranceHealth insurancePaid time offParental leaveTuition reimbursementVision insurance Full job description    The Lead Data Scientist should have a demonstrated passion for data science and datadriven analytics to implement the National Cardiovascular Data Registry’s NCDR data science strategy The role will be responsible for contributing to the advancement of NCDR data analytics through the application of novel approaches using data science tools and techniques The role will coach junior data science team members and collaborate across groups to achieve NCDR goals   This position is based in Washington DC where we have a hybrid work environment roughly 40 of the time in the office    Major Duties and Responsibilities   Build and deploy data science products and share analytic insights using machine learning statistical modelling and NLP techniques for NCDR internal and external stakeholders    Coach and mentor the junior data scientists   Leverage Exploratory Data Analysis to analyze and gain insights from clinical registry data to provide critical results enable datadriven decisions enhance product offerings and reduce cost and complexity    Work collaboratively across groups NCDR business lines IT ACC members and other stakeholders to execute data science initiatives    Collaborate with the Data Quality Program to enhance data quality solutions and strategies    Adhere to the data science team documentation and coding best practices to facilitate collaboration and effective communication   Design and develop data visualization and dashboard solutions using standard tools such as Power BI   Integrate external data sources to discover relevant and informative trends External data sources are including but not limited to clinical claims registry and Patient Reported Outcomes PRO data   Research data science and analytic methodologies applicable to the realworld evidence RWE team   Understand NCDR platform architecture such as Common Data Model CDM and legacy star schema   Complete all mandatory privacy and security training requirements and comply with ACC privacy and security policies     Required Qualifications       Bachelor’s degree preferably in a related field   Masters with 5 years relevant experience or Bachelors with 7 years relevant experience    Proficiency in R or Python and SQL   Demonstrated experience with statistical modeling machine learning building and deploying machine models andor leveraging unstructured data    Proficiency with PowerBI or other data visualization tools Tableau ggplot plotly   Ability to communicate technical concepts and implications with nontechnical stakeholders   Proficient with Microsoft Office familiarity with Microsoft Access Visio SharePoint software and GitHub   Willingness to travel limited     Desired Qualifications     Experience with data analysis for healthrelated organizations including FDA NIH and the life science industry   Proactively identify and mitigate roadblocks and troubleshoot solutions   Familiarity with Agile life cycle methodology        About Us   At the American College of Cardiology we bring our hearts to work   We are a 500person organization dedicated and committed to our mission to transform cardiovascular care and improve heart health for the past 70 years When you join our team you become part of a passionate culture that envisions a world where innovation and knowledge optimize cardiovascular care and outcomes   Every day we are committed to supporting our more than 56000 members and their patients around the globe and in doing so ensure our staff have a positive environment of teamwork collaboration professionalism and excellence To learn more about why ACC has been recognized as one of Modern Healthcares Best Places to Work in Healthcare please visit our site at wwwaccorgjobs  What We Offer   ACC values all members of our College family including ACC staff As the foundation of the organization ACC staff enjoy worldclass benefits and a culture of worklife balance Our benefit offerings include insurance medical dental vision basic life and short and longterm disability and supplemental options generous paid time off preloaded vacation and sick 12 holidays and an organizational shutdown during the last week of the year parental leave 2 community service days and halfday summer Fridays tuition assistance and a very competitive 10 retirement contribution after a year of service and much more You can visit our careers site for an overview of our full offerings httpswwwaccorgaboutaccjobsattheacc Please note that these offerings may change at any time    COVID Considerations   As an employer in the public health space and an organization that serves members who are essential medical personnel ACC requires all staff to be fully vaccinated against COVID19 upon hire Proof of vaccination will be required Individuals can request an exemption from this requirement due to a medical condition or sincerely held religious belief and those requests for reasonable accommodations will be evaluated individually   ACC is proud to be an equal opportunity and affirmative action employer We celebrate diversity and are committed to creating an inclusive environment for all candidates and employees All employment is decided on the basis of qualifications merit and business need Equal Opportunity Employer including individuals with disabilities and veterans   ACC is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures If you need assistance or an accommodation due to a disability you may contact Crystal Nott Sr Director People Resources  Engagement at cnottaccorg or 2023756423        </data></node>
<node id="n990" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=b5b620050958d052&amp;bb=9GK_E0ECeRmyEyJEGZ3o0cX16Ae0_uj9x3nwhVWDqekG0NSRRiiSuVrEb9GohW5Bej-vvTnWgj2nCSf2Z7CnfVMXfMgl4H0AEZCwRyQzUbnEdfrAr_TbSg%3D%3D&amp;xkcb=SoBr67M3CNkF_ewiIp0FbzkdCdPP&amp;fccid=23ddedaf27ad006f&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationSan Francisco CA BenefitsPulled from the full job descriptionADD insuranceCaregiver leaveDental insuranceEmployee stock purchase planFlexible spending accountHealth insuranceHealth savings accountShow morechevron down Full job description Company Description  It all started with an idea at Block in 2013 Initially built to take the pain out of peertopeer payments Cash App has gone from a simple product with a single purpose to a dynamic ecosystem developing unique financial products including AfterpayClearpay to provide a better way to send spend invest borrow and save to our 47 million monthly active customers We want to redefine the world’s relationship with money to make it more relatable instantly available and universally accessible  Today Cash App has thousands of employees working globally across office and remote locations with a culture geared toward innovation collaboration and impact We’ve been a distributed team since day one and many of our roles can be done remotely from the countries where Cash App operates No matter the location we tailor our experience to ensure our employees are creative productive and happy  Check out our locations benefits and more at cashappcareers Job Description  Cash Security is a securityenabling engineering organization focused on scaling security as a discipline through innovation As a security team Security Governance develops implements and promotes frameworks and standards aimed at securing our customer’s data giving privacy and security considerations a voice across the organization and simplifying Cash’s securityrelated regulatory and compliance obligations  Governance Partners act as a bridge between the Cash App functional area they support and Leadership across security engineering and compliance teams to drive security enablement The Security Governance Partner for Cash App Machine Learning and Data Science MLDS works directly with the teams that build Cash App’s machine learning pipelines and infrastructure and the Cash data scientists to identify and communicate constraints and to evaluate potential solutions while partnering closely with MLDS Security Engineering to communicate requirements and shape the security posture of the MLDS organization  You will   Act as a security domain expert in partnership with Compliance Legal and Engineering  Collaborate deeply across roles and functions with Security Engineering Machine Learning Engineering and Modeling and Data ScienceBusiness Intelligence  Identify prioritize and balance security efforts with other objectives  Help Cash automate governance and compliance functions and develop reusable tools for common tasks using scripting languages like Python or data tools like Prefect  Participate in technical design discussions evaluate security properties of systems and services drive risk decisions and influence technical architecture  Understand challenges and roadblocks to achieving the desired security posture and push requirements to Security Engineering to drive longterm change  Develop implement and promote security standards and frameworks  Interpret and communicate security and compliance constraints to key stakeholders  Monitor applicable changes to security and privacy related laws regulations and industry standards with an eye towards Generative AI and other forwardlooking technologies    Qualifications  You have   3 years of experience leading projects or programs in a security environment  5 years working in a securityfocused role in a technologyheavy industry  Proficiency with at least one programming language eg Python Kotlin Java  Conceptual understanding of machine learning and data science tools and processes  Solid technical background with cloud computing architectures and security patterns  Ability to drive alignment and change in a matrixedenvironment with minimal supervision  Boundless curiosity persistence and a grounded approach to getting things done  Process orientation and an efficiency mindset to keep the organization unblocked and accountable to security  Working knowledge of one or more relevant compliance regulations such as SECFINRA CCPA CPRA GDPR PCI DSS SOX   Tech stack we use and teach   Java and Kotlin  Python  AWS GCP and Kubernetes  SQL Snowflake  Apache Spark  DynamoDB Kafka Apache Beam and Google DataFlow  Tableau Airflow Looker Mode Prefect  Tecton  Jupyter notebooks    Additional Information  Block takes a marketbased approach to pay and pay may vary depending on your location US locations are categorized into one of four zones based on a cost of labor index for that geographic area The successful candidate’s starting pay will be determined based on jobrelated skills experience qualifications work location and market conditions These ranges may be modified in the future  Zone A USD 148700  USD 223100 Zone B USD 141300  USD 211900 Zone C USD 133800  USD 200800 Zone D USD 126400  USD 189600   To find a location’s zone designation please refer to this resource If a location of interest is not listed please speak with a recruiter for additional information  Fulltime employee benefits include the following   Healthcare coverage Medical Vision and Dental insurance  Health Savings Account and Flexible Spending Account  Retirement Plans including company match  Employee Stock Purchase Program  Wellness programs including access to mental health 11 financial planners and a monthly wellness allowance  Paid parental and caregiving leave  Paid time off including 12 paid holidays  Paid sick leave 1 hour per 26 hours worked max 80 hours per calendar year to the extent legally permissible for nonexempt employees and covered by our Flexible Time Off policy for exempt employees  Learning and Development resources  Paid Life insurance ADD and disability benefits   These benefits are further detailed in Blocks policies This role is also eligible to participate in Blocks equity plan subject to the terms of the applicable plans and policies and may be eligible for a signon bonus Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies Pay and benefits are subject to change at any time consistent with the terms of any applicable compensation or benefit plans  We’re working to build a more inclusive economy where our customers have equal access to opportunity and we strive to live by these same values in building our workplace Block is a proud equal opportunity employer We work hard to evaluate all employees and job applicants consistently without regard to race color religion gender national origin age disability veteran status pregnancy gender expression or identity sexual orientation citizenship or any other legally protected class  We believe in being fair and are committed to an inclusive interview experience including providing reasonable accommodations to disabled applicants throughout the recruitment process We encourage applicants to share any needed accommodations with their recruiter who will treat these requests as confidentially as possible Want to learn more about what we’re doing to build a workplace that is fair and square Check out our ID page  Additionally we consider qualified applicants with criminal histories for employment on our team assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance    We’ve noticed a rise in recruiting impersonations across the industry where individuals are sending fake job offer emails Contact from any of our recruiters or employees will always come from an email address ending with blockxyz squareupcom tidalcom or afterpaycom clearpaycouk  Block Inc NYSE SQ is a global technology company with a focus on financial services Made up of Square Cash App Spiral TIDAL and TBD we build tools to help more people access the economy Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions business software and banking services With Cash App anyone can easily send spend or invest their money in stocks or Bitcoin Spiral formerly Square Crypto builds and funds free opensource Bitcoin projects Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution  While there is no specific deadline to apply for this role on average US open roles are posted for 70 days before being filled by a successful candidate  </data></node>
<node id="n991" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0f17ec594bc97b90&amp;bb=E5D0-e-RDhZJMYuphDBGvtgMcvFCWCddSeKQF2syTyxawXySAJDAk9_FUOdXwJhjqNgq9GW6vRNXIr7x-8HoYwHnUg40JsSyNgUS9I3aOpk4O8NaFaNvmhjrRC0wfwES&amp;xkcb=SoCl67M3CNkCGdQ8MZ1VbzkdCdPP&amp;fccid=29593153878c7ce7&amp;cmp=dbSeer&amp;ti=Data+Scientist&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in ScalaYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay110000  130000 a yearJob typeFulltimeContract Location7950 Jones Branch Drive McLean VA 22102 BenefitsPulled from the full job description401k401k matchingDental insuranceHealth insurancePaid time off Full job descriptionWho We Are dbSeer is a new breed of business analytics consulting firm dedicated to helping our customers take full advantage of their data by transforming it into actionable insight Leveraging expertise in a variety of data and analytics technologies and by employing our unique analytics framework we promote an agile approach and project methodology that simplifies each project into a series of short ROIjustified steps aimed at incrementally reaching the target solution We match bestofbreed analytics technologies to the needs of our clients to provide the right solution and obtain the best results in the shortest possible time with the least cost and effort Founded in 2013 dbSeer focuses on helping our clients solve their unique analytics data architecture and design and data engineering challenges We recognize that analytics are critical to your business success when properly designed and implemented As a strategic partner we collaborate with you to plan design develop and deploy solutions that help you transform data into insight ultimately improving your business operations Contributions As a Data Scientist you will work with our customers to create descriptive predictive and prescriptive models and insights to drive impacts to the organization Work with business stakeholders tech and data engineers to problem solve and develop modeling strategy and solutions with innovative approach Responsibilities  Design develop and evaluate complex predictive models and advanced algorithms Identifies meaningful insights from large data and metadata sources Test hypothesesmodels analyze and interpret results Develop and code moderately complex algorithms and automated processes Use modeling and trend analysis to analyze data and provide insights Lead small projects and initiatives Manage numerous priorities using proven project management methodologies and sound development practices to ensure quality delivery Build client trust and respect establish client relationships and develop rapport with client  Qualifications  Bachelor’s or Master’s degree in Data Science Statistics Mathematics Computers Science Engineering or degrees in similar quantitative fields  · 24 years of experience building scaling and optimizing ML systems  Strong Python programming skills Understanding of relational databases Strong problem solving and analytical skills Excellent communication skills ability to collaborate effectively across teams and convey technical concepts to nontechnical stakeholders  Job Types Fulltime Contract Pay 11000000  13000000 per year Benefits  401k 401k matching Dental insurance Health insurance Paid time off  Experience level  2 years  Ability to Relocate  McLean VA 22102 Relocate before starting work Required  Work Location In person </data></node>
<node id="n992" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d7fea3ae6ab493c2&amp;bb=E5D0-e-RDhZJMYuphDBGvo5FME2hLkGq46qELN08FEN7Wve02a89o7kXrS-WoVZtJbIcFNJSFrSvQ0nslpWMAmeQAnS1BH-USAjG0z-BEiNZ13JHmGho_g%3D%3D&amp;xkcb=SoCD67M3CNkCGdQ8MZ1pbzkdCdPP&amp;fccid=7c60906a93c1d77e&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationMcLean VA 22102 BenefitsPulled from the full job description401kDental insuranceFlexible scheduleHealth insurancePaid time offVision insuranceWork from home Full job description Benefits   401k Competitive salary Dental insurance Flexible schedule Health insurance Paid time off Vision insurance   Join our dynamic team at DGS a cuttingedge data solutions company for wireless networks where innovation meets excellence in the realm of Radio Frequency RF awareness and environmental monitoring technologies We are seeking a qualified candidate for the role of Senior System Engineer specializing in 5G wireless communication You will play a pivotal role in the design development and optimization of cuttingedge 5G systems and technologies    We are seeking a qualified Telecoms Data Scientist to play a crucial role in analyzing large datasets relating to telecommunications network performance user behavior and network optimization Expertise in 5G Wireless communication is preferred    In this role you will…   Collect preprocess and analyze large volumes of telecommunications data including network logs RF spectrum measurements and environmental sensor data Clean transform and organize raw data to prepare it for analysis ensuring data integrity and quality Work with simulated data and environments as well as MATLAB tools Conduct exploratory data analysis to uncover insights and trends related to RF spectrum utilization network performance and environmental factors Develop statistical models and machine learning algorithms to extract actionable insights and optimize RF spectrum usage Collaborate with crossfunctional teams including RF engineers software developers and product managers to translate datadriven insights into innovative solutions Design and implement algorithms for RF interference detection mitigation and localization contributing to the enhancement of RF environmental awareness capabilities Stay abreast of the latest advancements in telecommunications RF spectrum management and data science techniques incorporating new methodologies and technologies into our analytical workflows The ideal candidate would possess…   Master’s or Doctorate degree in Computer Science Data Science Electrical Engineering Statistics or a related field Proven experience as a Data Scientist or similar role with a focus on analyzing telecommunications data Strong background in RF communication principles wireless networking technologies and RF spectrum management Deep understanding of working with decoded LTE5G Data Expertise in data analysis statistical modeling and machine learning techniques with proficiency in Python R or other programming languages Experience working with largescale datasets and distributed computing frameworks Familiarity with telecommunications network performance measurement tools and protocols Excellent analytical problemsolving and criticalthinking skills Strong communication and collaboration abilities with the ability to present complex findings clearly and concisely Ability to thrive in a fastpaced innovative environment and manage multiple projects simultaneously   Foxhound Federal LLC is an equal opportunity and affirmative action employer Foxhound Federal is committed to administering all employment and personnel actions on the basis of merit and free of discrimination based on race color religion sex sexual orientation gender identity national origin protected veteran status or status as an individual with a disability Consistent with this commitment we are dedicated to the employment and advancement of qualified minorities women individuals with disabilities protected veterans persons of all ethnic backgrounds and religions according to their abilities    Flexible work from home options available  </data></node>
<node id="n993" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0f9f74e479f40791&amp;bb=E5D0-e-RDhZJMYuphDBGvoSiD-94V78gTdcIjn7Kh6B4Z02U4FU3cl2TdqIHvAv7FBH3-4r2oj_8RuL1fAvj273NaOdMT1O45bxiqr4N7z1ZTSuXP_Loqg%3D%3D&amp;xkcb=SoA367M3CNkCGdQ8MZ1obzkdCdPP&amp;fccid=877d7de2a3934dfe&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNo Location630 Komas Drive Salt Lake City UT 84108 Full job description  Your work will change lives Including your own    The Impact Youll Make  With treatments for hundreds of diseases in our sights weve built a data science team with domain expertise in computer science bioinformatics physics biology mathematics applied statistics and more We work sidebyside with biologists oncologists chemists software engineers and many others together we develop the tools and methods to turn our experimental data into treatments for pathologies that affect the lives of countless individuals As a data scientist supporting the development of our industrialized workflows youll work with a highly dynamic team that is focused on improving how we move from ideation through to advanced candidate drugs in a way that accelerates decisionmaking and automates as much as possible to scale the impact that we can have  Youll have access to unbelievable scales of data we currently run up to 22 million experiments each week our groundbreaking Phenom1 foundation model trained on  1 billion inhouse images and our maps of biology and chemistry that contain  5 trillion relationships across multiple biological and chemical contexts  In this role you will leverage this data as you  Develop methods metrics benchmarks and models to help drive drug discovery in a standardized way Convert exploratory analysis into productionquality functions that can be incorporated into inhouse Python packages and that support atscale generation of data packages to accelerate decisions on passing programs through internal stage gates Create and analyze enormous sets of connected data for a variety of programs to learn how best to advance drug discovery in an industrialized way Collaborate with engineering teams to mature your models and analyses and put them into productionized flows Deliver quickly and iteratively both supporting inflight programs and building improvements for the longterm in shortlived agile workstreams Learn to leverage new code packages and data science techniques as needed  Location  Making Salt Lake City your home base is ideal however we will consider onsite work in our Toronto Ontario offices as well  The Team Youll Join  We are an applicationoriented group whose goal is to discover drugs at scale using the toolkit of computational science in collaboration with our counterparts in other engineering software and data engineering laboratory automation scientific biology chemistry clinical science and operational laboratory operations regulatory affairs disciplines We are valuedriving  data science at Recursion is not just an accelerating function it is a core part of our value proposition As data scientists we are responsible for showing up as leaders and visionaries helping to shape how Recursion delivers on our mission We work on what matters and deliver in timescales of weeks not quarters We focus on the impact that we are trying to make and the why of what we are trying to deliver and are resilient if the how of what we are doing needs to change  The Experience Youll Need  2 years practical experience applying probability statistics and machine learning to realworld datasets in service of academic or business applications and recommendations  Strong preference for experience in the field of biosciences particularly pharmaceuticals or working on projects that require regular crossdisciplinary collaboration  Experience working within a fastpaced interdisciplinary team to solve businessrelevant problems and communicating complex concepts and methods to audiences with diverse technical backgrounds High fluency with the Python data stack numpy pandas scikitlearn etc Experience in collaborative data product development and peer code review including version control tools like git Experience developing releasing and maintaining data products in a continuoususe production environment Nice to have experience in creating compelling visualizations of highdimensional data that enable clear decisionmaking and interpretation prompt engineering for LLMs cheminformatics OR analysis of RNA sequencing data  How Youll be Supported  You will be assigned a peer trail guide to support you as you onboard and get familiar with Recursion systems Receive realtime feedback on code quality and best practices from a team of peer experts Ability to participate and learn from your colleagues in our regular allhands journal club  tech talks for Data Science Option to attend conferences to learn more from colleagues networks and more to better your skillset  LIEP1   The Values That We Hope You Share  We Care We care about our drug candidates our Recursionauts their families each other our communities the patients we aim to serve and their loved ones We also care about our work We Learn Learning from the diverse perspectives of our fellow Recursionauts and from failure is an essential part of how we make progress We Deliver We are unapologetic that our expectations for delivery are extraordinarily high There is urgency to our existence we sprint at maximum engagement making time and space to recover Act Boldly with Integrity No company changes the world or reinvents an industry without being bold It must be balanced not by timidity but by doing the right thing even when no one is looking We are One Recursion We operate with a company first team second mentality Our success comes from working as one interdisciplinary team  Recursion spends time and energy connecting every aspect of work to these values They arent static but regularly discussed and questioned because we make decisions rooted in those values in our daytoday work You can read more about our values and how we live them every day here  More About Recursion  Recursion is a clinical stage TechBio company leading the space by decoding biology to industrialize drug discovery Enabling its mission is the Recursion OS a platform built across diverse technologies that continuously expands one of the worlds largest proprietary biological and chemical datasets Recursion leverages sophisticated machinelearning algorithms to distill from its dataset a collection of trillions of searchable relationships across biology and chemistry unconstrained by human bias By commanding massive experimental scale — up to millions of wet lab experiments weekly — and massive computational scale — owning and operating one of the most powerful supercomputers in the world Recursion is uniting technology biology and chemistry to advance the future of medicine  Recursion is headquartered in Salt Lake City where it is a founding member of BioHive the Utah life sciences industry collective Recursion also has offices in London Toronto Montreal and the San Francisco Bay Area Learn more at wwwRecursioncom or connect on X formerly Twitter and LinkedIn  Recursion is an Equal Opportunity Employer that values diversity and inclusion All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin age disability veteran status or any other characteristic protected under applicable federal state local or provincial human rights legislation   </data></node>
<node id="n994" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=a66a845bd7ea0174&amp;bb=E5D0-e-RDhZJMYuphDBGvmJjk6JeDhEztZYQMfpHWux265ZbQVoet4r8ZUvC9oiBMbQfct9R-K43QSHRMn0ajyeTx2sHsXifaEsOuY43mJAz2QGNPpM1IA%3D%3D&amp;xkcb=SoC567M3CNkCGdQ8MZ1vbzkdCdPP&amp;fccid=7442885bc0fa7c14&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in UNIXYesNoEducationDo you have a Masters degreeYesNo Location2225 Lawson Ln Santa Clara CA 95054 BenefitsPulled from the full job description401k matchingEmployee stock purchase planFamily leaveFlexible spending accountHealth insurance Full job description Company Description  At ServiceNow our technology makes the world work for everyone and our people make it possible We move fast because the world can’t wait and we innovate in ways no one else can for our customers and communities By joining ServiceNow you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity We know that your best work happens when you live your best life and share your unique talents so we do everything we can to make that possible We dream big together supporting each other to make our individual and collective dreams come true The future is ours and it starts with you With more than 7700 customers we serve approximately 85 of the Fortune 500® and were proud to be one of FORTUNE 100 Best Companies to Work For® and Worlds Most Admired Companies™ Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow Unsure if you meet all the qualifications of a job description but are deeply excited about the role We still encourage you to apply At ServiceNow we are committed to creating an inclusive environment where all voices are heard valued and respected We welcome all candidates including individuals from nontraditional varied backgrounds that might not come from a typical path connected to this role We believe skills and experience are transferrable and the desire to dream big makes for great candidates Job Description   Experience in quality assurance andor application development Strong test automation skills in Java  Python Handson experience testing largescale production systems in Python or Java Excellent communication collaboration reporting analytical and problemsolving skills Ability to take a project from scoping the requirements and building the test cases Maintain existing automation test frameworks Familiarity with testing AIML models and evaluation of the model quality Ability to analyze results from the test set and create defects based on result patterns Work with developers to design specific testing strategies for features being developed and automate them Create comprehensive test plans execute and automate them Support engineering organizations in troubleshooting or addressing issues with applications and devtest environments    Qualifications   Bachelors in Computer Science  Computer Engineering or related subjects with 3 years of work experience Masters in Computer Science  Computer Engineering or related subjects with 2 years of work experience 2 years of experience with technologies relevant to SN and coding skills with highquality results Experience working within different automated testing frameworks including Java JUnit Python Selenium TestNG and other opensource projects Experience with the agile methodology for software development teams Ability to understand several testing techniques eg performance unit integration automated their strengths and weakness and ability to use them to best effect  including tracking and addressing of any discovered issues Ability to use tools such as IDE debugger build tools source control ServiceNow instances profilers system administrationUnix tools to assist with daily tasks For positions in the Bay Area we offer a base pay of 121600  188400 plus equity when applicable variableincentive compensation and benefits Sales positions generally offer a competitive On Target Earnings OTE incentive compensation structure Please note that the base pay shown is a guideline and individual total compensation will vary based on factors such as qualifications skill level competencies and work location We also offer health plans including flexible spending accounts a 401k Plan with company match ESPP matching donations a flexible time away plan and family leave programs subject to eligibility requirements Compensation is based on the geographic location in which the role is located and is subject to change based on work location    Additional Information  ServiceNow is an Equal Employment Opportunity Employer All qualified applicants will receive consideration for employment without regard to race color creed religion sex sexual orientation national origin or nationality ancestry age disability gender identity or expression marital status veteran status or any other category protected by law At ServiceNow we lead with flexibility and trust in our distributed world of work Click here to learn about our work personas flexible remote and requiredinoffice If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying you may contact us at talentacquisitionservicenowcom for assistance For positions requiring access to technical data subject to export control regulations including Export Administration Regulations EAR ServiceNow may have to obtain export licensing approval from the US Government for certain individuals All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the US Government Please Note Fraudulent job postingsjob scams are increasingly common Click here to learn what to watch out for and how to protect yourself All genuine ServiceNow job postings can be found through the ServiceNow Careers site   From Fortune © 2022 Fortune Media IP Limited All rights reserved Used under license Fortune and Fortune Media IP Limited are not affiliated with and do not endorse products or services of ServiceNow  </data></node>
<node id="n995" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=5d0fda287d8a969b&amp;bb=E5D0-e-RDhZJMYuphDBGvtgMcvFCWCddlxW4laYXN863fcoajAsRR8RsZ9fGhvG6ks0AdQbUJGj3FsDXL-9Q--pvKsnKeKvoq3qqQFZkLvrRLMavUKrMpw%3D%3D&amp;xkcb=SoB567M3CNkCGdQ8MZ1ibzkdCdPP&amp;fccid=db71ef259173ba17&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRemote BenefitsPulled from the full job description401k matchingDental insuranceHappy hourHealth insurancePaid time offTuition reimbursementVision insurance Full job description Founded in 2003 IT Concepts’ core values – customercentricity teamwork driven to deliver innovation and integrity – ensure we work together to be the best realize objectives and make a positive impact in our communities We intentionally created and sustain our ITC culture that embraces change experimentation continuous learning and improvement We bring our design thinking problem solving approach that challenges assumptions prioritizes curiosity and invites complexity to deliver innovative efficient and effective solutions As we continue to grow in the support of our government customers we are looking for driven and innovative individuals to join our team  IT Concepts Inc ITC is seeking an experienced Data Scientist to join our team of diverse technical professionals to assist the Department of Veterans Affairs VA in analyzing clinical data and Informatics solutions delivered to the VA enterprise to support organizational learning and a HighReliability Organization  The qualified applicant will become part of ITC’s team supporting our Department of Veterans Affairs client The candidate should be skilled in reporting and analytics tools and methods and communicate effectively with executive leadership  Responsibilities  Develops and manages information to support strategic decisionmaking Performs complex analyses and communicates findings to business owners and senior leaders Conducts and facilitates and educates and trains on analyses data research data manipulation risk assessment solution development and strategic alignment Provides consulting and analytic services to leadership Provides technical support mentoring and training to less senior analysts Collaborates with crossfunctional teams to develop data visualizations with meaningful insights Analyzes and interprets data trends and patterns with performance objectives in mind Defines data assets to develop performance measurements key performance indicators and objectives and key results   Requirements   BS degree plus 9 years of relevant experience or a Master’s degree plus 7 years of relevant experience or a Doctoral degree plus 5 years of relevant experience Relevant experience creating and analyzing clinical data and creating data visualizations that communicate clinical outcomes improvements and costs   Excellent written and oral communication skills must provide examples if chosen for an interview Must have experience with  Creating custom reports using Power BI Power BI pipelines SQL Python R DAX   Preferred  Experience in Agile SAFe Agile VA experience VistACPRS a plus  Clearance Requirements  Must be able to obtain and maintain a public trust clearance  Benefits  The Company  We believe in generating success collaboratively enabling longterm mission success and building trust for the next challenge With you as our partner let’s solve challenges think innovatively and maximize impact As a valued member of our team you have the unique opportunity to work in a diverse range of technology and business career paths all while supporting our nation and delivering innovative technology solutions We are a close community of experts that pride ourselves on creating an environment defined by teamwork dedication and excellence We hold three ISO certifications 270012013 2000012011 90012015 and two CMMI ML 3 ratings DEV and SVC Industry Recognition Growth  Inc 5000’s Fastest Growing Private Companies DC Metro List Fastest Growing Washington Business Journal Fastest Growing Companies Top Performing Small Technology Companies in Greater DC Culture  Northern Virginia Technology Council Tech 100 Honoree Virginia Best Place to Work Washington Business Journal Best Places to Work Corporate Diversity Index Winner – MidSize Companies Companies Owned by People of Color Department of Labor’s HireVets for our work helping veterans transition SECAF Award of Excellence finalist Victory Military Friendly Brand Virginia Values Veterans V3 Cystic Fibrosis Foundation Corporate Breath Award  Benefits  We offer great benefits – Competitive Paid Time Off Medical Dental and Vision Insurance Identity Theft Protection Legal Resources Coverage 401k with company matching with NO vesting period ITC Health benefits have a 0 premium for certain plans for eligible employees We invest in our employees – Every employee is eligible for education reimbursement for certifications degrees or professional development Reimbursement amounts may fluctuate due to IRS limitations We want you to grow as an expert and a leader and offer flexibility for you to take a course complete a certification or other professional growth and networking We are committed to supporting your curiosity and sustaining a culture that prioritizes commitment to continuous professional development We work hard we play hard ITC is committed to incorporating fun into every day We dedicate funds for activities – virtual and inperson – eg we host happy hours holiday events fitness  wellness events and annual celebrations In alignment with our commitment to our communities we also host and attend charity galasevents We believe in appreciating your commitment and building a positive workspace for you to be creative innovative and happy  AAEO  VEVRAA  ITC is an Affirmative ActionEqual Opportunity employer and a VEVRAA Vietnam Era Veterans Readjustment Assistance Act Federal Contractor As such any personnel decisions hire promotion job status etc on applicants andor employees are based on merit qualifications competence and business needs not on race color citizenship status national origin ancestry sexual orientation gender identity age religion creed physical or mental disability pregnancy childbirth or related medical condition genetic information of the employee or family member of the employee marital status veteran status political affiliation or any other factor protected by federal state or local law ITC maintains a strong commitment to compliance with VEVRAA and other applicable federal state and local laws governing equal employment opportunity We have developed comprehensive policies and procedures to ensure our hiring practices align with these requirements As a part of our VEVRAA compliance efforts ITC has established an affirmative action plan that outlines our commitment to the recruitment hiring and advancement of protected veterans This plan is regularly reviewed and updated to ensure its effectiveness We encourage protected veterans to selfidentify during the application process This information is strictly confidential and will only be used for reporting and compliance purposes as required by law Providing this information is voluntary and it will not impact your eligibility for employment Our commitment to equal employment opportunity extends beyond legal compliance We are dedicated to fostering an inclusive workplace where all employees including protected veterans are treated with dignity respect and fairness  How to Apply  To apply to IT Concept Positions Please click on the “Apply for this Job” button at the bottom of this Job Description or the button at the top “Application” Please upload your resume and complete all the application steps You must submit the application for IT Concepts to consider you for a position If you need alternative application methods please email careersuseitccom and request assistance  Accommodations To perform this job successfully an individual must be able to perform each essential duty satisfactorily Reasonable Accommodations may be made to enable qualified individuals with disabilities to perform the essential functions If you need to discuss reasonable accommodations please email careersuseitccom   </data></node>
<node id="n996" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=aa794da9e8364804&amp;bb=E5D0-e-RDhZJMYuphDBGvn11i8U9IIggVBTLvSJIkPoqbN835n2XYS9Y3t8hKzLZ9wo0KzUYHYbRaW84ZDFeUHLBPlhyceeR1_Wb8lZS1let2u5fnWx77Q%3D%3D&amp;xkcb=SoDe67M3CNkCGdQ8MZ1nbzkdCdPP&amp;fccid=6fcff32d4b8d1e53&amp;cmp=Tetra-Fields-LLC&amp;ti=Statistician&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VBAYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay100000  110000 a yearJob typeFulltimeShift and schedule8 hour shiftDay shiftMonday to Friday LocationRemote Full job descriptionOverviewWe are seeking a skilled Statistician to join our team As a Statistician you will play a crucial role in analyzing and interpreting data to provide valuable insights and support decisionmaking processes This is an exciting opportunity to apply your expertise in statistical analysis and contribute to clinical quality measure CQM and electronic CQM eCQM development at CMS Duties Collect clean and organize large datasets for analysis Develop statistical models and algorithms to analyze data and identify trendspatterns Apply advanced statistical techniques to solve complex problems Collaborate with crossfunctional teams to design experiments and conduct hypothesis testing Create visualizations and reports to effectively communicate findings to stakeholders Stay uptodate with industry trends and advancements in statistical methodologies Requirements Masters degree in Statistics Mathematics or a related field Doctoral degree preferred Strong proficiency in statistical software eg R Python and programming languages eg SQL Java Experience with VBA Hadoop and machine learning techniques Knowledge of natural language processing and quantum engineering is a plus Solid understanding of experimental design hypothesis testing and regression analysis Excellent analytical skills with the ability to interpret complex data sets Strong attention to detail and problemsolving abilities Effective communication skills to present findings to both technical and nontechnical stakeholders Join our dynamic team of statisticians and contribute your expertise to drive datainformed decisions We offer competitive compensation packages professional development opportunities and a collaborative work environment Apply now to be part of our innovative team Job Type Fulltime Pay 10000000  11000000 per year Experience level  7 years  Schedule  8 hour shift Day shift Monday to Friday  Work Location Remote </data></node>
<node id="n997" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e2ab440840a121d8&amp;bb=E5D0-e-RDhZJMYuphDBGvrY7iowMGAYZyP3gdlADgZwA8mq0AGCV-x8LjCiuKYWDWBqz9A8fP6-odyTc-D05_yKzU99vHRbZRUEWaI5AKMpXow3BpVxSWw%3D%3D&amp;xkcb=SoBD67M3CNkCGdQ8MZ1kbzkdCdPP&amp;fccid=8174bdd96a638006&amp;cmp=Intellibus&amp;ti=Senior+Technical+Consultant&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeShift and scheduleMonday to Friday LocationArlington VA Full job descriptionAre you an Artificial Intelligence Expert working at a Large Financial Institution and being told by your leadership that you are too handson or detailoriented or think and work like a startup Imagine working at Intellibus to engineer platforms that impact billions of lives around the world With your passion and focus we will accomplish great things together We are seeking an experienced Artificial Intelligence AI Technical Advisor to provide expert guidance and strategic direction in AIrelated initiatives and projects The AI Technical Advisor will play a key role in advising senior leadership collaborating with crossfunctional teams and driving the development and implementation of AI solutions The ideal candidate will have a deep understanding of AI technologies industry trends and best practices along with a proven track record of successfully implementing AI projects in complex environments We are looking for someone who can  Strategic Guidance Provide strategic advice and insights to senior leadership on leveraging AI technologies to achieve business objectives and drive innovation Apply knowledge in quantitative analysis machine learning statistical modeling and software development to data analysis problems for our sponsors Work independently and on teams to engineer automated solutions for sponsor mission challenges Advise and lead large AI teams Explore promising research and maintaingain the technical edge required for projects Share and develop new approaches and methods Collaborate to document and support software analytics and clearly present status and results to internal and external stakeholders Collaborate with stakeholders to identify AI opportunities and define strategic AI initiatives aligned with organizational goals Technical Expertise Stay abreast of the latest advancements and trends in AI technologies machine learning natural language processing computer vision and related fields Evaluate emerging AI technologies and assess their potential impact on the organizations business processes and competitive advantage Project Leadership Led the design and implementation of AI projects from conception to deployment ensuring alignment with business requirements and technical feasibility Provide technical leadership and guidance to crossfunctional teams including data scientists engineers and developers throughout the project lifecycle Solution Architecture Collaborate with architecture teams to design scalable and robust AI solutions that meet performance security and scalability requirements Develop architectural patterns and best practices for AI development and deployment in enterprise environments Technical Evaluation and Due Diligence Conduct technical evaluations and due diligence on AI platforms tools and thirdparty vendors to assess their suitability for integration into the organizations ecosystem Provide recommendations and make informed decisions based on technical assessments and evaluations Collaboration and Communication Foster collaboration and knowledge sharing across teams by providing technical guidance training and mentorship in AI technologies and methodologies Communicate complex technical concepts and insights to nontechnical stakeholders clearly and understandably Quality Assurance and Governance Establish quality assurance processes and governance frameworks for AI projects to ensure adherence to best practices regulatory requirements and ethical guidelines Monitor and evaluate AI models and solutions to ensure ongoing performance reliability and compliance  Qualifications  Bachelors degree in Computer Science or a related field is preferred Relevant work experience may be considered in lieu of a degree Excellent communication and interpersonal skills with the ability to effectively collaborate with crossfunctional teams and stakeholders Proven leadership abilities with experience mentoring junior developers and driving technical excellence within the team  We work closely with  Artificial Intelligence Machine learning Tensorflow Caffe CNTK Python Java C  C  Our Process  Schedule a 15minute Video Call with someone from our team 4 Proctored GQ Tests  2 hours 3045 mins Final Video Interview Receive Job Offer  If you are interested in reaching out to us please apply and our team will contact you within the hour Job Type Fulltime Schedule  Monday to Friday  Experience  Artificial Intelligence 7 years Preferred Machine learning 7 years Preferred Java 7 years Preferred  Ability to Relocate  Arlington VA Relocate before starting work Required  Work Location In person </data></node>
<node id="n998" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d9c833e2ee24d154&amp;bb=E5D0-e-RDhZJMYuphDBGvmevdnKylonp6jiR7M_8vffpZxRMi3e3O6X2MYTxdsYIf_hnJkl2k_PVTX9hlJjKfPhX-9asYmeRN4refeU3Bbe1iXQ-QoLcmw%3D%3D&amp;xkcb=SoDQ67M3CNkCGdQ8MZ16bzkdCdPP&amp;fccid=bc3378ca3341d04f&amp;cmp=American-Unit-Inc&amp;ti=Data+Scientist&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in UNIXYesNo Job detailsHere’s how the job details align with your profilePay50 an hourJob typeContractShift and schedule8 hour shift LocationAmerican Unit Inc in Irving TX 75061 BenefitsPulled from the full job description401kDental insuranceHealth insurance Full job descriptionData Scientist Irving Texas 6 Months CTH Customer VERIZON Job Description Project Details Verizon is seeking a Data Scientist to join its Data Operations and Targeting team This team is heavily focused on Data Analysis where they will be partnering with different stakeholders to help take in requirements From those requirements they will be tasked with pulling different data sets adding intelligence and trending analytics to help the team with targeting automation simplification of processes as well as ad hoc data manipulation and analysis to be used by marketing and strategy on sales plays In addition they will assist strategically in developing national and regional marketing campaigns both standalone and within a journey framework to help align solutions to strategy These campaigns can be complex and involve multiple database environments and marketing tools so this candidate will be expected to utilize specific approaches to ensure accurate implementation of regulatory legal and privacy policies Basic Qualifications for Consideration  57 years of experience in roles related to data engineering marketing analytics or similar fields SQL expertise Utilize advanced SQL skills to efficiently pull and manipulate data from various sources including cloud GCP and Teradata databases Previous work with very large databases and datasets Join Functions Tables Stored Procedures Python Expertise Machine learning application Ability to code from scratch creating tables and automating codedataprocesses Text analysis CommunicationWorking in CrossFunctional Teams Strong written and verbal communication skills with the ability to interact with many diverse work teams at various organizational levels ETL experience with large data sets Data extraction transaction and loading Other datasets in the ETL process  Preferred Skills  Experience using Alteryx to help automate and streamline different workflows Experience using Teradata  Unix – for data analytics  data warehousing Data Visualization specifically using Tableau Ability to take large data sets and understand what it is saying what information is the data telling you what the trends the data is presenting and being able to make an informed decision from the data Strong analytic skills to determine all required resources organize information and processes and execute the desired marketing targeting and strategy Marketing Background Preferred Customer and prospect targeting experience for marketing list development modeling andor analytics using SQL Experience with customer and prospect demographic data internally and externally sourced  Job Type Contract Salary 5000 per hour Expected hours 40 per week Benefits  401k Dental insurance Health insurance  Compensation package  Hourly pay  Experience level  10 years 11 years 5 years 6 years 7 years 8 years 9 years  Schedule  8 hour shift  Ability to Relocate  Irving TX 75015 Relocate before starting work Required  Work Location In person </data></node>
<node id="n999" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=4b95e91b93f0a35f&amp;bb=E5D0-e-RDhZJMYuphDBGvnE1Mku5-H-fWWBA97bvo7mMNF9BvyozOZWCFpulG7_ozOKDdT0cB2Gg-5u4ehlWGfuqas0n5Em7i9GTP9UNoSIlZanIYsIL-w%3D%3D&amp;xkcb=SoBN67M3CNkCGdQ8MZ15bzkdCdPP&amp;fccid=f65dc27fae2e9bf3&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Model trainingYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationCollege Park MD Full job description SMX is looking for a Data Scientist If you are dedicated to fostering a positive company culture and interested in shaping and growing our communications capabilities SMX is an excellent place to grow your career This is a role that will have to report to the customers site This person will support DIA and will need to report to their facility This is a fulltime position with onsite work performed at a clients office 3 daysweek located in Washington DC or Reston VA or College Park MD with possibility for some flexibility to work remote  Essential Duties and Responsibilities    Create and write technical data design documents and participate in technical design discussions that transform operational requirements into data driven solutions Use your firm understanding of data model training and prediction in order to develop data models and analytical dashboards Formulate and complete endtoend data models that include data gathering analysis modeling and ongoing scaled deliverables and presentations Structurepreprocess datasets to find usable information and to prepare for modeling Review and collaborate on recommendations and designs with other subject matter experts Develop solutions actions items and issues and remain accountable for their completion Work in an agile environment to continuously deliver high outputs Have a high attention to detail working closely with plans operations and technical stakeholders to drive measurable results Produce highquality wellwritten technical documentation Work effectively within a deliverycentric multifunctional team Collaborate with crossfunctional stakeholders to understand their operational need Provide transparent and regular status updates in requested format Work in a fastpaced Agile environment with fluid requirements and changing priorities     Required SkillsExperience  Active TSSCI CI Poly clearance This is a fulltime position with onsite work performed 3 daysweek at a clients office located in Washington DC or Reston VA or College Park MD with possibility for some flexibility to work remote 3 years of technical experience Excellent written and verbal communication skills to both technical and nontechnical audiences for collaborating with team members and customers  Desired SkillsExperience  Bachelors degree in a technical field preferred or equivalent years of experience and certifications Knowledge of cloud servicestools   This position requires three days a week on site at customer location in Washington DC or Reston Virginia or College Park MD The rest of the week can be remote there is some flexibility     cjpost  LIonsite  LIhybrid     At SMX® we are a team of technical and domain experts dedicated to enabling your mission From priority national security initiatives for the DoD to highly assured and compliant solutions for healthcare we understand that digital transformation is key to your future success  We share your vision for the future and strive to accelerate your impact on the world We bring both cutting edge technology and an expansive view of whats possible to every engagement Our delivery model and unique approaches harness our deep technical and domain knowledge providing forwardlooking insights and practical solutions to power secure mission acceleration  SMX is committed to hiring and retaining a diverse workforce All qualified candidates will receive consideration for employment without regard to disability status protected veteran status race color age religion national origin citizenship marital status sex sexual orientation gender identity or expression pregnancy or genetic information SMX is an Equal OpportunityAffirmative Action employer including disability and veterans  Selected applicant will be subject to a background investigation   </data></node>
<node id="n1000" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=4a2408ebcdffde5c&amp;bb=E5D0-e-RDhZJMYuphDBGviW6yslm4c7pn06F_YiQqHceTN5SWpH8UrmBBTImcyKQcK6jfkFV_CPJYZfzxbm-iP7zSAIGtfzMcQOxDDiS6QjkKkLP0tWflw%3D%3D&amp;xkcb=SoB367M3CNkCGdQ8MZ1_bzkdCdPP&amp;fccid=1d28e943087f3cab&amp;cmp=Finder-Software-Solutions&amp;ti=Computer+Vision+Engineer&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay75000  109000 a yearJob typeFulltimeShift and scheduleMonday to Friday LocationVetted Security Solutions in Saint Petersburg FL 33714 BenefitsPulled from the full job description401k401k matchingDental insuranceHealth insuranceHealth savings accountLife insurancePaid time offShow morechevron down Full job descriptionWhat you will be doing  You will be designing and implementing algorithms and machine learning models for computer vision applications Building custom computer vision models to run on edge devices and the cloud Oversee data collection model training and model optimization strategies Implement model strategies Improve upon current models deployed in the field Working on novel labeling methods with the data infrastructure team to reduce labeling costs while improving model outputs  Requirements  BSMS in Computer Science or other related field or an equivalent combination of professional experience and education 3 years of industry experience developing algorithms and computer vision models using PyTorch or TensorFlow Experience in NVIDIA Deepstream Proven history of designing training and deploying CNN computer vision models to production Must have production experience in Computer Vision and Deep Learning 3 years of industry experience using OpenCV library and related tools Deep knowledge of image processing and computer vision Experience designing and operating labeling systems Great programming skills in Python along with relevant packages like numpy scikitlearning and matplotlib Deep understanding of data structures and algorithms Strong written and verbal communication and collaboration skills Developing computer vision models Experience with MLAI models pipeline inference or training Basic SQL skills with databases like MySQL and Microsoft SQL server  Additional nicetohave skills  Familiarity with NVIDIA DeepStream SDK and its main components including TensorRT Triton Inference Server  Tao Toolkit Experience in deploying CV solutions using NVIDIA Jetsons Experience with CC Have experience with deep learning libraries and tools like Keras CUDA CUDNN OpenVINO ONNX Knowledge of GStreamer Handson coding experience C  CUDA  Job Type Fulltime Pay 7500000  10900000 per year Benefits  401k 401k matching Dental insurance Health insurance Health savings account Life insurance Paid time off Parental leave Vision insurance  Experience level  3 years  Schedule  Monday to Friday  Ability to Relocate  Saint Petersburg FL 33714 Relocate before starting work Required  Work Location In person </data></node>
<node id="n1001" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=69c1c6cf36f19790&amp;bb=E5D0-e-RDhZJMYuphDBGvo5FME2hLkGq4JVOAk3GdOi-x2F0bWAjWBkpiEk-ZlvXzx95vvT8fCE-0YQ04oPiOjO20gGhv3XMAEY5AAEP8KsDaHuw7lxVZA%3D%3D&amp;xkcb=SoBe67M3CNkCGdQ8MZ19bzkdCdPP&amp;fccid=ef4d2a1362147d41&amp;cmp=Kid-Company&amp;ti=Ai%2Fml+Engineer&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay120000  130000 a yearJob typeFulltimeShift and scheduleMonday to Friday Location3101 Park Boulevard Palo Alto CA 94306 BenefitsPulled from the full job descriptionHealth insurance Full job descriptionJob Posting MLAI Principal Engineer Company Kid Company wwwkidcoai Job Title MLAI Principal Engineer Location San Francisco Bay Area CA Duration Fulltime About Us Kid Company is a startup founded by Robert LoCascio who Founded and ran LivePerson for 28 years which was the leading AI B2B Conversational Commerce company for the enterprise The mission of KID is to create the world’s best Responsive AI device that taps into the imaginations of people starting with children and allows them through safe AI be their best creators of music art video and games Description We are actively seeking an exceptional Principal Engineer with expertise in Machine Learning AI to lead the development and integration of advanced AI capabilities into our innovative Responsive AI device The successful candidate will spearhead the design and implementation of cuttingedge machine learning algorithms and AI technologies to revolutionize the way individuals interact and create with our AI device Responsibilities The Principal Engineer will  Lead the development and integration of machine learning algorithms and AI technologies into our Responsive AI device Spearhead the design and implementation of advanced AI capabilities to enhance user experiences Collaborate with crossfunctional teams to drive the innovation and evolution of our AI devices AI capabilities Research and implement stateoftheart machine learning and AI models to achieve industryleading performance  Requirements  A Bachelor’s or Masters in Computer Science Electrical Engineering or a related field with a focus on machine learning AI or related disciplines Proven experience in developing and implementing machine learning algorithms and AI technologies in realworld applications Expertise in deep learning frameworks such as TensorFlow PyTorch or Keras and proficiency in using them to build and deploy machine learning models Strong understanding of natural language processing computer vision and reinforcement learning Experience in deploying machine learning models in production environments and optimizing their performance Proven ability to lead and mentor a team of machine learning engineers and data scientists Must be in CA area  Benefits  Competitive salary of 120000 per year Opportunity to work with a pioneering team at the forefront of AI technology Professional growth and advancement within the company  Application Process If you meet the above qualifications and are passionate about pushing the boundaries of machine learning and AI technology we encourage you to submit your resume and a detailed cover letter to ronakidcoai with the subject line Principal Engineer  Machine Learning AI Application  Your Name Additionally we recommend visiting our website kidcoai to explore the Hiring Pages for current Job Vacancies Job Type Fulltime Salary 120000 per year Equity Startup Equity Work Schedule Monday to Friday in Palo Alto CA or San Francisco CA Job Type Fulltime Pay 12000000  13000000 per year Benefits  Health insurance  Experience level  4 years 5 years  Schedule  Monday to Friday  Work Location Hybrid remote in Palo Alto CA 94306 </data></node>
<node id="n1002" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=77e066b51bf905e7&amp;bb=E5D0-e-RDhZJMYuphDBGviOGKqa5pzcZC_b-kk6Qd0DDVKvxggnI6Mg3D2ykGwTo-raSNcpQpkU27NSZoGN8rwtMAHsRUoEWA0pERs-FI9rBJgiCs2gOPQ%3D%3D&amp;xkcb=SoAD67M3CNkCGdQ8MZ1zbzkdCdPP&amp;fccid=15c26b2f7c7309c1&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNoEducationDo you have a Masters degreeYesNo LocationLancaster PA 17602 Full job description Clark Associates a distinguished leader in the foodservice equipment and supplies industry is seeking a dedicated and visionary individual to join our Augmented Inventory Management AIM team As the foremost and rapidly expanding foodservice equipment and supplies dealer in the USA we leverage a network of over 20 strategically positioned national distribution centers to promptly deliver an extensive catalog of 70000 products to our valued customers within an efficient 12 business day timeframe   Role Overview  As a member of our Augmented Inventory Management team you will play a pivotal role in shaping the future of inventory management within our dynamic organization This team composed of highly analytical professionals is at the forefront of employing cuttingedge techniques to streamline decisionmaking processes enhance accuracy and achieve remarkable scalability through the integration of machine learning technologies   Responsibilities   Model Development Create advanced statistical and machine learning models that optimize inventory management processes fostering efficient allocation and utilization of resources  Collaboration Interface with stakeholders in procurement and operations to devise systematic solutions for intricate inventory management challenges  Continuous Enhancement Research and implement enhancements to existing models keeping abreast of emerging methodologies in the field  Actionable Insights Extract meaningful insights from data and formulate actionable recommendations that tangibly bolster the companys financial performance  Solution Validation Implement robust monitoring mechanisms to validate the efficacy of deployed solutions    Minimum Qualifications   Experience Over 3 years of practical experience using SQL and optimizationstatisticalmachine learning programming Python preferred R and Matlab accepted  Education Bachelors degree in operations research data science statistics or a closely related quantitative field alternatively over 4 years of experience with an optimization data research or applied science team  Adaptability Demonstrated ability to swiftly acclimate to evolving environments rapidly learn new skills and apply them to intricate problemsolving scenarios    Preferred Qualifications   Supply Chain Expertise Familiarity with supply chain and warehouse stock optimization practices  Operations Research Experience Familiarity with building optimization models Eg ORTools CPLEX  Forecasting Acumen Exposure to time series forecasting models  Reporting Proficiency Experience with reporting tools like Power BI and Excel for sharing results  Communication Exceptional written and verbal communication skills with the ability to translate technical processes for nontechnical audiences  Advanced Education Masters degree in data science statistics operations research industrial engineering or a closely related highly quantitative field    Remote work qualifications   Access to a reliable and secure highspeed internet connection Cable or fiber internet connections at least 75mbps download10mbps upload are preferred as satellite connections often cannot support the technologies used to perform daytoday tasks  Access to a home router and modem  A dedicated home office space that is noise and distractionfree The space should have strong wireless connection or a wired Ethernet connection wired connection is preferred if possible  A valid physical address apartment suite etc PO Boxes are not supported as a physical address is required for you to receive your computer equipment  The desire and ability to work and communicate with other team members via chat webcam etc  Legal residents of one of the following states  H1B Visa Sponsorship Not Available W2 only   </data></node>
<node id="n1003" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=90880c38eee1762e&amp;bb=E5D0-e-RDhZJMYuphDBGvv1Hmy61DVRHb4p3uefS7zpRiFgqkkKf-7IzPGH816zSFSWRiJJyJ93VBtC68rjFK0VkszOKM5_gYeDAmgkPkGMGTRUHNXD5gw%3D%3D&amp;xkcb=SoCk67M3CNkCGdQ8MZ12bzkdCdPP&amp;fccid=caed318a9335aac0&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNoEducationDo you have a Bachelors degreeYesNo LocationSan Jose CA 95110 BenefitsPulled from the full job description401k matchingADD insuranceDisability insuranceEmployee assistance programFlexible spending accountHealth insuranceHealth savings accountShow morechevron down Full job descriptionResponsibilities  TikTok is the leading destination for shortform mobile video Our mission is to inspire creativity and bring joy TikTok has global offices including Los Angeles New York London Paris Berlin Dubai Singapore Jakarta Seoul and Tokyo    Creation is the core of TikToks purpose Our platform is built to help imaginations thrive This is doubly true of the teams that make TikTok possible Together we inspire creativity and bring joy  a mission we all believe in and aim towards achieving every day To us every challenge no matter how difficult is an opportunity to learn to innovate and to grow as one team Status quo Never Courage Always  At TikTok we create together and grow together Thats how we drive impact  for ourselves our company and the communities we serve  Join us    About the team  The Applied AI DCC Team is part of the Monetizing Integrity team in TikTok global business solutions We support Data Cycling Center to provide enterprises with affordable and trusted data and models    Responsibilities   Design and build core capabilities by leveraging content understanding capabilities such as natural language processing machine learning or computer vision to extract insights and improve monetization strategies Develop creative solutions and build prototypes for business problems using algorithms based on the latest deep learning machine learning statistics and optimization techniques Independently manage data projects from 0 to 1 and collaborate with product managers to define user stories and success metrics to guide the development process Verify the business value and estimated revenue of the project using methods such as AB testing Collaborate with engineering teams to deploy and scale data science solutions  Qualifications    Minimum Qualifications   BS or above in Computer Science Software Engineering Data Science or a related field Knowledge of underlying mathematical fundamentals in statistics machine learning and analytics 3 years experience in data modelinganalysis with industry experience in MLDL and one of CVNLPSpeech Experience with exploratory data analysis statistical analysis hypothesis testing and model development Fluency in SQL Hive Presto or Spark and having experience working with large datasets High proficiency in Python and SQL and MLDL frameworks such as TensorFlow PyTorch  Preferred qualifications   Experience in CICD such as git and cloud services such as AWSGCPAzure will be highly desirable Intellectual curiosity along with excellent problemsolving and quantitative skills including the ability to desegregate issues identify root causes and recommend solutions  TikTok is committed to creating an inclusive space where employees are valued for their skills experiences and unique perspectives Our platform connects people from across the globe and so does our workplace At TikTok our mission is to inspire creativity and bring joy To achieve that goal we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach We are passionate about this and hope you are too    TikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities pregnancy sincerely held religious beliefs or other reasons protected by applicable laws If you need assistance or a reasonable accommodation please reach out to us at httpsshorturlatcdpT2  Job Information  The base salary range for this position in the selected city is 128000  290000 annually    ​    Compensation may vary outside of this range depending on a number of factors including a candidate’s qualifications skills competencies and experience and location Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work and this role may be eligible for additional discretionary bonusesincentives and restricted stock units    ​    Our company benefits are designed to convey company culture and values to create an efficient and inspiring work environment and to support our employees to give their best in both work and life We offer the following benefits to eligible employees    ​    We cover 100 premium coverage for employee medical insurance approximately 75 premium coverage for dependents and offer a Health Savings AccountHSA with a company match As well as Dental Vision ShortLong term Disability Basic Life Voluntary Life and ADD insurance plans In addition to Flexible Spending AccountFSA Options like Health Care Limited Purpose and Dependent Care    ​    Our time off and leave plans are 10 paid holidays per year plus 17 days of Paid Personal Time Off PPTO prorated upon hire and increased by tenure and 10 paid sick days per year as well as 12 weeks of paid Parental leave and 8 weeks of paid Supplemental Disability    ​    We also provide generous benefits like mental and emotional health benefits through our EAP and Lyra A 401K company match gym and cellphone service reimbursements The Company reserves the right to modify or change these benefits programs at any time with or without notice </data></node>
<node id="n1004" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=3dfd8ddd4fdcc47b&amp;bb=AbDN_Sau9azpIrnPUsr1yWW9nA385qSdd-ZSeFFyeCX88eU9vSeURI4VqNlDPpLr220Mehs45acoUxsmnX3lOHnnsDFrcDqVNvgevGbWBC9eWfeJJGIpfg%3D%3D&amp;xkcb=SoDe67M3CNkeuKxylh0IbzkdCdPP&amp;fccid=29944148d86fe659&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNo LocationRemote Full job description  A Little About Us  Innovative collaborative minds wanted The world loves Postgres We envision a world where organizations thrive by harnessing the full power of Postgres the worlds fastest growing and most loved and used open source database Our mission is to enable data teams everywhere to harness the full power of Postgres whether on premises or in the cloud with high availability reliability scalability and security  Were 1 in Postgres We aspire to become 1 in Postgres AI Weve been major contributors to Postgres since the beginning and we are proud to call thousands of boundarypushing customers our partners Proud though we are we are not resting on our laurels Theres plenty of work to do The good news is that everything we do will impact Postgres which is to say that it will impact the world No pressure  EDB empowers organizations to take control of their data As one of the leading contributors to the vibrant and fastgrowing Postgres community EDB is committed to driving innovation in AI data and enterprise database technology Our work is fueled by creative dedicated people who are committed to help our customers and the community take Postgres everywhere Join us   This position is 100 remote for candidates based in the US  We are looking for an experienced Analytics Software Engineer with significant Rust experience and a strong working knowledge of Postgres Ideal candidates will be able to demonstrate selfstarting skill sets alongside a proven track record of delivery in a complex Cloud andor onprem environment  Your impact will be  You will own delivery for key aspects of the EDB Analytics andor Data  AI capability You carry responsibility for designing engineering solutions to defined Customer Use Cases in the Analytics and AI space  What you will bring  5 years of experience working as a DevOps Engineer or a Software Engineer Experience in Rust Python GoLang JavaScript Bash or PowerShell etc Experience with Postgres is a must You feel at home with different types and formats of data Understanding of Kubernetes monitoring and environments Experience modeling and benchmarking Analytics workloads Experience with GitHub Jenkins Docker and Jira You are comfortable working in remote team setups  What will give you an edge  Experience buildingusing Analytics Machine Learning and AI Libraries particularly Apache Data fusion and Apache Spark to deliver business value Analyzing data and building visualizations Applied knowledge andor experience establishing customer requirements and turning that into delivered capability Experience with DBRE or DBA practices    EDB is committed to supporting our employees overall well being by offering a range of benefits and resources to promote a healthy worklife balance and wellness We provide access to Modern Health to aid employees in health and wellness tips and practices as well as Wellness Fridays extending to June 2024 Check out our career site for more information on perks and benefits and reach out to our Talent Acquisition team for region specific benefits  We know it takes a unique mix of people and skills to help us in our mission to supercharge Postgres and we understand that not everyone will check every box Wed love to hear from you and we want you to apply  EDB is proud to be an equal opportunity workplace We celebrate diversity and are committed to creating an inclusive environment for all employees EDB was built on a commitment to trust and respect each other and to embrace an array of people and ideas These values remain at the center of our culture and are key to our companys integrity  EDB does not seek or accept unsolicited resumes or CVs from recruitment agencies EDB and its affiliates are not responsible for and will not pay any fees commissions or any other similar payment related to unsolicited resumes or CVs except as required in a written signed agreement between EDB and the recruitment agency or party requesting payment of a fee  LIRemote BIRemote   </data></node>
<node id="n1005" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=ef61fa9637f0cd9a&amp;bb=AbDN_Sau9azpIrnPUsr1yYsOjGoITTq9Q5hHYA6pX-jqhGrV04Cu3pDfg7SxWLw9X5dkn5B49I80CJ8pTQNEd7LitUfKpKZ4ScTsLaOh69Clkr0XfuadVQ%3D%3D&amp;xkcb=SoBQ67M3CNkeuKxylh0PbzkdCdPP&amp;fccid=6f995776e115ab84&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationWaynesboro VA BenefitsPulled from the full job descriptionHealth insuranceReferral programRetirement planVision insurance Full job description Innovative Refrigeration Systems is looking to hire a Lead Machine Learning Engineer fulltime onsite in Waynesboro VA You would be joining our software engineering department to build machine learning models optimizing energy efficiency for heavy industrial refrigeration  cold storage facilities Our aim is to create a more sustainable future for our Fortune 1000 clients powering America’s food supply chain This is an extremely rewarding and impactful greenfield initiative with a company uniquely positioned to challenge the status quo Responsibilities  Building physicsbased reinforcement machine learning models around industrial refrigeration PLC  IoT data to optimize for energy efficiency and temperature safety Fine tuning and deploying ML models to our backend platform Working closely with our product energy and software teams to guide future development of our energy management platform  Qualifications  Bachelors degree Must be a selfstarter with ability to learn from our mechanical engineers and adapt their knowledge of refrigeration and thermodynamics to ML models 2 years of handson experience building and optimizing ML systems Comfortable in cloud infrastructure provider ML environments Expert in your ML tools of choice R Python Scikit Tensorflow etc  Bonus Skills  Masters or PhD in computer science mathematics mechanical or chemical engineering or any similarrelevant field Background education or otherwise in any of energy optimization predictive maintenance thermodynamics heatmass transfer industrial refrigeration or mechanicalchemical engineering Prior experience as a software engineer building backend systems around ML models  Benefits  Retirement plan we match dollar for dollar up to 15 Health insurance with 75 of monthly premium covered for employees and their families Dental  Vision  Life  Disability  Supplemental insurance Competitive vacation and holiday pay Employee referral incentives Professional development opportunities  About Innovative Refrigeration Systems We are a vertically integrated market leader in cold storage and industrial refrigeration providing engineering controls construction service and software solutions to America’s largest food and beverage corporations Our software team is responsible for multiple industrial softwareasaservice platforms in process safety environmental health  safety and energy management  </data></node>
<node id="n1006" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=f7735ae8a02fa498&amp;bb=AbDN_Sau9azpIrnPUsr1yXpwiiYiuNPI-SDif8jrMNRcyq9__eK9tp0jPwTGlqGX33cPzl9EA5hs9xk5lBkFR4WXxsCdzkEbMwX5AR5XPik%3D&amp;xkcb=SoDk67M3CNkeuKxylh0ObzkdCdPP&amp;fccid=d723225da214c842&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Software developmentYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationPeoria IL BenefitsPulled from the full job description401kBenefits from day oneDental insuranceHealth insurancePaid time offVision insurance Full job description Career Area Business Technologies Digital and Data   Job Description  Your Work Shapes the World at Caterpillar Inc  When you join Caterpillar youre joining a global team who cares not just about the work we do – but also about each other We are the makers problem solvers and future world builders who are creating stronger more sustainable communities We dont just talk about progress and innovation here – we make it happen with our customers where we work and live Together we are building a better world so we can all enjoy living in it  The Opportunity   Embedded within the Global Machine Pricing organization gathers and documents userclient requirements supports the development and testing of new products or services collects feedback from clients and recommends improvements Also performs analytical tasks and initiatives on pricing merchandising and commercial data for Caterpillar machine products to support datadriven business decision and development  Responsibilities    Assisting in the development testing and delivery of analytics and digital solutions Analyzing existing offerings preparing technical documentation and recommending upgrades  Identifying userclient requirements for a product or service and defining technical specifications Preparing technical documentation and sharing them with senior analysts and technical teams  Handle the complete development of new dashboards including concept design backend development data validation frontend visualization and publication with proper security  Work with various business partners to understand business needs and devise the appropriate solution whether that is a dashboard a onetime report an automated report or other similar solution  Supporting senior analysts by fulfilling their data requests preparing reports and providing technical assistance Participating in initiatives to improve processes efficiency and employee productivity  Directing the data gathering data mining and data processing processes in huge volume creating appropriate data models  Leading to define requirements and scope of data analyses presenting and reporting possible business insights to management using data visualization technologies Conducting research on data model optimization and algorithms to improve effectiveness and accuracy on data analyses    Skill Descriptors   Accuracy and Attention to Detail Understanding the necessity and value of accuracy ability to complete tasks with high levels of precision   Level Extensive Experience   Evaluates and makes contributions to best practices  Processes large quantities of detailed information with high levels of accuracy  Productively balances speed and accuracy  Employs techniques for motivating personnel to meet or exceed accuracy goals  Implements a variety of crosschecking approaches and mechanisms  Demonstrates expertise in quality assurance tools techniques and standards   Business Data Analysis Knowledge of business data analysis ability to collect identify analyze and interpret business data using various kinds of techniques to meet business needs and requirements  Level Extensive Experience   Sets standards for business data analysis tools and techniques advises on their application and ensures compliance  Provides solutions to data requirements in the business data analysis process  Directs the implementation of business data collection processes and educates junior employees on new data sources  Evaluates the quality of business data collected and the effectiveness of data analysis methods  Oversees the use of business data analysis software and troubleshoots for common errors Suggests conclusions with results drawn from business data analysis     Analytical Thinking Knowledge of techniques and tools that promote effective analysis ability to determine the root cause of organizational problems and create alternative solutions that resolve these problems  Level Working Knowledge   Approaches a situation or problem by defining the problem or issue and determining its significance  Makes a systematic comparison of two or more alternative solutions  Uses flow charts Pareto charts fish diagrams etc to disclose meaningful data patterns  Identifies the major forces events and people impacting and impacted by the situation at hand Uses logic and intuition to make inferences about the meaning of the data and arrive at conclusions    Effective Communications Understanding of effective communication concepts tools and techniques ability to effectively transmit receive and accurately interpret ideas information and needs through the application of appropriate communication behaviors  Level Extensive Experience   Reviews others writing or presentations and provides feedback and coaching  Adapts documents and presentations for the intended audience  Demonstrates both empathy and assertiveness when communicating a need or defending a position  Communicates well downward upward and outward  Employs appropriate methods of persuasion when soliciting agreement Maintains focus on the topic at hand     Programming Languages Knowledge of basic concepts and capabilities of programming ability to use tools techniques and platforms in order to write and modify programming languages  Level Working Knowledge   Participates in the implementation and support of specialized programming languages  Highly proficient in Power BI SQL and Snowflake Experience with Python and Data Science preferred  Conducts basic reviews on writing a specific programming language within a specific platform  Assists with the design and development of specialized programming languages  Follows an organizations standards policies and guidelines for structured programming specifications Diagnoses and reports minor or routine programming language problems     Query and Database Access Tools Knowledge of data management systems ability to use support and access facilities for searching extracting and formatting data for further use  Level Working Knowledge   Defines creates and tests simple queries by using associated command language in a specific environment  Applies appropriate query tools used to connect to the data warehouse  Obtains and analyzes query access path information and query results  Employs tested query statements to retrieve insert update and delete information Works with advanced features and functions including sorting filtering and making simple calculations     Business Assessment Knowledge of the activities tasks practices and deliverables for assessing and documenting business opportunities ability to assess the benefits risks and success factors of potential applications  Level Working Knowledge   Applies tools and techniques for gathering business requirements  Defines the activities deliverables and tools of a business assessment process  Documents best practices and techniques for assessing application opportunities  Participates in conducting and documenting business assessments Participates in the preparation of risk assessment and benefits analysis     Requirements Analysis Knowledge of tools methods and techniques of requirement analysis ability to elicit analyze and record required business functionality and nonfunctionality requirements to ensure the success of a system or software development project   Level Working Knowledge   Follows policies practices and standards for determining functional and informational requirements  Confirms deliverables associated with requirements analysis  Communicates with customers and users to elicit and gather client requirements  Participates in the preparation of detailed documentation and requirements Utilizes specific organizational methods tools and techniques for requirements analysis     This Job Description is intended as a general guide to the job duties for this position and is intended for the purpose of establishing the specific salary grade It is not designed to contain or be interpreted as an exhaustive summary of all responsibilities duties and effort required of employees assigned to this job At the discretion of management this description may be changed at any time to address the evolving needs of the organization It is expressly not intended to be a comprehensive list of “essential job functions” as that term is defined by the Americans with Disabilities Act  Additional Information  This employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer such as H L TN F J E O As a global company Caterpillar offers many job opportunities outside of the US which can be found through our employment website at wwwCaterpillarcomCareers   Employee benefit details  Our goal at Caterpillar is for you to have a rewarding career Our teams are critical to the success of our customers who build a better world  Here you earn more than just an hourly wage because we value your performance we offer a total rewards package that provides day one benefits medical dental vision RX and 401K along with a yearly STIP bonus  Additional benefits include paid vacation days and paid holidays prorated based upon hire date  Final details  Please frequently check the email associated with your application including the junkspam folder as this is the primary correspondence method If you wish to know the status of your application – please use the candidate login on our career website as it will reflect any updates to your status  LI  Posting Dates April 1 2024  April 14 2024   Any offer of employment is conditioned upon the successful completion of a drug screen  EEOAA Employer All qualified individuals  Including minorities females veterans and individuals with disabilities  are encouraged to apply  Not ready to apply Join our Talent Community   </data></node>
<node id="n1007" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=05b55520f7eb2d0b&amp;bb=AbDN_Sau9azpIrnPUsr1yaufNDWaWo5iMRX0F4TRcIWpmxtmTQVF-SUwp3QmzDAPyUXOCXLMrMjVv33a-VxE_xUETMFGCFUp9r-wMAKiPY8%3D&amp;xkcb=SoDN67M3CNkeuKxylh0MbzkdCdPP&amp;fccid=c1099851e9794854&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNoEducationDo you have a Doctoral degreeYesNo Job detailsHere’s how the job details align with your profilePay170700  300200 a yearJob typeFulltime LocationCupertino CA BenefitsPulled from the full job descriptionDental insuranceEmployee stock purchase planHealth insuranceRSURetirement plan Full job description Summary   Posted Jan 30 2024    Weekly Hours 40   Role Number200516560   Play a part in building the next revolution of machine learning technology Were looking for passionate midlevel and senior researchers to work on ambitious curiosity driven longterm research projects that will impact the future of Apple and our products In this role youll have the opportunity to work on innovative foundational research in machine learning As a member of the team you will be inspired by a diversity of challenging problems collaborate with worldclass machine learning engineers and researchers to impact the future of Apple products and publish some of your results in highquality scientific venues    Key Qualifications    Demonstrated expertise in machine learning research  Publication record in top machine learning vision and NLP conferences eg NeurIPS ICML ICLR AAAI CVPR ICCV ECCV ACL EMNLP etc  Handson experience working with deep learning toolkits such as PyTorch  Strong mathematical skills in linear algebra and statistics  Ability to formulate a research problem design experiment implement and communicate solutions  Ability to work in a diverse collaborative environment Experience with generative models energy based models diffusion models variational methods autoregressive models for vision speech and language large language models training reinforcement learning etc is desirable      Description   In this position you will conduct cutting edge research in Machine Learning in collaboration with your colleagues in MLR You will advance the state of the art in diverse topics such as generative models language modeling reinforcement learning etc by developing new techniques and demonstrating them on different domains such as speech vision and language An integral part of the job involves publishing of the results to top level ML conferences and participating in the research community through conferences workshops etc You will also have the opportunity to collaborate with broader teams across Apple in the use of the latest ML techniques    Education  Experience   PhD in Computer Science or related technical field with a wellestablished track record in publishing in top Machine Learning conferences like NeurIPS ICML ICLR etc    Additional Requirements   Pay  Benefits      At Apple base pay is one part of our total compensation package and is determined within a range This provides the opportunity to progress as you grow and develop within a role The base pay range for this role is between 17070000 and 30020000 and your base pay will depend on your skills qualifications experience and location  Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs Apple employees are eligible for discretionary restricted stock unit awards and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan You’ll also receive benefits including Comprehensive medical and dental coverage retirement benefits a range of discounted products and free services and for formal education related to advancing your career at Apple reimbursement for certain educational expenses  including tuition Additionally this role might be eligible for discretionary bonuses or commission payments as well as relocation Learn more about Apple Benefits  Note Apple benefit compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program  Apple is an equal opportunity employer that is committed to inclusion and diversity We take affirmative action to ensure equal opportunity for all applicants without regard to race color religion sex sexual orientation gender identity national origin disability Veteran status or other legally protected characteristics    </data></node>
<node id="n1008" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=876ce683bf41d1d6&amp;bb=AbDN_Sau9azpIrnPUsr1yfwQ5pFGx4M6hwYupkmIEwpxDPm2G3XnrH9mI1GKcZb2BLkOAAr-EXvAKWJueVaLQouR-VYO8PjGnzW-xHWSV-s%3D&amp;xkcb=SoAk67M3CNkeuKxylh0DbzkdCdPP&amp;fccid=c1099851e9794854&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in PythonYesNoEducationDo you have a Masters degreeYesNo LocationSeattle WA BenefitsPulled from the full job descriptionDental insuranceEmployee stock purchase planHealth insuranceRSURetirement plan Full job description Summary   Posted Mar 1 2024    Role Number200540711   Apple is where individual imaginations gather together committing to the values that lead to great work Every new product we build service we create or Apple Store experience we deliver is the result of us making each other’s ideas stronger That happens because every one of us shares a belief that we can make something wonderful and share it with the world changing lives for the better It’s the diversity of our people and their thinking that inspires the innovation that runs through everything we do When we bring everybody in we can do the best work of our lives Here you’ll do more than join something  you’ll add something Apple Vision Pro is a revolutionary spatial computer that seamlessly blends digital content with your physical space It will allow us to do the things we love in ways never before possible  all while staying connected to the people around us In this position you will join a team of computer vision and machine learning researchers and engineers to discover and build solutions to previouslyunsolved challenges and push the state of the art We are looking for a selfdriven and motivated computer visionmachine learning engineer experienced in applying machine learning to mobile computer vision algorithms As a member of a fastpaced prototyping team you have the unique and rewarding opportunity to shape upcoming products that will delight and inspire millions of people every day    Key Qualifications    Track record of successfully building and shipping products  Strong experience in mobile machine learning and pipelining  Strong experience in 3D geometry and computer vision  Experience in 3D Reconstruction is a plus  Experience in modern Novel View Synthesis methods suchas NeRF Gaussian Splatting etc is a plus  Excellent coding skills  Excellent communication and collaboration skills  Excellent problem solving and analytical thinking skills  Creativity and curiosity for solving highly complex problems  Experience in Python and C Experience with GPU programming or Metal is a plus      Description   You’ll be working in a team of computer vision and machine learning researchers and engineers to prototype and ship world class algorithms that pushes the state of the art Your job responsibilities will include Inventing and implementing state of the art computer vision algorithms to solve cutting edge problems Designing such algorithms to work reliably and efficiently on mobile devices Collaborating with other teams in software and hardware to ensure the full pipeline runs efficiently and utilizes Apple hardware effectively Cooperating with your teammembers to prepare presentations papers and talks to explain your inventions    Education  Experience   MS or PhD in computer vision machine learning computer science computer engineering or relevant industry experience with a track record of successful projects    Additional Requirements   Pay  Benefits      At Apple base pay is one part of our total compensation package and is determined within a range This provides the opportunity to progress as you grow and develop within a role The base pay range for this role is between 13150000 and 24330000 and your base pay will depend on your skills qualifications experience and location  Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs Apple employees are eligible for discretionary restricted stock unit awards and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan You’ll also receive benefits including Comprehensive medical and dental coverage retirement benefits a range of discounted products and free services and for formal education related to advancing your career at Apple reimbursement for certain educational expenses  including tuition Additionally this role might be eligible for discretionary bonuses or commission payments as well as relocation Learn more about Apple Benefits  Note Apple benefit compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program  Apple is an equal opportunity employer that is committed to inclusion and diversity We take affirmative action to ensure equal opportunity for all applicants without regard to race color religion sex sexual orientation gender identity national origin disability Veteran status or other legally protected characteristics    </data></node>
<node id="n1009" labels=":Skill"><data key="labels">:Skill</data><data key="name">identify patterns</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1010" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">uncover valuable insights</data></node>
<node id="n1011" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data science methodologies</data></node>
<node id="n1012" labels=":Skill"><data key="labels">:Skill</data><data key="name">time synthesize</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1013" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytics lifecycle</data></node>
<node id="n1014" labels=":Skill"><data key="labels">:Skill</data><data key="name">data grooming</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1015" labels=":Skill"><data key="labels">:Skill</data><data key="name">data exploration</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1016" labels=":Skill"><data key="labels">:Skill</data><data key="name">model prototyping</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1017" labels=":Skill"><data key="labels">:Skill</data><data key="name">model validation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1018" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">algorithm productionalization</data></node>
<node id="n1019" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical algorithms</data></node>
<node id="n1020" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">datadriven decisions</data></node>
<node id="n1021" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">forecasting models</data></node>
<node id="n1022" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">testing hypotheses</data></node>
<node id="n1023" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical modelling</data></node>
<node id="n1024" labels=":Skill"><data key="labels">:Skill</data><data key="name">econometrics</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1025" labels=":Skill"><data key="labels">:Skill</data><data key="name">clean the data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1026" labels=":Skill"><data key="labels">:Skill</data><data key="name">collect data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1027" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">generalized linear models</data></node>
<node id="n1028" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">time series analysis</data></node>
<node id="n1029" labels=":Skill"><data key="labels">:Skill</data><data key="name">random forests</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1030" labels=":Skill"><data key="labels">:Skill</data><data key="name">xgboost</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1031" labels=":Skill"><data key="labels">:Skill</data><data key="name">svm</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1032" labels=":Skill"><data key="labels">:Skill</data><data key="name">topic modeling</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1033" labels=":Skill"><data key="labels">:Skill</data><data key="name">generative ai</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1034" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">large language models</data></node>
<node id="n1035" labels=":Skill"><data key="labels">:Skill</data><data key="name">llm</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1036" labels=":Skill"><data key="labels">:Skill</data><data key="name">anomaly detection</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1037" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">performance tuning</data></node>
<node id="n1038" labels=":Skill"><data key="labels">:Skill</data><data key="name">data streams</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1039" labels=":Skill"><data key="labels">:Skill</data><data key="name">node</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1040" labels=":Skill"><data key="labels">:Skill</data><data key="name">mine</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1041" labels=":Skill"><data key="labels">:Skill</data><data key="name">model performance</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1042" labels=":Skill"><data key="labels">:Skill</data><data key="name">decision tree</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1043" labels=":Skill"><data key="labels">:Skill</data><data key="name">neural networks</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1044" labels=":Skill"><data key="labels">:Skill</data><data key="name">regression</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1045" labels=":Skill"><data key="labels">:Skill</data><data key="name">distributions</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1046" labels=":Skill"><data key="labels">:Skill</data><data key="name">statistical tests</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1047" labels=":Skill"><data key="labels">:Skill</data><data key="name">distributed data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1048" labels=":Skill"><data key="labels">:Skill</data><data key="name">decision trees</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1049" labels=":Skill"><data key="labels">:Skill</data><data key="name">presenting data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1050" labels=":Skill"><data key="labels">:Skill</data><data key="name">cloud</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1051" labels=":Skill"><data key="labels">:Skill</data><data key="name">run</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1052" labels=":Skill"><data key="labels">:Skill</data><data key="name">dbatlas</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1053" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">crossvalidating models</data></node>
<node id="n1054" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">recommendation system</data></node>
<node id="n1055" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">recommender systems</data></node>
<node id="n1056" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">distributed application</data></node>
<node id="n1057" labels=":Skill"><data key="labels">:Skill</data><data key="name">ensemble modeling</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1058" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical techniques</data></node>
<node id="n1059" labels=":Skill"><data key="labels">:Skill</data><data key="name">ml ops</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1060" labels=":Skill"><data key="labels">:Skill</data><data key="name">sklearn</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1061" labels=":Skill"><data key="labels">:Skill</data><data key="name">mllib</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1062" labels=":Skill"><data key="labels">:Skill</data><data key="name">mlflow</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1063" labels=":Skill"><data key="labels">:Skill</data><data key="name">cplex</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1064" labels=":Skill"><data key="labels">:Skill</data><data key="name">gurobi</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1065" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">mathematical modeling</data></node>
<node id="n1066" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data analytics modeling</data></node>
<node id="n1067" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical modeling</data></node>
<node id="n1068" labels=":Skill"><data key="labels">:Skill</data><data key="name">ggplot</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1069" labels=":Skill"><data key="labels">:Skill</data><data key="name">microsoft access</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1070" labels=":Skill"><data key="labels">:Skill</data><data key="name">apache beam</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1071" labels=":Skill"><data key="labels">:Skill</data><data key="name">descriptive</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1072" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">prescriptive models</data></node>
<node id="n1073" labels=":Skill"><data key="labels">:Skill</data><data key="name">trend analysis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1074" labels=":Skill"><data key="labels">:Skill</data><data key="name">data stack</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1075" labels=":Skill"><data key="labels">:Skill</data><data key="name">scikitlearn</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1076" labels=":Skill"><data key="labels">:Skill</data><data key="name">test automation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1077" labels=":Skill"><data key="labels">:Skill</data><data key="name">handson</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1078" labels=":Skill"><data key="labels">:Skill</data><data key="name">test set</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1079" labels=":Skill"><data key="labels">:Skill</data><data key="name">ide debugger</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1080" labels=":Skill"><data key="labels">:Skill</data><data key="name">data research</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1081" labels=":Skill"><data key="labels">:Skill</data><data key="name">data manipulation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1082" labels=":Skill"><data key="labels">:Skill</data><data key="name">risk assessment</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1083" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data visualizations</data></node>
<node id="n1084" labels=":Skill"><data key="labels">:Skill</data><data key="name">data trends</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1085" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical models</data></node>
<node id="n1086" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">identify trendspatterns</data></node>
<node id="n1087" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">hypothesis testing</data></node>
<node id="n1088" labels=":Skill"><data key="labels">:Skill</data><data key="name">caffe</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1089" labels=":Skill"><data key="labels">:Skill</data><data key="name">cntk</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1090" labels=":Skill"><data key="labels">:Skill</data><data key="name">stored procedures</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1091" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data model training</data></node>
<node id="n1092" labels=":Skill"><data key="labels">:Skill</data><data key="name">prediction</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1093" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytical dashboards</data></node>
<node id="n1094" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">model optimization</data></node>
<node id="n1095" labels=":Skill"><data key="labels">:Skill</data><data key="name">cnn</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1096" labels=":Skill"><data key="labels">:Skill</data><data key="name">scikitlearning</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1097" labels=":Skill"><data key="labels">:Skill</data><data key="name">cuda</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1098" labels=":Skill"><data key="labels">:Skill</data><data key="name">cudnn</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1099" labels=":Skill"><data key="labels">:Skill</data><data key="name">openvino</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1100" labels=":Skill"><data key="labels">:Skill</data><data key="name">onnx</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1101" labels=":Skill"><data key="labels">:Skill</data><data key="name">gstreamer</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1102" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">reinforcement learning</data></node>
<node id="n1103" labels=":Skill"><data key="labels">:Skill</data><data key="name">ortools</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1104" labels=":Skill"><data key="labels">:Skill</data><data key="name">ab testing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1105" labels=":Skill"><data key="labels">:Skill</data><data key="name">presto</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1106" labels=":Skill"><data key="labels">:Skill</data><data key="name">cicd</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1107" labels=":Skill"><data key="labels">:Skill</data><data key="name">scikit</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1108" labels=":Skill"><data key="labels">:Skill</data><data key="name">generative models</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1109" labels=":Skill"><data key="labels">:Skill</data><data key="name">language modeling</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1110" labels=":Skill"><data key="labels">:Skill</data><data key="name">3d geometry</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1111" labels=":Skill"><data key="labels">:Skill</data><data key="name">gpu programming</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1112" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-2d0c01d4f49448da8b2b9cb8af8481f4</data><data key="resume">Jessica    Claire                        San Francisco     CA      609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Accomplishments       Diversity Committee at University of Illinois at UrbanaChampaign  PROJECT LEAD Data Visualization Final Course Project CONSULTANT Business Intelligence Group School of Information Sciences INFORMATION TECHNOLOGY LEAD Association for Computing Machinery INSTRUCTOR Jeevan Jyot NGO Mumbai India Projects Yelp Data Analysis University of Illinois at UrbanaChampaign          Jan  2018  Apr2018 Built a simple text classifier using Pythons Pandas NLTK and Scikitlearn libraries  Created a sentiment analysis model that predicts whether a user liked a local business or not based on their review on Yelp  Performed analysis on businesses as well as users data and outlined the analysis using interactive visualizations in python  Evaluating Thought Leadership in Insurance University of Illinois at UrbanaChampaign   Oct  2017  Dec  2017 Helped the client in Accessing Thought Leadership Reports of their Competitors in the Insurance industry  Performed regressive analysis of the reports generated visualizations to depict complex data and processes  Data Visualization Final Course Project UIUC          Sep  2017  Dec 2017 Designed dashboards to display visualizations with interactive components of IPywidgets  Performed data aggregation and audio integration using Python  Distributed Document Clustering Using a Hybrid Approach University of Mumbai Sept  2016  Apr  2017 Developed a Hybrid algorithm comprising of KMeans Particle Swarm Optimization PSO Latent Semantic IndexingLSI Algorithms for distributed clustering of documents  Used Hadoop MapReduce Framework for clustering 20000 documents 20NewsGroups and 21578 documents Reuters21578 on single and multiple nodesmachines  Android Joystick Shri Bhagubhai Mafatlal Polytechnic          Sept  2013  Apr  2014 Developed an Android Application that turned smart phones into Computer Remote Controllers to allow users to wirelessly operate a remote desktop via Bluetooth connectivity  Provided different features for customizing the controller for user flexibility and comfort         Professional Summary      Experienced Data Analyst committed to maintaining cutting edge technical skills and uptodate industry knowledge        Skills           Python and R  proficient  Tableau Power BI  SQL MySQL Hadoop  Microsoft Excel proficient  Excellent communication skills      Photoshop SharePoint Adobe Creative  Suite  HTML5 CSS3 JavaScript  Java Android  Excellent problemsolving abilities                       Work History       Data Analyst       012018      Present         –    Long Island City                 Implementing data preprocessing using python to clean a dataset containing over million entries and generating valuable insights from the clean dataset through visualizations  Using Natural Language Processing toolkit NLTLK package to perform topic segmentation and analyzing the trends in equipment features over years at John Deere  Using Google Analytics to analyze data of the mobile applications and make data driven decisions to improve customer support and experience           Technology Consultant       012018      Present     Jones Lange Lasalle Inc    –                     Providing strategic consulting for a leading company in the electrical data networking industry to implement a governance framework on their intranet portal  Developing a business model for the client to improvise their intranet design optimizing item placement usability and searchability           Data Analyst Intern       112017      012018     Hewlett Packard Enterprise    –                     Worked with AACSB process manager for data extraction and cleaning using R and analyzed the data using descriptive visualizations in Tableau  Compiled information on faculty activities collected data for surveys performed data quality control activities using Microsoft Excel  Transformed the data using data wrangling in a format specified by the management           Application Developer and Content Management Intern       122015      012016     Do It Best Corp    –              India       Interacted with senior professionals to develop and design business processes to enhance the functionality of the Android application  Implemented the business processes on the backend using JSON  Used Tableau for data collected from multiple sites to generate insights for the senior professionals to carry out decision making          Certifications     Data Visualization Applied Business Research Data Statistics  Information Big Data Analytics Cloud computing Data Mining  Business Intelligence Data Structure  Algorithms Advanced Database Management Systems Software Project Management Programming for Analytics and Data Processing Competitive Intelligence  Knowledge Management        Education       MS       Information Management       Expected in   Dec 2018                University of Illinois      Urbana Champaign     IL     GPA        Status         Information Management GPA 350         BE       Information Technology       Expected in   May 2017                University of Mumbai      Mumbai          GPA        Status         Information Technology GPA 395         Diploma       Information Technology       Expected in   May 2014                Shri Bhagubhai Mafatlal Polytechnic                GPA        Status         Information Technology GPA 375        Publications     Published a technical paper on the project Android Joystick IJARCCE Journal Vol 5Issue11 Nov 2016       Skills     Adobe Illustrator Photoshop Big Data BI Business Intelligence business processes Business Research Competitive Intelligence consulting CSS3 Client customer support Data Processing Data Mining Data Modelling Data Visualization Database Management decision making features Google Analytics HTML5 Java JavaScript JSON Knowledge Management lEADERSHIP Microsoft Excel Microsoft Office SharePoint MySQL Natural Language Processing networking Programming Project Management Python quality control SQL Statistics strategic surveys Tableau</data><data key="id">230388201717447766043406647718212528575</data></node>
<node id="n1113" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-1edf3bdef6744bc69520a01d3285bbf8</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Summary     Highly talented and accomplished professional with extensive skills and experience in complex work environments Coordination planning and support of daily operational and administrative functions Detailoriented and well organized when completing projects able to multitask effectively Experienced working in fast paced environments demanding strong organizational technical and interpersonal skills       Skills         Microsoft Word Excel Publisher PowerPoint Outlook Lawson Time Matters and Internet                       Experience      072012   to   Present     Data Analyst      Lewis Pr    –    New York     NY             Collect and compile data for statefederal requirements which includes audit documentation regarding professional learning activities  Assist the Title II Department with the allocation management and tracking of Title II  Part A program funds  Maintains and develops electronic files records for easy access of reports  Prepare spend down reports for various departments that use Title II funds  Enter requisitions for activities allowable under Title IIA  Provides ongoing technical support to publicprivate school and other district personnel including conducting meetings regarding compliance  Compiles and summarizes program data for all required Federal and State reports  Assist with the preparation of documents for meetings involving stake holders  Assist in preparation of presentations and meetings  Assist with procurement process of compliance with documentation  Conduct surveys with personnel and stakeholders to assess the needs of the district and compile data for reports  Works closely with various departments to track expenditures and gather information necessary as requested by Title II Department          062012   to   112012     Data Analyst      Lewis Pr    –    Chicago     IL             Compiled and evaluated student data for the purpose of assessing program effectiveness student growth and provider quality including completion of federal and state reports  Assisted with the development of district policies and procedures for effectively implementing monitoring and evaluating the Title I Supplemental Educational Service program in accordance with federal law  Evaluated and processed vendor invoices against student attendance reports and performance data to ensure reliability and validity  Assisted with preparation processing and review of contractual agreements for all vendors as well as enter contract information in Lawson system  Developed and created marketing programs to effectively communicate with parents district personnel and the community regarding the SES program services  Facilitated training and workshops for schooldistrict personnel parents vendors and community members regarding Title I program guidelines  Monitored school sites where tutorials take place to ensure that providers are in compliance with guidelines set forth          092009   to   112012     Administrative Assistant      Primrose School    –    Oldsmar     FL             Provided administrative support for the Supplemental Educational Services Program including acting as a liaison between the tutorial providers school personnel and parents adhering to and interpreting state and federal program guidelines and relaying that information to the public and meeting deadlines for reports  Prepared spreadsheets and data reports summarizing enrollment data by schoolprovider  Monitored school sites where tutorials take place to ensure that providers are in compliance with guidelines set forth  Prepared correspondence create email distribution lists maintain calendars organize mass mailings and file management  Maintained and track database of over 2400 students  Verified freereduced lunch status of students requesting to participate in the program  Assisted with completion of all state reportssurveys regarding SES  Provided executivelevel administrative support to the Program Director of Title I Office with a demonstrated ability to improvise improve procedures and meet demanding deadlines  Collaborated and met with other departments to complete special projects including summer school  Conducted surveys regarding summer school and compile data for reports          042002   to   122008     Legal Assistant      State Of Ohio    –    Wayne County     OH             Provided administrative support to Managing Partner and Associates in general practice law firm with a demonstrated ability to improvise improve procedures and meet demanding deadlines  Acted as a liaison between clients and attorneys arranged meetings maintained calendars drafted correspondence maintained and organized files sent documents by facsimile photocopied proofread prepared memos sent documents by overnight mail scanned documents and downloaded documents  Managed client files opening and closing files maintaining client databases and attorney billing  Prepared and interpreted legal documents such as but not limited to Contracts of Sale Promissory Notes and Powers of Attorney  Assisted accounting department in maintaining bank accounts including managing subaccounts for clients writing checks and communicating with the bank regarding deposits and wire transfers  Supervised and trained parttime and summer employees including delegating responsibilities overseeing and reviewing tasks completed and collecting time sheets  Provided support to the office manager in handling payroll and accounts payablereceivable responsibilities  Assisted with firm marketing including preparation of packets for presentations building and fostering working relationships with affiliated companies to increase clientele          Education and Training      Expected in   May     Juris Doctor          ATLANTAS JOHN MARSHALL LAW SCHOOL                GPA               Expected in   May 2001     BS     Business Administration     CHEYNEY UNIVERSITY OF PENNSYLVANIA                GPA       magna cum laude Business Administration        Interests     Notary Public Commissioned in the State of Georgia       Skills     accounting accounts payable administrative support Attorney billing closing Computer experience Contracts clientele client clients databases database documentation email facsimile file management forth funds IIA Lawson law legal documents Notes Director Managing marketing meetings access Excel mail Office Outlook PowerPoint Publisher Microsoft Word Works office manager payroll personnel policies presentations procurement quality spreadsheets surveys technical support workshops       Additional Information       LICENSES Notary Public Commissioned in the State of Georgia</data><data key="id">317372527761722178757702714562875550369</data></node>
<node id="n1114" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional Summary     5  years of experience on Database development and administration knowledge of relational database on SQL Server particularly on SSMS SSRS SSIS TransactSQL and SQL Server Agent Extensive exposure to creation of Database Objects Stored Procedures Triggers Views User Defined Functions and Cursors           Core Qualifications         SQL  Server Tools          SQL Server Management Studio SSMS  Data Warehouse Tools          MS SQL Server 200520082012 Integration Services  Business Intelligence Tools        SQL Server 200520082012 Business Intelligence Development Studio  Programming Languages	     TSQL  Reporting Tools          SQL Server 200520082012 Reporting Services  Development Environment        Visual Studio 200520082012  Operating Systems	         Windows XP20032008 Server Win 3x9598 Vista Win 7  Advanced Excel          Vlookup Subtotal Pivot table and Chart                       Education       Admas University               Expected in        –      –       BSc Degree        Information Science          GPA           Information Science          Experience       Liberty Healthcare Corporation      Data Analyst        OK                   112014      Current     Experience in Constraints rules and default setting Primary Foreign Unique and Default Key  Developed Joins and SubQueries to simplify complex queries involving multiple tables  Experienced in using temporary tables table variables common table expression CTE to enhance optimized SQL queries form improved performance of queries  Experience in crating and updating Clustered and NonClustered Indexes to keep up the SQL Server Performance  Well versed in Normalization DeNormalization techniques both in OLTP and OLAP system  Experienced in using Try catch block introduced in SQL Server 2005 and error handling  Great deal of experience on authoring managing and deploying ad hoc enterprise advance and interactive reports using SQL Server Reporting Servers  Expert in Data Extraction Transforming and Loading ETL using various tools such as SSIS DTS Bulk Insert data cleansing and profiling  Well experienced using different transformations tools like Aggregate Cache Transformation Conditional split Copy columns Sort column Data conversion Derived column Merge Merge join Union all Import and Export columns and OLE DB command  Data Extraction form OLE DB server encrypt and compressed with the TF PGP and TF compression task to the remote server location and FTP Server on the CSV format  Implementation of point in time backup and recovery of databases executing package scheduling and managing jobs on SQL Server Agent and maintain good documentation  Managed and maintained users security and permissions and migration database objects form one server to another server database to another database  Exposure to Business Analysis and requirements gathering in Business and Health Care Domains mainly on HIPAA  Knowledge of complete Software Development Life Cycle and work experience in Agile and environment  Strong research analytical coordination interaction skills team player and able to quickly grasp new technologies and products           EBS      Data Analyst SQL Server DeveloperSSISSSRS                           092012      102014     EBSEthiopian Broad Casting Service aims to promote Ethiopian and African countries values Cultures and traditions on a global scale  The muchneeded information provided by EBS would  Help bridge the cultural divide and narrow the communication gap for Ethiopians residing in  North America and around and world  Involved in gathering business requirements  Installation and configuration of SQL Server  Created database tables wrote stored procedures for developers and users  Created SQL scripts and defined functions check constraints indexes and views  Created triggers to enforce data and referential integrity  Create new SSIS package 2008R2 to extract date from legacy to SQL Server objects  Extensively used SSMS and SSIS ImportExport system for performing ETL operations  Performed data conversion from fat file and excel in to a normalized database structure  Configured Server for sending automatic mails  Developed monitored and deployed SSIS packages 2008R2  Installation and configuration of reporting server  Generated Snapshot Drill Down and parameterized reports using SSRS Prepared Adhoc reports through report builders and published through Report Manager Design and created different types of reports like Sub Reports DrillThrough Cascading Drill Down adhoc Reports in visual Studio and deploy and manage on Microsoft SQL Server Reporting Services Configuration Manager  Ethiopian Magical Farm PLC Jessica Ababa Ethiopia                 SQL ETL DeveloperSSISSSRS                           022010      072012     Responsibility Designed and developed the databases  Created store procedures views and tables and generated TSQL script for application  Business Intelligence Development Studio BIDS to create edit and deploy SSRS and SSIS  Designed deployed and maintained of various SSRS Reports in SQL Server 2008R2  Created ETL packages using SSIS to extract data from relational database and then transform and load in to the database  Developed deployed and monitored SSIS packages including upgrades DTS to SSIS  Developed executed documented and maintained appropriate BI procedures  Collaborated with network operators application developers and DBAs to enhance end user experience  Scheduling Jobs and Alerting using SQL Server Agent          Professional Affiliations              Skills     Ad Agile aims automate backup Business Analysis BI Business Intelligence business management Hardware data collection Data conversion Data migration data modeling DTS data warehouse databases database delivery documentation edit ETL fat FTP logic managing Access MS Excel excel Excel          V Microsoft SQL Win 7 Win 3x 9598 Windows XP migration enterprise network OLAP OLE Operating Systems DB Pivot table PLC Programming quality relational database reporting requirement requirements gathering research Scheduling Servers scripts script Software Development sorting MS SQL Server Microsoft SQL Server SQL SQL  Server SQL Server tables team player TSQL TSQL translating Unique upgrades Vista Visual Studio</data><data key="id">17155897641748297909372247622701130843</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-1d5c0e96b48d4a65b163501a6a6adfb1</data></node>
<node id="n1115" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-444737d288e04f429913c622d738c6d4</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Career Overview      Over  5 years  of experience  in a technical support role  electro mechanical maintenance IT and customer service Exceptional in troubleshooting installation configuration and desktop hardware support OS support for ATMs Proficient in the installation of customer proprietary software on different PCs Performed preventive maintenance on equipment when performing site visits Provided technical support and training for other technicians within the company across the country as well as customers Knowledge of LANWAN and printer network setups Knowledge of Active directory user setup Strong attention to detail Very well organized and able to escalate to the proper channels on issues that need more resources         Qualifications          MS Win 2000 XP 7 and 81  Other Applications MS Office Suite 20032007 and 2010 Excel Word Power Point Outlook Desktop Support TCPIP DNS Configuration Network setup Pinging LANWAN Configuration PC hardware and operating systems maintenance and troubleshooting Network Security Able to provide tech support over the phone Familiar with servicing printers Strong analytical skills  Proficiency in TCPIP protocols  Excellent problem solving skills                         Work Experience        092011   to   Current   Data Analyst    CH Robinson Worldwide Inc         Omaha     NE            LEAN trained on process improvement for developing new GEN paperwork process  Currently analyze data for changes adds and deletes onto the transaction switch  Verify to make sure all data elements related to their account is correct prior to the system change  Also provide technical support to ATM clients and vendors  Help customers trouble shoot minor issues with their ATMs to minimize down time  Also provide hardware and software support to customer vendor to help resolve technical issues  Managed network platform conversion for ATM clients from one processor to another migrating 1000 terminals in 6 months  Also provided support for ADA compliance standards for all ATM clients            082008   to   012011   Field Service Manager    Dish Network Corporation         Spokane     WA            Managed 37 technicians between Texas and Oklahoma  Assisted with 7 ATM software and hardware upgrades for financial institutions  Provided customer support for ADA compliance standards  Also performed upgrades from software to hardware to make ATMs ADA compliant  Provided tech support for both customers and technicians across the country  Performed work load balance between other technicians with my service area  Prioritized service calls to meet customer SLAs Hired and trained new staff when necessary  Traveled to other areas for tech and customer visits or to provide support on ATMs that have been down for long periods of time  Met with area tech leads every other week via conference calls to discuss any issues or changes coming to any of the areas            032008   to   072008   Customer Service Technician    Ametek Inc         Warrendale     PA            Performed software and hardware maintenance on Wincor ATMs  Used diagnostic software on PCs to troubleshoot hardware issues on ATMs to help pin point where errors could be found  Performed PMs on ATMs when time permitted to prevent calls backs            032005   to   032008   Sr Field Customer Service Engineer    Hyperoptic         Field     KY            Provided leadership to technicians in the Houston area and other areas that didnt have any leadership  Helped hire and train new technicians across the country  Help support 6 hardware and software upgrades for financial institutions  Provided tech support for both customers and technicians across the country  Prioritized service calls to meet customer SLAs  Traveled to other areas for tech and customer visits or to provide support for on ATMs that have been down for a long period of time  Performed PMs on ATMs and their PCs if time permitted to prevent call backs            022004   to   032005   Customer Service Engineer    Tecniflex         City     STATE            Serviced bank pneumatic tube systems for financial institutions  Serviced safe electronic and mechanical locks  Opened safety deposit boxes for financial institutions when requested  Performed PMs on safe doors every 6 months  Serviced ATM machines and PCs          Education and Training        Expected in   1996   Associates       Applied Science Electronics Engineering    Itt Tech     Houston     Tx      GPA        Applied Science Electronics Engineering           Expected in   2017   Bachelor of Science       Information Systems    University of Houston Downtown     Houston     TX      GPA               Languages      Bilingual in Spanish        Skills      ADA ATM balance hardware upgrades hardware conversion clients customer support DNS doors financial LAN leadership mechanical Excel MS Office Suite Outlook Power Point Win 2000 Word Network setup Network Security network Operating Systems PCs PC hardware printers process improvement safety SLA Spanish switch TCPIP technical support tech support software support Desktop Support phone troubleshoot troubleshooting upgrades WAN</data><data key="id">215948562580113635362500948079374459167</data></node>
<node id="n1116" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-132fa8c6bfc44dd89773bd25f8872de8</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Professional Summary     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market       Core Qualifications         Customer Service Focused        SAP Experience Organized          Team leadership Typing 60 WPM          Process implementation Data management          Microsoft Office Suite Staff development          Microsoft Outlook                       Experience      102012   to   Current     Data Analyst          –    Nashua     NH             Complete weekly financial reporting for multiple agents and upper management Calculate and disburse large dollar weekly funding to agent companies Load and maintain agent and customer contract information into our systems Internal and external customer service Field inbound calls from customers Process outgoing and incoming mailings Process customers credit card ACH and check payments Respond to and track all credit chargebacks for our department within a tracking system I designed with IT team Respond promptly and accurately to email ticketing requests Created Spartas first employee manuals including training and development Trained new employees and outside users on our PBS system Assist with meeting schedules and travel arrangements for the CIO Oversaw daily office operations for Sparta Performed accounts receivable duties including invoicing researching chargebacks discrepancies and reconciliations Review multiple reports to determine the status of collections and agent reserve balances Effectively communicate with sales marketing management and administrative teams on a daily basis Ensure superior customer experience by addressing customer concerns demonstrating empathy and resolving problems on the spot Direct calls to appropriate individuals and departments          122009   to   102012     Inventory Planner      Lockheed Martin Corporation    –                      Bel Ridge M O Proactively managed multiple supplier lines for over two hundred locations within the SAP system Monitored inventory and placed orders to ensure proper stocking levels were maintained Managed a major line conversion from Metallics to LH  Dottie Worked with branch district and corporate management on key inventory aspects Achieved and maintained exceptional performance numbers Internal and external customer service Completed new stock recommendation workflows Worked closely with suppliers to obtain special deals Developed and improved vendor relationships Reviewed forecasts and various reports to maintain proper inventory levels Eliminated excess and unwanted merchandise by working with suppliers to secure returns of DNO materials Recruited to Corporate Purchasings Training Group in charge of training new employees and revamping training manuals Accepted into the Corporate Purchasing Leadership program LEAP designed to foster and develop future company leaders Member of the Positive Outlook Program POP responsible for organizing employee morale building events Assisted with planning and scheduling internal employee blood drives with the American Red Cross          122007   to   122009     Purchasing Assistant      Assurant    –              Bel        Expedited purchase orders Tracked inventory shipments Internal and external customer service Completed new stock recommendation workflows Prepared spreadsheets detailing item information Communicated with vendors regarding inventory needs Entrusted to mentor new hire Purchasing Assistants Completed reporting for Inventory Planners Receptionist duties Compiled and maintained up to date company information and contacts          052006   to   122007     Office Assistant      Pacific Office Automation    –    La Jolla     CA             Receptionist duties Prepared statements for billing company Completed insurance precertifications Scheduled patient appointments Promptly answered all incoming phone calls Followed up on patient billing questions Acquired and maintained patient charts Enforced high level of patient and office confidentiality Managed several physicians daily appointments and workflow Developed training office and procedural documents Assisted in completing employee evaluations Created and managed employees weekly work schedules Managed inventory and office supplies Facilitated record retrieval and access by maintaining organized chart filing system Composed and drafted all outgoing correspondence and reports for physicians Oversaw daily office operations          Education      Expected in        Technical Diploma                          GPA               Expected in   March 2006     Medical Billing and Coding          SanfordBrown College      Collinsville     IL     GPA               Professional Affiliations              Skills     accounts receivable administrative billing charts conversion credit addressing customer concerns Customer Service Data management email filing financial reporting insurance Inventory inventory levels invoicing Leadership Team leadership marketing materials Medical Billing mentor access Microsoft Office Suite office Microsoft Outlook Outlook organizing Coding Purchasing Receptionist reporting researching sales SAP scheduling spreadsheets Staff development phone training manuals travel arrangements Typing 60 WPM workflow</data><data key="id">320134674989111212261471899670864145920</data></node>
<node id="n1117" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-9686e648a5d246e4a6f2aab74e38cdf5</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     tyee       Highlights         Microsoft Excel Word Outlook Power Point FileZilla QuickCap                       Accomplishments              Experience       Data Analyst       092015      062016     Altice Usa    –    Tyler     TX            Loading daily weekly or monthly enrollment files into QuickCap and performing a reconciliation on members added  subtracted vs  the previous file  Performing a monthly reconciliation on enrollment files vs  capitation payments  Performing a monthly reconciliation of per member per month utilization and costs by client  Creating canned and custom reports via Data Reporting Services with the assistance of the Director of Data Analytics for the purpose of analyzing all aspects of business performance           Security Analyst       092014      011     Cisco Systems Inc    –    MinneapolisSt Paul     MN            Allocated billions in trades by end of the day Monitor an average of 200 trade breaks daily with a 6070 settlement rate at end of day Traded confirmation and allocation to client sub accounts Ensure accuracy and timeliness of real time customer trade confirmations to clients Allocated monitored and settled across all global markets for equity and fixed income trades Manages exceptions relating to suballocated trades Created and analyzed report dashboards and metrics for various daily weekly and monthly reporting Facilitation of the account opening and documentation process with Operations           Site Supervisor       082010      012014     Bay City School District    –    Bay City     MI            Tutored Junior and High School students grades improved 20 following tutoring  Planned and scheduled monthly activities for afterschool programs and boxing gym           Customer Service Coordinator       052007      122010     Swift Refrigerated    –    Chicago     IL            Scored 100 on productivity and awarded best worker for the month  Assisted in car reservations and developed great customer service  Managed international and domestic car reservation portfolios          Education       Bachelor of Science       Economics French       Expected in   May 2014                Westminster College      Salt Lake City     UT     GPA        Status         Economics French Mens Soccer team  devoted 20 hours and gained valuable leadership and teambuilding experience  Fourth Year Senior Senator as a member of the Education SubCommittee worked with undeclared students to help determine their major         Associates of Science              Expected in   Jun 2012                Salt Lake City Community College      Taylorsville     UT     GPA        Status         Public Relations Officer assisted with college activities and marketing        Interests     Soccer Rugby Reading Travel  Fashion       Skills     client clients customer service documentation equity fixed income leadership teambuilding Director marketing Microsoft Excel Outlook Power Point Word Public Relations real time reporting tutoring       Additional Information       InterestsHobbies Soccer Rugby Reading Travel  Fashion</data><data key="id">198606948153707398538730183405779849620</data></node>
<node id="n1118" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary      Flexible and versatile Data Analyst specializing in developing innovative solutions to organizational problems Advanced knowledge of Business Intelligence tools experienced in confirming the accuracy of data in various systems and developing manage and understanding complex spreadsheets Strong organizational technical and analytical skills        Highlights           Microsoft Excel certified Advanced Excel modeling  Data Analyst Business Intelligence Spreadsheet management  Ability to quickly comprehend complex spreadsheetsdata  QlikView and Qlik Sense      Visual Basic for Applications Bilingual in Spanish and English  Analytical thinking  Responsibility  Leadership  Fast learning of new concepts  Excellent communication skills  Resultsoriented                       Accomplishments         Led successfully a 3person team of India analysts in transferring a report process Hewlett Packard   Reduced processing time by 70 in reports implementing macros  Promoted to Lead Analyst after just 14 months of employment ITESO         Experience       Data Analyst       082012   to   Current     Atlas Executive Consulting    –    San Diego     CA            Responsabilities Manipulation of data running pivot tables Vlookups and formulas in Excel creating reports that managers consume  Analyzing business data to develop Key Performance Indicators  and building dashboards to visualize important trends and goals for decision making  Provide strategic recommendations to managers  attend administrative assistants requests providing them information of their   Tools Excel QlikviewVBA   Achievements    Automation of 4 reports which reduced the processing time  response time and risk of error  Develop Qlickview dashboards  Several recognition by my supervisor           PSG MX Supply Chain – Planning Support of Costumer Laptops       112010   to   082012     Hewlett Packard    –    City     STATE            ResponsibilitiesTracked vendors orders by several reports Backlog Shipped Deliveries Inventory and in Transit to ensure the ontime delivery and the customers satisfaction Placed Purchase Orders in a certain instance of SAP based on Planner demands     Tools  SAP Constant Velocity use VA02 VA03 ME21N Excel     Achievements    Identified bottlenecks and implemented new and improved processes and policies  Implement HP GBS India transferring weekly supply planning activities and supervising the process November 2010 – March 2011            Information Management – Developer  Support       092010   to   112010     Hewlett Packard    –    City     STATE             Responsibilities  Developed metrics used to determine inefficiencies and areas for improvement Provide Support to users via SharePoint with any problem within a specific tool to ensure quality data    Tools  Microsoft Excel SharePoint         Education       Bachelor of Information Technology     Information Technology      Expected in   2010     ITESO      Guadalajara     Jalisco     GPA               Languages      Bilingual SpanishEnglish        Certifications      •Visual Basic for Applications  •Qlikview Developer and Desinger        Skills       •Microsoft Excel and Access Reporting Formulas macros  5 years  Advanced •Visual Basic for Applications 1 year Intermediate•Qlikview Developer and Desinger 1 year Intermediate</data><data key="id">95026819123011019074713040927512276840</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-680578b15dab4c5384ca3017393355df</data></node>
<node id="n1119" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-67ff27c81a33470c95c1fe7b99e76e6d</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary     Incredibly motivated professional with a variety of academic and civilian administrative experience Possesses analytical research and problem solving skills Detail oriented organized and able to manage projects to meet deadline requirements Proficient written and oral communication skills with extended experience utilizing various forms of correspondence Precise judgment and decisionmaking skills to ensure the client’s needs are being met Effective while working within a group or independently       Skills           Problem Solving  Communication  Critical Thinking      Analytical   Attention to Detail  MultiTasking                       Experience        042014   to   Current   Data Analyst    Atlas Executive Consulting         Washington     DC            Utilized SQL Developer to streamline processes and reports obtained from the Enterprise Data Warehousing EDW and Enterprise Resource Planning ERP information systems  Responsible for maintaining and updating multiple tables with in the EDW application  Constructed and modified SQL queries to support data reporting initiatives  Reviewed and validated financial statements including but not limited to balance sheets and statements of revenue to identify and correct data discrepancies that impact financial reports  Presented project and reporting results to senior management and clients to assess reporting issues and provide solutions  Managed the CORPROD inbox to receive feedback concerning financial reports and queries  Worked closely with Subject Matter Experts SMEs and Database Administrators DBA to provide status updates and technical guidance to run financial and inventory reports  Utilized project management skills to ensure weekly reports and on going project objectives are met            062011   to   042014   Financial Management Analyst    Department Of Justice         Batavia     NY            Budgeted allocated and monitored the use of financial resources that support the supply chain and operational strategic plans  Analyzed business requirements to prepare and coordinate various budget proposals and exhibits with clients and Headquarters for submission  Provided guidance to management concerning financial management policies and procedures  Allocated and reallocated funds to the appropriate fund centers to ensure sufficient monies are available when and where needed at the execution level  Analyzed key business indicators including sales obligations net operating revenue and cash and financial statements to identify variances  Collaborated with multiple departments to articulate budget variances financial performance and corrective actions to the supply chain and operational management in order to improve business performance  Received validated accepted funding documents and identified the need to create reimbursable internal orders to track costs incurred in ERP systems  Utilized Navy Enterprise Resource Planning system to allocate execute and control funds identify weaknesses within financial areas  Reviewed program manager budget requests to evaluate estimates and requirements  Supervised two Financial Technicians on completing funding documents and other financial matters            032009   to   062011   Financial Management Intern    American Advanced Management Inc         Coalinga     CA            Developed financial analysis skills including but not limited to balance sheet reconciliation components of financial contracts accounting principles for government entities and the financial audit process  Assisted with generating and analyzing monthly financial reports and physical inventory  Participated in the yearly forecasting initiatives  Developed and utilized spreadsheets databases and other applications to complete assignments            022001   to   082004   Personnel Clerk    United States Marine Corp         City     STATE            Performed personnel and general administrative duties utilizing           manual and automated information systemsPrepared documents           maintained personnel records retrieved pay and personnel           information  Developed and maintained working knowledge of the Marine Corps           Total Force System MCTFS which encompasses the online Diary           SystemOLDSthe Unit DiaryMarine Integrated Personnel System           UDMIPS  Created entries for individual service records audited service           records for required entries and documentation completed various           personnel and pay related forms and documents  Researched proper unit diary entry requirements entered           transactions into MCTFS via the unit diary audited and corrected           feedback reports from the MCTFS system and prepared individual           pond and allotment request          Education and Training        Expected in   2016   Master of Science       Business Administration    University of Mary Washington     Fredericksburg     VA      GPA       335400          Expected in   2008   Bachelor of Arts       Business Management    Charleston Southern University     Charleston     SC      GPA       300400</data><data key="id">136447749940820370967945296361976212262</data></node>
<node id="n1120" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-6d3564777b044580b0b6cdbe640c5f67</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Professional Summary     To obtain a position as a Financial Analyst Ambitious financial professional with superior analytical and problem solving skills       Skills           Strong organization  Excellent attention to detail  Flexible team player  Data trending      Critical thinking  Quality assurance  US GAAP principles  Strong problemsolving capabilities                       Work History        102012   to   Current   Data Analyst    Ascension Health         Mount Juliet                 Vendor Risk Management Reviewed banking rules and regulations to ensure thirdparty vendors are within compliance of the client  Used Microsoft Excel to organize vendor information using pivot tables and Vlookups Communicated with team members regarding questions or documentation concerning Vendor Risk Management  Analyzed risk assessment documents of vendors to ensure compliance of risk management auditing and testing consumer complaints and regulations  Reviewed team members files to ensure high quality            102010   to   102012   Document Processor Associate    Customers Bank         Phoenixville     PA            Processed documents and checks for large corporate banks Operated folder and postage machines Mailed checks via US  Postal service and FedEx Used Microsoft Excel to create spreadsheets to organize loans Used internet skills to research location of checks and documents Met compliance dates Started as a temporary contractor in April 2011 rolled permanent            102012   to   Current   AML ANALYST    PriceWaterhouseCoopers                          Researched businesses and individuals using Google and Accuity  that were suspected of illegal money laundering  Used Excel and Macro skills to organize  clients files and perform calculations of multiple transactions that may have occurred  Used my business writing skills to create written reports Analyzed businesses or individuals profiles to determine if they were  participating in illegal transactions of money laundering  Guidewire Validated test cases per analytic review of testing procedure lines Retrieved and interpreted User Stories from Business Analysts  Composed test cases that tested the buttonfield functionality navigation flow and page processingvalidation for Guidewire screens as directed by User Stories  Parameterized variables and entered data values into the Test Data Harness database for corresponding screens  Developed reusable scripts by contributing to the Page Reference Library and Login Navigate scripts  Peer reviewed test cases and adjusted design steps accordingly  Escalated questions and technical difficulties to managers  Provided daily status updates to manager          Education        Expected in   5 2009   Bachelor of Science       Mathematics Finance    University of South Carolina     SC           GPA        Mathematics Finance Emphasis in Finance and Risk Management Insurance  Advanced courses in Finance Management            Expected in      Microsoft Master Certification          January 2014                           GPA               Accomplishments       Cooperative Ministries Taught financial literacy to a group adults in the financial assistance program January 2014  Midlands Boys and Girls Club Presented the Earn Your Future program to children between the ages  of 10  14 years teaching them the importance of saving and budgeting May 2013         Certifications     Microsoft Masters Certifications       Skills     auditing banking business writing client database documentation Internet Savvy internet skills Java Script Lotus Notes Access Microsoft Excel Excel money Microsoft Office Outlook PowerPoint Word navigation Page postage machines quality research risk assessment Risk Management SAS scripts spreadsheets SQL Stories tables validation written</data><data key="id">330827980237578865000834288081252328482</data></node>
<node id="n1121" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-a58fdb3e60634812ad5ff07707b17f8e</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional Summary     Obtain a role that will allow me to be innovative and utilize strategic thinking skills to manage and execute superior project strategy within an organization Over twelve years work experience in information technology and managing projects Acknowledged for creativity and ability to find innovative ways to deliver business solutions Proven team player with the ability to engage and inspire others Excellent written and verbal communication skills           Education       University of Pittsburgh               Expected in   1 2001     –      –       Bachelor of Science        Information Science          GPA           Information Science          Skills      Programming Languages  SAS SQL C Cworking knowledge HTML  Operating Systems  UNIX Windows 2000XPVista78  Software  HyperionBrio Enterprise Guide Web Report Studio Information Map Studio Cactus IntellisoftIntelliCredIntelliContract Crystal Reports AppWorx MicroStrategy MS Office 2013 Suite Viso SAS BIOLAP MAXIMUS GeoAccessNetworks Quest Analytics SPSS working knowledge       Work History       Alakaina Family Of Companies      Data Analyst                           2009      Current     Created and maintained SAS programs to pull data to assist with audits for government agencies as well as private auditors Deloitte to ensure that the company has adequate provider coverage across our network  Performed ETL functionality to measure compare and query data by way of SAS and SQL programming in order to display charts and graphs for senior management to assist with business decisions  Reviewed and analyzed report results against established baselines to determine trends and outliers  Lead programmer for Hospitalprovider Termination Process for our Contracting division to remove Hospitals or groups of individual providers from our network for various reasons  Generated Plug and Play reports to assist the managers of the Contracting division with rate negotiations with vendors on the fly to more effectively negotiate decisions to award payment increases  Identified the need for and developed automated processes reports as well as streamlined various existing processes to relinquish resources for repetitive jobs  Provided and Analyzed data to assist Contract Analysts with their reporting prior to negotiating fees for service and rates in order to come to a happy medium for all parties involved  Developed Credentialing and Recredentialing reports using the Cactus application for the Credentialing business unit to send to a third part vendor to credential our providers  This finalized the providers participating status within our network to assure that they were being compensated accordingly  Initiated and administered change management process for report application migration effectively exceeding project deadlines and expectations  Modified an existing CredentialingRecredentialing system to send recredentialing packets to providers via RightFax to be more economic and efficient  Processed Adhoc data request for all business units to assist with decision making on various provider related issues  Trained team members to use Enterprise Guide querying software to make them self sufficient in obtaining data  Developed and maintained a QA process and testing guidelines for our Online Provider Directory to ensure accuracy of the data that our Member Relations department shares with our members  The Online Directory  resulted in an increase of our members using the new technology to assist with creating customized paper directories allowing them to select a provider in their area  This decreased our member call center activity freeing resources for other initiatives  Assisted in the creation of statements of work sent to outside vendors to help facilitate new corporate initiatives  Responsible for creating Business Requirements Report Specifications Policy and Procedures and Desktop processes in accordance to our business rules  Recognized for creative solutions resulting in improved business results           Altec      Informatics Programmer Analyst                           012003      012009     Developed and maintained Genesys Call Center reports used to track of productivity of Agents  Queues as well as being used as supporting documentation in legal cases in the event of an employee termination  Project lead on project with Verint Witness Actionable Solutions to develop a way to track individual agent phone and PC activity to ensure that our members and providers were being treated with respect and dignity  Processed Adhoc data request for all business units to assist with their daily work flow  Maintained a monthly Key Indicator report for Board of Directors that identified areas where the business was thriving and also areas that need a little more attention  Created Information Cubes using Enterprise Guide to allow senior management to decipher data with the click of a button and drill down to very detailed levels of reports  Developed and maintained stored procedures to assist the business units in being able to run their own reports through AddIns placed in MS Office Developed Web Reports using Web Report Studio SAS BI so that adhoc reports that required simple parameter changes could be run by the Enduser           UNIVERSITY OF PITTSBURGH MEDICAL CENTER      Systems Analyst                           012001      012003     Developed and maintained various UNIXMARS scripts to generate daily weekly and monthly reports  Processed data requests for various administrators including but not limited to transplant reports and weekly Patient Observation reports  Trained nursescase managers to manipulate data using office software and Cognos Utilized analytical expertise to provide direction and recommendations to senior managers on key business decisions and process improvement initiatives  Wrote UNIX shell scripts to query clinical databases to provide data to senior managers to track trends in specific medical procedures and lengths of stay with respect to diagnosis codes and CPT codes</data><data key="id">201222298694913699117632498749477647101</data></node>
<node id="n1122" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary     Obtain a position where I can utilize my Accounting and Financial Concept skills Customer Service skills French Speaking skills and Critical Thinking skills to better and improve the growth of the Company           Highlights       Excel Word and Power Point Microsoft Outlook CMS program  Deans List for 6 consecutive quarters                     Education       Stephens Henager College    Orem     Utah      Expected in   2015     –      –       Forensic Accounting                  GPA                                   Expected in        –      –       Bachelor Degree                  GPA                     Accomplishments      Graduated Suma Cum Laude with a Bachelor Degree in Forensic Accounting at Stephens Henager College   Honor student and earn the deans list 9 times during the school year        Experience       Altice Usa      Data Analyst   Prestonsburg     KY                   012014      012014     Review and analyze clients Application in compliance with Companys Guidelines  Determine Clients qualification by reviewing and analyzing clients Credit Report and Financial Statements  Enrolled Clients information in Data System           Landmark Aviation      Monitoring Agent   Kahului     HI                   012012      012014     Monitor residential and commercial security systems  Assist to handle individuals crisis as they come up           Emd Millipore      Executive Coordinator   Breinigsville     PA                   012007      012012     Provide and process travel service to Delta Air Lines Executive Members  Provide and process travel service to Hearing Impaired  Provide and process travel service to Federal Air Martials           Incharge Institute      Credit Counselor   City     STATE                   012000      012006     Assess clients financial situation  Determine the optimal solution in resolving clients financial distress  Offer and explain possible programs such as credit counseling bankruptcy options budgeting improvement and debt management          Work History       Dispatch Police Department Emergency Medical Service and Fire Department                                  Languages     Write and Read French       Skills     Accounting budgeting CMS counseling Credit Client clients financial Financial Statements Read French Excel Microsoft Outlook Power Point Word</data><data key="id">7819023529030261471911163313029828293</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-8c4d404034ab4f52bf911fc2d074418d</data></node>
<node id="n1123" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-804d5755ff3449d889503d217324e787</data><data key="resume">JC     Jessica    Claire                             San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Experience        102019   to   062019   Data Analyst    Angi Homeservices Inc         Rockledge     FL            Built an end to end analyticsbased solution for animal shelters to verify optimality of current capacity with a vision to deliver recommendations to improve animal outcomes and health in the future  Created a cloudbased AWSFlaskHTML UI to source animal shelter data  present optimal solution  Utilized Delphi method to generate synthetic data for missing values using information from animal experts  Leveraged SQLPython to preprocess and optimize animal shelter data  Dealt with a tricky client with unclear outcomes and impacted by COVID19 times  yet delivered a complete extendable analytics UI solution  Predicted the outcome of animals with 73 accuracy based on characteristics of the animals at the time of intake            012019   to   062019   Analyst    Ameresco         Half Moon Bay          IND       Traders of import export licenses in India Operationalized and negotiated the pricesrate of various MEISSEIS by predicting the price of the MEISSEIS licenses using Timeseries analysis and supervised machine learning model resulting in savings of INR 1000000  Created a new billing system from scratch to increase efficiency and reduce time spent in creating bills using Excel VLOOKUP HLOOKUP and Formulas which resulted in time spent in creating bills reducing by 50            062018   to   092018   Analyst Intern    Spencer Stuart         Chicago          IND       Although junior quickly earned responsibility for creation of data architecture  automation for BSEs Mutual Funds  Processed 1 million weekly transactions to enable audits executive reporting  Presented to CXOs weekly          Work History        102019   to   062019   Data Analyst    UC Davis Koret Shelter Medicine       San Francisco     CA     Built an end to end analyticsbased solution for animal shelters to verify optimality of current capacity with a vision to deliver recommendations to improve animal outcomes and health in the future  Created a cloudbased AWSFlaskHTML UI to source animal shelter data  present optimal solution  Utilized Delphi method to generate synthetic data for missing values using information from animal experts  Leveraged SQLPython to preprocess and optimize animal shelter data  Dealt with a tricky client with unclear outcomes and impacted by COVID19 times  yet delivered a complete extendable analytics UI solution  Predicted the outcome of animals with 73 accuracy based on characteristics of the animals at the time of intake            012019   to   062019   Analyst    Sanghvi International Pvt Ltd       Mumbai IND          Traders of import export licenses in India Operationalized and negotiated the pricesrate of various MEISSEIS by predicting the price of the MEISSEIS licenses using Timeseries analysis and supervised machine learning model resulting in savings of INR 1000000  Created a new billing system from scratch to increase efficiency and reduce time spent in creating bills using Excel VLOOKUP HLOOKUP and Formulas which resulted in time spent in creating bills reducing by 50            062018   to   092018   Analyst Intern    MarketPlace Technologies Pvt Ltd Bombay Stock Exchange       Mumbai IND          Although junior quickly earned responsibility for creation of data architecture  automation for BSEs Mutual Funds  Processed 1 million weekly transactions to enable audits executive reporting  Presented to CXOs weekly          Accomplishments       Predicting Heart Disease Using Classification Trees  Used classification trees to predict whether a person has heart disease classifying those without heart disease True Negatives and with heart disease True Positives with accuracy rates of 81 and 85 respectively Car Brands Mapping  Built a position map for car brands using Principal Component Analysis PCA and Principal Component Regression PCR in R and gave recommendations as to what Infinity should do to improve its car design Airbnb in San Francisco Opportunities  Possibilities  Constructed a random forest classifier to discern aspects of a listing affecting ratings received on Airbnb  Built a linear regression model to understand factors affecting price of a rental on Airbnb ACTIVITIES  LEADERSHIP President IT Students Associations VIT July 2016  August 2017  Orchestrated 3 Python and 2 R workshops for over 250 student attendees and organized LAN gaming events with 300 participants Joint Sports Secretary Student Council VIT July 2016  August 2017  Led and managed a team of 100 volunteers and organized a 5day long Sports Festival with 28 events and 3000 entries         Education        Expected in   July 2018   Bachelor of Engineering       Information Technology    University of Mumbai Vidyalankar Institute of Technology     Mumbai           GPA                 Expected in   June   Master of Science       Business Analytics    University of California     San Francisco     CA      GPA               Summary     A motivated data enthusiast with 5 years of SQL and data analysis experience 3 years of ObjectOriented Programming experience and 2 years of Python and R experience Selftaught Machine Learning and Big Data and applied to various settings by taking 3 Coursera Courses 3 Udacity Courses 2 Udemy Courses and am an AWS Certified Cloud Practitioner Motivated to make a difference and hence currently analyzing publicly available animal shelter data       Highlights           Tools SQL Python Jupyter Notebook Flask R Tableau Spark MongoDB Excel HTML MapReduce  Skills AB Testing Data Analysis Database Management Data Mining Data Visualization Hypothesis Testing Modeling Machine Learning Statistical Analysis Dashboards  Automation  Billing system  Client      Data Analysis  Data Mining  Data Visualization  Database Management  Delphi  Funds  HTML  Machine Learning  Excel  Modeling  MongoDB  Python  Reporting  SQL  Statistical Analysis  Tableau  Vision                       Skills      Tools SQL Python Jupyter Notebook Flask R Tableau Spark MongoDB Excel HTML MapReduce  Skills AB Testing Data Analysis Database Management Data Mining Data Visualization Hypothesis Testing Modeling Machine Learning Statistical Analysis Dashboards  Automation billing system client Data Analysis Data Mining Data Visualization Database Management Delphi Funds HTML Machine Learning Excel Modeling MongoDB Python reporting SQL Statistical Analysis Tableau vision</data><data key="id">228453806890064931472132132102300266426</data></node>
<node id="n1124" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-71d1cf147eaf47719ca0e8329b94d47d</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Logical Data Analyst skilled in requirement analysis software development and database management Selfdirected and proactive professional with 5 years of vast experience collecting cleaning and interpreting data sets Natural problemsolver possessing strong crossfunctional understanding of information technology and business processes        Skills           Query Tools  Analytical Problem Solving  Reporting Tools  Data Mining  Data Quality      Data Analysis  Data Visualization and Presentations  Database Programming and SQL  Linear and Logistic regression  R and Python                       Experience       Data Analyst        012022      Current     Granicus    –    Chicago     Il Or Denver            Conducted data analysis to prepare forecasts and identify trends  X13ARIMASEATS model to seasonal adjustment for time series analysis 2 delinquencies  Generated reports and obtained data to develop analytics on key performance and operational metrics  Partnered with data scientists to support ongoing research and analytics initiatives  Worked with internal teams to understand business needs and changing strategies  Created and designed business intelligence databases spreadsheets or outputs  Recommended metrics and models based on observed trends  Generated standard or custom reports summarizing business financial or economic data  Managed diverse projects for data capture storage and forecast analysis  Developed a new credit scoring matrix to target personal loans using linear and logistic predictive models  Machine learning using descriptive models in analyzing the internal scoring model for personal loans  Use descriptive analytics to implement new credit criteria for personal loans DebttoIncome ratio to Free Cash Flow  Experience using YeoJohnson Transformation and centerscale a vector to attempt normalization  Implemented internal reporting using power bi to Forecasting using 2  delinquency comparisons  Built compelling data stories and visualizations to influence decision makers           Data Specialist       022021      122021     Ascension Health    –    Wichita     KS            Worked with internal processes using various modeling and machine learning in R to analyze inpatient care to hospice  Generated customized reports on client data providing valuable insights into requested data points  Audited data regularly to guarantee data integrity and quality  Reconciled data to identify anomalies  Corrected data errors and entered missing information into new data parcels being processed for clients maintaining compliance with guidelines  Reviewed and updated account information in company computer system  Extracted imported and exported data into various database applications  Gathered data from multiple sources to assimilate meaningful inputs for databases  Created reports and audited charts to maintain concise records  Utilized Microsoft Power BI to create interactive and reporting dashboards used for internal operations  Developed and implemented a new payroll system using SSRS  Imported and implemented new data by creating a linked server with Microsoft SQL Server Management Studio           Report Developer       052020      102020     Cgi Group Inc    –    Springfield     VA            Monitored and reported on key performance indicators and shared results with management team  Assessed and prioritized departmental reporting needs functions and strategies and reported findings to management  Streamlined reporting processes by evaluating data sources compiling data and redesigning output  Drafted and formatted reports in alignment with data quality requirements           Data Analyst       102017      032020     Granicus    –    Columbus     OH            Audited internal data and processes to identify and manage initiatives improving business performance  Generated reports and obtained data to develop analytics on key performance and operational metrics  Created impactful business reports utilizing large datasets and Crystal Reports SQL and MS Excel  Worked in a close team to build data support and report generation used for both internal and external reporting  Assisted in the administration and auditing of SQL databases  Created and maintained custom queries views and stored procedures using TSQL for data and driven datasets  Assisted in change management validation and verification to ensure data quality  Managed Crystal to distribute daily weekly and monthly reports and dashboards for internal clients  Generated standard or custom reports summarizing business financial or economic data          Education and Training       Bachelor of Science       Business Administration – MIS       Expected in   052017                Utah Tech University       St George UT          GPA        Status                  Master of Science       Business Analytics       Expected in                   University of Utah      Salt Lake City     UT     GPA        Status                 Technical Skills  Certifications       Microsoft Technology Associate in Database Administration Fundamentals  Predictive analytics R and Python Machine learning Modeling using KNN linear and logistic regression X13ARIMASEATS  Crystal Reports Business Objects SSRS TSQL MYSQL server 2008 Oracle plsql</data><data key="id">277258491714249799748455016096466562575</data></node>
<node id="n1125" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-148e1ea78d4a471582aad5638a248a51</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary      Experienced Data Analyst who responds to shifting business needs and priorities in a systematic and effective way  Excels at implementing operational assessments and conducting functional requirements analysis for businesses of all sized  Committed to maintaining cutting edge technical skills and uptodate industry knowledge        Skills           Technical help desk experience  Database design  Excellent communication skills  Strong analytical skills       Technical specification creation  LANWAN Network upgrades  Enterprise Technology  Excellent diagnostic skills  Crosstier components implementation                        Experience       Data Analyst       082016   to   092016     Lockheed Martin Corporation    –    Jacksonville     FL             Worked with SAP removing duplicatesWorked with CRM interface to correct dataAnalysis of customer dataAssisted with cleaning 442 000 customer account records  Extract Install Base data from the centralized repository and evaluateprioritize refresh opportunities based Developed and implemented complex Internet and Intranet applications on multiple platforms  Performed analysis and identified cost saving opportunities and potential program enhancements  Certificate of Completion  Architectural DraftingAutoCAD and Civil Engineering           Business Analyst       112015   to   072016     Halo Branded Solutions    –    Toledo     OH             Extracted Install Base data from the centralized repository and evaluateprioritize refresh opportunities  Packaged prioritized customer data for insidefield sales consumption   Managed research and reporting of customer install base for BDM Sales team  Created metrics associated with funnel build and conversion rates and value   General field and inside sales support   Daily use of SAP to pull serial data to create client reports  Daily use of Sales Force to create and maintain client account information  Excel used daily to analyse and create client reports   ​Microsoft Server used in conjunction with Access to pull large data sets for analytical reporting           Data Analyst       022009   to   072015     Lockheed Martin Corporation    –    San Antonio     TX             Excel and Access reports creation   Acted as a questionanswer source for client   Maintained client Database   Identified process inefficiencies through gap analysis   Performed monthly data cleanup of all Carrier invoices input into system   Recommended operational improvements based on tracking and analysis   Performed analysis and identified cost saving opportunities and potential program enhancements            Database Administrator       012006   to   012009     Lockheed Martin Corporation    –    Vineyard     UT             Acted as a questionanswer source for client  Was responsible for monthly analysis of data input  Analyzed and made recommendations on data when necessary  Was responsible for monthly metrics reports  Was responsible for process improvements  Was responsible for root cause and corrective action analysis          Accomplishments       Led a 6person team of multicountry analysts in completing a 400 million Asset Recovery project  Led team to meet 100 of SLAs by streamlining business processes and identifying areas for improvement  Boosted customer service ratings by 90 by developing new processes and improving work flow         Education and Training            Architectural Design     Expected in        New York Institute of Technology      New York     NY     GPA         Fine Arts coursework  Emphasis in Architectural Design  Arts Education coursework  Economics coursework  Computer Information Systems coursework  2D and 3D design coursework           Certificate of Completion     CAD     Expected in        Porter  Chester Institute      Stratford     CT     GPA         Emphasis on Automated Computer Aided Drafting  Coursework included Manual Drafting and Civil Engineering focus  Electrical Mechanical and Structural Engineering coursework  Computer Information Systems coursework  Construction training  2D and 3D design coursework  Course on Lighting Design</data><data key="id">253893878710372000388917904806064766790</data></node>
<node id="n1126" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-e1ba3544ad7e4853953356ad5fee2662</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary      Selfmotivated Research Manager offering over 22 years of leadership experience in the animal biomedical field Methodical with significant experience in data organization and analysis Excellent problemsolver with a history of automating processes and driving operational enhancements            Skills         Analytical Problem Solving  Attention to Detail  Data Compiling  Database Building      Data Quality  Microsoft Office  SelfDriven and Motivated  Working Collaboratively                     Education       Towson State University    Towson MD           Expected in   011992     –      –       Bachelor of Arts        PsychologyInterdisciplinary Study Animal Behavior          GPA            Relevant coursework Behavioral Statistics Behavioral Endocrinology Chemistry         Certifications       Certified Manager of Animal Resources  2017  Manager Development Program  2015           Experience       Emprise Bank      Data Analyst   Chanute     KS                   032022      Current     Determines future data and analytical needs to ascertain the effectiveness of colony management strategies  Constructed a MS Access database and developed the data table and form structure that accommodates information from historical records and to meet future data entry and analytical needs  Performs quality assurance to assess data validation on 26 years of historical data Transfers data from those records into newlydeveloped data tables           Emory University Yerkes National Research Center      Colony Manager Research Lab Manager   City     STATE                   072012      022022     Managed colony data for over 2200 nonhuman primates from 2 species optimized and maintained distinct data tracking spreadsheets for viral test history reproduction social history and animal housing allocation  Assessed and established colony management practices eg safety and training guidelines  Directed the development of plans for the breeding program eg introduction of breeder males into 15 social groups per year and social group management eg introduction or removal of animals group moves Influenced strategic planning to reach department and Center goals  Restructured the unit for improved organizational order creation of professional growth opportunities and effective management of nine technicians  Expanded data skills eg designed population projection tool increased Excel skillset to include pivot tables and formulas MS Access database and form design Visio to build timelines and matrilines and beginner RStudio and Cytoscape use  Orchestrated research support activities eg biological specimen requests grant and protocol management research design and budget preparation Strengthened relationships with scientists for effective collaboration           Emory University Yerkes National Research Center      Supervisor Research Specialist   City     STATE                   081999      012009     Successful growth of the SPF rhesus macaque colony and met program aims eg genetic characterization increased frequency of viral testing  Coordinated collection of specimens for research and colony needs  Increased collaboration to meet the needs of animal assignment requests Made animal data eg social hierarchy demographic and reproductive assessable to researchers and arranged animal training for safer procedures  Compiled researchrelated and colony data eg census reproductive success population growth for the Centers base grant and regulatory oversight           Emory University Yerkes National Research Center      Lead Research SpecialistResearch Specialist   City     STATE                   101996      081999     Executed the derivation of a Specific Pathogen Free SPF rhesus macaque colony and a SIVSTLV negative sooty managabey colony  Entrusted with funded research project objectives eg data collection and analysis for Dr Deborah Gusts two funded grants  Mastered technical abilities eg venipuncture animal training animal access and behavioral observation data collection  Designed and developed a record management system for the animal colony eg demographics reproductive parameters socializations and health interventions          Additional Information       Presenter at Americal Society of Primatologists and Breeding Colony Management Consortium national conferences  Contributing author in over 15 profession research publications including BBeisner etal Factors influencing the success of male introductions into groups of female rhesus macaques Introduction technique male characteristics and female behavior Americal Journal of Primatology 2021 and AChahroudi etal Target cell availability rather than breast milk specific factors dictates mothertoinfant transmission of SIV Journal of Medical Primatology 2014 Complete list upon request</data><data key="id">184884603335192536936723689782396859964</data></node>
<node id="n1127" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary      Seasoned Data Entry Operator with 5 years of experience building and maintaining electronic databases Received and reviewed new written records transcribed information into electronic databases verified and validated new and previouslyinput data fields prepared reports and contributed to refinement of data entry protocols Detailoriented professional effecting useful accurate databases        Skills           Meticulous Attention to Detail  Certified in 10Key  Data Transcription  Administrative Support Specialist  Advanced Clerical Knowledge  Collecting Information  Proficient With  Database   Filing and Data Archiving  Verifying Data Accuracy  Written and Verbal Communication  10Key Certification  Workflow Management  Data Verification  Time Management  Data Management      Service Oriented  Data Review  Data Compilation  Project Management  Report Generation  SelfStarter  Microsoft Office  Multitasking and Prioritization  Spreadsheet Management  Data Entry  Information Storage  Advanced MS Office Suite Knowledge  Data Processing  Data Integrity                       Experience       Data Analyst       062022   to   Current     Fedex Cross Border    –    Chattanooga     TN             Input client information into spreadsheets and company database to provide leaders with quick access to essential client data  Identified and corrected data entry errors to prevent duplication across systems  Reviewed and updated account information in company computer system  Proofread and edited documents to correct errors  Completed database backups to secure information  Processed customer and account source documents by reviewing data for deficiencies  Scanned through extensive documents and reporting to identify pertinent data  Remained focused for lengthy periods to accurately perform work with adequate speed  Contacted customers via phone or email to address data inquiries  Stored hard copies of data in organized files to optimize retrieval  Reviewed corrected or deleted data verifying customer and account information  Input new data to test customer and account system changes and upgrades  Followed data program techniques and procedures to maintain data entry requirements  Maintained database by entering new and updated customer and account information  Exceeded quality goals to support team productivity  Created reports and audited charts to maintain concise records  Organized billing and invoice data prepared accounts receivable and generated revenue reports to support audits  Responded to daily inquiries and requests within mandated timeframe to meet deadlines  Compiled sorted and verified electronic data against hard copies to support quality control efforts  Revised standard operating procedures to reflect current practices  Reviewed source documents to locate required data for entry  Gathered and documented statistical information to generate reports  Scanned and stored files and records electronically to reduce paper files and secure data           Correctional Officer III       052021   to   062022     Texas Department Of Criminal Justice    –    City     STATE             Protecting the citizens of Texas while promoting positive change in offender behavior to reintegrate offenders into society and to assist victims of crime  Performs moderately complex correctional work involving the care and custody of approximately 1341 G1G4 inmates  Work involves the direct supervision of inmate work groups in their daily assigned duties preventing escapes and maintaining discipline in conformance with strict rules regulations and standard operating procedures  Works under general supervision with moderate latitude for the use of initiative and independent judgment  Assumes a high level of responsibility for the care and custody of assigned inmates through knowledge of and adherence to laws rules regulations and standard operating procedures governing the Texas Department of Criminal Justice TDCJ  Searches for contraband and provides security counts feeds and supervises inmates in housing work and other areas accessed by stairs and maintains security of assigned areas involving long periods of sitting and standing climbing stairs and ladders to reach assigned areas and working at heights  Provides custody and security of inmates including observing actions of inmates squatting and bending to conduct “pat” and “strip” searches of inmates restraining and securing sometimes assaultive inmates and transferring and transporting inmates by walking or riding in various vehicles such as trailers vans buses and other forms of transportation  Supervises and provides security of inmates performing technical skills such as construction maintenance laundry food service and in varied industrial and agricultural operations which involve climbing stairs and ladders and climbing around the inside or outside of buildings works outdoors and indoors without air conditioning works around motorized or moving equipment and machinery and is subject to all types of weather  Ability to work protracted irregular and overtime hours as needed responds to emergencies including climbing stairs and ladders while searching for escaped inmates hearing calls for and calling for help giving first aid at the emergency site carrying an injured or unconscious inmate or employee various distances to safety up or down stairs and ladders and uses force and deadly force including the use of chemical agents or firearms to control inmates  Reads reviews and properly applies information found in inmate records which is related to the inmate’s health and safety and to the security of the facility provides leadership and technical guidance to other staff complies with policies procedures rules and regulations enforces inmate disciplinary rules and prepares and maintains records forms and reports  Inspected cells and conducted random searches of common areas  Monitored daily activities to identify and manage suspicious behavior improper conduct and signs of conflict  Observed inmate behavior to prevent crime escape attempts and other dangerous activities  Employed deescalation techniques verbal commands and physical and mechanical restraints to address unruly inmates  Detected potential threats and quickly defused conflicts  Maintained clear and open communications with facility areas to support safe operations  Informed inmates and visitors of rules safety and security procedures and responsibilities  Supervised residents during meal distribution and intake recreation time and worksite performance  Applied nonviolent response tools and physical restraint during problematic situations  Enforced resident behavior management protocols and drafted incident reports for infractions  Received property from incoming inmates provided receipts and inspected items for contraband  Transported inmates to and from medical and dental appointments funerals work details and court hearings  Processed inmate intake and reassignment by completing paperwork conducting searches and recording fingerprints  Directed visitor signin documenting processes and completing appropriate visitation logs  Reviewed housing assignments and reassigned inmates to meet allocation and safety needs  Supervised offender work assignments and evaluated performance for compliance with standard regulations  Coordinated activities with inmates to stimulate learning and skill development for life after incarceration  Oversaw public areas and grounds to verify cleanliness and inmate safety  Maintained inmate logs and entered information into electronic offender record systems for regulatory monitoring  Instructed inmates on work detail and oversaw worksite transportation           Military Contractor       062018   to   052021     Combine Technical Services    –    City     STATE            AVIATIONGround Units  APACHEJAGM AH64 V6 IOT BLACKHAWK UH60V IOT SCAMPII RPUAS LUT TUAS FOT MSTDMS2TD Spider I1A IOT STRYKER Manpack  Leader Radio   Duties Accomplishments and Related Skills  COMPUTER OPERATOR 1 IN TEST EVENTS TO INCLUDE Performing data entry on dual monitors with appropriate access database software’s  To enter update research verify and retrieve data into and from various systems  This would ensure information is recorded with accuracy and confidentiality  Compile verify accuracy and sort information according to priorities to prepare source data for computer entry  To Reviews data for deficiencies or errors correct any incompatibilities if possible and check output  Research and obtain further information for incomplete documents  Generate reports store completed work in designated locations and perform backup operations on a keypad external hard drive  Provided scan documents and print files when needed  In doing so to diligently keep information confidential  Respond to queries for information and access relevant files  Comply with data integrity and security policies  To ensure proper use of office equipment and address any malfunctions  Data programs CDL 3D Visualization mapping to track military aircrafts based of location  Ability to use Commercial Virtual Remote Environment CVR Hub Microsoft Teams Cloud Computing Program Office  That allows me to chat video virtual Meetings Screen Share Document Collaboration and storage  VOLUNTEERISM Volunteered over 100 hours in assisted individuals that expressed their computer issues as needed  I have done routine disk clean ups to make space on the hard drive this would quickly delete temporary and cache files or uninstalling old applications  Allowing the computer to run much efficiently  Most of the time I would have to do data recovery  I would back up on a separate type of media such as an external hard drive or USB flash drive or back it up online using cloud storages  Using both provides extra protection  I would advise them backing up personal data once a week and backing up business data daily  Most backup methods allow you to schedule automatic data backups as often as you want  DATA COLLECTION IN TEST EVENTS TO INCLUDE Maintained above a 98 quality rating exceeded standards while exceeding the standards with completion of test data throughout every pay period  Operational testing experience for all directorates Field and Data Management processes  Extensive experience planning training and organizing personnel and tasks  Strong background in Data control and Quality control and assurance  Accountable for 100 of the Military and Civilian Mission Test data collected and was able to capture more test information by engaging with test players through direct communication for 1215 hours a day  Data was captured by working 95 hours multiple a week using handheld communication devices in a field environment  Worked in all directorates for Operational test command  Experience in all forms required for testing RAM Performance and HSI accountability of employees and data accident procedures and equal opportunity procedures and processes for all employees  Proven ability to work well under pressure meet closing deadlines self starter able to work well in diverse groups or independently  Setup conference calls to host of daily task  Assisted in training new data collectors on data collection form completion and resolving anomalies  Assisted the Team Lead in the anomaly investigation process  Reported any problems encountered with the data collection process to the appropriate Data manager and Team Lead for resolution  Working closely with military personnel to track and report all problems with the software or programs Maintaining physical and data security  Following all instructions or directions at precise timing intervals monitoring setup and teardown procedures  Able to perform a wide range of standard and nonstandard clerical tasks which include inventory processing by helping with sensitive items reports equipment layout procedures and resolving issues that broke during mission to write up DA form 2404 or do a statement of charges and a FLIPL investigation  As well keeping accountability with a detailed hand receipt DA form 2062  All missions Fording Urban Mobility Live Fire Range Wrecker Team Recovery Operator Team Recovery Casualty Evacuations MOPP gear and emergency situations Flight landings CBRN procedures and missions  Administered and assisted in surveys after tasks and performed QC checks on all RAM Data and performance forms  Form completion and anomalies resolution          Education and Training       Associate Degree     Informational Technology applied Science     Expected in   052018     Vista College      Richland     Tx     GPA       GPA 375        Additional Information       Active Security Clearance SECRET 20190409  Awards and Honors  President’s list 201719 Ace Award 2010 4   Page</data><data key="id">39433607528190513994501630647081772002</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-018d695505f34c4f8151b779981cccf4</data></node>
<node id="n1128" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-06a569984b6d4fed89650ef9764321f3</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Professional Summary     Qualitydriven leader successfully led crossfunctional teams in operations maintenance logistics and optimization activities  History of using innovative approached to production efficiency and accuracy including training programs and resource allocation 20 years of US Naval Aviation Logistic Experience including     Supply Chain and Warehouse Management     Financial Analyst     Lean Six Sigma – Green Belt      Inventory Analyst Manager     Quality Assurance and Auditor Lead     Aviation Consolidated Allowance List     Command Inspector Team Lead 12 years of Documented Superior Senior Leadership experience 5 years experience as a member of the Quality ImprovementAssurance Inspection Team Leader Recognized expert as a “Material Analyst”   Reviewed and analyzed usage trends for the consumable and repairable aircraft material  Set high and low material levels Created and updated as needed “Route Stop Spares” and Deployment packups for the Navy C130 and F18 platforms Located mission critical material by the utilizing resources such as  OneTouch DoD EMALL FEDLOG ILS Naval Aviation Control Point item managers and numerous commercial vendors Solicited quotes negotiated and procured material from commercial vendors Extensive Knowledge of the Naval Aviation Logistics Command Management Information System NALCOMIS MS Word Excel PowerPoint Access Relational Supply Base Level Item Tracking System Electronic Retrograde Management SystemERP  and Fund Administration and Standardized Document Automatic SystemFASTDATA Security Clearance Secret        Core Qualifications           Guest services  Inventory control procedures  Merchandising expertise      Loss prevention  Cash register operations  Product promotions                       Experience       Data Analyst       012009   to   Current     Nelnet    –                      for the Command Fleet Logistics Support Command  Gather information from numerous Naval Aviation Enterprise databases 15 squadrons and several other resources  Analyze and explained trends to created PowerPoint and Excel Current Readiness briefs for the Commodore to delivery up line to the Naval Aviation Admiral           Senior Leading Chief Petty Officer       011989   to   012009     US Navy    –                      for the Aviation Supply Department managed and mentored 56 Sailors Marines and Civilians with Program Management  and Logistic Specialists in support of  C130  FA18 Aircraft  AIMD Test Bench Maintenance Assist Modules and IMRLGround Equipment requirements  Participated in several Lean Six Sigma process Improvement Events  Tracked and analyzed the delivery time for material to be pulled from the warehouse shelf and delivered to the squadron  Increased delivery time by 20 by utilizing a second printer and having the repairable manager pull and stage the material  Completed a Six month Individual Augment assignment in Bagram Afghanistan  As the only Humanitarian Assistance HA Officer in Afghanistan managed a team of 4 Service Members and 20 local nationals to procure and distribute 9 million worth of critical HA supplies to families and villages throughout the country  Commander Naval Air Reserve Force Command  Aviation Supply Logistics Leading Chief Petty Officer  I was responsible for guiding and assisting over 400 personnel in 23 Reserve Squadrons AE6B C130 C130 FA18 and HELO with technical supply support  Successfully researched and resolved over 200 high priority “supply assist” requests a month  Thoroughly completed 10 Quality Inspection Visits and Quality Assist Visits to 5 Reserve Air Stations  Completed training for over 400 personnel and conducted process improve requirements for each site  Performed 4 Aviation Consolidated Allowance Lists for 3 air stations that supported the FA18 aircraft  Analyzed and projected repairable aircraft asset requirements for the next 2 years           Material Control Chief          to            –                      Researched and procured aviation material that supported the maintenance of repairable assets for C130 P3 and 2 FA18 squadrons  Assigned as the “Supply Assistance” expediter for fleet squadrons  I thoroughly researched the critical and hard to find aviation items by utilizing all available resources  ILS vendors item managers Material Control clerk  responsible for the ordering tracking of all material to support the squadron  Expedited the mission critical documents and maintained constant communication with station supply  Operating Target Manager  validated monthly financial reports ensured all charges were valid  Reviewed and analyzed spending trends for the Aircraft Operations and Maintenance AOM and Navy Working Capital Funds budgets  Developed several Excel charts and graphs as a tool to improve the financial metrics and spending          Education       Graduate          Expected in   1 1988     Anchor Bay High School      New Baltimore     MI     GPA       1988  Aviation Supply “A” School         1994  Hazardous Material Control Management Technician NEC 9595 1997  Primary Leadership Development Program 1999  Joint Aviation Maintenance Material Management Course 2000  Computer Peripheral Equipment Operator 2000hr Apprenticeship 2001  Advanced Leadership Development Program      Business Administration Economics     Expected in   1 2004     Northwood University                GPA       Relational Supply Force Advance Technical Specialist NEC 2830        Professional Affiliations              Skills     budgets C charts databases delivery financial Funds graphs hr Leadership Development Logistics Excel PowerPoint 2000 Navy Naval NEC Enterprise next 2 personnel printer process Improvement Program Management Quality Six Sigma Technician</data><data key="id">133969219376573844550170899355951176973</data></node>
<node id="n1129" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-07363e8d66174fdfab3890305bc1d2aa</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Summary     To obtain employment with a company that will provide an opportunity for advancement were I can utilized my work experience toward a rewarding career       Highlights            Microsoft office MS Project 20072010 2013                         Experience      052016   to   Current     Data analyst      Mantech International Corporation    –    Omaha     NE             5 years experience in Design Development and Support of MS SQL Server with Specialization in MS SQL Server 2008R2012 in Production Analysis and Presentations  Create list reports cross tab reports blank reports and reports with lot of conditional formatting using Report Studio  Develop Detail and Summary reports Drill through and Drill down reports using SQL Server Reporting Services SSRS Develop queries to retrieve and analyze data from multiple databases  Find tables using Database diagrams in SQL server establish relationship in multiple tables using Primary and Foreign keys  Extensively use TSQL in constructing Stored Procedures Variables Tables User Define Functions Views and triggers  Create CrossTab Drilldown and Parameterized Reports using RDL Deploy and upload SSRS reports to SharePoint Server and Report Manager for the end users and involved in enhancements and modifications Developed queries to retrieve and analyze data from multiple tables in the database  Find tables using Database diagrams in SQL server established relationship in multiple tables using Primary and Foreign keys  Perform adhoc analytical request using SQL for internal and external parties  Explain results to QA and Operations managers using templates graphs charts Power points and Pivot charts with over 98 efficiency  Analyzed and interpret data to understand business impact correlationsdiscrepancies explain variance analysis budget analysis and propose changes and alternate solutions resulted in cutting down project cost from 3 million to 24million Analyzed and mine data in the Enterprise Data Warehouse and explained trends that appeared on standard reports thus facilitates interpretation to stakeholder  Used tools Excel and SQL daily to analyze query sort and manipulate data according to defined business rules and procedures with less than 2 discrepancies  Compiled data from multiple sources which may include simple data mining enter or import data into Spreadsheets Databases Variance Analysis Extracted data from multiple sources using SQL exported data to excel using SQL export and import button  Analyzed data explained findings to management  Eg  why a negative variance project should be abandoned and a positive variance project should be pursued  Retrieved and analyzed key data elements from data warehouse Performed analysis of findings and trends using statistical andor financial analysis modeling process documentation profitability analysis etc  Worked closely with Operations Managers develop graphs reports dashboards and presentations fraud activities patterns trends and operational metrics as need Created Weekly Monthly and quarterly reports for the Operations department          032013   to   042016     Information Technology Specialist      Mastec Inc    –    Manteca     CA             Managed and maintained the database update software grant user permission and security of the database Conducted high quality training for groups up to 510 employees monthly with a 100 success rate  Supervised and evaluated up to 15 employees 90 of which attained their next promotion level and also improved staff productivity and efficiency of each team member by 70  Constructed Edited and Tested computer programs Worked under pressure multitasked and met tight deadlines  Operated and maintain computer systems with all updates and upgrades and update maintenance officer on daily activities          012012   to   032013     Data Analyst      Mantech International Corporation    –    Richmond     VA             Designed wrote and maintained SQL queries from multiple data sources to support the business  Interface with LOB and MIS resources to ensure that the reports are accurate and deployed in a timely manner  Identified data process improvements and leverage technology to automate and streamlined existing processes  Produced parameter driven matrix subreports and integrated report hyperlink functionality to accessed external applications as needed  Generated matrix reports tabular drilldown drillthrough sub reports chart reports multi parameterized reports Implemented multiple grouping into several reports using visibility and toggle options Administrated and scheduled mailing subscriptions on SSRS reporting server Analyzed and interpreted data to understand business impact correlationsdiscrepancies and to proposed changesalternate solutions in every stage of the data process  Produced dashboard SSRS Reports under report server projects and published SSRS Reports to the reports server  Created SSRS Data Model projects using Microsoft Visual Studio 2012 and using Report Builder for report server to facilitate Adhoc reporting by the business users Analyzed and mine data in the Enterprise Data Warehouse to explained trends that appear on standard reports  Applies broad in depth retail business and technical knowledge to establish direction          Education      Expected in   2015     Masters          Colorado Technical University                GPA               Expected in   2015     IT and Project management in progress Colaberry school of Data Analytics  in                          GPA               Expected in   2006     Certification in Microsoft SQL Business Intelligence Concentration in Database Development    SSIS SSAS SSRS TRUSTECH Institute of Technology in                          GPA               Expected in        Certification in MS Project Professional 2007                          GPA               Expected in   2004     Bachelors     Political science and Public Administration     University of Buea                GPA       Political science and Public Administration        Skills     Ad automate budget analysis Business Intelligence charts data mining data warehouse Databases Database Database Development direction documentation financial analysis graphs interpretation mailing Excel Microsoft office MS Project SharePoint Microsoft SQL Windows 98 MIS modeling Enterprise next Presentations processes progress Project management promotion quality QA reporting retail Spreadsheets MS SQL Server SQL SQL Server Tables TSQL upgrades Variance Analysis Microsoft Visual Studio       Additional Information       Secret Security Clearance completed 8th January 2015</data><data key="id">308738995164409378844472140258632808333</data></node>
<node id="n1130" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-fbcc02cd252f43acb0dc41489718bc00</data><data key="resume">JC     Jessica    Claire                                        100 Montgomery St 10th Floor           555 4321000                 resumesampleexamplecom                         Summary      Selfmotivated Data Analyst offering 8 years of leadership experience across various industries Methodical with significant experience in data mining and statistical analysis Excellent problemsolver with history automating processes and driving operational enhancements        Skills           Data entry  Organization and efficiency  Leadership  Troubleshooting  Data mining  Database management  Data mapping  HTML      Scrum  UIUX  Design and development  Application development  JavaScript  CSS  Python                       Experience        032017   to   032019   Data Analyst    Tendril         Seattle     WA            Improved pattern recognition ability by 250 by analyzing patterns within signals of interest and tracking electronic signatures for 56 vessels over three years  Spearheaded and increased workflow improvements by 80 by designing and developing fullstack systems which include extracting data from different sources developing multiple formatting and structuring programs that can take large pools of unformatted data entries creating structured databases and application interfaces navigable by the untrained eye  These databases are utilized fleetwide by military personnel and AI systems to improve pattern recognition and predictive analysis for mission planning and operations  Achieved 100 operational effectiveness without fault by managing maintenance checks on all components of passive radar and jamming suite  Increased situational awareness and improved efficiency of operations by creating and distributing intelligence products fleetwide  Chaired 10 Subject Matter Expert SME boards and amplified electronic warfare watch conditions leading to 20 increase in operation proficiency in vital watch  Trained and mentored sailors in OPELINT and refined combat systems training team drill guides resulting in the qualification of 100 sailors in vital watch position  Solidified MOC certification of ComThirdFleet by deriving operational intelligence through evaluations of signal quality radar capabilities and life patterns within ELINT  Enhanced electronic intelligence by performing fusion analysis and analyzing radar capabilities from electronic intelligence  Managed timesensitive Indications and Warnings IW giving technical and tactical guidance to Warfare Commanders and national consumers in support of surface subsurface air and special warfare operations  Increased team productivity and improved task performance by publishing detailed stepbystep technical guides utilizing years of firsthand experience and disseminating psychological operations materials  Steered cryptologic and intelligence briefs radar cross section testing and electronic warfare training events  Assigned information operation data collection responsibilities  As a knowledge manager introduced and created work center SharePoint sites across three domains TSSCI GENSER and UNCLASS allowing precise and effective progress tracking log safekeeping and project management within the workplace  Successfully launched and established military deception plans decoy handling signals intelligence operation and cross fix on electronic intelligence intercepts electronic intelligence collection plans and order of battle  Conceptualized developed and briefed 48 intelligence products on SIGINTrelated patterns of life and findings to fleetlevel commands and strike group information operation operational directives  Generated and managed proforma signals analysis reports tactical and operational electronic intelligence reports and communications electronic warfare reports using electronic intelligence national databases and electromagnetic spectrum research  Extracted data from record message traffic and identified essential elements of friendly information violations  Deciphered mission threats fused multisource and electronic support data and correlated electronic intelligence signals to specific platforms  Resolved and minimized identification conflicts in collected electronic intelligence  Devised rapid evaluation guidelines and electronic order of battle to support warfare commanders’ requirements  Recommended and administered changes to emission control plans and conditions and electronic warfare countermeasures  Decoded tactical transmissions and determined onboard weapons systems limitations  Supervised information operations preplanned response electronic warfare status boards ships weapons passive countermeasure system status availability and frequency deconfliction  Constructed and managed online emitter libraries electronic warfare control ship procedures electronic support and electronic support collection logs  Monitored strategic and tactical situations and prioritized electronic intelligence collection efforts  Controlled access to restricted areas and safeguarded handled and stored classified materials regularly  Increased productivity through by implementing Microsoft Excel Visual Basic for Applications software program to streamline operational processes  Evaluated historical current and forecast data to determine opportunities for development and enhancement  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Harnessed version control tools to coordinate project development and individual code submissions  Mentored entrylevel Python development personnel providing both internal and external positions with technical guidance and education  Prepared forecasts and identified trends through data analysis and tracking  Maintained security and data integrity of databases  Organized subsystems to execute proper collection of data  Mined data to uncover insights and identify market trends and inflection points  Spearheaded diverse projects for data capture storage configuration and forecast analysis  Directed field studies and data collection to support sophisticated analysis            072015   to   032017   Data Entry Operator    Md Anderson         Sugar Land     TX            Raised procedural compliance in the 3M and Hazmat programs by training sailors in QA Craftsman  Awarded as “ingenious planner” for efforts in garnering 4 outstanding zone inspection scores and successful host ship events for a People’s Liberation Army Navy Surface Action Group visit  Responsible for the operation of the ANSLQ32 Electronic Warfare Suite  Scheduled reviewed and supported over 263 training sessions and evolutions on 185 topics leading to the advancement of all eligible OT Division Sailors  Recommended for advancement to First Class Petty Officer  Worked closely with team members to deliver project requirements develop solutions and meet deadlines  Identified corrected and reported data entry errors  Completed accurate and efficient data entry and database updates to support business operations  Input client information into spreadsheets and company database to provide leaders with quick access to essential client data  Documented data entry completions in corresponding logbooks  Executed data verification to ensure expedient error detection  Maintained records by creating monthly reports closing terminated records and performing chart audits  Verified and logged deadlines in response to daily inquiries and requests  Supported document reviews and auditing by locating and providing required data reporting on input procedures and other relevant circumstances as necessary  Coordinated scheduled and executed indepth data entry projects  Drafted reports to deliver information to upper management  Compiled data from source documents prior to data entry  Adhered to strict data confidentiality policies to prevent information leakage            012014   to   072015   Technician    Fox Corporation         Charlotte     NC            Led and managed the accomplishment of 130 preventative maintenance checks on vital electronic sensors and computer systems within the passive radar and active jamming suites ensuring 100 operational effectiveness and safety without fault  Identified evaluated and provided vital signals of interest to forces while conducting operations during strike group exercises giving situational awareness and enhancing strike group tactical operator efficiency  Piloted the training environment from for 22 sailors on professionalism maintenance damage control and SLQ32 Operations resulting in 28 individual qualifications  Operated and maintained electronic sensors and computer systems  Collected analyzed and exploited Electronic Intelligence ELINT in accordance with fleet and national tasking  Maintained machines in good working order by cleaning lubricating and adjusting  Collaborated with technicians to diagnose equipment breakdowns and address quality issues  Posted and adhered to lockouttagout hazardous material handling and hearing and eye standards  Demonstrated excellent mechanical knowledge of machine design use repair and maintenance  Assembled parts using bolts screws speed clips rivets and other fasteners  Inspected equipment and systems to identify issues and reported problems to repair technicians  Prepared operational reports and provided information to supervisors  Worked from complex and detailed manufacturing documentation and verbal instructions          Education and Training        Expected in             Full Stack Developer    San Diego Global Knowledge University     San Diego     CA      GPA               Websites Portfolios Profiles        httpswwwlinkedincominJessicaClaire4140031a3                      Certifications     Operational Risk Management and TimeCritical Risk Management  Navy Planning and Foreign Liaison Training  ILDC Leadership Course  San Diego Global Knowledge UniversityApril 2021 – November 2021  Full Stack Developer</data><data key="id">201570454511468162795933779522499910616</data></node>
<node id="n1131" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-be364a9888214b6a845ebc5eb1216576</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary      A passionate researcher and learner with ten years of experience supporting IT activities analyzing data translating longterm and shortterm goals into business solutions such as data modeling and design data architecture design process automation flow data ingestion and migration data Warehousing data lake business intelligence data quality to drive optimal operation processes for seamless data batch flow efficiency using Agile and scrum framework My solutions are deployed in a premise and cloud base environment for cost saving availability reliability and manageability            Skills         Skills  Business Tools  Microsoft Excel  SSRS PowerBI Tableau  SQL with Microsoft SQL server MySQL  Data Modelling  Data Warehouse  Snowflake Oracle  Hardware troubleshooting      AWS Services  JIRA for backlog managment  VLookup Pivot Tables XLookup  ETL ELT Operations  Data Lake Engine – Dremio  Process management                     Education and Training       Scrum Master Training               Expected in   082022     –      –       Scrum Master Training                  GPA                                   Expected in   012015     –      –       Business and Financial Data Modelling                  GPA                    University of Lagos    Lagos Nigeria           Expected in   062006     –      –       Bachelor of Science        Computer Engineering          GPA                     Experience       Ebay Inc      Data Analyst   Portland     OR                   082019      Current     Responsible for cross functional business requirements capture and translating them into data models to drive key operational risk management and CRM models  Responsible for data discovery ETL processes from RDBMS to data lakes and bulk insert big data into RDBMS  Developed reports on business position management billing forecast operational exceptions  Currently the IT interface for various department in setting up reports modifying rebuilding optimizing subscribe user existing reports in PowerBI Tableau and SSRS  Designed monitor data validation checks to ensure consistence between source and destination pre and post load  Work with crossfunctional to scale and optimize cloud solutions in production systems  Using cost explorer compute optimizer to advice engineering team on resizing and providing cost visibility  Work with engineering team on the best approach to optimally size their production system considering impacts and complexities  Liaises with key management to drive optimization cost within the organization  Working with engineering team to adequate provision low and complex resources such as s3 ec2 instance in nonproduction environment  Injecting CloudWatch logs and metrics through API calls into data lake s3 for KPI reporting on various AWS resources for cost tracking and optimization  Curate enterprise to create different LOB semantic layersmaterialized views and Communicate findings to the stakeholders through standard and ad hoc reports  Work with cloud engineering Team and Cloud Core on the identified costsavings opportunities as well as propose new ones  Developed a PowerBITableau application reports for various sections of the business for decision making  Work with the software development team to understand their various data model requirements develop test and deployed  Design document develop test and deploy of various business intelligence applications dashboards and reports for risk evaluation on gross margin forecasting and customer analytics reports  Curate data using MS Excel to create Pivot Tables VLOOKUP joins and other advanced functions for adhoc reports  Worked with multiple databases servers such as MySQL SQL Server Snowflake for creating various data model  Designed developed and maintained 360 customer profile snapshot enterprise active new existing and inactive contacts  Managed setting up multiple connection driver between data sources and analytics engine  Tableau  Developed KPI trends report for management insights decision support           Zippin      BI Analyst   Salt Lake City     UT                   062018      082019     Responsible for cross functional business requirements capture and translating them into data models to drive key operational risk management and CRM models  Developed reports on business position management billing forecast operational exceptions  Interface for various department in setting up reports modifying rebuilding optimizing subscribe user existing reports in PowerBI  Designed monitor data validation checks to ensure consistence between source and destination pre and post load  Work with crossfunctional to scale and optimize cloud solutions in production systems  Using cost explorer compute optimizer to advice engineering team on resizing and providing cost visibility  Work with engineering team on the best approach to optimally size their production system considering impacts and complexities  Injecting CloudWatch logs and metrics through API calls into data lake s3 for KPI reporting on various AWS resources for cost tracking and optimization  Curate enterprise to create different LOB semantic layersmaterialized views and Communicate findings to the stakeholders through standard and ad hoc reports  Developed a PowerBI application reports for various LOB of the business for decision making  Work with the software development team to understand their various data model requirements develop test and deployed  Manage various business intelligence applications dashboards and reports for risk evaluation on gross margin forecasting and customer analytics reports  Curate data using MS Excel to create Pivot Tables VLOOKUP joins and other advanced functions for adhoc reports  Worked with multiple databases servers such as MySQL SQL Server Oracle for creating various data model  Designed developed and maintained 360 customer profile snapshot enterprise active new existing and inactive contacts  Managed setting up multiple connection driver between data sources and analytics engine PowerBI  Developed KPI trends report for management insights decision support           Ebay Inc      Data Analyst   New York     NY                   052014      062018     Responsible for cross functional business requirements capture and translating them into data models to drive key operational risk management and CRM models  Developed reports on business position management billing forecast operational exceptions  Interface for various department in setting up reports modifying rebuilding optimizing subscribe user existing reports in PowerBI  Designed monitor data validation checks to ensure consistence between source and destination pre and post load  Work with crossfunctional to scale and optimize cloud solutions in production systems  Using cost explorer compute optimizer to advice engineering team on resizing and providing cost visibility  Work with engineering team on the best approach to optimally size their production system considering impacts and complexities  Injecting CloudWatch logs and metrics through API calls into data lake s3 for KPI reporting on various AWS resources for cost tracking and optimization  Curate enterprise to create different LOB semantic layersmaterialized views and Communicate findings to the stakeholders through standard and ad hoc reports  Developed a PowerBI application reports for various sections of the business for decision making  Work with the software development team to understand their various data model requirements develop test and deployed  Design document develop test and deploy of various business intelligence applications dashboards and reports for risk evaluation on gross margin forecasting and customer analytics reports  Curate data using MS Excel to create Pivot Tables VLOOKUP joins and other advanced functions for adhoc reports  Worked with multiple databases servers such as MySQL SQL Server Oracle for creating various data model           HiiT PLC      IT Business Technical Engineer   City     STATE                   012011      052014     Provides remote hands assistance to technical teams and customers  Managed system hardware upgrades  Participates in testing evaluation and implementation of related software products including new products software packages operating systems and facilities in order to ensure the software used in the project is compatible from endtoend  Knowledge of tape management systems including handling of media inventory control and offsite storage  Performs user Administration of report distribution  Maintains asset record updates tracking and assigning assets to their location and status registered deployed etc  Support end users’ requests in an area of reportssubscription automation and enlisting users in a report distribution list  Monitoring the performance of individual applications to ensure systems perform efficiently  Ensure adequate performance and operation of the systems  monitor the performance operation diagnose and address any problem  Interfaces with internal clients vendors managers IT and Product Development to resolve problems  Assist in monitoring all computer systems within the organization that are operational throughout an associate’s workday  Assist Revenue Systems Senior Analysts with architecting designing developingcoding testing and delivering moderately complex revenue systems  Responsible for the implementation and remediation of the client device security including patch management antivirus intrusion prevention and access control in line with regional guidance  Maintain a strong working relationship internally and externally across different global IT function groups business functions and international external service vender</data><data key="id">248932956378689958376038080235462071186</data></node>
<node id="n1132" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-d1d7b3fb5cae4c18bb8adc1145d27176</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary      Experienced Data Analyst versed in analyzing and manipulating various data in MercuryGate Transportation Management System and providing Support to Corporate and Independent Business Offices for multiple TMS issues  Excellent interpersonal and troubleshooting skills        Highlights           MercuryGate TMS  Resolve TMS Support Issues  Customer Enterprise Set up and Configuration  Regression Testing      Troubleshooting  Analytical Problem Solving Skills  Customer serviceoriented                       Experience        082008   to   Current   Data Analyst    Verizon Communications         Greensburg     PA            Troubleshoot various data issues in TMS for independent offices and implement solutions to resolve the issues  Coordinate with internal and external team members to meetbeat deadlines  Perform Regression Testing prior to TMS upgrades  Set up and configure new customer enterprises in TMS as needed  Create repeatable documented processes and tools for systematically manipulating data  Analyze and maintain various shipment data in MercuryGate TMS for independent offices including rail and drayage rates truckload and LTL rates and fuel schedules as received  Utilize Microsoft Access and Excel to manipulate data into standard format as provided by independent offices  Utilize Business Intelligence and Mercury Edge to create reports for customers and independent offices as requested  Manipulate and load all data correctly into TMS prior to the testing and golive deadlines for new customers  Prepare and implement plan for maintaining data as needed with external team members for time period between initial load and golive as well as postgolive            052008   to   082008   Cell Site Transport Analyst at TMobile    E  J Gallo Winery         Fort Lauderdale     FL            Develop Adhoc reports in Business Objects to proactively analyze possible risks of missing deadlines Risk Analysis Reports and to provide status to upper management   Provide backup support to Transport Engineers to ensure deadlines are continuing to be met during heavy workload periods              2001   to   022008   Engineering Support Analyst    SPRINT NEXTEL CORP         City     STATE            Provided support to Engineering teams by compiling data for reporting in Access and Excel proactively identifying possible delays in the T1 ordering process flow  Managed cell site projects from order initiation to installation  Coordinated T1 ordering process activities  Tested T1s for cell sites           Education        Expected in   2008   Bachelor of Science       Business Administration Business Information Systems    DeVry University     Irving     Texas      GPA   Magna Cum Laude GPA 3740    Business Administration Business Information Systems Magna Cum Laude GPA 3740 Team Leader for Senior Project and ECommerce Project        Skills     MercuryGate TMS Excellent Customer Service Skills Strong Troubeshooting Skills Support Multitasking Meeting deadlines Excellent Written and Oral Communication Skills</data><data key="id">112053869111601454216909186868679617741</data></node>
<node id="n1133" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Professional Summary      Dependable resultsdriven Data Analyst with extensive experience in collecting organizing interpreting and disseminating various types of statistical figures and strong focus on detail accuracy collaboration and delivering unparalleled client service Expertise in data analysis reporting structures and product and customer analytics Key strengths include excellent organizational skills effective time management and willingness to assist peers and colleagues during peak workload periods Proven ability to exercise good judgment and work well with minimal supervision and being able to make quick decisions in high pressure situations Strong technical project management and negotiation skills Knowledgeable about business process improvement methodology and productivity optimization strategies Recognized for integrity trustworthiness work ethic  objectivity My goal is to develop new skills and utilize my current skills to gain graduate assistantship to develop advocacy and leadership skills        Skills           SQL  Tableau  Python      MS Office Suite AccessProjectExcelPowerPointWordOutlook  Data Science Research Methodologies  Pattern and Trend Identification                       Work History       Data Analyst       082020   to   012022     Consumertrack    –    Los Angeles     CA             Analyzed Clients Financial Data and performed reconciliations to verify completeness of data sets  Providing support for data acquisition and extraction from clients’ accounting systems  Carrying out the manipulation and upload of more complex data sets from a variety of finance systems across various business areas and clients into deloitte’s platform  Carrying out Data manipulation using tools such as Excel and Microsoft SQL  Performing reconciliations to verify the completeness of datasets provided to clients  Carrying out data quality checks which will need to be raised to the Audit practice or client as appropriate  Assisting and guiding audit teams with data extraction from client systems  Been the engagement teams first point of call when using Analytics platform for troubleshooting or general functionality queries providing them with assistance and guidance in the use of our platform  Having a highlevel understanding of the system being supported and using Microsoft SQL to troubleshoot any issues raised by the Audit practice via the help desk system  Possible development and maintaining of various bespoke pieces of work built outside of our analytics platform on a client by client basis  Created reports for audit parameters using PowerBI  Hands on experience in Performance Tuning Query Optimization  Strong Knowledge on Power BI to import data from various sources such as SQL Server Azure SQL DB SQL Server Analysis Services Tabular Model MS Excel etc           Data Analyst       082017   to   012020     Consumertrack    –    Chicago     IL             Analyzed client’s sales marketing and finance departments to automate processes implement internal and external reporting structures and develop cost effective solutions  Conducted needs assessment to define user requirements  Identified overlaps and established metrics and database structure to support all organizational reporting requirements  Prepared regular status reports to enable continuous improvement and improve the overall performance  Collated data and performed volume calculations to present complex solutions in simple terms to clients and enable decision making  Designed implemented and analyzed results of marketing initiatives including email and onsite promo campaigns recommending most effective selling strategies and contributing to additional revenues of more than 29 Million  Developed a route optimisation tool for a leading retailer in India who serves millions of customers in more than 400 cities in every state of the country through digital platforms and over 1500 stores that cover over 16 million square feet of retail space  Improved collections process for a leading retail bank that reached more than Rs 245 million monthly outstanding on a portion of its unsecured credit portfolio during the economic downturn this was classified as early stage delinquents  Built analytical models to obtain the probability of collection from customers based on various actions  The client was able to realign call center agents to more impactful areas such as late stage delinquency and reduce staffing levels in early stage delinquency by 35  Resulted in annual cost savings of Rs 85MM without impacting the collection ratio of 98 for the early stage delinquents           HR Intern       072016   to   032017     American Modern Insurance Group    –    Evansville     IN             Worked with HR team to coordinate company events  Assisted human resources and recruiting teams by scheduling phone screens and onsite interviews and planning recruitment related events  Filed paperwork sorted and delivered mail and maintained office organization  Developed strong written and verbal communication skills  Supported efforts to optimize employee engagement diversity and inclusion to enhance performance management and retention           Programmer Analyst       032012   to   062015     Packaging Corporation Of America    –    Harrisonburg     VA             Developed programs and created files using SQL PLSQL  Created group data warehouse for distribution  Designed various management forecasting and end user reports  Analyzed business requirements and leveraged my technical skills to automate scripts to provide enduser support and regular maintenance to the databases  Previously index fragmentation on each index per table in the properties were checked manually and if it met a certain threshold we executed rebuilding of the index  Saved 45 of the time reducing the time spent on repetititve activities  Reducing 30 of the cost spent on repetitive activities  Used the time saved to solve quality issues which resulted in 45 increase in efficiency  Determined the right stored procedures views and functions for the application  Createddocumented specs and test cases for each PLSQL procedure          Education       Master of Science     Data Science     Expected in   012024     University of Memphis      Memphis     TN     GPA                MBA     Human Resources and Marketing Management     Expected in   072017     Bharathiar University                GPA                Bachelors of Engineering     Computer Science and Engineering     Expected in   042011     Anna University      ChennaiIndia          GPA               Certifications       Python for Data Analytics and AI Coursera – authorized by IBM  Data Science Methodology Coursera – authorized by IBM  Database and SQL for Data Science Coursera – authorized by IBM</data><data key="id">85043192382953542720504976987861958730</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-e92f727eaa244fef84b05d8309a8a573</data></node>
<node id="n1134" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-efbc7419715d41f59184b7791656b692</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary     Seeking an accounting position with a corporation where my analytical customer service and problem solving skills will increase profitability Selfmotivated individual with the ability to multitask Driven hard worker with the ability to adapt to changing environments           Highlights       Microsoft Word Microsoft Excel PowerPoint QuickBooks and Peachtree                     Education       Texas Southern University    Houston     Texas      Expected in   May 2015     –      –       Bachelors of Business Administration        Accounting          GPA           Accounting         Houston Community College    Houston     Texas      Expected in   May 2010     –      –       Associate of Arts        Accounting          GPA           Accounting          Accomplishments              Experience       The Nielsen Company      Data Analyst   Atlanta     GA                   062015      Present     Critically evaluates information gathered from multiple sources reconciles conflicts classifies the information in logical categories  Uses different visualization techniques and present the results of data exploration exercises  Understands the flow of data business processestechnical interfaces that would be created or impacted  Identifies and documents sources of existing data as well as the new data  Understands use of master and reference data including sources and contributors           International Paper Company      Customer Service Supervisor   San Antonio     TX                   052015      062015     Drive the delivery of exceptional customer service Expedite front lines direct flow of customers Communicate customer requests to management Maintain appearance of register area and keep supplies stocked Monitor compliance of cashiers with established Company policies and standards Ensure the accuracy and efficiency in ringing sales Maintaining all cash and media at the registers Follow guidelines prescribed by the Customer ServiceLogistics Manager Ensure validity of customer returns exchanges check authorizations and voids Monitor all areas of possible loss           Hcl Technologies Ltd      Junior Analyst   Morris     NJ                   042015      042015     Documented client organizations direction structure business processes and requirements  Researched client organizations industry position  Assisted in the collection and consolidation of required information and data  Assisted with the preparation of an Audit certification in which the company passed           Keiser University      Administrative Assistant   Port Saint Lucie     FL                   012015      032015     Administrative support including answering telephones completing processing and purging  filing paperwork associated with client accounts with the as400 software           Archer Daniels Midland Company      Hardlines Sales Associate   Abilene     TX                   092014      012015     Provides exemplary customer service to all guests and patrons support with inventory merchandising           Aimbridge Hospitality      Accounting Intern   Plano     TX                   052012      072013     Operate computers programmed with accounting software to record store and analyze information Classify record and summarize numerical and financial data to compile and keep financial records using journals and ledgers or computers Receive record and bank cash checks and vouchers Comply with federal state and company policies procedures and regulations Compile statistical financial accounting or auditing reports and tables pertaining to such matters as cash receipts expenditures accounts payable and receivable and profits and losses Reconcile or note and report discrepancies found in records           Archer Daniels Midland Company      Tax Intern   Alpharetta     GA                   012013      032013     Prepared or assisted with tax returns for individuals Maintained a customer base relationship Calculate form preparation fees according to return complexity Interviewed clients to obtain information on taxable income and deductible expenses allowances and due diligence Computed taxes owed or overpaid using tax software to complete entries on forms following tax form instructions and tax tables Checked data input and verify totals on forms to detect errors in arithmetic data entry or procedures           Orix      Work Study Assistant   Boston     MA                   082012      112012     Made sure the Satisfactory Academic Progress SAP Appeal decisions of the appeals committee was conveyed and mailed to each student Answered phone calls and assisting or referring the caller appropriately Delivered documents to the Presidents Office for signatures Filed confidential documents Retrieve required information from Banner software           NATIONAL ASSOCIATION OF MINORITY CONTRACTORS      Accounting Intern   City     STATE                   112011      052012     Operate computers programmed with accounting software to record store and analyze information Check figures postings and documents for correct entry mathematical accuracy and proper codes Debit credit and total accounts on computer spreadsheets and databases using specialized accounting software Operate 10key calculators typewriters and copy machines to perform calculations and produce documents Comply with federal state and company policies procedures and regulations Code documents according to company procedures Reconcile or note and report discrepancies found in records           HOUSTON INDEPENDENT SCHOOL DISTRICT      Audit Intern   City     STATE                   062011      082011     Audited cash receipts and disbursement vouchers for accuracy in purchasing procedures  Verified internal fund transfers in compliance with Houston Independent School District finance procedures manual  Examined fundraising activities conducted throughout a fiscal audit period using an activity fund program          Skills     10key Academic accounting accounting software accounts payable and receivable administrative Administrative support analytical skills as400 auditing business processes calculators cash receipts credit client clients Excellent customer service Customer Service data entry databases Debit delivery direction due diligence filing finance financial forms fundraising inventory Logistics Manager merchandising Microsoft Excel Office PowerPoint Microsoft Word organization skills Peachtree copy machines policies Progress purchasing QuickBooks sales SAP spreadsheets tables tax taxes tax returns telephones phone typewriters</data><data key="id">313151985355021990962065062926154020096</data></node>
<node id="n1135" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Profile      Logical Data Analyst skilled in eliciting and documenting data projects requirement analysis database management and generating insight through analytic and predictive data modelling Selfdirected and proactive professional with 5 years working with technology team using either Agile or Hybrid SDLC methodology Natural problemsolver possessing and strong crossfunctional understanding of information technology and business processes        Skills           Data Integrity Validation  Compatibility Testing  Statistics and SQL  Database Programming and SQL  Data Analysis  Analytical Problem Solving  Reporting Tools  Data Quality  Clear and Concise Communication  Query Tools  Stakeholder Communication      Business Requirements  Business Analysis  Sequence Diagrams  Data Visualization and Presentations  Data Mining  Attention to Detail  Requirements Definition  Database Building  BI Tool and System Design  Spreadsheet Development                       Experience       Data Analyst       072021      Current     Incloudcounsel    –    Chicago     IL            Undertaking analytical projects in collaboration with business areas across CEBA small business loan repayment division This will include scoping planning and leading colleagues to deliver data products to agreed specifications and timelines  Charged with providing reports and dashboards to leadership team regarding metrics such as numbers of inbound calls duration of calls and many more using Excel SQL Power BI dashboard  Developed different visualization and reported results of data analysis to internal clients and senior managers and maintain regular reportingvisualizations  Developed and maintained data assets including basic data modelling development of business rules to support data quality and integrity and integration with analysis and visualization tools  Worked with the team to analyzed the reason behind customer redialing the customer center  Analysis report showed that lengthy wait time was the primary trigger to redialing Also the analysis showed that the center received more calls on Thursdays and Fridays compared to other days of the week  The insight resulted in the project team reshuffling the reps and placing more of them on days with more calls This therefore dropped customer wait time by 20 after 3 months post implementation  Also generated monthly reports that monitored the repayment of the principal loan and conducted indebt analysis on the segments  This result allowed the organization to understand the propensity of defaults Its showed that applicant companies that were open less than 2 years before taking the loan were more likely to defaults  The insight lead the team to allocate reps that will educate this applicants This thereby lead to an increase the down payments by 15 in 3 months  Worked with internal teams to understand other business needs and recommended metrics and models based on observed trends  Tools Used Azure DevOps MS SQL Power BI Microsoft MS Excel Outlook PowerPoints MS Word           Reporting Analyst       042020      062021     Carefirst    –    Reston     VA            Worked on the CEBA loan acquisition team that was created to respond to the Covid19 government loan application for small to medium business  Researched and analyzed data on metrics such as total number of applications approved versus pending  Managed and undertook a wide range of applied qualitative quantitative statistical data research and analysis projects including research preparation and organization of information or resources as required to meet deadlines  Provides sound technical presentation to stakeholders on the interpretation of findings and key drivers  Researched and prepared recommendations briefings correspondence reports presentations and policy papers which are logical and concise for the consideration of the management team  Created daily reports for the management team showing the amounts of people that applied for the loan and how many of them were approved versus declined  The insight generated from one of my reports showed 78 of the loan declined were due to applicant not providing the right documents to support their applications  Engaged and liaised with internal stakeholders in the development and implementation of a documentation repository so that reps could easily search for FAQ and respond accurately to the clients This response therefore resulted in 30 increase in approval rate within 2 months of implementation  Spent 80 of my time analyzing data using SQL building data model and creating reports dashboards on Power BI I also allocated 20 of my time collaborating with the business to understand reporting needs gathered requirements and feedbacks  Tools Used Azure DevOps MS SQL Power BI Microsoft MS Excel Outlook PowerPoints MS Word           HR Data Analyst       052019      042020     Total System Services Inc    –    KS     State            Was charged with providing insight to improve recruitment retaining and compensation allowance of employees  Conducted analytical research regarding employee tenure verses performance analysis  Conducted comprehensive analysis of compensation and provided insight to the executive team  Insight generated from analysis lead to an improved process in retaining and compensation where we realized that higher performing employees are short tenure compared to lower performing employees  The insight lead the company to revise the compensation process which had a residual payments that will graduate with tenure This new process thereby lead to a reduces turnover rate by 20 in the first 6 months  Extracted and analyzed a variety of HR data using analytical tools such as Excel SQL and Tableau to create monthly business intelligent reports which was presented to senior managers  Managed all aspect of employees journey from recruitments to organizational departure Owned the reports and track all phase to ensure an healthy HR KPI and also provided regular insight to the HR management  Cleaned up data and maintained data accuracy to ensure the integrity of HR data  Interacted with nontechnical stakeholders to deliver analyses and answer questions on deliverables also provided supporting information to substantiate research findings           Product Owner       052018      052019     Leidos Holdings Inc    –    Orlando     FL            Worked as the Subject Matter Expert and Data Analyst in this data migration project  Establish and maintain product backlog to better achieve Business strategic goals and objectives based on Business stakeholders’ prioritization and needs  Ensuring backlog is visible to all Establish start and completion dates for each initiative while providing monitoring controlling the progress update to Projectinitiative sponsor stakeholders and management  Participate in project management activities eg Scrums and apply project management methodology where applicable eg High level scoperequirements in Project Charter BRD – Business Requirements documentation Requirements Traceability documentation Test planning and test cases and obtain signoff  Lead various stakeholders on requirements functional and nonfunctional gathering and refinement including detailed requirements solicitation and documentation  Responsible for bridging the gap between IT and the business to conduct analysis and solutions to meet business objectives this could include technical solutions business processes data etc  Perform analysis of functional and nonfunctional requirements to identify information database content structure application subsystems procedures process and current and future process maps  Articulate solution ideas but also balance them against what’s technologically feasible and functionally reasonable  Maintain a proactive relation with internal and external stakeholders engage with business leaders and users to understand how datadriven changes to process products services software and hardware can improve efficiencies and add value  Identify and describe product features to build a shared understanding of the problem and solution with the product development team  Provide clarification to development team on product features  Move functionality and deliverables between releases as required  Performed QA testing at the time of Quality assurance developed over 50 test cases and ensured that all test cases were Okayed before deployment  Ensured that all 20 reports that had to be migrated from the Legacy software were properly migrated to the new Success Factor software Making sure that all logics were accurately replicated in the Success factor software  Establish test entrance and exit criteria for the QA and BAT testing with support from Developers  Playing the dual role of both the Product Owner and the Data Analyst on this project allowed me to achieve zero defects by the time of project deployments due to the fact that i was constantly pointing out defect and ensuring that the defects were all addressed before the product was deployed          Education and Training       Post Degree Certificate        Human Resource Management       Expected in   052018                University Of Leicester      East Leicester NS          GPA        Status                  Certification in Business Data Analysis Training Course       Information Technology       Expected in   022017                Topron Consulting       Mississauga ON          GPA        Status                  Bachelor of Science       Psychology       Expected in   052015                University of Toronto      Toronto     ON     GPA        Status   </data><data key="id">17485253515650161857749988264634932387</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-78d9e7669e7d461985e69b6e0445d97c</data></node>
<node id="n1136" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-48b5f8baaca745808bc02e0a4194f2bc</data><data key="resume">Jessica  Claire                                                   555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional Summary      Jr Data Analyst with the ability to adapt quickly in new and changing business social and cultural environments            Core Qualifications        TRACS Wise Business Objects SAP and MicroStrategy PowerPoint AS400 ISO 9001 SAP JDE Business Wise Tracs Business Continuity CRM Quantum MicroStrategy JDE JDA Java SAP Professional Studies in Computer Information Science Windows Server and Crystal Reports     communication and self motivated individual                     Education       DeVry University    Van Nuys     CA      Expected in   1 2015     –      –       Associate of Arts        Computer Information and Support          GPA            Currently finishing my last year at Devry for the Business Management Degree                         Expected in   1 2001     –      –       Associate Degree        Network Systems Administration          GPA           Network Systems Administration        Certifications     A Certified Advance Topic in Office MeritCEI California Excel CertifiedIntermediate Microsoft Access SkillsA CertifiedMCSE Certified Hubble Certified Active Directory TRAINING Internship Program NBC for USH Tech Services          Experience       CH Robinson Worldwide Inc      Data Analyst   Sugar Land     TX                   2014      102014     Daily Metadata Management of crucial unit level activities for global and programming avails consolidation of data volumes and announcements provided solutions to analytical projects and complex tasks  Bi weekly ingests template setup data prepping and standardizing Metadata rules for Digital Schedule System  Responsibilities Involved in daily preparation and requisite data mining documentation of film and TVcatalog titles  Used Analytics to unlock combinations for Metadata to properly setup naming conventions  Avails IDs Unique IDs Ratings ITC and process data set up for WB2B to flow downstream  Involved in locale and language solutions to track reporting changes to the Metadata  Provided valueadding consulting solutions that enable ontime SLAs  Provided financial support to VP for concurring cost to enable standardization of digital avails schedule system  Worked as a constant Program Tester to execute final ingest templates of theatrical new release  Digital Asset Management and support to the DAMM team for the setup of digital products in SAP  Localization of Product Greenlighting  Delivery to ensure there is consistency of metadata while complying with client delivery specs  Involved in PO setup and WO creation that would allow digital distribution team to streamline daily operations  Supported Sony Google Amazon LG AppleITunes and ingesting data to DASS System for the master data  Corrected SAPMSB approved titles to flow down to validation report  Setup Digital Asset to green light EST start and end date for Sony LG Google Amazon DLA VUDU WAUKI and XBOX  Added value and delivered DASS program on time and maintained Metadata for AppleITunes  Awarded to manage domestic announcements for US and Canada Programming Avails for master data  Approved MSB titles and Video VersionsMPMs by tracking correct ZVVR in SAP  QA 1 2 3 changes to Big Data benefits to properly delegate ITC pricing for ITunes and Apple  Maintained Metadata with Microsoft Access to open SAP Checklist           Anthology      Data Support Analyst   Boca Raton     FL                   042012      092013     Supported home video distribution team with data analysis streamlining daily operations and reconstructed data workflow maintained on time service level agreements for on time deliverables  Involved in weights and measures entering data flow into SAP for custom dashboard reporting  Credited reports for account activation providing billing discrepancies for returns  Billed submission to WalMart Sams club target with PO reports and invoice reports  Involved in daily payment tracking to ensure production cost fees are up to date  Escalated and corrective action plan for complaints made by the client to support rapid response  Supported Master Data team with physical DVDBD single and corrugate product setup in SAP and Multi Titles  Took ownership of custom month end cumulative pricing reports for summit pictures  Managed ship to list platform to Direct Proper Physical Distribution for CINEDIGM films  Used SAP transaction codes ZMPSDS MM03 and MM02 for product setup for the master data team  Involved in royalty and benefits tracking with business objects in Tracs determining royalty shares by researching benefit contribution  Customer Service Management for Physical Media Product to set up Domestic Announcements  Supporting Daily operations to Master Data for new materials UPC and vendor IDs  Consolidated data in Tracs and uploaded data using JDA  Created queries for royalty reports using Microsoft Access           Aaa Northern California Nevada  Utah      Marketing Analyst   Petaluma     CA                   032010      2012     Developed Infomercial Financial Model PL in 20112012 calendar year for high profile clients  Supported rescue the sale campaigns by validating short form scripts and rebuttals  Facilitated seamless integration of rescue sale scripts with ongoing responsibility to maintaining correct pricing  Changed offer codes and reconstructed TM infomercial scripts by writing rebuttals used in TM Scripts  Compiled analytical sales results to determine conversion rates for ongoing TV campaigns  Tracked media spending results and media earn revenue by building conversion reports  Tracked conversion reports to determine continuation of campaigns driven by media spending  Provided billing solutions to determine minutes used in USCanada then properly collecting client  Booked all vintage marketing associated travel expenses and concurring fees to AMEX spending  Involved in weekly billing and cost analysis to Invoice receipts and approvals  Designed on time budget deployments in JDE for NBCs accounting unit so it can be tightly integrated           Penn National Gaming      Sourcing Financial Analyst   York     PA                   042008      2010     Competitive bids for application to variation to determine price margins for theme park projects Provided high levels of proposal writing reported directly to the finance director of operation  Streamlined the order entry system and improved accuracy to correct prices  Decreased repossession rate from 6 to less than 3 by collecting refunds due to NBC  Collecting receipts and resolving billing discrepancies made by USH by delegating the use of Cards  Awarded event based bid proposals for Univision  NBC theme park operations  Managed vendor job walks to process the new set up for Mutual Propane and Implementing the takeover by submitting new fixed rates to theme park attractions and awarding bids to the vendors  Managed vendor job walks for new smoke and duct detectors to city walk reporting bids to hilltop operations  Consolidated Data in tracs and uploaded data using Microsoft Access  Created Queries for royalty reports using Microsoft Access           Corelogic      Technical Support Analyst   Chantilly     VA                   012007      022008     Fostered and developed client relations and provided end user sales training and support  Amplified sales support by creating a corrective action plan for the pressure team which increased sales by 73  Involved in installation configuration troubleshooting support and maintenance of PC hardware and peripherals including desktop computers printers scanners modems and networked printers  Supported all incoming calls and rerouted calls to proper desktop support team  Prepared landing gear packets to incorporate business process for articles of inspection  Supported private sales of landing gear by brokering price margins and executing proper traceability of landing gear product  Billed and invoiced clients to provide closure for offshore deals properly delegating invoice receipts approvals  Provided a solution for more efficient process by creating and executing allocation plans in JDA           The Intercept Group      Helpdesk Support Associate   City     STATE                   2000      102006     Served as a top Quality Assurance associate that reported directly to Lead Developers for a small company specializing in creation of individualized signature callreporter software  Tested all signature applications as to program integrity and security reporting run time errors  Assisted in creation of fixes and reprogrammed user with technical support on how to apply regulatory updates  Maintained log chart for clients run time errors and log charts for backups to restore cumulative data  Implemented and maintained Microsoft Windows NT network systems  Maintained and established user accounts network security and troubleshot regulatory errors  Maintained financial information and fostered new client relationship          Professional Affiliations              Skills     A Certified accounting Active Directory streamline Apple AS400 Asset Management benefits billing budget Bi Business Objects business process catalog charts Competitive network systems consulting conversion cost analysis CRM Crystal Reports client clients client relations Customer Service data analysis data mining Delivery documentation downstream DVD film finance Financial ISO 9001 Java JDE team building lighting director marketing materials Microsoft Access MCSE Excel Office PowerPoint Windows Microsoft Windows NT MicroStrategy modems network security order entry PC hardware peripherals pricing printers Programming proposals proposal writing QA 1 Quality Assurance reporting reporter researching sales sales support sales training SAP scanners seminars Scripts service level agreements SLA summit technical support desktop support TV troubleshooting Unique validation Video Wise workflow articles</data><data key="id">187396261187520328602024392620217795442</data></node>
<node id="n1137" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-07c72533983646c9b29a72a4de4bbbd9</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Logical Data Analyst skilled in requirement analysis software development and database management Selfdirected and proactive professional with over 10 years of vast experience collecting cleaning and interpreting data sets Natural problemsolver possessing strong crossfunctional understanding of information technology and business processes Selfmotivated Data Analyst offering over 10 years of leadership experience across various industries Methodical with significant experience in data mining and statistical analysis Excellent problemsolver with history automating processes and driving operational enhancements        Skills           Data Warehousing  Attention to Detail  Reporting Tools  Data Quality  Gap Analysis  Report Writing  Defect Tracking  Data Modeling  Python Programming  Strong Work Ethic  Simulation Modeling  Flexible Schedule  Informatica Platform  Data Mining  Process Improvement  Configuration Management  Software Development Life Cycle SDLC  Microsoft Visio  Data Validation  Process Mapping  Requirements Definition  Search Engine Optimization  Business Performance Analysis  Statistical Analysis  Oracle Business Intelligence      Product Development  Analytical Problem Solving  Sequence Diagrams  Enterprise Application Integration  Project Management  Technical Writing  Data Analysis  Technical Analysis  Data Integrity Validation  Data Science With R Programming  Clear and Concise Communication  Usability Testing  Business Management  Bug Life Cycle BLC  Database Maintenance  Statistics and SAS  Microsoft Access  Report Preparation  Business Needs Analysis  Tableau  Data Visualization and Presentations  Database Management  Query Tools  Data Mapping                       Experience       Data Analyst       112022      Current     Leidos    –    Laughlin Air Force Base     TX            Generated reports and obtained data to develop analytics on key performance and operational metrics  Worked with internal teams to understand business needs and changing strategies  Conducted data analysis to prepare forecasts and identify trends  Collected tracked and reviewed data to evaluate business and market trends  Utilized data analysis to monitor process efficiencies and identify data integrity exceptions  Interacted with nontechnical stakeholders to deliver analyses and answer questions on deliverables  Audited internal data and processes to identify and manage initiatives improving business performance  Leveraged software to compile and model data  Audited initial datasets to confirm validity of imported data corroborating changes against updates in data tapes  Recommended metrics and models based on observed trends  Assisted integration of internal and external data tools and products maintaining stability and performance across systems  Synthesized business intelligence or trend data to support recommendations for action  Provided technical support for existing reports dashboards or other tools  Analyzed and tracked data to prepare forecasts and identify trends  Gathered and organized data to analyze current industry trends  Maintained or updated business intelligence tools databases or dashboards  Generated standard or custom reports summarizing business financial or economic data  Identified needs of customers promptly and efficiently  Managed diverse projects for data capture storage and forecast analysis  Collected tracked and evaluated current business and market trend data  Identified or monitored current and potential customers using business intelligence tools  Managed timely flow of business intelligence information to users  Assessed programs to identify risks or problems to determine appropriate responses  Created or reviewed technical design documentation to drive accuracy of reporting solutions  Analyzed competitive market strategies through related product market or share trends  Communicated with customers competitors and suppliers to stay abreast of industry or business trends  Maintained library of model documents templates or other reusable knowledge assets  Identified and analyzed industry or geographic trends with business strategy implications  Synthesized current business intelligence or trend data to support recommendations for action  Disseminated information regarding tools reports or metadata enhancements           Data Analyst       102018      102022     Leidos    –    Lehigh     KS            Generated reports and obtained data to develop analytics on key performance and operational metrics  Worked with internal teams to understand business needs and changing strategies  Collected tracked and reviewed data to evaluate business and market trends  Utilized data analysis to monitor process efficiencies and identify data integrity exceptions  Provided supporting information to substantiate research findings  Assisted with efforts to track evaluate and report on impact of programs using multiple data sources  Audited initial datasets to confirm validity of imported data corroborating changes against updates in data tapes  Recommended metrics and models based on observed trends  Assisted integration of internal and external data tools and products maintaining stability and performance across systems  Synthesized business intelligence or trend data to support recommendations for action  Analyzed and tracked data to prepare forecasts and identify trends  Provided technical support for existing reports dashboards or other tools  Identified needs of customers promptly and efficiently  Generated standard or custom reports summarizing business financial or economic data  Managed diverse projects for data capture storage and forecast analysis  Identified or monitored current and potential customers using business intelligence tools  Collected tracked and evaluated current business and market trend data  Disseminated information regarding tools reports or metadata enhancements  Synthesized current business intelligence or trend data to support recommendations for action           Data Analyst       102013      102018     Leidos    –    Lexington     KY            Generated reports and obtained data to develop analytics on key performance and operational metrics  Worked with internal teams to understand business needs and changing strategies  Conducted data analysis to prepare forecasts and identify trends  Utilized data analysis to monitor process efficiencies and identify data integrity exceptions  Collected tracked and reviewed data to evaluate business and market trends  Provided supporting information to substantiate research findings  Assisted with efforts to track evaluate and report on impact of programs using multiple data sources  Audited internal data and processes to identify and manage initiatives improving business performance  Leveraged software to compile and model data  Disseminated information regarding tools reports or metadata enhancements  Synthesized current business intelligence or trend data to support recommendations for action  Identified and analyzed industry or geographic trends with business strategy implications          Education and Training       Master of Science       Science       Expected in   052013                Mercer University      Macon     GA     GPA        Status                  Bachelor of Science       Science       Expected in   052004                University Of Ibadan      Ibadan Nigeria          GPA        Status   </data><data key="id">152271698224934749271695500972366907138</data></node>
<node id="n1138" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-b1cf01f13b5a400ebf82266cd5ffbf57</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary      Qualityfocused Data Entry Clerk experienced in data processing coding and transcription Skilled at entering data quickly with strong attention to detail and accuracy Team player with outstanding communication skills and flexibility in working with others            Skills         Requirements Definition  Statistical Analysis  Data Quality  Configuration Management  Reporting Tools  Analytical Problem Solving  Attention to Detail  Data Analysis  Flexible Schedule  Strong Work Ethic  Data Visualization and Presentations  Stakeholder Communication  Decision Making  Customer Communication  Business Analysis      Database Building  Mail Management  Accounting Support  Meeting Planning  Data Entry Documentation  Excel Spreadsheets  Research and Analytical Skills  Microsoft Office Suite  Microsoft Office  Schedule Management  Computers and Technology  Document and File Management  Strong Organizational Skills  Multitasking and Time Management  Verbal and Written Communication                     Education and Training       Data Science Certificate    Plano TX           Expected in   072022     –      –               Data Science          GPA                    Udemy    Plano TX           Expected in   072022     –      –               Project Management Certificate          GPA                    ISTA    Safi Morocco           Expected in   062012     –      –               Information Technology          GPA                    Salah Eddin    Safi Morocco           Expected in   072010     –      –       High School Diploma                  GPA                    Ibno Zohr University    Agadir Morocco           Expected in        –      –               Business Economics          GPA                     Experience       Humana Inc      Data Analyst   Clermont     FL                   012022      042023     Worked with internal teams to understand business needs and changing strategies  Conducted data analysis to prepare forecasts and identify trends  Utilized data analysis to monitor process efficiencies and identify data integrity exceptions  Collected tracked and reviewed data to evaluate business and market trends  Analyzed and tracked data to prepare forecasts and identify trends  Provided technical support for existing reports dashboards or other tools  Gathered and organized data to analyze current industry trends  understand the business analyze process the data using python           Columbia Academy      Infant and Toddler Teacher   Fulton     MD                   062019      042020     Supervised circle time free play outside play and learning and developmental activities  Communicated with childrens parents and guardians about daily activities behaviors and problems  Taught children foundational skills in colors shapes and letters  Controlled classroom environments with clearly outlined rules and positive reinforcement techniques  Implemented handson playbased strategies for experiential learning  Organized and led activities to promote physical mental and social development  Read aloud and played alphabet games to encourage early literacy  Developed and enforced positive strategies to encourage good behavior  Supported childrens emotional and social development by adapting communication tactics for differing client needs  Introduced learning activities and imaginative play to teach children to explore  Sparked creativity and imagination by helping children discover new things each day  Monitored childrens play activities to identify additional learning opportunities or behavioral issues  Enhanced sensory abilities by giving children access to numerous textures and shapes  Observed behavioral issues to alert parents or guardians  Engaged with children on individual basis to build positive trusting relationships  Read stories to children and taught painting drawing and crafts  Organized creative and fun activities enhancing childrens physical emotional and social wellbeing  Built and strengthened positive relationships with students parents and teaching staff  Participated in workshops trainings and conferences to improve educational skills  Incorporated music and art activities to encourage creativity and expression           Salient Crgt      Administrative Assistant   Tampa     FL                   062014      112014     Answered phone calls and emails to provide information resulting in effective business correspondence  Provided secretarial and office management support while building cooperative working relationships  Scheduled appointments meetings and events for management staff  Responded effectively to sensitive inquiries or complaints  Coordinated appointments meetings and conferences  Managed physical and digital files monitored spreadsheets and updated reports to coordinate project materials  Developed administrative processes to achieve organizational objectives and improve office efficiency  Prepared and prioritized calendars and correspondence  Composed correspondence reports and meeting notes  Created spreadsheets in Microsoft Excel for recordkeeping and reporting  Kept office equipment functional and supplies wellstocked to promote efficient operations  Sorted and distributed incoming faxes letters and emails for office distribution           Dignity Health      Front Desk Receptionist   Chandler     AZ                   012014      042014     Handled payment processing and provided customers with receipts and proper bills and change  Welcomed patrons to front desk and engaged in friendly conversations while conducting checkin process  Input customer data into reservation systems and updated to reflect room changes  Greeted visitors to provide information and direct to appropriate personnel  Explained policies and procedures to visitors  Completed basic bookkeeping and document filing  Prepared daily shift close reports and balanced cash register to accurately reflect transactions          Languages       English                 Full Professional          Negotiated                               French                 Full Professional          Negotiated                               Arabic                 Native Bilingual          Negotiated     </data><data key="id">168839310054732547394763470435214273355</data></node>
<node id="n1139" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/senior-supply-chain-data-analyst-decision-support-3e0fbfae5312436d8a49a103a1f0f10f</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Excellent communication interpersonal intuitive and leadership skills  Experienced in Reporting and Data Analytics  Computer proficient with Windows Excel Access Office Cognos BI Lawson SAP etc  Ability to organize plan and execute various tasks simultaneously  Quick learner with the ability to achieve immediate and longterm goals       Skills           Guest services  Inventory control procedures  Merchandising expertise      Loss prevention  Cash register operations  Product promotions                       Experience       Senior Supply Chain Data AnalystDecision Support       102013      Current     Ipsos    –    Cincinnati     OH            Responsible for the development of monthly AdHoc reporting and dashboards focused on Cost Savings Analysis using MS Excel and Access  Work closely with internal customers to create data sets to gain foresight into their business needs  Developed MetricKPI dashboardreports using Excel and Oracle  Develop data models using Access and Excel to provide monthly reporting for business managers within the organization  Provide Financial Analysis to Directors on Large RFPs and Request for Enhancement Projects  Prepared presentations for Directors and Managers to explain monthlyquarterly performance to shareholders           Business Process AnalystEngineering       062011      092013     Choptank Transport    –    Allen     TX            Provided analytical support for weeklymonthly reporting using MS Excel and Cognos Report Developer  Assisted engineers process validation  Identified and evaluated improvements in methods procedures and workflow that increase productivity and decrease operating costs  Maintained configuration and assisted in the development of production reporting applications and related interfaces with warehouse management and distribution management systems  Prepared and maintained engineering documentation dealing with distribution center operations material handling or warehouse management issues  Researched and gathered information needed for strategies business case documentation dealing with distribution center operations material handling or warehouse management system issues  Assisted Business Process Engineer regarding new warehousedistribution methods and equipment to improve existing operations and to accommodate new business requirements  Reviewed production schedules and engineering specifications to ensure efficiencies in procedures and activities  Coordinated and maintained quality control objectives and activities to resolve production problems maximize product reliability and minimize cost           Help Desk SupportIntern       072017      112017     Waggener Edstrom    –    City     STATE            Microsoft domain model support  PC imaging and deployment  Windows Active Directory 2005 administration  MS Exchange Server 2005 administration  Documented problem resolutions  Supported multi user applications  Provided user training as necessary  TicketIncident tracking  Supported and trained end users  Provided support to end users on software and hardware problems via phones and email           Data AnalystIntern       042017      082017     Weyerhaeuser    –    City     STATE            Performed analytical support to the Weyerhaeuser Hardwoods and Industrial products business  This included  Maintained business unit KPIs and prepared supporting analysis and schedules graphs charts and related information  Developed pivot tables access and SQL queries and SOPs           Data Analyst       092017      122017     Robert Half Thales Avionics    –    City     STATE            Fulfilled equipment orders for onsite technicians  Maintained onsite inventory of parts by performing physical cycle counts while updating information in SAP system  Created new parts in SAP including part description quantity plant assignment and quality testing information  Assisted QA with the analysis of rejected parts submitted by technicians           Senior CustomerTechnical Support Representative       011      112017     Verizon Wireless    –    City     STATE            Handled general inbound call center customers and dealer inquiries concerns and reports in a challenging high callvolume environment  Analyzed billing inquiries process adjustments and enter service orders utilizing online computer systems  Analyzed pricing structures relative to airtime usage and advised customers of the best service plan for their account  Retained and effectively communicated current information regarding all marketplaces serviced by Verizon Wireless  Assisted customers with troubleshooting of all equipment data roaming and enhanced services offered by Verizon Wireless  Served as Tier II and III to representatives assisting customers with equipment data and service area issues           Sales Support Representative       011      112017     Airtouch Cellular    –    City     STATE            Processed and fulfilled equipment orders by working directly with Distribution Centers and logistics providers to ensure ontime delivery  Activated new lines of service and implemented all changes on existing accounts requested by agents  Educated agents on current price plans and promotions  Handled commercial and individual accounts with written and verbal price plan analysis  Evaluated and offered special pricing structure to qualified large business accounts  Worked closely with Major Account Executives on account level changes and fulfilling large equipment orders for our large business customers          Education and Training       BS       Information Technology Business Systems Analysis       Expected in   October 2010                University of Phoenix                GPA        Status         Information Technology Business Systems Analysis 32        Skills     Active Directory Ad billing business case Business Process call center charts Cognos hardware delivery documentation email Engineer Financial Analysis graphs imaging inventory logistics Access MS Excel Excel MS Exchange Server Windows Oracle Developer pivot tables presentations pricing quality QA quality control reporting SAP SOP SQL user training phones troubleshooting validation workflow written       Activities and Honors</data><data key="id">258445310291813121195717175915815811692</data></node>
<node id="n1140" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Career Overview      ClientsEmployers Thompson Reuters Health Care Truven Health Analytics Blue CrossBlue Shield Michigan Microsoft Wayne County Michigan         Skill Highlights           Oracle 1110g98i SQL Server MS Access DB2 zOS  Data Warehouse Data Modeling  Erwin 9873 System Architect 8 Kimball and Inmon Methodologies Conceptual Logical Physical Dimensional Data Modeling Star and ExtendedStar Schema MDM Data Mapping  SQLPlus Explain Plan Data Mining SQLLoader OLAP OLTP Job Scheduling Design Data Profiling Data Mapping Data Scrubbing Data Cleansing  BI Business Intelligence     Dimensional Data Modeling Analytics Data Governance MDM Programming Languages  SQL TransactSQL SQLPlus Stored Procedures VBNET CNET  MainFrame COBOL  Environments  Windows SunUltra HPUX Red Hat  Project Management  MS Project MS Office Serena CAPanvalet  Data Warehouse Data ModelerBusiness Analyst Contract Position                       Technical Skills      VBNET CNET NET Application Development Architect  Automation Budget Business Analyst BI Business Intelligence  COBOL Coldfusion CA hardware concept concept development contracts  Data migration Data Mining Data Modeling Data Warehousing  Database development DC Dialog email Erwin Erwin 9 ESRI ESRI ArcSDE ETL GIS Drawing HPUX DB2 PROFS Imaging IMS DB Information Systems J2EE Languages LINUX Macromedia MainFrame Access Access Database Microsoft Access  C Microsoft Excel Microsoft Exchange Microsoft Office Microsoft Project Windows Windows platform Windows NT Microsoft Word  Modeling OLAP OS Oracle Developer OracleSQL Panvalet pricing p Programming Project Management Quality Red Hat Reporting SharePoint Portal Server  Siebel SQLPlus SQL Server SQL SQL Loader Sun System Architect 8 Systems Architecture System Integration Tables TransactSQL  TSO UNIX Shell Scripting UNIX Scripts VBA Microsoft Visual Basic VB         Work Experience      042011   to   082014     Data Warehouse Data ModelerBusiness Analyst      Ait Laboratories    –    Birmingham     AL             Translates business requirements and models into feasible and acceptable data warehouse designs designs and builds appropriate data repositories and data movements dimensional databases to ensure that business needs are met  Creates database models which serve as blueprints for project engagements of all complexities   Designs and oversees the implementation of data transformation and change processing procedures to ensure that data from multiple sources is aggregated normalized and updated and then published to multiple target environments Interacts with clients while designing internal and external data interfaces to ensure that database development needs are according to client specifications coordinates with clients external data providers and business team to ensure that business model is accurately represented and understood by all relevant parties   Establishes data standards and policies including security and healthcare PHI compliance data modeling systems architecture design and migration system architectures to ensure access to and integrity of data assets   Analyzes changes in on line transactional processing as it affects the data warehouse business processes and external information sources and recommends modifications to improve quality of applications and meet additional requirements   Lead and coordinate activities of data modelers analysts and ETL developers   Develop strategies and parameters to ensure cost effectiveness and system efficiency   Evaluate reusability of current data for additional analysis while maintaining integrity and confidentiality of information conducts data cleaning to rid the system of old unused or duplicate data for better management or quicker access   Develop data warehousing hardware and software platforms and integrates systems to ensure that integration is effective and meets client specifications          102005   to   042011     Business Intelligence      Fidelity National Information Services    –    Charlotte     NC             Work with Business Units to develop approve and implement new Healthcare Data Warehouse Subject Area Data Models Subject Areas consisting of Claim Membership Provider Lab Pharmacy Medicare Advantage Blue Health Intelligence and Care Management Develop Data Mart Data Models Manage reviews updates versioning and publishing of Data Models Monitor track and report progress   Resolve issues mitigate risks and devise contingency plans Architect and Implement centralized data model repository dramatically increasing data modeling productivity Technology Architecture  Federal Programs Business Unit  Medicare Advantage Systems Coordinate system integration and testing Work with Project Management Application Testing and Application Development to establish project plans resources hours dates issues  risks Assign tasks to developers  Monitor track and report progress Resolve issues troubleshoot testing results mitigate risks and devise contingency plans  Manage development testing implementation and follow up Work with business unit claim and pharmacy vendors to develop and approve problem incident and change request documents into business requirement and solution approach documents Data Modeling  Provider Registration and Credentialing Create data migration plan mapping current state to future state data elements Data Warehouse ArchitectureData Warehouse Modeling          082002   to   082005     Microsoft Software Developer Tools Sales Specialist      Microsoft    –    City     STATE             Developed strategies to gain adoption of the NET platform in large enterprise accounts   Created and delivered ROI TCO developer tools product feature and licensing presentations to customers   Performed Siebel Sales Data mining and research to qualify leads and prospects  Built 1 million  sales pipeline for each fiscal year   Coordinated Telesales Rep efforts with Microsoft Project Engaged partners to close business develop marketing campaigns and conduct proofofconcept development   Negotiated pricing and terms Created and delivered resellerpartner incentives    Managed technical team to deliver NET Proofofconcepts Architected proofofconcept NET applications with Microsoft staff developers and customers  using Net and C           021996   to   112002     Business Analyst      Wayne County Michigan    –    City     STATE             Responsible for Microsoft Visual Basic Application Development for County Prosecutors Office and County Sheriff to track warrant requests from all county municipalities including the City of Detroit   Responsible for Microsoft Access Application Development at Human Relations department to track vendor certifications and contracts   Managed Microsoft Visual Basic Application Development for Accounts Payables application to automatically send check requests to the County Treasurer for printing   Managed 20 technicians to collect and assess Y2K impact on over 1000 desktops and printers and 20 servers   Developed Y2K Implementation Plan GIS Architected Designed and Implemented the Countys Geographical Information Systems OracleSQL Server data warehouse  Installed Oracle SQL Server and ESRI ArcSDE  Spatial Data Engine Modeled In Erwin Real Property and Taxbased Data Layers  Converted logical data models to physical created DDL generated tables Developed and executed data migration plans data load processes using SDELoader Access SQL Loader and SQLPlus          Education and Training      Expected in   1998     Bachelors Degree     Computer Science     Wayne State University      Detroit     Michigan     GPA</data><data key="id">50268375443711598882491696221600115197</data><data key="url">https://www.livecareer.com/resume-search/r/data-warehouse-data-modeler-business-analyst-332f51ac4f5f47fb9bc32931a9abc7b0</data></node>
<node id="n1141" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/sql-devloper-dba-data-analyst-3355f0f1c12b4eb9a813a0ab85705db5</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Profile       Has a Master’s of Science Degree in Management Information Systems with Business Intelligence Concentration  Has  5  years of experience working as a SQL server Developer SQL DBA and Data Analyst with excellent analytical and problemsolving skills to work with minimal supervision  Strong knowledge and experience of RDBMS like MS SQL Server 2008 R22012 MS Access MySQL and TSQL queries stored procedures   Strong knowledge and experience in Business Intelligence –SSIS SSRS SSAS Power BI  Power Pivot Power Query Power View Power Map and Pivot Table  Experience in Windows server 20082012 Active Directory AdministrationUsers and Computers DNS DHCP  Experience with data analysis and visualization tools like Excel and Tableau              Core Qualifications         Business Intelligence Data Analytics  Database Development Administration  Windows Server Administration  Windows Azure      Data warehouseData Mining  Microsoft Office  Web Design  Amazon Web Services                     Education       Nova Southeastern University    Fort Lauderdale     FL      Expected in   2015     –      –       Master of Science        Management Information Systems          GPA           Concentration Business Intelligence         Miami Dade College    Miami     FL      Expected in   2003     –      –       Associate of Science        Computer Programming and Analysis          GPA                    Haremaya University Of Agriculture    Haremaya     Harer      Expected in   1988     –      –       Bachelor of Science        Agriculture Plant Science          GPA                     Technical Skills       SQL Server MS Access MS Excel MYSQL   SSIS SSAS SSRS Power BI  TSQL VBA MS Access Excel Macros  Tableau      RapidMiner XLMiner Data Mining   HTML CSS WordPress Dreamweaver Photoshop JavaScript         Professional Experience       Market Footwear Fred Lurie Associate Inc      SQL DevloperDBAData Analyst   City     STATE                   092012      032015    Responsibilities   Created ETL packages involving various data sources SQL Server Flat Files Excel source files  etc  using MS SSIS  Developed data quality solution using SQL Server data services DQS as part of ETL  Analyzed data using using SSAS Excel Tableau and Power BI tools for decision making process  Built Self Service Business Intelligence  dashboards using Power BI tools in Excel  Wrote TSQL queries  for data Analysis and Reporting  Developed reports using MS SQL Server Reporting Services SSRS and Excel  Helped decision makers to understand data on reports and dashboards and gain insights  Installed administered and maintained MS SQL Server 20082012 including upgrades security configuration and service packs   Designed implemented and maintained SQL Server databases   Designed and implemented tables views functions stored procedures and triggers in SQL Server  Planned and implemented SQL Server Security and database permissions   Performed  Database Performance Analysis and Tuning using Database Engine Tuning Advisor SQL Server Profiler and SQL Server Extended Events  Setup HighAvailability as part Disaster Recovery Strategy for the SQL server 2012 databases Failover Clustering Database Mirroring Log Shipping and Replication             Cifra Systems      SQL Server developerDBA   City     STATE                   072009      082012     Responsibilities    As a midlevel DBA Installed configured administered and secured SQL servers 20082008 R2   Created and optimize database objects eg Tables Views indexes cursors stored procedures functions and Triggers   Scheduled and automated maintenance plans using SQL Server Job Agent   Scheduled and automated fulldifferentialtransactional backups and implemented recovery strategies   Configured and monitored database Mirroring Replication and Log Shipping as HADR strategy  Planned and implemented SQL Server Security and database permissions   Extensively used Joins and subqueries for complex queries involving multiple tables  Created indexes clustered and nonclustered  to speed up query performance  Designed Excel dashboard  for tabular and chart reports with drill down  functionality           Miami Christians Fellowship      Web DesignerSQL DBADeveloperMedia Specialist   City     STATE                   022005      042009     Designed desktop database application in MS Access using VBA  Developed financial report using MS Access and Excel  Created website for the organization utilizing HTML WORDPRESS CSS PHP MSSSQL and MYSQL  Performed regular website maintenance and update while also designing graphics for banners flyers and posters  Recorded and edited videos to be distributed in DVD  CD format and upload them to organization’s website  Trained staff members how to utilize computers use software to maintain website and other media support services            Hotsilhouette Inc      Web Developer and Database Support   City     STATE                   072003      2005    Responsibilities   Developed and maintained  companys websites using Dreamweaver HTML CSS ASPNET JavaScript  Designed and managed backend SQL Server databases for web applications  Wrote complex TSQL queries to manipulate  data  Designed and edited graphics and photos for websites using Photoshop     Installed Configured and managed SQL Server and Access databases  Analyzed sales data from different sources and created dashboards using MS Excel  Provided technical computer support to end users</data><data key="id">186341187280622283409609294755555521868</data></node>
<node id="n1142"><data key="name">compile data</data></node>
<node id="n1143"><data key="name">conducting meetings</data></node>
<node id="n1144"><data key="name">procurement process</data></node>
<node id="n1145"><data key="name">workshops</data></node>
<node id="n1146"><data key="name">data reports</data></node>
<node id="n1147"><data key="name">databases</data></node>
<node id="n1148"><data key="name">publisher</data></node>
<node id="n1149"><data key="name">technical support</data></node>
<node id="n1150"><data key="name">relational database</data></node>
<node id="n1151"><data key="name">ssms</data></node>
<node id="n1152"><data key="name">transactsql</data></node>
<node id="n1153"><data key="name">database objects</data></node>
<node id="n1154"><data key="name">triggers</data></node>
<node id="n1155"><data key="name">views</data></node>
<node id="n1156"><data key="name">user defined functions</data></node>
<node id="n1157"><data key="name">development studio</data></node>
<node id="n1158"><data key="name">reporting servers</data></node>
<node id="n1159"><data key="name">csv</data></node>
<node id="n1160"><data key="name">visual studio</data></node>
<node id="n1161"><data key="name">dts</data></node>
<node id="n1162"><data key="name">data migration</data></node>
<node id="n1163"><data key="name">requirements gathering</data></node>
<node id="n1164"><data key="name">power point</data></node>
<node id="n1165"><data key="name">microsoft outlook</data></node>
<node id="n1166"><data key="name">recommendation workflows</data></node>
<node id="n1167"><data key="name">forecasts</data></node>
<node id="n1168"><data key="name">filezilla</data></node>
<node id="n1169"><data key="name">quickcap</data></node>
<node id="n1170"><data key="name">analyzed report</data></node>
<node id="n1171"><data key="name">manipulation of data</data></node>
<node id="n1172"><data key="name">vlookups</data></node>
<node id="n1173"><data key="name">analyzing business data</data></node>
<node id="n1174"><data key="name">•visual basic</data></node>
<node id="n1175"><data key="name">analytical  attention</data></node>
<node id="n1176"><data key="name">enterprise data warehousing</data></node>
<node id="n1177"><data key="name">edw</data></node>
<node id="n1178"><data key="name">enterprise resource planning</data></node>
<node id="n1179"><data key="name">erp</data></node>
<node id="n1180"><data key="name">data reporting</data></node>
<node id="n1181"><data key="name">data trending</data></node>
<node id="n1182"><data key="name">java script</data></node>
<node id="n1183"><data key="name">lotus notes</data></node>
<node id="n1184"><data key="name">web report studio</data></node>
<node id="n1185"><data key="name">appworx</data></node>
<node id="n1186"><data key="name">biolap</data></node>
<node id="n1187"><data key="name">maximus</data></node>
<node id="n1188"><data key="name">quest analytics</data></node>
<node id="n1189"><data key="name">hlookup</data></node>
<node id="n1190"><data key="name">recommendations</data></node>
<node id="n1191"><data key="name">sqlpython</data></node>
<node id="n1192"><data key="name">classification trees</data></node>
<node id="n1193"><data key="name">true negatives</data></node>
<node id="n1194"><data key="name">principal component analysis</data></node>
<node id="n1195"><data key="name">principal component regression</data></node>
<node id="n1196"><data key="name">pcr</data></node>
<node id="n1197"><data key="name">random forest classifier</data></node>
<node id="n1198"><data key="name">linear regression model</data></node>
<node id="n1199"><data key="name">notebook</data></node>
<node id="n1200"><data key="name">mapreduce</data></node>
<node id="n1201"><data key="name">requirement analysis</data></node>
<node id="n1202"><data key="name">interpreting data</data></node>
<node id="n1203"><data key="name">seasonal adjustment</data></node>
<node id="n1204"><data key="name">credit scoring matrix</data></node>
<node id="n1205"><data key="name">logistic predictive models</data></node>
<node id="n1206"><data key="name">descriptive models</data></node>
<node id="n1207"><data key="name">scoring model</data></node>
<node id="n1208"><data key="name">anomalies</data></node>
<node id="n1209"><data key="name">management studio report developer</data></node>
<node id="n1210"><data key="name">compiling data</data></node>
<node id="n1211"><data key="name">knn</data></node>
<node id="n1212"><data key="name">linear and logistic regression</data></node>
<node id="n1213"><data key="name">reports business objects</data></node>
<node id="n1214"><data key="name">plsql</data></node>
<node id="n1215"><data key="name">diagnostic skills</data></node>
<node id="n1216"><data key="name">gap analysis</data></node>
<node id="n1217"><data key="name">data compiling</data></node>
<node id="n1218"><data key="name">rstudio</data></node>
<node id="n1219"><data key="name">cytoscape</data></node>
<node id="n1220"><data key="name">data entry</data></node>
<node id="n1221"><data key="name">verifying data accuracy</data></node>
<node id="n1222"><data key="name">data verification</data></node>
<node id="n1223"><data key="name">data review</data></node>
<node id="n1224"><data key="name">data compilation</data></node>
<node id="n1225"><data key="name">report generation</data></node>
<node id="n1226"><data key="name">six sigma</data></node>
<node id="n1227"><data key="name">define functions</data></node>
<node id="n1228"><data key="name">mine data</data></node>
<node id="n1229"><data key="name">graphs reports</data></node>
<node id="n1230"><data key="name">fraud activities</data></node>
<node id="n1231"><data key="name">report builder</data></node>
<node id="n1232"><data key="name">extracting data</data></node>
<node id="n1233"><data key="name">unformatted data</data></node>
<node id="n1234"><data key="name">pattern recognition</data></node>
<node id="n1235"><data key="name">predictive analysis</data></node>
<node id="n1236"><data key="name">forecast data</data></node>
<node id="n1237"><data key="name">forecast analysis</data></node>
<node id="n1238"><data key="name">data modelling</data></node>
<node id="n1239"><data key="name">xlookup</data></node>
<node id="n1240"><data key="name">elt</data></node>
<node id="n1241"><data key="name">data discovery</data></node>
<node id="n1242"><data key="name">cloudwatch</data></node>
<node id="n1243"><data key="name">regression testing</data></node>
<node id="n1244"><data key="name">negotiation skills</data></node>
<node id="n1245"><data key="name">pattern</data></node>
<node id="n1246"><data key="name">trend identification</data></node>
<node id="n1247"><data key="name">data acquisition</data></node>
<node id="n1248"><data key="name">performing reconciliations</data></node>
<node id="n1249"><data key="name">data quality checks</data></node>
<node id="n1250"><data key="name">tabular model</data></node>
<node id="n1251"><data key="name">analytical models</data></node>
<node id="n1252"><data key="name">quickbooks</data></node>
<node id="n1253"><data key="name">peachtree</data></node>
<node id="n1254"><data key="name">reference data</data></node>
<node id="n1255"><data key="name">validation</data></node>
<node id="n1256"><data key="name">compatibility</data></node>
<node id="n1257"><data key="name">sequence diagrams</data></node>
<node id="n1258"><data key="name">business rules</data></node>
<node id="n1259"><data key="name">visualization tools</data></node>
<node id="n1260"><data key="name">statistical data research</data></node>
<node id="n1261"><data key="name">data model</data></node>
<node id="n1262"><data key="name">requirements traceability</data></node>
<node id="n1263"><data key="name">test planning</data></node>
<node id="n1264"><data key="name">nonfunctional requirements</data></node>
<node id="n1265"><data key="name">product owner</data></node>
<node id="n1266"><data key="name">data analyst</data></node>
<node id="n1267"><data key="name">data workflow</data></node>
<node id="n1268"><data key="name">collecting cleaning and interpreting data</data></node>
<node id="n1269"><data key="name">microsoft visio</data></node>
<node id="n1270"><data key="name">business performance analysis</data></node>
<node id="n1271"><data key="name">analytical problem solving</data></node>
<node id="n1272"><data key="name">business analysis</data></node>
<node id="n1273"><data key="name">database building</data></node>
<node id="n1274"><data key="name">quality testing</data></node>
<node id="n1275"><data key="name">sqlplus</data></node>
<node id="n1276"><data key="name">sqlloader</data></node>
<node id="n1277"><data key="name">data scrubbing</data></node>
<node id="n1278"><data key="name">data cleansing</data></node>
<node id="n1279"><data key="name">vbnet</data></node>
<node id="n1280"><data key="name">esri</data></node>
<node id="n1281"><data key="name">arcsde</data></node>
<node id="n1282"><data key="name">microsoft project</data></node>
<node id="n1283"><data key="name">spatial data</data></node>
<node id="n1284"><data key="name">data layers</data></node>
<node id="n1285"><data key="name">–ssis</data></node>
<node id="n1286"><data key="name">power pivot</data></node>
<node id="n1287"><data key="name">power view</data></node>
<node id="n1288"><data key="name">power map</data></node>
<node id="n1289"><data key="name">rapidminer</data></node>
<node id="n1290"><data key="name">xlminer</data></node>
<node id="n1291"><data key="name">aspnet</data></node>
<node id="n1292" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-engineer-2803c6ab603a4a0681e372b5316189e3</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Professional Summary      Energetic Data Engineer in developing robust code for highvolume businesses Strong decisionmaker with 6 years of experience in Data engineering and help firms in designing and executing solutions for complex business problems involving large scale data warehousing realtime analytics and reporting solutions Ability to translate business questions and concerns into specific quantitative questions that can be answered with available data using sound methodologies        Skills           Python 3x R SQL  Hadoop Apache Spark  Hive Pig Kafka Sqoop Oozie  Teradata Snowflake      Amazon S3EMRLambda  Git Jenkins Splunk  MS Office  Microsoft Visual CNET                       Work History       Data Engineer       042020   to   Current     Avanade    –    Bangor     ME             Combining data from multiple source systems Profile Systematics etc and multiple platforms Snowflake OneLake Hubs and computing canonical goldstar metrics “once and for all” to cut operational costs  Develop Spark jobs to transform data and apply business transformation rules to loadprocess data across enterprise and application specific layers  Experience in buildingoperatingmaintaining fault tolerant and scalable data processing integrations using AWS  Configured S3 buckets with various life cycle policies to archive the infrequently accessed data based on requirement  Good working experience on submitting the Spark jobs which shows the metrics of the data which is used for Data Quality Checking  Working on building efficient data pipelines that transform high volume data into a format used for analytical fraud prevention and ML use cases  Extensively used Splunk Search Processing Language SPL queries Reports Alerts and Dashboards  Excellent knowledge of source control management concepts such as Branching Merging LabelingTagging and Integration with tool like Git  Performing Data Quality checks like row count schema validation Hash key validation for all data movement between applications           Data Engineer       082019   to   042020     Avanade    –    Burlington     NC             Responsible for designing and developing various analytical solutions for gaining analytical insights into large data sets by ingesting and transforming these datasets in the Big Data environment using technologies like Spark Sqoop Oozie HIVE  Scheduling jobs to automate the process for regular executing jobs worked on using Oozie  Developed Oozie workflow schedulers to run multiple Hive and Pig jobs that run independently with time and data availability  Familiar with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining  machine learning and advanced data processing  All the projects which I have worked for are Open Source Projects and has been tracked using JIRA  Parallel copying of files between various clusters using Distcp Kafka in Hadoop           Data Scientist Intern       092018   to   122018     Ascend Learning    –    Memphis     TN             Acquire clean integrate analyze and interpret disparate datasets using a variety of statistical data analysis and data visualization methodologies reporting and authoring findings where appropriate  Developed Linear Mixed Effects Models for Boston Edfi dataset to estimate the teacher contribution to the student test scores  Random intercept model which utilized a preposttest design and included fixed effects for student demographics and a growth model where students were nested by time random slope and intercept and teacher was tested  Generated various Clustering models for entire West Virginia department of education and evaluated cluster’s performance           Big Data Engineer       012014   to   072017     Verizon    –                      Identify customers digital analytical needs and engage with customer and principal architects daily understand the business requirements for big data analytical solutions and break down large scale requirements into detailed system specifications  Moving data to HDFS framework using SQOOP from Teradata SQL Server  Good working experience on Hadoop tools related to Data warehousing like Hive Pig and also involved in extracting the data from these tools on to the cluster using Sqoop  Solved performance issues in Hive with understanding of joins groups bucketing partitions and working on them using HiveQL  Deployed programs written in PySpark to run Spark MLlib for analytics and reduced customers churn rate by 25          Education       Master of Science     Business Analytics     Expected in   4 2020     The University Of Texas At Dallas      Richardson     TX     GPA               Websites Portfolios Profiles        httpswwwlinkedincominJessicaClaire    httpwwwgithubcomJessicaClaire                  Skills       Python 3x R SQL  Hadoop Apache Spark  Hive Pig Kafka Sqoop Oozie  Teradata Snowflake    Amazon S3EMRLambda  Git Jenkins Splunk  MS Office  Microsoft Visual CNET         Work History       Data Engineer     042020   to   Current     Capital One Financial Corp   –   Wilmington     DE      Combining data from multiple source systems Profile Systematics etc and multiple platforms Snowflake OneLake Hubs and computing canonical goldstar metrics “once and for all” to cut operational costs  Develop Spark jobs to transform data and apply business transformation rules to loadprocess data across enterprise and application specific layers  Experience in buildingoperatingmaintaining fault tolerant and scalable data processing integrations using AWS  Configured S3 buckets with various life cycle policies to archive the infrequently accessed data based on requirement  Good working experience on submitting the Spark jobs which shows the metrics of the data which is used for Data Quality Checking  Working on building efficient data pipelines that transform high volume data into a format used for analytical fraud prevention and ML use cases  Extensively used Splunk Search Processing Language SPL queries Reports Alerts and Dashboards  Excellent knowledge of source control management concepts such as Branching Merging LabelingTagging and Integration with tool like Git  Performing Data Quality checks like row count schema validation Hash key validation for all data movement between applications           Data Engineer     082019   to   042020     Verizon Wireless   –   Tampa     FL      Responsible for designing and developing various analytical solutions for gaining analytical insights into large data sets by ingesting and transforming these datasets in the Big Data environment using technologies like Spark Sqoop Oozie HIVE  Scheduling jobs to automate the process for regular executing jobs worked on using Oozie  Developed Oozie workflow schedulers to run multiple Hive and Pig jobs that run independently with time and data availability  Familiar with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining  machine learning and advanced data processing  All the projects which I have worked for are Open Source Projects and has been tracked using JIRA  Parallel copying of files between various clusters using Distcp Kafka in Hadoop           Data Scientist Intern     092018   to   122018     Hoonuit   –   Minneapolis     MN      Acquire clean integrate analyze and interpret disparate datasets using a variety of statistical data analysis and data visualization methodologies reporting and authoring findings where appropriate  Developed Linear Mixed Effects Models for Boston Edfi dataset to estimate the teacher contribution to the student test scores  Random intercept model which utilized a preposttest design and included fixed effects for student demographics and a growth model where students were nested by time random slope and intercept and teacher was tested  Generated various Clustering models for entire West Virginia department of education and evaluated cluster’s performance           Big Data Engineer     012014   to   072017     Tata Consultancy Services   –              Identify customers digital analytical needs and engage with customer and principal architects daily understand the business requirements for big data analytical solutions and break down large scale requirements into detailed system specifications  Moving data to HDFS framework using SQOOP from Teradata SQL Server  Good working experience on Hadoop tools related to Data warehousing like Hive Pig and also involved in extracting the data from these tools on to the cluster using Sqoop  Solved performance issues in Hive with understanding of joins groups bucketing partitions and working on them using HiveQL  Deployed programs written in PySpark to run Spark MLlib for analytics and reduced customers churn rate by 25</data><data key="id">280519406331163022609303386259277165013</data></node>
<node id="n1293" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-engineer-3fb6a76ff6b44876ada5d6b8ca0e51f7</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Involving in various projects related to Data Modeling Data Analysis Design and Development for Data warehousing environments Practical understanding of the Data modeling Dimensional  Relational concepts like StarSchema Modeling Snowflake Schema Modeling Fact and Dimension tables Comprehensive knowledge and experience in process improvement normalizationdenormalization data extraction data cleansing data manipulation Exceptional troubleshooting skills with ETL Technologies        Highlights         Hadoop Hive Avro Kafka MapReduce Looker Programming Languages SQL Java Scala PHP Shell Script HTML Database Tools Aster Database PostgreSQL MySQL Operating Systems Linux Unix Microsoft Windows                       Accomplishments              Experience       Data Engineer       082012      Current     Avanade    –    Greenville     NC            Experience with full development cycle of a Data Warehouse including requirements gathering design implementation and maintenance  Data modeling based on Kimball methodology developed and architected ETL processes for different projects RMA inventory purchase order etc Reengineer some of the current ETL processes to streamline the data acquisition and integration process using our homegrown ETL tools  Built eventdriven data pipeline that comprises multiple steps to gather high volume and velocity data from both push based and pull based sources which includes design and implement web service to collect data JSON object over http request convert data in JSON format into Avro then feed into Kafka land data in Kafka on Hadoop and Aster  Designed and built Looker API using Scala which makes other teams access the data in data warehouse more easily and gracefully  Establish and maintain SQL queries and routines  Write adhoc queries based upon the schema understanding for diverse needs of our business users           Java Intern       022012      032012     Dominion Enterprises    –    Fredericksburg     VA            Implemented code for small features  bugfixes in Java           PHP Developer       062011      092011     Meetoncruise    –    City     STATE            Implemented the backend logic using PHP  Utilized JQuery and AJAX to provide dynamic and interactive user interface  Designed and implemented data model in MySQL database to support the website          Education       Master of Science       Electrical and Computer Engineering       Expected in   2012                Polytechnic Institute of New York University      Brooklyn     NY     GPA        Status         Electrical and Computer Engineering         Bachelor of Science       Automation Engineering       Expected in   2010                Nanjing University of Aeronautics and Astronautics      Nanjing     Jiangsu     GPA        Status         Automation Engineering        Skills     streamline ad AJAX API data acquisition Data modeling data warehouse Database engineer ETL features HTML http PHP inventory Java JQuery JSON Linux logic access Microsoft Windows MySQL Operating Systems PostgreSQL processes Programming requirements gathering Shell Script SQL Unix user interface website</data><data key="id">101004532999732917696975093886375431500</data></node>
<node id="n1294" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary      Over 5 years of engineering and finance experience as a Data Developer with cross platform integration experience using Hadoop and Spark architecture Handson experience configuring as well as installing Hadoop Ecosystem  HDFS MapReduce Pig Hive Oozie Flume HBase Spark Sqoop Flume and Oozie Strong understanding of various Hadoop services MapReduce and YARN architecture Vast knowledge in importing as well as exporting data in HDFS using SQOOP Automated transfer of data from Hbase by developing Map Reduce jobs Expertise in analysis using PIG HIVE and MapReduce Experience in HDFS data storage and support for running mapreduce jobs Involved in Infrastructure set up and installation of HDP stack on Amazon Cloud Experienced with ingesting data from RDBMS such as SQL Teradata into HDFS using Sqoop and Oracle Expertise in Hadoop architecture HDFS Mapreduce Oozie Sqoop Spark Hive Zookeeper and NoSQL databases Deployed and configured clusters in Cloudera Manager Set up backups and disaster recovery for Data Node and Name Node metadata as well as sensitive data on clusters Expertise in implementing and designing HDFS access controls directory and file permissions user authorization that facilitates stable secure access for multiple users in a large multitenant cluster Knowledge on exporting as well as importing stream data into HDFS using Flume Spark and Kafka messaging systems Utilized various schedulers on the Job tracker to share resources within clusters for Map Reduce jobs such as FIFO scheduler Fair Scheduler and Capacity Scheduler Monitored jobs with YARN and provisioned configured and installed HBase Kafka Hive Oozie Sqoop Ranger Storm Flume Spark as well as maintained total architecture AWS services for cloud migration such as S3 Redshift EMR Glue Athena and DynamoDB Expertise in Vertica DB architecture High Availability and column orientation Great knowledge of Agile and Scrum methodologies Behavior Driven Development Domain Driven DesignTest Driven Development and continuous integration as well as delivery Skilled in defining functional and gathering user interface requirements for applications and websites Expertise in real time analysis by utilizing RDD Datasets Data Frames and Streaming API in Apache Spark Expertise in using Spark Resilient distributed datasets and dataframe APIs over the Cloudera platform to perform analytics on Hive data by integrating Hadoop with Kafka Expertise in uploading Click stream data from Kafka to HDFS Expert in utilizing Kafka for messaging and publishing subscriber based messaging systems  Worked with NoSQL databases such as Cassandra HBASE PostgreSQL MongoDB Redis DynamoDB        Skills           SQL transactional replications  SAN technologies  Reverse engineering skills  Warehouse models  Security Protocols  Enterprise information architecture      Quality analysis  Deep learning  Data mining  Data analytics  Critical thinking                       Experience       Data Engineer       072021   to   Current     Principal Financial Group    –    Pittsburgh     PA            Analyze design and build Modern data solutions using Azure PaaS service to support visualization of data  Understand current Production state of application and determine the impact of new implementation on existing business processes  Extract Transform and Load data from Sources Systems to Azure Data Storage services using a combination of Azure Data Factory TSQL Spark SQL and USQL Azure Data Lake Analytics Data Ingestion to one or more Azure Services  Azure Data Lake Azure Storage Azure SQL Azure DW and processing the data in In Azure Databricks  Created Pipelines in ADF using Linked  ServicesDatasetsPipeline to Extract Transform and load data from different sources like Azure SQL Blob storage Azure SQL Data warehouse writeback tool and backwards  Developed Spark applications using Pyspark and  SparkSQL for data extraction transformation and  aggregation from multiple file formats for analyzing  transforming the data to uncover insights into the  customer usage patterns  Responsible for estimating the cluster size monitoring  and troubleshooting of the Spark databricks cluster  Experienced in performance tuning of Spark  Applications for setting right Batch Interval time  correct level of Parallelism and memory tuning   Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Contributed to maintaining  Type  databases in conjunction with data development and software engineering teams  Assisted solution providers with definition and implementation of technical and business strategies  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Data Engineer       072020   to   062022     Principal Financial Group    –    Portland     OR             Worked on MongoDB by using CRUD Create Read Update and Delete Indexing Replication and Sharding features  Involved in designing the row key in HBase to store Text and JSON as key values in the HBase table and designed row key in such a way to getscan it in sorted order  Oozie was integrated with the Hadoop stack which consists of MapReduce Hive Sqoop and Pig and Unix shell scripts  While working on Hive tables used Hive QL designed and Implemented PartitioningStatic Dynamic Buckets on Hive  Performed Cache and Persist as well as Checkpointing when utilizing Spark Streaming APIs to build common learning data models to get near realtime data from Kafka and persisted into Cassandra  Conducted cluster coordination services with Zookeeper and monitored the workload capacity planning and job performance through Cloudera Manager  Built applications by utilizing Maven and integrated with Continuous Integration servers such as Jenkins to build jobs  Deployed maintained and configured Test and multinode Dev Kafka Clusters as well as handled clusters and implemented data ingestion for real time processing in Kafka  Created cubes in Talend for various aggregation types of data from PostgreSQL and MS SQL server to visualize data  Monitored Name Node health status in Hadoop as well as the number of Data Nodes and Task trackers running along with automating jobs to pull data from various MySQL data sources to push result set data to HDFS  Created story telling dashboards through Tableau Desktop to publish on Tableau Server and integrated GitHub for version control tools to maintain the versions in projects  Deployed Spark applications in python and utilized Datasets and DataFrames in Spark SQL for processing data faster  Loaded transactional data with Sqoop from Teradata created managed and external tables in Hive and worked with semistructured and structured data of 5 Petabytes in size  Constructed MapReduce jobs to validate clean and access data and worked with Sqoop jobs with incremental load to populate and load into Hive External tables  Designed strategies to optimize distribution of weblog data over clusters in addition to exporting and importing stored web log data into Hive and HDFS through Sqoop  Responsibilities of building scalable data solutions that are distributed through Hadoop and Cloudera as well as developed and designed automated test scripts in Python  Integrated Apache Storm with Kafka to perform web analytics and to perform clickstream data from Kafka to HDFS  Developed SQL scripts and designed solutions to implement Spark with Hive Generic UDFs for incorporating business logic within Hive queries  Developed data pipelines in AWS using S3 EMR Redshift to extract data from weblogs to store into HDFS  Transmitted streaming data from Kafka to HBase Hive and HDFS by integrating Apache Storm and wrote Pig scripts for transforming raw data from various data sources to form baseline data  Participated in Agile meetings Ford Credit Customer Data domain conducted daily scrum meetings and spring planning  Expanded and optimized data pipelines and architecture as well as optimized data flow and collection  Created pipelines from scratch using PySpark and scheduled jobs using Airflow  Stream processed data in Kafka streaming wrote Producer Consumer Connector and Streams API to handle stream of records subscription of topics consume input and build reusable producers and consumers  Worked with unstructured datasets such as IoT sources sensor data XML and JSON document sources  Worked with on prem clusters as well as clusters on the cloud and used GCP Big Query Data Fusion and DataFlow  Scaled up architecture with Google Kubernetes and set up load balancing  Worked on various optimization techniques in Spark such as cache and persist using accumulators  bucketing and partitioning garbage collection tuning data serialization windowing functions and broadcast variables  Used numerous Spark transformations such as groupByKeyreduceByKey flatMap filter sample union etc  Utilized Dataproc to spin up clusters Dataprep for data analysis and Dataflow for streaming data using Apache Beam  Dealt with VPC controls on Google Cloud Platform and CMEK encryption for data security  Environment Hadoop HDFS HBase SparkPython and Scala Azure Databricks Scala Hive Kafka MapReduce Sqoop ETL Java Python PostgreSQL SQL Server Teradata UnixLinux           Big Data Developer       052017   to   052020     Cognizant Technology Solutions    –    Mount Laurel     NJ             Performed query tuning in HiveQL as well as performance tuning transformations in Pyspark using Spark RDDs and Python  Used lambda functions to create a Serverless Data intake pipeline on AWS  Using python constructed a Spark Streaming pipeline to receive realtime data from Apache Kafka and store it in DynamoDB  Implemented Apache Spark data processing module to handle data from multiple RDBMS and Streaming sources then compiled Apache Spark applications using Scala and Python  Extensive experience designing and scheduling multiple Spark Streaming  batch Jobs in Python pyspark and Scala  Achieved highthroughput scalable faulttolerant stream processing of live data streams using Apache Spark Streaming  Involved with the use for creating and saving data frames using various Python modules with pyspark  Sqooped data and performed Hive queries for data ingestion from relational databases to analyze historical data  Experienced with Elastic MapReduce EMR as well as setting up environments on amazon AWS EC2 instances for pipelines in AWS  Expertise in handling Hive queries using Spark SQL such as window functions and aggregations  Ran Spark applications on Docker using EMR and used AWS Glue data catalog as the metastore in Spark SQL  Configured different File Formats like Avro parquet for HIVE querying and processing based on business logic  Utilized Sequence files RC files Map side joins bucketing partitioning for Hive performance enhancement and storage improvement  Implemented Hive UDF to implement business logic and performed extensive data validation using Hive  Involved in loading the structured and semi structured data into spark clusters using Spark SQL and Data Frames API  Utilized AWS CloudWatch to monitor the performance environment instances for operational and performance metrics during load testing  Scripting Hadoop package installation and configuration to support fully automated deployments  Involved in chefinfra maintenance including backupsecurity fix on Chef Server  Deployed application updates using Jenkins  Installed configured and managed Jenkins  Triggering the SIT environment build of the client remotely through Jenkins  Deployed and configured Git repositories with branching forks tagging and notifications  Worked on MongoDB database concepts such as locking transactions indexes Shading replication schema design  Viewing the selected issues of web interface using SonarQube  Developed a fully functional login page for the companys user facing website with complete UI and validations  Installed Configured and utilized AppDynamics Tremendous Performance Management Tool in the whole JBoss Environment Prod and NonProd  Installed and installed Hive in a Hadoop cluster and assisted business usersapplication teams in finetuning their HIVE QL for optimal performance and efficient use of cluster resources  Utilized Oozie workflow for ETL Process for critical data feeds across the platform  Configured Ethernet bonding for all Nodes to double the network bandwidth  Configured Kerberos Security Authentication protocol for existing clusters  Constructed the use of Zookeeper failover controller ZKFC and Quorum Journal nodes for high availability for significant production clusters and automatic failover controller created  Installation and deployment of many Apache Hadoop nodes on an AWS EC2 system as well as development of Pig Latin scripts to replace the old traditional process with Hadoop and data feeding to AWS S3  Experience with AWS CloudFront including the creation and management of distributions that provide access to an S3 bucket or an HTTP server running on EC2 instances  Developed Python scripts UDFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop And Developed enterprise application using Python  Constructed Spark application performance optimization including determining the appropriate Batch Interval time Parallelism Level and Memory Tuning  Experience and handson knowledge in Akka and LIFT Framework  Used PostgreSQL and NoSQL database and integrated with Hadoop to develop datasets on HDFS  Environment HDFS Map Reduce Hive 110 Kafka Hue 390 Pig Flume Oozie Sqoop Apache Hadoop 26 Spark SOLR Storm Cloudera Manager Red Hat MySQL Prometheus Docker Puppet YARN SparkSQL Python Amazon AWS Elastic Search Tableau Linux  Key Skills SQL Apache hive Apache SparkDatabricks Jupyter Notebook Anaconda Python Django Pandas Flask Keras NumPy Scikitlearn MatPlotLib Tensorflow  Time Series Forecasting AB testing Bayesian methods PowerBI MicrosoftWord Excel Powerpoint Java Data Visualization Analytical Skills Cost Accounting Corporate Finance Knowledge Statistical AnalysisRStudio          Education and Training       Bachelor’s     Computer Science     Expected in        Uttara Institute of Business and Technology                GPA</data><data key="id">26624075482424204881256882949061507743</data><data key="url">https://www.livecareer.com/resume-search/r/data-engineer-71257baffbce48d4a96368645e3008da</data></node>
<node id="n1295" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-engineer-b9cc3bb45c274243bede854fa43fe37c</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      I am an Automation Developer with a love for troubleshooting and electronics This passion for automation started at Micro Center in 2017 where I began finding ways to simplify my reports to upper management These reports took several hours to compile and took the time I could have spent guiding my agents Eventually I started finding ways to gather that data through an API After discovering I could collect this information and use APIs to simplify tasks I started programming software to do these things This new skill helped change how I approached my teams problems  complaints and eventually led to me creating tools to improve our efficiency and productivity Since then my passion has grown into a career that keeps me wanting to learn more        Skills           Systems Engineering  Lab Test Technician  Cybersecurity  Data Engineering      API Design  Deployment  Software Developer Python NodeJS PHP Bash Git Etc  Automation  Disaster Recovery                       Experience       Data Engineer       062021      112023     Bank Of America Corporation    –    Aurora     IL            Deployed Linux cloud servers and applications to meet the needs of the company PostgreSQL Metabase Apache Spark TableAU  Secured servers and applications using ACLsﬁrewallsIDSIPS and following standard security practices  Built several ETLs in PythonPHPJavaScript for several ticketing platforms  Designed disaster recovery plans for our systems  Constructed complex SQL queries to aid in reporting and automation  Tested data regularly to ensure accurate reporting  Managed several PostgreSQL databases  Automated reports and redundant tasks through the use of APIs and databases  Created and implemented complex business intelligence solutions  Created conceptual logical and physical data models for use in different business areas  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified protected and leveraged existing data  Planned and installed database management system software upgrades to enhance systemic performance           Test Engineering Technician       082020      062021     General Atomics    –    Houlka     MS            Traveled daily to integrators to provide quality assurance for PowerSpec systems and review the build process  Compiled reports for integrators to log and classify defects  Answered product questions for PowerSpec systems  Regularly assisted neighboring departments on build projects and quality controlled parts for buyers  Worked directly with vendors for BIOS updates and patches on PowerSpec systems  Created and deployed system images for PowerSpec products  Built extensive QC reports through rigorous testing methodologies including benchmarking components and testing for compatibility  Performed problem solving and resolution for technical quality inspection and customer quality issues  Developed and maintained solutions for integration and testing phases  Tested functionality performance and compliance of each product against design specifications to maintain strong development standards and high customer satisfaction  Completed unit and regression tests on software and individual modules  Created and optimized automated testing tools for repetitive tasks  Created and maintained database of common and known testing defects  Worked with offsite teams to complete timely tests and facilitate smooth product releases  Promoted high customer satisfaction by resolving problems with knowledgeable and friendly service           Call Center Lead       012019      082020     Metlife    –    Omaha     NE            Encouraged team members to improve productivity and service levels by modeling correct behaviors and coaching employees  Resolved team support issues with efficient approach to keep call center operating smoothly and customers satisfied with services  Routinely updated guides to reduce questions on the ﬂoor  Provided accurate and detailed reports for management  Handled customer escalations and ensured they went to the appropriate team  Maintained Zendesk and the ticketing procedures to ensure both a smooth and efficient experience for my agents  Worked with management to update legacy procedures and remove redundant processes through automation  Managed customer concerns with calm demeanor and knowledgeable service  Created an efficient workflow in order to provide a better experience for consumers  Kept records of customer interactions or transactions thoroughly recording details of inquiries  Worked directly with stores and customer relations team to improve customer retention  Built and maintained call center server to house reports and automations           Technical Consultant       092017      012019     Applied Systems Inc    –    University Park     IL            Troubleshot and resolved problems with programs and systems  Utilized knowledge of applications programming and systems functionality to assist employees with technical needs  Assisted customers with various types of technical issues via email live chat and telephone  Handled customer service issues by providing guidance or escalating for advanced support  Served as first point of contact for escalated technical service calls emails and live chat  Troubleshot hardware issues and worked with service providers to facilitate repairs for end users  Developed and maintained strong relations with customers to meet quality expectations  Documented customer complaints and inquiries for use in technical documentation and bug tracking  Reviewed support cases for technical and troubleshooting accuracy and identified needed process improvements  Maintained uptodate case documentation for future reference  Demonstrated advanced product knowledge to solve customer issues  Delivered remote assistance for technical issues using screen sharing mouse and keyboard control and other tools          Education and Training       High School Diploma       Electronic Classroom       Expected in   062012                Electronic Classroom of Tomorrow      Columbus OH          GPA        Status   </data><data key="id">272474282630622662458174313553127824080</data></node>
<node id="n1296" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary       Data Engineering  with experience in Design Development Implementation and support of  Data Warehousing  for over  8 years  Experienced in complete  Software Development Life Cycle SDLC   software Testing Life Cycle STLC   SDLC methodologies  Extensively worked on  Informatica Designer Tools  Source Analyze Warehouse Designer Mapping Designer Mapplet Designer and Transformation Developer and Workflow Manager Tools Task Developer Worklet and Workflow Designer Workflow Monitor and informatica Power exchange Experience in working with using Informatica in  SAP HANA   Oracle   MS SQL   Teradata  and  DB2 environments  Hands on experience in migrating on premise ETL to  Google Cloud PlatformGCP  using cloud native tools such as  BIG Query   Cloud Data Proc   Google Cloud Storage   Composer   Practical understanding of the Data modeling concepts like  Star  Schema Modeling snowflake Schema Modeling Fact and Dimension tables  Also experience in Optimizing Database querying data manipulation using SQL and  PLSQL  in Oracle flat files and SQL server database Experienced in implementing  Change Data Capture CDC  using informatica Power Center for Oracle and SAP Systems Experienced in debugging mapping by analyzing the data flow and evaluating transformations Experienced in performance tuning of data flow through source target sessions and mappings  identifying and resolving performance bottlenecks at various stages  using techniques like Database tuning and Session Partitioning Experienced in test strategy developing test plan details test cases and writing test scripts by decomposing business requirements and developing test scenarios to support quality deliverables Involved in test planning and execution for various Test Phases  Unit Test   System and User Acceptance Testing  Expert in  analyzing designing developing installing configuring  and  deploying  MS SQL Server suite of products with Business Intelligence in SQL Server Reporting Services SQL Server Analysis Services and SQL Server Integration Services Performed data profiling data cleansing data conversion exception handling and data matching using  informatica IDQ   Excellent communication skills good organizational skills selfmotivated hardworking ability to grasp quickly and learn fast and open to new technologies         Skills  Tools           ETL Tools Informatica Power Center 1051 Power Exchange 961901 Informatica Data Quality 961 Power connect for SAP BW power connect for JMS power connect for IBM MQ series power connect for Mainframes DTS MDM ERWIN  Scheduling TIDAL UC4 CONTROLM AUTOSYS OpCon  Oracle SAP HANA MS SQL Server Snowflake MongoDB Teradata DB2 AWS  Python Java SQL UNIX Unix Shell Scripts HTML XML JSON Microsoft Office Apache Spark Kafka Kubernetes Hive Scala  Data analysis Data management Data warehouse Big Data PLSQL RDBMS NoSQL Vertica Spark Kafka Oozie Maven      Debugging Coding Designing Quality analysis ETL SAP BW  AWS Cloud Tools EC2 Elastic Loadbalancers Elastic Container Service Docker Containers S3 Elastic Beanstalk Cloud Front Elastic Files System RDS Dynamo DB DMS VPC Direct Connect Route53 Cloud Watch Cloud Trail Cloud Formation IAM EMR ELB Lambda functions REST API Airflow Data Pipeline RedShift  Google Cloud Platform GCP Cloud Storage Big Query Composer Cloud Dataproc Cloud SQL Cloud Functions Cloud PubSub Dataflow AI Building Blocks Looker Cloud Data Fusion Dataprep Firestone  Azure Azure Storage Database Azure Data Factory Azure Analysis Services                       Experience       Data Engineer       082019      Current     Lockheed Martin    –    Eagan     MN            Responsible in Identifying all the Legacy systems analyze their  data models  mathematical and  Scientific models and all the business components to be migrated to Arkansas Integrated Eligibility system ARIES  NexGen solution   Business and Data Mapping Analyst worked with Client to gather business and functional requirements  Experience in building largescale data pipelines and datacentric applications using Big Data tooling like Hadoop Spark Hive and Airflow in a production setting  Experience in working with Rest APIs for data extraction  Designing and coordinating with  Informatica Power center  admin to set up services users Groups database components like tables indexes procedures and synonyms Unix groups and User access for ARIES system  Designing all the components of ARIES system and step by step conversion approach using the  Ralph Kimball  and  Bill Inmom  data warehouse design methodologies to determine the feasibility of the design within the time and cost constraints  Development of  ETL  Extract Transform and Load code components like mappings Workflows Sessions and database objects like stored procedures functions and  Unix shell scripts  from business requirements and design plan  Designing and developing the mathematical models and analytical reports in  Cognos  analyze  Developing integrations using  Informatica Cloud  Data Integration  IICS  –  CDI  Service  For Google Analytics  Responsible Identifying the performance bottleneck in the ARIES systems during Integrated  Testing  IT and  System Testing  ST and PT  Performance Testing  phases and implement Performance tuning techniques  Confer with the scrum master client partners Business Managers and adhere to the  agile   Experience in multiple database technologies such as traditional RDBMS MS SQL Server Oracle MySQL PostgreSQL MPP AWS Redshift Snowflake Teradata Distributed Processing Spark Hadoop EMR NoSQL MongoDB DynamoDB Cassandra Neo4J Titan  Conduct code review sessions with peer developers to ensure code quality  Wrote scripts and processes for  data integration  and bug fixes  Utilized  Python  to handle debugging and automation scripting tasks  Created and implemented complex business intelligence solutions  Create  Managing buckets on  S3  and store DB and log backup upload images for CND server  Setup databases on  Amazon RDS  or  EC2 Instances  as per requirements  Handson experience with snowflake utilities SnowSQL SnowPipe Big data model techniques using python  Expert in migrating data from various systems into Salesforce CRM using ETL tools   Informatica   Hands on Experience in  Data Management Data Security Data Modeling Data Quality Workflow Automation Formulas  Validations   Collaborated with Legacy team to define data extraction methodologies and data source tracking protocols  Used  SparkSQ L to read parquet data and create tables in  Hive  using  Scala API   Hands on experience with data ingestion tools like  Sqoop Kafka Flume   Used various  spark transformations  and  Actions  for cleansing the input data  Installed and configured  Apace Airflow  for workflow management and created workflow in python  Worked on Tableau Desktop versions 782 Tableau Reader and Server  Build data pipelines in Airflow in GCP for ETL related jobs using different airflow operators  Experience in GCP Dataproc GCS Cloud functions BigQuery           BIETL Developer       112018      072019     Wells Fargo    –    Loveland     CO            Analyzing the source data coming from Mainframe sources and working with business users and developers to develop the Model  Created xsd and used it in  xml generator Transformation   Also called  WSDL  files using web consumer transformation in  Informatica   Effectively and efficiently communicated systems solutions to business problems to team members business unit representatives management and other impacted project teams  Analyze and modify existing stored procedures functions and queries in order to integrate them into previously built reporting application utilizing  SSIS  and  SQL Server Management Studio   Change ETL process from  importing flat files  and  COBOL  as source for previously built application utilizing informatica Power center 96 to read data from existing Reporting Server SQL Server databases  Interface with report users to determine requirements for new and existing reports  Develop test and deploy  ETL  jobs with reliable errorexception handling and rollback framework Manage automation of file processing as well as all ETL processes within a job  workflow   Worked as a Data Analysts to match the current data mappings with old mainframe mappings  Created Design Documents Context Diagrams on every Projects and did code review  Created Reusable lookup to send emails Notification to Users  This Lookup was used by multiple developer in multiple Projects  STOP and START the  Netezza  appliances in case of issues  Prepared best practices documentation for teams in writing  NOSQL   Conducted sessions to help explain teams about the  Netezza  architecture and design  As a part of Informatica decommission objects Project Used  SVN  to archive the ETL Code and delete the unwanted mappings and workflows  Extracted data from  flat file  and staged into a single place and applied business logic to load them in the  Teradata database   Lead in few complex Projects and Performed Unit testing and created  QA documents   Participated in solution brainstorming and provided technical instruction and coaching to others within organization  Developed technical project deliverables  Created spreadsheets using  Power BI  and  Power Pivot  by importing data from the sources directly  Created visually impactful dashboards in Excel and  Tableau  for data reporting using  PivotTables  and  VLOOKUP   Involved in migrating ETL code lower environments to like dev testLoad to production environment  Used  Debugger  for debugging Mappings  Created  Tidal  Jobs and  Runbooks  to schedule jobs in Tidal  Scheduling ETL Jobs using  UC4 scheduler   Migrated ETL code using Deployment Groups from Dev to Prod environments  Created SSIS package for loading the data coming from various interfaces like OMS Orders Adjustments and Objectives and also used multiple transformation in SSIS to collect data from various sources  Worked on SSIS Package DTS ImportExport for transferring data from Database Oracle and Text format data to SQL Server  Created  SSIS  packages for File Transfer from one location to the other using FTP task  Manage and document our platform infrastructure This can go from installing a new Consul server to resolving performance issues in a MongoDB cluster through setting up a continuous integration pipeline           Informatica Developer       122017      102018     Cognizant Technology Solutions    –    Horsham     PA            Involved in all the phases of the development like Analysis Design Coding  Unit Testing  System Testing and UAT  Extracted data from  heterogenous sources  and performed complex transformations to load data into the target systems  Resolved various performance issues by examining the logs current design and  removing the bottlenecks   Created reusable ETL components which need to be run at the mapping session and workflow levels  Wrote complex SQL queries and performed extensive data analysis in  Oracle 11g   Peer reviewed developers code and ensured they fall into the enterprise guidelines  Worked extensively with session parameters Mapping Parameters Mapping Variables and Parameter files for Incremental Loading  Created Informatica mappings to load Payment flow BT cash data Currency conversion data from  SAP HANA DSL layer  to  SAP HANA DPL layerDLL SAP BA layer  Implemented SAP BW and  BOBJ  with different SAP data source  Used  Qlik  and Informatica for  ETL   reporting  Worked closely with  SAP ABAP  team to create logical systems RFC destinations and to create tRFC port for RFC destinations  Followed best practices that were defined at the enterprise level and also peer reviewed the code for the same  Created and reviewed scripts to create new tables queries for new enhancements and bug fixes in the existing  data warehouse   Used Debugger and various other techniques like tracing to fix the defects errors and data issues  Extensively worked with various Lookup caches like  Static cache Dynamic cache and Persistent cache   Involved in developing the Deployment groups for deploying the code between various environment Dev QA  Worked in the Data Integration Team to perform data and application integration with a goal of moving more data more effectively efficiently and with high performance to assist in businesscritical projects coming up with huge data extraction  Migrated data from SQL Server to  Netezza using NZMigrate utility   Experience in integration of various data sources like  Oracle DB2 SQL server csv XML and Flat Files  into staging area  Developed code to extract transform and load ETL data from inbound flat files and various databases into outbound  flat files  and  XML  files using complex business logic  Involved in deploying objects from DEV to UATPROD during monthlyquarterly releases  Developed Slowly Changing Dimension Mappings for  Type 1  SCD  and  Type 2 SCD  Monitored and improved query performance by creating views indexes and sub queries Extensively involved in enhancing and managing Unix Shell Scripts  Developed workflow dependency in  Informatica  using Event Wait Task Command Wait  Involved in  L3 Support  by fixing load failures and defects during peak and offPeak hours  Working with Informatica and  SAP ECC   We are extracting data in Informatica from  SAP ECC  via  Business Content Data sources  Design the ETL architecture for the conversion of source from legacy to new  ERP SAP   Built Sqoop scripts to extract data from the SAP  Responsible for data integration  Epicor and SAP  and data management  Validated DW data with standard  SAP reports            ETL Informatica Developer       082013      122015     ObjectOne Information Systems    –    City     STATE           · Responsible for developing ETL mappings for the reporting requirement  for data feeds  · Created mapplets and many other reusable components reduce redundancy  · Provided the best solution for their requirement Help the Clients in designing of the system and process to make it more robust and meaningful  · Created the ETL component and test scripts Involved in client meetings for their inconsistent requirements  · Sourced data from  SAP HANA  using  SAP Adapter  in Informatica Target Tables  · End to end processing from the source to target Load the data from various File systems to the  DWH  and  DM  Maintain the Versioning for the objects by using  VSS   · Successfully conducted Load Testing and Performance Testing  · Resolve the issues coming from the end to end processing Perform the enhancements if required by the business users requirement  for data feeds  · Involved in preparing Source to Target mappings and Application Design Document Worked closely with all upstream and downstream application owners to make sure interface agreement documents are clear  · Successfully conducted Load Testing and Performance Testing Automated regression test suite for actuate reporting and true portal  · Used  ALM  to track and report system defects and bugs writing modification request for the bugs in the application and helped developers to track the problem and resolve the technical issues         Education and Training       Master of Science       Computer And Information Systems       Expected in   122017                New England College      Henniker     NH     GPA        Status   </data><data key="id">60738515287105690973581748617742576500</data><data key="url">https://www.livecareer.com/resume-search/r/data-engineer-2c2f724ef72e40e2b35028b711268d0e</data></node>
<node id="n1297" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-engineer-e1345820d6384278b986c6b6dfd4e897</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary       Over 3 years of professional IT experience in  Data Engineering  and  Data Analytics  using various languages and tools like  SQL Python BigHadoop   Extensive experience on Data Engineering field including Ingestion Datalake Datawarehouse Reporting and Analytics  Strong knowledge and experience on Data Analysis Data Lineage Big Data pipelines Data quality Data Reconciliation Data transformation rules Data flow diagram including Data replication Data integration and Data orchestration tools  Knowledge and experience on AWS services like RedshiftRedshift spectrumS3GlueAthena Lambdacloudwatch and EMRs like HIVE Presto  Extensively used ETL methodology for performing Data Migration Extraction Transformation and loading using  Talend  and designed data conversions from wide variety of source systems  Experienced in Data Ingestion projects to inject data into  Data lake  using multiple sources systems using Talend Bigdata  Good technical Skills in  SQL Server   ETL  Development using  Informatica tool   Expertise in writing  SQL   PLSQL  to integrate of complex OLTP and OLAP database models and data marts worked extensively on  Oracle SQL SERVER   Experience in all the life cycle phases of the projects on  large data sets  and experience with performance  tuning  and  troubleshooting   Ability to work effectively with associates at all levels within the organization  Strong background in mathematics and have very good analytical and problemsolving skills         Skills           Big Data Ecosystems  HDFS MapReduce Spark Kafka Hive Airflow Stream Sets HBase Flume Zookeeper Nifi Sentry Ranger  Scripting Language  Python PowerShell Scripting Pig Latin HiveQL  Cloud Environment  Amazon Web Services AWS Microsoft Azure  NoSQL Database  Database  MySQL Oracle Teradata MS SQL SERVER PostgreSQL DB2  Version Control  Git SVN Bitbucket  ETL Tools  Tableau Microsoft Excel Informatica Power BI R Google Data Studio                        Experience       Data Engineer       012022      Current     Honeywell    –    Michigan     ND            Worked on Architecture Design for Multistate implementation or deployment  Implement One time Data Migration of Multistate level data from SQL server to Snowflake by using Python and SnowSQL  Day today responsibility includes developing ETL Pipelines in and out of data warehouse develop major regulatory and financial reports using advanced SQL queries in snowflake  Stage the API or Kafka Datain JSON file format into Snowflake DB by FLATTENing the same for different functional services  Build Docker Images to run airflow on local environment to test the Ingestion as well as ETL pipelines  BuildingMaintaining Docker container clusters managed by Kubernetes Utilization of Kubernetes and Docker for the runtime environment of the CICD system to build test and deploy  Created Airflow DAGs to schedule the Ingestions ETL jobs and various business reports  Created  Airflow  Scheduling scripts in Pythonx  Cluster capacity planning along with operations team and management team and Cluster maintenance as well as creation and removal of nodes  HDFS  support and maintenance  Strong knowledge of  Rack awareness  topology in the  Hadoop cluster   Involved in Loading data from  LINUX file system  to  Hadoop Distributed File System   Responsible for building scalable distributed data solutions using  Hadoop   Experience in managing and reviewing  Hadoop log files   Data migration from  RDMS  to  Hadoop  using  Sqoop  for analysis and implemented  Oozie  jobs for automatic data imports from source  Created  HBase  tables to store various data formats of  PII  data coming from different portfolios  Strong in Exporting the analyzed and processed data to the  Relational databases  using  Sqoop  for visualization and for generation of reports for the team           Data Engineer       012021      122021     Honeywell    –    Nashville     TN            Prepared ETL design document which consists of the database structure change data capture Error handling restart and refresh strategies  Worked with different feeds data like JSON CSV XMLDAT and implemented Data Lake concept  Developed Informatica design mappings using various transformations  Designing and building multiterabyte full endtoend Data Warehouse infrastructure from the ground up on Confidential  Redshift  for large scale data handling Millions of records every day  Optimizing and tuning the  Redshift  environment enabling queries to perform up to 100x faster for Tableau and SAS Visual Analytics  Wrote various data normalization jobs for new data ingested into  Redshift   Advanced knowledge on Confidential  Redshift  and MPP database concepts  Migrated on premise database structure to Confidential  Redshift  data warehouse  Developed UDF in Scala to implement the business logic  Developed spark applications in Scala on distributed environment to load huge number of JSON files with different schema in to Hive tables  Most of the infrastructure is on AWS used   AWS EMR  Distribution for Hadoop   AWS S3  for raw file storage   AWS EC2  for Kafka  Used  AWS Lambda  to perform data validation filtering sorting or other transformations for every data change in a DynamoDB table and load the transformed data to another data store  Created  Airflow  Scheduling scripts in Python  Programmed  ETL functions  between Oracle and Amazon Redshift           SQL Developer       062019      122023     Ihs Markit    –    Southfield     MI            Gathered business requirements and converted them into new TSQL stored procedures in visual studio for database project  Performed unit tests on all code and packages  Analyzed requirement and impact by participating in Joint Application Development sessions with business client online  Performed and automated SQL Server version upgrades patch installs and maintained relational databases  Performed front line code reviews for other development teams  Modified and maintained SQL Server stored procedures views adhoc queries and SSIS packages used in the search engine optimization process  Updated existing and created new reports using Microsoft SQL Server Reporting Services Team consisted of 2 developers  Created files views tables and data sets to support Sales Operations and Analytics teams  Monitored and tuned database resources and activities for SQL Server databases          Education       Master of Science       Data Analytics Engineering       Expected in   122022                George Mason University      Fairfax     VA     GPA        Status           38 GPA           Bachelor of Science       Electronics  Communication Engineering       Expected in   062019                KL University      Guntur          GPA        Status   </data><data key="id">150574715265165963952823813869633129863</data></node>
<node id="n1298" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-science-data-engineer-intern-bd32a7569cbb4dd2a9f164b4801eef35</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market           Skills         Machine LearningData Mining  NLP  Linear Regression neural networksdeep learning Naive Bayes SVM Logistic Regression  decision trees Kmean KNN N Grams edit distance gradient descent  Statistical Programming  Packages  R Python NumPy Matplotlib scikitlearn pandas ggplot2 Shiny dplyr caret e1071keras  Business Intelligence  Visualization  Tableau Qlik View Qlik Sense Excel OBIEE  Hadoop Ecosystem Components  Spark Hive Sqoop Flume Kafka Impala  Databases  Oracle MySql PostgreSql  Oracle ERP  Fusion Middleware  Oracle EBusiness Suite 11i  R12 Oracle SOA Oracle Service Bus  Other Languages  Tools  SQL PLSQL core java Scala RStudio Jupyter SqlDeveloper Toad Atom  Certifications  Tableau SQL  PLSQL                       Education and Training       University of Utah David Eccles School of Business    Salt Lake City     Utah      Expected in   August 2017     –      –       Master of Science        Information Systems          GPA           Information Systems Data Science and Analytics specialization   Recipient of 15000 Graduate Fellowship from David Eccles School of Business Academic Capstone Project  Big Data  Building statistical regression and classification models cleaning exploring data and developing interactive web interface using R Shiny which helps the company to classify clients loan type and predicting the amount of loan they will take in future Kaggle  House Prices Predictions  Applied different machine learning simple and advanced models on housing predictors for predicting the sales prices of houses used imputation methods for filling missing and null values in the data set Independent Study  Research  Apache Spark using Scala and Python Rajeev Gandhi Memorial College of Engineering and Technology          India                        Expected in   May 2012     –      –       Bachelor of Science        Computer Science and Engineering          GPA           Computer Science and Engineering Designed Hand Draw Shape Recognition interface which helps the user to invoke desired application just by drawing the shape linked to the application          Experience       Envestnet      Data Science  Data Engineer Intern   Secaucus     NJ                   012017      Present     Working on Big Data Ingestion using Sqoop for transferring data from multiple MySql database servers to transient storage in amazon EMR Hcatalog and using Hive to transfer data to persistent storage in amazon S3 bucket  Developing Sqoop and Hive scripts for data ingestion  Using R and spark in amazon EMR for filtering exploring analyzing providing insights on data and developing reports           First American Corporation      Oracle Technical Consultant  Data Analyst   City          India              022013      072016     Created SQL scripts for daily extracts adhoc requests  reporting and for analyzing large data sets  Designed ER diagrams conceptual models logical and physical models created database objects  Tables Indexes Sequences and Views  Developed Oracle Business Intelligence reports created and modified Oracle database objects  Tables Views and Indexes which increased the performance of Oracle Business Intelligence reports by 60 in production environment  Prepared SQL  Loader scripts for loading data from other systems into oracle ERP system worked with onsite business team in performing data fixes  Created PLSQL interfaces for doing business validation transferring data between ERP modules and loading data to base tables           CMC Limited      Data Analyst Intern   City          India              112012      012013     gt Developed SQL scripts worked on oracle 11i database and oracle reports          Skills     Academic ad Apache Big Data Business Intelligence Draw clients Data Mining Databases database EBusiness edit ERP filling drawing java Machine Learning Excel Middleware MySql NLP networks neural Oracle Oracle database PLSQL PostgreSql Programming Python reporting Research sales servers scripts SQL SQLLoader Tableau Tables Toad type validation View</data><data key="id">266612002933194965787459684177947415296</data></node>
<node id="n1299" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/senior-data-engineer-ffd843d6febb4217800488911db2a3f8</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Professional Summary      Business Intelligence Consultant with a 10year career in data warehousing business intelligence reporting and data management architecture Progressive developer and technical team lead with a strength in design  development as well as driving performance reducing inefficiencies and cutting costs Possess comprehensive knowledge and hands on experience in both ETL and Reporting tools Knowledge of Credit Card account life cycle management in Finance domain Functional knowledge of Collections and Recovery operations Expertise in planning executing and spearheading various SDLC and Agile Scrum projects in compliance to quality standards Known for effective communication with excellent relationship building  interpersonal skills strong analytical problem solving  organizational abilities Proven track record of taking ownership and diving deeper into issues to identify the root cause and troubleshoot towards resolution        Skills           ETL Tools  Ab Initio  Scripting Languages  Unix shell scripting Python SparkScala  Cloud Services Microsoft Azure Cloud Services Data Lake  DataBricks and Data Pipelines Azure Data Factory  BI Reporting Skills  Tableau Desktop Tableau Server Power BI Reporting  SAP Business Objects SAP BOUniverse Designer Tool SAP BO XI WEBI ReportingSAP BO XI Deski Reporting  Database  Oracle  Teradata Hive      Scheduler tools  Autosys ControlM  Relational Database Management  Business Intelligence Reporting  Project Management  Data Analysis and Visualization                       Work History      062017   to   Current     Senior Data Engineer      Thoughtworks    –    Memphis     TN            A multitenure loyalty program across all Williams Sonoma brands – the first time that customers can earn points by purchasing across the multiple brands WSI Pottery Barn West Elm etc I work in the customer data warehouse CDW which is Teradata and work with their 3rd party marketing systems vendor There are various data points and integrations with the data warehouse which holds all customer information points transactions loyalty IDs etc   Worked with Requirements Analysts and PDMs to identify understand and document business needs for data flow Analyze requirementsUser stories at business meetings and strategize impact of requirements on different platformsapplications Worked with Project Management in creation of project estimates for each Agile story  Engaged in projects that uses Scala and Azure DatabricksDatafactoryDatalake to extract process and load Adobe Clickstream data received for Williams Sonoma ECOM website Deployed custom codes in Scala to read JSON extract CSV files from Adobe add column headers handle badmalformed records handle invalid records before finally writing cleansed data into ParquetDelta file format partitioned by date and hour  Build and review design deliverables Perform dependency analysis between new objects created in Ab Initio graphs Conduct IT and UAT testing of each of these graphs Record errors reported during Unit and Integrated testing  Prepare production Implementation Plan to deploy codes successfully to production environment  Coordinate daily standups short meetings where teammates talk through whats being worked on yesterday what was done today and if blocked Involved in variety of other scrum meetings such as planning meetings where team pulls in stories requirements grooming meeting to look at backloglist of stories and flush out timeline and work expected demo show work that was completed in sprint etc  Reporting  Visualization in Tableau and Power BI  Design and deploy rich Graphic visualizations with Drill Down and Drop down menu option Prepare Dashboards using calculations parameters Apply various reporting objects like Filters prompts Calculated fields Groups Parameters Work on the development of Dashboard reports for the Key Performance Indicators for the top management          032010   to   062017     Senior DeveloperTechnical Team Lead      Tech Mahindra Synchrony Financial Services    –    City     STATE            Synchrony Financial is a consumer financial services company offering consumer financing products including credit promotional financing and loyalty programs and installment lending through Synchrony Bank its wholly owned subsidiary Synchrony is the largest provider of private label credit cards in the US The company provides private label credit cards for such brands as Amazon JC Penney CheapOAir OneTravel Sams Walmart Lowe’s Guitar Center Gap BP We as Data warehouse solution providers store this credit card holder information from the instance a credit card is applied till the account is closedcharged off   Understanding the full scope of requirements from business Estimating time and effort involved from gathering project requirements till project implementation  Answering technical queries driving product initiatives and metric collection and analysis  CoOrdinating between project stake holders – Business Client and offshore teams during all phases of project  Preparing design documentation design reviews development performing code reviews to ensure coding standards are followed testing and deployment of application enhancements  Experience in Ab Initio toolsets including the following  o Parallel and serial flow batch graphs conditional components other components like reformat normalize and join lookup and graphs processing ASCII and EBCDIC layouts  o Knowledge on Abinitio architecture including the GDE Cooperating system EME and other related items  o Fine tuning of Abinitio graphs based on performance enhancement by proper usage of memory in the components  o Well versed with various Ab Initio components such as Join Rollup Partition Departition Dedup sorted Scan Normalize DeNormalize  o Expertise knowledge improving Performance and Troubleshooting of the AbInitio graphs and monitoring ABInitio run time statistics  o Experience with advanced Abinitio metaprogramming air commands and other admin related tasks including the creation of save and performing migration from one server to the other  Establish support such as acquiring conditioned test data support from third partyteam etc for Integration Testing to simulate production environment and to ensure correctness and quality of buildparameter set up  Getting sign off from IT managers for deploying parameterset up onto production environment  Documenting implementation plan and Hour by Hour plan listing down the tasks in sequence of their execution on the date of implementation  Coordinate release activities across multiple teams          Education      Expected in   4 2008     Bachelors     Information Technology     College of Engineering Bhubaneswar      India          GPA               Accomplishments      • Tableau Desktop Specialist Certified No expiration  These are Open Badges that I have been awarded and that attest to my skills  httpswwwyouracclaimcombadges556948fdd8114f8097d245ab6c530d9clinkedinprofile   o Tableau Desktop Specialist title use their foundational knowledge of Tableau Desktop and data analytics to solve problemsDesktop Specialists can connect to prepare explore and analyze data and share their insights        Skills       ETL Tools  Ab Initio  Scripting Languages  Unix shell scripting Python SparkScala  Cloud Services Microsoft Azure Cloud Services Data Lake  DataBricks and Data Pipelines Azure Data Factory  BI Reporting Skills  Tableau Desktop Tableau Server Power BI Reporting  SAP Business Objects SAP BOUniverse Designer Tool SAP BO XI WEBI ReportingSAP BO XI Deski Reporting  Database  Oracle  Teradata Hive    Scheduler tools  Autosys ControlM  Relational Database Management  Business Intelligence Reporting  Project Management  Data Analysis and Visualization         Work History      062017   to   Current     Senior Data Engineer       Kforce Inc Williams Sonoma Inc   –   San Francisco     CA     A multitenure loyalty program across all Williams Sonoma brands – the first time that customers can earn points by purchasing across the multiple brands WSI Pottery Barn West Elm etc I work in the customer data warehouse CDW which is Teradata and work with their 3rd party marketing systems vendor There are various data points and integrations with the data warehouse which holds all customer information points transactions loyalty IDs etc   Worked with Requirements Analysts and PDMs to identify understand and document business needs for data flow Analyze requirementsUser stories at business meetings and strategize impact of requirements on different platformsapplications Worked with Project Management in creation of project estimates for each Agile story  Engaged in projects that uses Scala and Azure DatabricksDatafactoryDatalake to extract process and load Adobe Clickstream data received for Williams Sonoma ECOM website Deployed custom codes in Scala to read JSON extract CSV files from Adobe add column headers handle badmalformed records handle invalid records before finally writing cleansed data into ParquetDelta file format partitioned by date and hour  Build and review design deliverables Perform dependency analysis between new objects created in Ab Initio graphs Conduct IT and UAT testing of each of these graphs Record errors reported during Unit and Integrated testing  Prepare production Implementation Plan to deploy codes successfully to production environment  Coordinate daily standups short meetings where teammates talk through whats being worked on yesterday what was done today and if blocked Involved in variety of other scrum meetings such as planning meetings where team pulls in stories requirements grooming meeting to look at backloglist of stories and flush out timeline and work expected demo show work that was completed in sprint etc  Reporting  Visualization in Tableau and Power BI  Design and deploy rich Graphic visualizations with Drill Down and Drop down menu option Prepare Dashboards using calculations parameters Apply various reporting objects like Filters prompts Calculated fields Groups Parameters Work on the development of Dashboard reports for the Key Performance Indicators for the top management          032010   to   062017     Senior DeveloperTechnical Team Lead       Tech Mahindra Synchrony Financial Services   –   Chicago     IL     Synchrony Financial is a consumer financial services company offering consumer financing products including credit promotional financing and loyalty programs and installment lending through Synchrony Bank its wholly owned subsidiary Synchrony is the largest provider of private label credit cards in the US The company provides private label credit cards for such brands as Amazon JC Penney CheapOAir OneTravel Sams Walmart Lowe’s Guitar Center Gap BP We as Data warehouse solution providers store this credit card holder information from the instance a credit card is applied till the account is closedcharged off   Understanding the full scope of requirements from business Estimating time and effort involved from gathering project requirements till project implementation  Answering technical queries driving product initiatives and metric collection and analysis  CoOrdinating between project stake holders – Business Client and offshore teams during all phases of project  Preparing design documentation design reviews development performing code reviews to ensure coding standards are followed testing and deployment of application enhancements  Experience in Ab Initio toolsets including the following  o Parallel and serial flow batch graphs conditional components other components like reformat normalize and join lookup and graphs processing ASCII and EBCDIC layouts  o Knowledge on Abinitio architecture including the GDE Cooperating system EME and other related items  o Fine tuning of Abinitio graphs based on performance enhancement by proper usage of memory in the components  o Well versed with various Ab Initio components such as Join Rollup Partition Departition Dedup sorted Scan Normalize DeNormalize  o Expertise knowledge improving Performance and Troubleshooting of the AbInitio graphs and monitoring ABInitio run time statistics  o Experience with advanced Abinitio metaprogramming air commands and other admin related tasks including the creation of save and performing migration from one server to the other  Establish support such as acquiring conditioned test data support from third partyteam etc for Integration Testing to simulate production environment and to ensure correctness and quality of buildparameter set up  Getting sign off from IT managers for deploying parameterset up onto production environment  Documenting implementation plan and Hour by Hour plan listing down the tasks in sequence of their execution on the date of implementation  Coordinate release activities across multiple teams</data><data key="id">137893745859917600174161396390961745079</data></node>
<node id="n1300" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Desktop and Server Support Application Server Technician Offer my Windows Server 20122008 R22003 Windows XP78 Mac OS X Linux Application Server Microsoft Office 2010 and Office Mac 2011 Active Directory and Network LANWAN switches and routers experience Desktop and server hardware and software technical support experience Strong analytical troubleshooting communication and customer service skills  Skilled Windows System Administrator offering 10 years of experience building and maintaining multiplatform technology services with a solid understanding of current Windows Mac and UNIX application systems       Highlights           Systems	Windows XP78 Desktop	Max OS X 108 109 1010	iO78 mobile and tablet  Windows 20032008 Server	Android mobile		Red Hat Enterprise Linux  Unix web applications	LANWAN  Mobile Phone	iPhoneiPad Apple	Blackberry		Android  Applications	Microsoft Office 20072010	Office Mac 2010		Final Cut 7 and 10  Microsoft Office 365	Pages OS X		Numbers OS X  Adobe Creative Suite cloud	Adobe Acrobat XXI	Adobe Lighthouse 45  Parallels	WebEx  Utilities	Symantec Antivirus	Voltage Email Encryption	Barracuda Backup Cloud  PGP DesktopServer	Symantec Ghost  Server Applications	Active Directory	PowerShell Scripting	WINS	DFS  DHCP	FilePrint Services	LDAP	Group Policy GP  Exchange 2010	NTFS security		HyperV	DNS                         Accomplishments       Requirements Analysis    Completed business requirements analysis including the evaluation of systems specifications for the Soundview Throgs Neck Community Health Center     Strategy and Planning    Developed and communicated Electronic Health Record security policies and standards to all users  Established policies and procedures for medical record documentation     IT Training    Successfully trained all employees to use Electronic Health Record and Billing systems     Network Support    Acted as first point of contact for all major technical issues including power outages system failures and disaster recovery  Oversaw infrastructure of three offices and acted as support for helpdesk technicians of Yeshiva University          Experience       Data Support Services Engineer       072013      092013     Montefiore Medical Center formally SVTNCMHC Health Care Services    –    City     STATE            Managed the daytoday IT operations for the Montefiore Medical Center  Assisted in the migration of technology services from Yeshiva Universitys servers to Montefiores servers including computers user logins data files printer settings software applications and email archives  Provided QA testing in the migration of the centers Electronic Health Record Mindlinc and Billing IMA system  Trained all staff in the use of Montefiores technical services including clerical registration billing electronic health record and emailprintingdata file usage           Technical System Support Engineer       072003      072013     Yeshiva University Health Care Services Soundview Throgs Neck Community Health Center SVTNCMHC    –    City     STATE            Desktops Mobile Phones Printers and Application Servers Provided all levels of enduser desktop server mobile phonetablet and printer technical support for 2 healthcare center locations in the Bronx  Backend technical support for all server and network services  Responsible for managing 15 application servers including 4 domain controllers  Deployed and maintained a Windows 2008R2 and 2003 Active Directory environment  Managed all aspects of server and application security using Active Directory LDAP and Linux  Planned and implemented the physical deployment and migration of Windows 7 desktops from Windows XP  Consulted daily with the executive clinical and administrative staff concerning the overall quality and possible improvement of technology systems for the medical center  Trained all staff in the use of SVTNYeshiva Universitys technical services including clerical registration billing electronic health record and emailprintingdata file usage  Provided QA support and testing of our internal and external billing system IMA  Work closely with outside vendors to design and maintain the EHR Billing and Backup applications  Implemented and maintained the centers data backup system using Barracuda Cloud Backup  Identified designed and implemented the requirements for the centers disaster recovery system  Responsible for managing and maintaining the centers audiovideo conference room system          Education       Graduate Certificate       Digital Media and Project Management       Expected in                   The New School      New York     NY     GPA        Status                  BBA       Computer Information Systems       Expected in                   Baruch College City University of New York      New York     NY     GPA        Status                  Skills      Active Directory administrative Adobe Adobe Acrobat Antivirus Apple audio Backup Billing billing system clerical Encryption Desktops DHCP disaster recovery DNS Email Ghost LAN LDAP Linux Mac managing Max Exchange Microsoft Office Office Windows 7 Windows Windows XP migration Enterprise network OS printer Printers quality QA Red Hat Servers Scripting Symantec technical support Phones Phone Unix Utilities video WAN web applications</data><data key="id">12880653418622435105799992611550629230</data><data key="url">https://www.livecareer.com/resume-search/r/data-support-services-engineer-fd6014dbd6d04903b67269cf3fc6cd59</data></node>
<node id="n1301" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/aws-data-engineer-fcef99cc1f1c4bdeb25cbb8d5b3ac32c</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases         Skills          ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                      Experience       AWS Data Engineer       012022      022022     Accenture Contractor Jobs    –    Rochester     NY            Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Used Spark SQL for Scala  amp Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process  Configured computing networking and security systems within cloud environment  Implemented cloud policies managed technology requests and maintained service availability           Data Engineer       012016      112019     Verizon    –    Beaverton     OR            Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Big Data Engineer  Hadoop Developer       102013      122015     Two95 International Inc    –    Boca Raton     FL          AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Python Developer        092012      102013     Fiserv    –    City     STATE            AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes          Education and Training       Post Graduate        Data Engineering        Expected in   022022                Purdue University      West Lafayette     IN     GPA        Status                  Post Graduate        Data Science And Business Analytics        Expected in   092021                University of Texas At Austin      Austin     TX     GPA        Status                  Bachelor of Arts       Business Administration And Management       Expected in   122009                Califonia State University       Fullerton CA          GPA        Status   </data><data key="id">339690346324715490168338998099130748299</data></node>
<node id="n1302" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/voice-data-network-engineer-22138788e64d42bdb03363162c2bd70a</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary      Technical Customer Service Specialist with a vast knowledge of web applications software and framework seeking to assist clients in all troubleshooting endeavors Outstanding networking adept at developing customer relationships looking for an opportunity to bring exceptional team building skills to a management position in a vibrant customer service department          Highlights           RoutersHubsSwitches        Juniper Nexus Cisco 2600 2800 3600 3700 3800 7200 2900 3500  Authentication TACACS  RADIUS  Network Analyzers Ethereal Sniffer Pro  Network Technologies        STP RSTP HSRP VRRP IRB Port Channel Cisco ASA 5500 SA  Operating Systems Windows20002003 Win98ME2000 proxp  Routing Protocols TCPIP RIP EIGRP OSPF and BGP  WAN Technologies T1E1 DS3 FrameRelay DSL MPLS leasedline  LAN Technologies 8021q ISL Patching up BackupRestoration DNS Infoblox DHCP  VOIP SIPH323 MGCPTDMSS7 Avaya  Voice gateways                         Accomplishments       10 years of IT experience in design development implementation troubleshooting and maintenance of mediumto large scale Network infrastructure Experience in Static Routing and configuring dynamic Routing Protocols  RIP v1 and v2 IGRP EIGRP OSPF and BGP Expertise in network protocols Switching Routing and Security technologies Experience on Cisco Catalyst Series 6500 Switches and Virtual switching system  Deployed the switches in high availability configuration  Good knowledge of Spanning Tree Protocol  IEEE 8021d IEEE 8021w IEEE 8021s PVST Well experienced in VLAN implementation VTP domain settings Stacking Load balance and Redundancy technologiesXRRP VRRP HSRP PAGP LACP Configuration of Access Control Lists ACL Quality of  ServiceQoS VPN NATPAT policies Experience in implementation of  PPP ISDN HDLC Frame Relay  T1 DS3 MPLS Installation and maintenance of RADIUS TACACS    Cisco Secure ACS Good understanding of the OSI Reference Model and the TCPIP Model Network Management CiscoWorks LMS ProCurve Manager Plus PCM  SonicwallOpen View Network analysis and troubleshooting tools  Sniffer Pro Wireshark Agilent Network and analysis tools Experience working with Cisco Nexus 2148 Fabric Extender and Nexus 5000 series to provide a Flexible Access Solution for a datacenter access architecture  Experience configuring Virtual Device Context in Nexus 7010  Experience convert PIX rules over to the Cisco ASA solution  Worked on Extensively on Cisco Firewalls Cisco PIX 506E515E525  ASA     550055105540 Series  Proficient in the operation and administration of Juniper NetScreenISGSSGbased firewalls Juniper SRXseries services gateways Juniper JMMXseries routers Juniper SAseries SSL VPN appliances F5 loadbalancing products LTM GTM FirePass and various Cisco IOSbased platform  Assisted in converting PIX rules over to the Cisco ASA solution  Good knowledge on VOIP protocols like H323 SIP MGCP and SS7 and interfacing of TDM to VOIP system    Worked on Avaya  Voice gateways for VOIP implementation using Cisco Catalyst switches Experience in System Software installations Implementation of DNS TCPIP DHCP and IAS in Network environments Windows 2003 Windows 2000 and Windows 9598 Sound knowledge of networking methods techniques and processes Extensive knowledge of installing advanced configurations on Cisco Call Manager platforms Possess good working knowledge of TCPIP networks Comprehensive knowledge of Cisco network protocols and transport systems Indepth knowledge of Cisco Unified Communications like UCM UCCX MPE Unity NSTS and DRSN Well experienced in Planning Designing implementation and management of mediumtolarge scale of enterprise networks         Experience       Voice  Data Network Engineer       012015   to   Current     Criterion Systems    –    Washington     DC             Responsible for presales and postsales support for the design and implementation of call center and corporate voice solutions  Implemented call routing call management and computer telephony integration  Configured installed and supported Cisco Unified Communication Manager CallManager 8x Cisco Messaging Systems UnityUnity Connection 8x CER VoiceGateways Cisco routers and switches  Performed system upgrades and patches based on request schedule or events  Created dialplans route patterns route groups route lists calling search space partitions  Patched and upgraded Cisco Unified Communications applications  Created Voice Mail boxes call handlers create and reset passwords MWIs administered and maintained Active Directory on Unity  Used tools such as Advanced Settings Tool Message Store manager DOH PropTest Services Tools Depot  Checked Outlook accounts in Exchange servers to view warning and limits text messages associated w Unity Voice Mail created Subscriber services skill sets and prompts for UCCX deployment  Created Subscriber services skill sets prompts for UCCX deployment  Established and maintained collaborative relationships with customers recognized for consistently providing excellent customer service and retention  Instrumental in developing telecommunications departmental Service Level Agreements SLAs and standard processes which optimized daily operations           Network Engineer       072012   to   112014     Criterion Systems    –    Beltsville     MD             Performed responsibilities of assisting voice engineer in designing and installing Cisco network equipment  Handle first level of designing and installing Cisco Call Manager and associated Voice applications  Responsible for the administration configuration and implementation of IP Telephony systems  Handle the tasks of designing and maintaining documentation related to network layout  Perform responsibilities of maintaining security policies on Cisco firewall solutions  Monitor and ensure that the requirements of high availability are in place for Cisco environment  Manage team tasks against SLAs as well as work with project plans adhering to project timelines resource constraints and within budget  Primary support for Cisco Unity Unity Connection Unity Express AIM and NMCUE Call Manager VOIP Migration Project Involved in migrating Lehmans Branches to centralized VOIP platform that will replace the existing Standalone phone system  Cisco router 3745 series and switch series 3560 are used to support to add VLANs to all the routers and switches to support the VOIP phones  Worked on Avaya Gateway equipment is added to the network to support the ports from Cisco Switches across the network  Successfully performed installation and configuration for IPSEC VPN on Cisco routers PIX series VPN Concentrator 3000 and ASA 5500 security appliances  Worked extensively in Configuring Monitoring and Troubleshooting Ciscos ASA 5500PIX security appliance Failover DMZ zoning  configuring     VLANsroutingNATing with the firewalls as per the design  Assisted in migration of centralized voice mail system based on Cisco Unity connection and support interfacing of legacy TDM PBX of the sited in voice network system  Familiar with H323 IOS gateways and SRST telephony design and implementation and call manager deployment in the voice network  Hands on experience on implementation and troubleshooting of ACLs and QoS for the networks Involved in supporting voice infrastructure including call routing the voice gateways and its troubleshooting methods  Worked on updating CATOS for that switches and IOS for the routers to support the VOIP standalone system for 6500 Multilayer switch and 3500 switch series  HP Openview SNMP network node manager is used to monitor the network and in the times of green zones and DMZ zones           Sr Voice Data Analyst       2012   to   062012     Criterion Systems    –    Germantown     MD             Troubleshoot network software and hardware issues Remote corrective actions supporting Monitored all functions around the voice and data network  Responsibly monitorered network and Tier 23 support  Interfacework with repairrestoration of remote site equipment Assisting field technicians in maintenance of remote site equipment Responsibly cross trained other members of the Network Engineering team to broaden the knowledge base within the team  Managed emergency change needs of the business  Proven ability to learn new systems as required to effectively support customer needs  Troubleshoot wireless problems using different RF analysis tools  Handle and prioritize high call volumes and customer inquiries  Other duties as assigned by the Director of Telecommunications           Network Engineer       102009   to   112011     MORGAN STANLEY    –    City     STATE             Involved in Network Redesign for branch for data environment  Convert Branch primary WAN circuit T1 to MPLS and branch router security migration  Performing BGP Changes on branch router and headend router  Conversions to BGP WAN routing  Which will be to convert WAN routing from OSPF to BGP OSPF is used for local routing only which involves new WAN links  Replace network hardware with new 7200 routers and 3825 switches in all branch locations  Design and implementation of GRE for multicast and unicast communication on an existing IP VPN Managed and engineered 58 JuniperCheckpoint firewalls  Experience on designing and troubleshooting of complex BGP and OSPF routing problems  Involved in designing and implementing QOS and policy map to 3800 series routers for all the branches Worked closely with network monitoring staff and will do network design testingcertification prior to production implementation  Implements configurations and implementation instructions into change management system and insures that all approvals and processes are adhered to compliant with Audit  Worked on Layer 2 protocols such as VTP STP RSTP PVST MST and other VLAN troubleshooting issues and configuring switches from scratch and deployment Experience in branch relocation connect workstation servers etc  Rack and stack preconfigured new hardware and connect the circuits  Worked with carrier to test and turnup circuits  Worked with carriers like ATT Verizon Wind stream and Level3 for deploying WAN circuits and implementation of MPLS cloud Performs system level documentation on platforms and assists in project tracking and documentation  Experience with developing network design documentation and presentations using VISIO  Troubleshoots with problems regarding the network changes and migrations           Network Engineer       112006   to   092009     PITNEY BOWES    –    City     STATE             Responsible for Design integration configuration maintenance performance monitoring and security of network infrastructure including local area networks LAN wide area networks WAN  firewalls DHCP DNS Installing the Network devices in datacenter environment and clearly articulate complex network designs and drawings through documentation Visio as well as verbal training sessions  Experience in Configuring SitetoSite and Remote Site VPNs NATPAT policies  Managing Cisco Secure ACS for TACACS  RADIUS authentications  Monitoring customer data networks and providing fault isolation and remote troubleshooting  Experience on designing and troubleshooting of EIGRP routing issues  Responsible for the management of network at the client environment  Supporting and performing projects for the client WAN environment at a global level  Implementation of network system upgrades and modifications including planning testing scheduling and coordination  Ensures that change management and defined security procedures for all network systems are executed in accordance with customer policies and procedures  Interacting with Carriers for installation of new WAN circuits at Customer premises and makes sure circuit installed with no issues and ready to use before users move in to the branch  Providing Teir3 technical support for LANWAN issues and oncall for technical escalation on a rotational basis Remedy Ticketing system  Well experienced in troubleshooting bug related issues with help of Cisco TAC service  Providing networking services coordinate tasks and ensure their execution and documentation in accordance with established corporate standards           Systems  Network Engineer       052005   to   092006     WESTERN UNION    –    City     STATE              Resolved customer complaints and concerns with strong verbal and negotiation skills  Responsible for clients new branches turn up and managing the network environment  Installation of Operating Systems application softwares and Troubleshooting hardware issues Management of WAN connected over Frame relay and DSL Management of Pix Firewall  Cisco Routers and switches infrastructure Configuration of routers and switches for OSPF EIGRP VLANs STP Trunks Ether Channel Load Balancing Configuration of Routers and Firewalls for sitetosite IPSEC VPN tunnels Implemented Secured Wireless Network based on WPA and MAC Authentication  technologies Integration of AVAYA IPPhones with Switches in VLAN environment  Policy configuration and management of firewalls  DNS changes as per client requests  Planning about new product requirements Contacting Vendors for product purchases  Contacting Service providers to troubleshoot issues on WAN connectivity  Involved in planning and designing team for network upgradeschanges  Configuring NAT Policies Global VPN Client Policies Establishing SiteSite VPN Connections Maintenance of DHCP Server  Enabling highavailability and highcapacity features such as Spanning Tree and link aggregation on Switches Responsible for service request tickets generated by the helpdesk in all phases such as troubleshooting maintenance upgrades patches fixes and all around technical support Basic Configurations and Maintenance of various Switches Routers Firewalls and Wireless Access Points at different client locations Responsible for interaction with third party technicians on site Performed network support involving daily wiring maintenance of cabling switch maintenance network port reenabling and cable room Documentation of all new software installation procedures troubleshooting procedures Responsible for implementation of new network infrastructure includes Cabling Switching Routing Establishing MAN Connectivity between  their different braches Involved in Loop back tests and troubleshooting issues during establishment of T1 Line Dynamic VLAN implementation using IEEE 8021x protocol and RADIUS Server Integration of Active Directory Services ADS with RADIUS Server and creation of remote access polices on RADIUS Implementation of Access Control ListsACLs  QoS Implementation of Portfast on all access switches and setting bridge priorities on L3 Switches Environment Cisco 2900 3560 1800 3700 3800 7200 2900 3500          Education       Bachelor of Technology          Expected in   2003     Goa Institute of Technology                GPA               Certifications      CCNP Cisco Certified Network Professional  CCNA Cisco Certified Network Associate        Skills     Active Directory ADS articulate AVAYA Backup Basic BGP budget cable Cabling call center change management Cisco router Cisco Cisco Routers hardware network systems computer telephony integration Client excellent customer service DOH designing DHCP Documentation DNS DSL EIGRP engineer fast features FirewallsFrameRelay gateways Gateway HP Openview Hubs IP local area networks LAN layout leasedline MAC Director Managing Messaging Access Exchange Outlook 2000 Windows2000 Win98 Migration Network Engineering network design network hardware network support Network networking networks Operating Systems OSPF PBX phone system Policies presentations processes project plans Protocols Express RIP router Routers Routing sales sales support scheduling servers Service Level Agreements SLA SNMP software installation SS7 Store manager Switches switch Cisco Switches T1 TCPIP TDM technical support telecommunications Telephony Phones Troubleshoot Troubleshooting upgrades view VPN VISIO voice and data Voice Mail VOIP WAN wiring</data><data key="id">285353067357438442010566533524756651993</data></node>
<node id="n1303" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-engineer-ii-30ba17cad1264ff7923637259cc2cb9f</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Selfmotivated Data Engineer offering 5 years of leadership experience across various industries Methodical with significant experience in data mining and statistical analysis Excellent problemsolver with a history of automating processes and driving operational enhancements Effective at making important team decisions focused on moving products through planning preproduction and production phases        Skills           Data Integrity Validation and Analysis  Database Programming and SQL  Product Planning      Analytical Problem Solving  Strong Work Ethic  Application Support                       Experience       Data Engineer II        032021      Current     Usaa    –    Argyle     TX            Work with stakeholders to identify improvements to processes and data delivery using technical aptitude to deliver ad hoc tools reporting and analytics with a focus on the needs of the Residential Portfolio  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts  Maintain JIRA projects and workflows to meet project metrics  Work with Residential Portfolio to reduce CIP Construction InProgress by 69 across Residential Cox Business and Network Transformation programs by developing clear processes to mitigate risk providing visibility through enhanced reporting and database aligning  Build and validate the hypothesis of product ideas and infrastructure improvements for Fiber Reinforcement New Build Rebuild and Expansion Projects  Provide for database management to support the integrity of data for enterprisesupported tools and applications  Work with software development team to design and customize applications SiteTracker applications and trackers to meet market exceptions  Utilized existing reporting to perform data blending as needed to serve business needs  Develop processes to improve workflow and department efficiency  Designed and developed data quality dashboards with visualization in Tableau Power BI and Salesforce  Work with the development team to define and implement customer change requests to enhance product functionality  Complete pilot testing implement best data practices and optimize workflow to enhance performance and drive efficiencies in the organization to meet or exceed customer and business expectations  Utilize WATTS and SiteTracker experience as a user andor report development  Identify strengths and weaknesses of existing reports suggests areas of improvement and help enhance existing data reports to meet evolving requirements  Interpret and analyze data using exploratory mathematic and statistical techniques based on scientific methods           ANALYTICS CONSULTANT       032019      042021     Syndigo    –    Boise     ID            Partnered with WBS developers to automate manual processes decreasing errors and saving time creating a 6 increase in margins 2021 fiscal year  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Worked to create UI mockups and prototypes that illustrate how sites function and look like  Provided executives with analytics and decisionsupport tools used a basis for reorganization consolidation and relocation strategies  Executed tests collected and analyzed resulting data and identified trends and insights to achieve maximum ROI in paid search campaigns  Evaluated project requirements and content standards for each project to produce copy in line with a creative structure  Recommended changes to website architecture content and linking to improve SEO positions           DATA QUALITY ANALYST       052018      092020     Illumina    –    Virginia     MN            Worked mainly with wellknown engineering clients to provide customers with quality products  Dashboard development for project projection project closing and project profit  Responsible for estimating negotiating and agreeing on budgets and times lines while overseeing the production process  Responsible for drafted proposals per specifications and estimation skeleton with a focus on Data Centers  Developed RPAs that work alongside ERP to run cost analytics and tracked quality data metrics  Provide code compliance knowledge and enforcement for both owner and contractor  Drive and own backlog grooming and management prioritize iteration and drive acceptance testing and delivery of iterations  Defining road maps and prioritizing backlogs of work to meet with a vision of providing services that meet customers standards  Prioritized functionality backlog after golive to resteer team to achieve the desired ROI  Performed root cause analysis based on rework data to create corrective and preventive measures throughout the product development  Collaborated with Business Analysts and the software development team to identify and convert business goals into data requirements  Developed and streamlined productivity tracker to provide 34month predictions  Responsible for managing the progression of a project from RFP to  CO          Education and Training       Master of Science       Computer Science  Analytics        Expected in                   Georgia Tech Master of Science in Computer       Atlanta GA          GPA        Status                  Bachelor of Science       Civil Engineering   Industrial Microbiology       Expected in                   University of Georgia      Athens     GA     GPA        Status                 Activities and Honors       GatherUp  Nonprofit organization working with the community to provide recourses for further growth  NAMIC  Marketing  2021 2H Synergy Award</data><data key="id">148616979939238102062955431006346284182</data></node>
<node id="n1304" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-center-cloud-engineer-architect-8eec0f1efb0947ffab14c9580532f9d1</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Summary      Senior system engineer with 20 years of experience in software design development and architecture Full software development life cycle requirement gathering prototyping design implementation testing release and maintenance Has strong customer interfacing experience Has strong project lead and vendor management experience Capable of working with various teams to drive requirement gathering analysis and application design Polished communication and presentation skills able to demo products and solutions to both internal and external audience        Highlights          Performance and scalability optimization  Development environment software  Complex problem solver  Strong decision maker  Excellent communicator     Strong in C C  Microsoft Visual C  PHP MySQL   Linux Scripting   Cloudera Hadoop Certified  Ceph storage clustering  Apache Mesos   Docker and Linux Containers                       Accomplishments      Promoted to Lead Engineer after 18 months of employment at Dell  Promoted to strategist position at Dell after 5 years of employment  Holder of several US issued patents  US patent 20090100194  httpwwwfaqsorgpatentsapp20090100194   US patent 20130086262  httpwwwfaqsorgpatentsapp20130086262   US patent 20120198349  httpwwwfaqsorgpatentsapp20120198349         Experience      022013   to   072015     Data Center Cloud EngineerArchitect      Abbott Laboratories    –    Madison     MS             Solid experience with Mesosphere and the latest offering of DCOS Data Center Operating System Currently integrating DCOS on cloud hardware using Chef as a configuration management tool   Experience with Data Center Cloud Servers Responsible for designing and building a rack level softwarehardware solution for hyperscale cloud customers   Responsible for evaluating ARM 64 bit as a server solution in a data center   Evaluating CoreOS and Docker containers as a solution for Dell servers in a data center   Experience with Ceph as a cold storage server solution  Currently working as a solution architect to support the sales team by providing technical guidance on Dell cloud servers  Responsible for proposing solutions to customers and responding to customer RFI and RFP  Travel to customer site to present about Dell cloud servers          2007   to   022013     Principal Engineer      Johnson  Johnson    –    Athens     GA             Worked on competitive analysis to compare the Cisco UCS solution to Dells Active System solution  Worked on automating OS deployment using WSMAN and Dells life cycle controller The solution was intended to help services team in the field to have a quick tool to deploy the Active System solution  Worked on integrated solutions using PowerEdge servers PowerConnect switches and Compellent storage array The integrated solution is intended to build a business ready configuration called vStart  Was Responsible for the integration of Hadoop Big Data with HPC clustering stack  Was the lead technical engineer to lead an engineering group of 10 The team was responsible for developing custom solutions to address unique customer requirements  Was responsible to support the sales team in EMEA by providing technical insight into Dell products  Developed plugins for Microsoft SCOM System Center Operation Manager Most development was done using C Visual Basic scripting and XML  Traveled to Dell India to train group of engineers on Dell system management products  Interface and interlock between several teams to lock down requirements and drive to results  Worked with Dell marketing on new feature requirements  Developed a Windows and Linux solution to monitor Dell systems for storage and BMC alerts and send SNMP traps to a system management console          032000   to   2007     Senior Software Engineer      Transcore    –    Hayward     CA             Developed firmware using C and C to authenticate users via Active Directory using industry standard LDAP protocol  Have a good working knowledge of USB protocols Worked with CATC to debug several USB devices  Developed a USB Linux kernel mode mass storage stack to make a USB slave device appear as a USB disk  Was the lead engineer of a group that developed Voice over IP clientserver application that allowed chat over the network The technology used GSM610 Audio Codec and Win32 Wave API  Developed Internet Explorer plugin using ActiveX technology to be used with Dell Remote Access Cards remote media feature  Worked on defining a protocol to send SCSI commands over  TCPIP The protocol was used as the basis for the remote media feature on Dells Remote Management Card DRAC   Developed application software for remote KVM keyboard video and mouse to provide the console redirection feature for Dells Remote Management Card DRAC   Was responsible for the integration of a high performance serial device driver using Microsoft DDK for Windows NT as well as Windows 20002003 following the WDM architecture          041999   to   032000     Software Engineer      Transcore    –    Miramar     FL             Was responsible for the design implementation and integration of tools such as compiler linker and assembler in an integrated development environment IDE  The IDE is used by firmware programmers to develop on Zilogs family of microcontrollers  This project required intense use of C MFC and Multithreaded Programming          031997   to   031999     Software Engineer      Transcore    –    Peachtree Corners     GA             Was the primary engineer responsible for developing Windows 98 and Windows NT device driver using WDM Win32 Driver Model Microsofts SDK and Microsofts DDK Driver Development Kit to allow host to target communication between Kodaks digital camera and the PC  Successfully developed application and interface software using C and object oriented techniques that allowed interfacing a digital camera to the IEEE 1394 high performance serial bus  Was part of the team to develop firmware embedded software for Kodaks high end digital camera  Was part of the team to develop the camera SDK software development kit using multithreaded concepts and Visual C the SDK was used by outside developers to control and acquire images from the Kodak digital camera          111994   to   031997     Software Engineer      ULTRA SCANCALSPAN CORPORTATION    –    City     STATE             Was part of the team responsible for coding and maintaining fingerprint match software using C and C on a UNIX workstation  Developed Windows 95 driver using C to allow communication with an ultrasonic fingerprint scanner from a PC over the parallel port  Integrated fingerprint match software into several applications using Visual C MFC and the Win32 SDK Software Development Kit  Designed and implemented firmware using Assembly and C to interface the MOTOROLA DSP56166 to an ultrasonic fingerprint scanner  Successfully developed firmware to allow transfer of images from a scanner to the PC over the parallel and the serial port Assembly and C were used in this project          Education      Expected in        BS     Electrical and Computer Engineering     State University of New York      Buffalo     New York     GPA   with Cum Laude GPA 3640            Skills      Win32 API ActiveX Big Data IPMI WSMAN C  C Visual C Visual Basic Visual Source Safe ClearCase Linux gnu tools  storage clustering competitive analysis  Database Dell servers device drivers XML Windows Audio API  PHP ASP  JAVA LDAP Active Directory MySQL Microsoft SQL MFC API Windows 2000  Windows 2008 Windows 20012  networking object oriented  Programming Red Hat Linux Ubuntu Linux  technical sales  SCSI protocols  Storage technology  scripting SNMP   UNIX USB VB scripting Voice over IP</data><data key="id">326757138740783914221199641260286969063</data></node>
<node id="n1305" labels=":CV"><data key="labels">:CV</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Profile     Professional and dedicated worker committed to delivering high quality performance and superior customer service to insure team success Utilize strong ethics customer relations and training  Over 8 years of experience in Storage management including designing installing configuring  and administering TSM  Netbackup Experience in both UNIX and Windows environment       Core Qualifications           Extensive knowledge of TSM administration procedures    Adept at server software installation and maintenance      Familiar with UNIX and Perl scripting     Good problem solving skills                          Professional Experience        072015   to   Present   Data Protection Engineer    Bmo         Pleasant Prairie     WI            TSM Implementation and administration TSM 6X 5X Implementation and administration on AIX Linux Solaris Windows platforms Tivoli data Protection for Oracle SQL implementation Bare metal Recovery implementation IBM 3592 EO5 LTO2LTO3 and LTO4  DataDomain configuration Tivoli Storage Management Disaster Recovery Implementation Installing TSM client on Unix and Win 200x servers TSM tuning for faster backup and restore Server to server configuration Expertise in TSM DR implementation and recovery Lanfree configuration  on UnixAixLinuxHPUX TSM library manager and library client configuration and Implementation Managing 8 TSM server environment with 2000 clients different site with Data Doman and physical library TSM administration and node backup centralized scheduling and reporting Avamar Client registration and configuration Avamar Client backup and restore Participate in SRT Schedules Retention and Targets yearly review with customers            042012   to   052015   Data Protection Engineer    Bmo         Portage     WI            TSM 6X 5X Implementation and administration on AIX Linux Solaris WinTel platforms Tivoli data Protection for Oracle SQL implementation Bare metal Recovery implementation IBM 3592 EO5 LTO2LTO3 and LTO4  DataDomain configuration Tivoli Storage Management Disaster Recovery Implementation Installing TSM client on Unix and Win 200x servers TSM tuning for faster and backup and restore Server to server configuration Expertise in TSM DR implementation and recovery Lanfree configuration  on UnixAixLinuxHPUX TSM library manger and library client configuration and Implementation Managing 10 TSM servers environment with 3000 clients different site with Data Doman and physical library            042007   to   052012   IT Specialist    Autodesk Inc         Colorado     TX            Netbackup 5x 6x implementation and administration on Window and Unix Netbackup client installation and configuration Provided  247 oncall support rotation Responsible for troubleshooting and daily backup issues Monitored and maintained tape volumes Troubleshot ACSLStape library software  issues Wrote a Runbook for both TSM and Netbackup Netbackup to TSM migration Netbackup client implementation Backup and restore using TSM Tivoli data Protection for Oracle SQL implementation            122006   to   042007   System Administrator    Advantest America Corporation         Fremont     CA            Coordinated hardware and software installations and upgrades to insure work is performed in accordance with Agency policy  Coordinated and monitored troubleshooting to isolate and diagnose command system problems  Daily system checks of all TSM servers tape library devices and server backup operations  Improved processes by writing scripts to automate task that is done on regular base            042000   to   062005   Design Automation Engineer    Intel Corp         Multiple Cities     NJ            Extensive experience in gate level simulation  Evaluated and QA new simulation tools  Assisted Design Engineers with tool issues they encountered and solve the problem  Interfaced between Intel and tool vendors to resolve issues and new releases  Provided automation and made improvements to existing flows  Trained new users on Gate Level Simulation tools and any new methodology  Experience in test vector generation test bench writing and regression running  Supported GLS and RTL VTPSIM Vector Test Pattern Simulation for all CPD Components Platform Division and tool owner for VTPSIM  Supported Design Engineering Environment for all Chipset Engineering including Software and License installations  Responsible for Local DBA Database Administration for TIBETBug tracking tool  Developed utility to generate indicator data for managers  Created Auto clone feature for Design Group which enabled them to submit bugs in two different projects at the same time  Provided training for Design Product Design Automation and Marketing Engineers on bug tracking tool  Setup training class for Design Automation Group            051997   to   042000   Test Engineer    Drs Technologies         Johnstown     PA            Sole author of STIL2S9K tool that generated all S9K test mode patterns for Camino MCH MTH and Carmel MCH  This tool was the first tool that enabled PCG Platform Components  Group to generate scan transition fault vectors with multiple timing sets  Kafka tool owner and support  This tool converts VCD Variable Change Dump file to STIL Standard Test Interface Language format  It was the first tool that enabled PCG to convert VCD to STIL  Worked with Teradyne Engineers to develop a STIL2J973 vector generation tool for J973 testers  Ran tests and gave feedback to the vendor  Helped out Product Engineers in generating test mode and functional vectors and validated the pattern before silicon arrived  Site owner for software tools Fabio and Purify  These tools were required to be run on the products test tape for white paper process          Education        Expected in   1994   Bachelor of Science       Computer Engineering    California State University     Sacramento     CA      GPA       Computer Engineering        Skills     AIX Agency automate Automation Backup hardware Client clients Database Administration DBA Disaster Recovery functional HP UX IBM Intel Linux Managing 8 Marketing Win WinTel Windows Window 2000 migration Oracle processes QA reporting scheduling server configuration servers scripts Simulation Solaris SQL Tivoli troubleshooting TSM TSM 6X Unix upgrades Netbackup Netbackup 5x author</data><data key="id">88130048337986624128176699613407932497</data><data key="url">https://www.livecareer.com/resume-search/r/data-protection-engineer-95cf566a964e41eaa5970c3e1d68ce54</data></node>
<node id="n1306" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/senior-software-engineer-data-manager-92600f55fb9a4018a4a7688a4607e44f</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary      Senior Software Engineer with 26 years of experience as an enterpriselevel database web applications developer technical lead and project manager  Expert Microsoft SQL database administrator with SQL server integration services SSIS and SharePoint experience  ​  Proven leader and strategic thinker able to architect cuttingedge software solutions and lead development teams through the entire software development life cycle SDLC  Recipient of NASAs Space Flight Awareness Honoree award for significant contributions in the preservation and management of Lunar sample data         Skills           Databases Microsoft SQL Server 20002014 MongoDB MySQL  Languages SQL Microsoft NET C and VB ColdFusion 11 Java JavaScript PowerShell Python CC  Development ToolsMethodologies SDLC Agile  Waterfall Methodologies Object Oriented Programming OOP Application Programming Interfaces API MVC Visual Studio Eclipse Git Subversion UML SSIS SSRS HTML5 CSS SASSLESS web services REST jQuery Bootstrap Accessibility508 Compliance Website Security UX Design  Graphic DesignMultimedia  Adobe Creative Suite CC Photoshop Illustrator InDesign Dreamweaver Premiere Pro Lightroom Inkscape  Software  Adobe Acrobat Pro Microsoft SharePoint Visio and Project R                         Experience        012007   to   Present   Senior Software EngineerData Manager    Ssi Schaefer Systems International North America         Columbus     OH            Designed and developed web database applications to showcase NASAcurated astromaterials sample collections including comprehensive searchable databases with sample processing and availability information scientific analyses references and multimedia photos 3D models videos for use by space and planetary scientists  Technologies used included Adobe ColdFusion Microsoft NET C JavaScriptJQuery SVG REST various APIs and Microsoft SQL among others  Architected managed and developed internal Enterpriselevel databases and applications using both traditional waterfall and agile methodologies  Led teams of 25 developers and interns both internal and in distributed teams and provided technical mentorship  Significant expertise as database architect and administrator with experience creating SQL server integration services SSIS and reporting service SSRS solutions and optimizing query performance  Regularly employed scripting technologies such as PowerShell and Python to implement work automation and provide data visualization  As webmaster for the directorate public websites was a recognized expert in all aspects of website management including the administration of IIS and Apache web servers ColdFusion and Java application servers and SQL servers  Maintained adherence to NASA and government standards and regulations accessibility508 compliance information privacy websiteapplication security  Developed and maintained relationships and collaborations within NASA and global research and technology communities that leveraged crossfunctional expertise to improve the quality and accessibility and transfer of scientific data and support NASAs Open Data initiatives            012001   to   012007   IT Consultant    Splunk         Aldie     VA            Windows 10 Windows Server 2003  2012 R2 Linux IIS 758 Apache ColdFusion Server Designed and developed custom ecommerce and informational websites for small to mediumsized clients using various technologies including Adobe ColdFusion and Flash Microsoft Visual Basic and ASP Java JavaScript MySQL and Microsoft SQL on ApacheLinux and IISWindows systems  Provided website management and system administration services to consumers and small businesses  Coordinated full range of project development from initial proposal to final delivery            011998   to   012001   IT Program Manager    Polar Tank         Houston     TX            Managed enterprise software projects with budgets in excess of 3M which were consistently completed within budgetary and time constraints  Prepared proposals managed budgets and developed business system requirements and functional specifications for proposed projects  Supervised 10 business analysts project managers application developers and contractors during entire software development lifecycle  Designed and implemented relational and OLAP databases for integration with web applications and developed middletier components using Microsoft Visual Basic ASP ADO and COMCOM technologies using the Microsoft Visual Studio IDE and SQL Server 2000 with Analysis Server  Experienced using ERWin Data Modeler and UML for database design  Successfully completed global implementation of wide area networks and computer installations in 20 international locations by directing internal resources and local and international external vendors            011994   to   011998   DSP Marketing Programs Manager    TEXAS INSTRUMENTS         City     STATE            Designed and implemented intranetextranet web applications for the sales organization customers and product distributors that greatly reduced marketing support and training costs for the division  Web applications were implemented in C and Perl on a UNIX platform for the NCSA Mosaic browser and for Netscape Navigator  Managed team of 34 developers            011993   to   011994   Technology Transfer Team Leader    SEMATECH         City     STATE            Managed an average of 30 projects a month for the Lithography division to fulfill technology transfer requirements of consortium            011991   to   011993   Design Engineer    IBM CORPORATION         City     STATE            Designed 486based computer motherboards and peripheral interfaces for IBMs PC Value Line          Education and Training        Expected in   1991   MS       Electrical Engineering    Solid State Devices  Circuits  University of Michigan     Ann Arbor     MI      GPA       Electrical Engineering        Awards              Skills     Microsoft NET 3D ADO Adobe Creative Suite Adobe Acrobat Adobe Dreamweaver Photoshop Premiere Agile Apache API ASP automation budgets C ColdFusion COM COM CSS clients data visualization database applications Databases database architect database design delivery directing ecommerce Eclipse ERWin extranet Flash functional government Graphic Design UX HTML5 IBM IDE IIS IIS 75 Illustrator InDesign Java JavaScript JQuery Linux marketing C Microsoft SharePoint Microsoft SQL SQL Server 2000 Windows 2000 MongoDB motherboards Multimedia MVC MySQL Enterprise Netscape Navigator networks Object Oriented Programming OOP OLAP Perl Programming project development proposals proposal Python quality reporting research sales scientific SDLC servers scripting software development Microsoft SQL Server SQL SQL server system administration UML UNIX Visio Microsoft Visual Basic VB Microsoft Visual Studio Visual Studio Web applications web servers Web Development Website websites website management webmaster Windows Server 486       Additional Information       AWARDS JETS Superior Performance Team Award 2014  Apollo Sample Curation Team NASA Group Achievement Awards 2014  Apollo Sample Curation Team  2013  Hayabusa Curation Team  Stardust Interstellar Curation Team JSC Directors Innovation Group Achievement Award 2013  Antarctic Meteorite Curation Team NASA Space Flight Awareness Awards 2009  Honoree  GRANTS 2016 NASA PDART CoPI Creating and Serving Novel Data Products of Astromaterials Combining ImageBased 3D Reconstructions and XRay CT Data of Astromaterials 2015 NASA PDART CoPI MoonDB Restoration and Synthesis of Lunar Petrological Data         Websites Portfolios Profiles</data><data key="id">191782290901319920686020512632969913854</data></node>
<node id="n1307" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/sr-data-engineer-867553e4eb4548f0bdb1253aa98a392b</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional Summary       Over 8 years of progressive professional experience in analysis Design Development and Implementation as a Sr Data Engineer and Data ModelerData Analyst  Strong experience in Data Migration Data Cleansing Transformation Integration Data Import and Data Export  Good knowledge in Data Quality  Data Governance practices  processes  Highly Qualified with about 5 years of experience in building Big data applications creating data lakes to manage structured and unstructured data  Well versed with Agile with Scrum Waterfall Model and Testdriven Development TDD methodologies  Server Reporting Services SSRS and SQL Server Integration Services SSIS  Worked with Java based ETL tool Talend and Implemented Integration solutions for cloud platforms with Informatica Cloud  Extensive experience in setting up CI CD pipelines using tools such as Jenkins Github Nexus and Maven  Experience developing On  premise and Real Time processes  Experience on Migrating SQL database to Azure data lake Azure data lake analytics Azure SQL database Data Bricks and Azure SQL Data Warehouse and Controlling and granting database access and migrating on premise databases to Azure Data Lake Store using Azure Data Factory  Strong experience in Big Data technologies like Spark Spark SQL Pyspark Hadoop HDFS Hive Sqoop Flume Kafka Spark Streaming  Experience in job workflow scheduling and monitoring tools like Oozie M2  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa  Experience in designing dashboards reports performing adhoc analysis and visualizations using Tableau Power BI  Handson experience on NoSQL databases like Snowflake HBase Cassandra and Mongo DB  Experience in building and architecting multiple Data pipelines end to end ETL and ELT process for Data ingestion and transformation in GCP and coordinating tasks among the team  Expertise in DBMS concepts  Experience in GCP Dataproc GCS Cloud functions Big Table and Big Query  Experience in writing and executing unit system integration and UATscripts in data warehouse projects  Worked on different file formats like Parquet Avro ORC and Flat files  Designing star schema Snowflake schema for Data Warehouse ODS architecture  Experience with Apache Spark ecosystem using SparkCore SQL Data Frames and RDDs  Experienced in data manipulation using python  Handson experience working at Amazon Web Services AWS using Elastic Map Reduce Redshift and EC2 for data processing AWS glue RDS S3 Lambda  Proficient in installing configuring and using Apache Hadoop ecosystems such as MapReduce Hive Pig Flume Yarn HBase Sqoop Spark Storm Kafka Oozie and Zookeeper  Experience in integrating Kafka with Spark streaming for high speed data processing  Extensive experience in developing and driving strategic direction of SAP operating system SAP ECC and SAP business intelligence SAP BI system  Develop effective working relationships with client teams to understand and support requirements develop tactical and strategic plans to implement technology solutions and effectively manage client expectations  An excellent team member with an ability to perform individually good interpersonal relations strong communication skills hardworking and high level of motivation             Skills         Technical Skills  HadoopSpark Ecosystem  Hadoop MapReduce Hiveimpala YARN Kafka Flume Sqoop Oozie Zookeeper Spark  Databases Oracle My Sql SQL Server PostgreSQL HBase Snowflake Cassandra Mongo DB  Cloud computing  Amazon Web Services AWS Amazon Redshift MS Azure Azure blob storage Azure Data Factory Azure Synapse  Google cloud PlatformBig Query Big Table Dataproc      BI Tools Business Objects XI Tableau 91 Power BI  Query Languages SQL PLSQL TSQL  Scripting Languages Unix Python  Operating Systems Linux Windows Ubuntu Unix  SDLC Methodology Agile Scrum Waterfall UML                     Education       CBIT    Hyderabad           Expected in   2013     –      –       Bachelor’s        Computer Science          GPA                     Work History       Humana Inc      Sr Data Engineer   Riverton     WY                   102020      Current     Involved in requirements gathering analysis design development change management deployment  Involved in development of real time streaming applications using PySpark Apache Flink Kafka Hive on distributed Hadoop Cluster  Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Milib  Extracted data from heterogeneous sources and performed complex business logic on network data to normalize raw data which can be utilized by BI teams to detect anomalies  Designed and developed Flink pipelines to consume streaming data from Kafka and applied business logic to massage and transform and serialize raw data  Developed a common Flink module for serializing and deserializing AVRO data by applying schema  Developed Spark streaming pipeline to batch real time data detect anomalies by applying business logic and write anomalies to Hbase table  Implemented layered architecture for Hadoop to modularize design  Developed framework scripts to enable quick development  Designed reusable shell scripts for Hive Sqoop Flink and PIG jobs  Standardize error handling logging and metadata management processes  Indexed processed data and created dashboards and alerts in splunk to be utilized action by support teams  Responsible for operations and support of Big data Analytics platform Splunk and Tableau visualization  Managed developed and designed dashboard control panel for customers and Administrators using Tableau PostgresQL and REST API calls  Designed and Developed applications using Apache Spark Scala Python Nifi S3 AWS EMR on AWS cloud to format cleanse validate create schema and build data stores on S3  Developed CICD pipelines to automate build and deploy to Dev QA and production environments  Supported production jobs and developed several automated processes to handle errors and notifications  Also tuned performance of slow jobs by improving design and configuration changes of PySpark jobs  Created standard report Subscriptions and Data Driven Report Subscriptions  Tools Hadoop Map Reduce Spark Spark MLLib Tableau SQL Excel PIG Hive AWS PostgresQL Python PySpark Flink SQL Server 2012 TSQL CICD Git XML Tableau           Cognizant Technology Solutions      Sr GCP Data Engineer   Lake Forest     CA                   012018      092020     Developed multi cloud strategies in better using GCP for its PAAS and Azure for its SAAS  Involved in loading and transforming large sets of the structured semistructured dataset and analysing them by running Hive queries  Developed custom python program including CICD rules for Google cloud data catalog for metadata management  Design and develop spark job with Scala to implement end to end data pipeline for batch processing  Do fact dimensional modeling and proposed solution to load it  Processing data with Scala spark spark SQL and load in hive partition tables in parquet file format  Develop spark job with partitioned RDD like hash range custom for faster processing  Develop and deploy the outcome using spark and Scala code in Hadoop cluster running on GCP  Develop near real time data pipeline using flume Kafka and spark stream to ingest client data from their web log server and apply transformation  Performs data analysis and design and creates and maintains large complex logical and physical data models and metadata repositories using ERWIN and MB MDR  Assist service developers in finding relevant content in the existing reference models  Like Access Excel CV Oracle flat files using connectors tasks and transformations provided by AWS Data Pipeline  Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries  Worked on developing Pyspark script to encrypting the raw data by using Hashing algorithms concepts on client specified columns  Responsible for Design Development and testing of the database and Developed Stored Procedures Views and Triggers  Developed Pythonbased API RESTful Web Service to track revenue and perform revenue analysis  Compiling and validating data from all departments and Presenting to Director Operation  Develop SQOOP script and SQOOP job to ingest data from client provided database in batch fashion on incremental basis  Use DISTCP to load files from S3 to HDFS and Processing cleansing and filtering data using Scala Spark Spark SQL HIVE Impala Query and Load in Hive tables for data scientists to apply their ML algorithms and generate recommendations as part of data lake processing layer  Build data pipelines in airflow in CP for ETL related jobs using different airflow operators both old and newer operators  Created Big Query authorized views for row level security or exposing the data to other teams  Good knowledge in using cloud shell for various tasks and deploying services           Honeywell      Big Data Engineer   Altamonte Springs     FL                   062016      122017     Implemented a generic ETL framework with high availability for bringing related data for Hadoop  Cassandra from various FRANCHI sources using spark  Experienced in using Platfora a data visualization tool specific for Hadoop and created various Lens and Viz boards for a realtime visualization from hive tables  Queried and analysed data from Cassandra for quick searching sorting and grouping through COL  Implemented various Data Modelling techniques for Cassandra  Joined various tables in Cassandra using spark and Scala and ran analytics on top of them  Participated in various upgrades and troubleshooting activities across enterprises  Knowledge in performance troubleshooting and tuning Hadoop clusters  Applied Spark advanced procedures like text analytics and processing using inmemory processing  Implemented Apache Drill on Hadoop to join data from SQL and No SQL databases and store it in Hadoop  Created architecture stack blueprint for data access with NoSQL Database Cassandra  Brought data from various sources into Hadoop and Cassandra using Kafka  Experienced in using Tidal enterprise scheduler and Ooze Operational Services for coordinating the cluster and scheduling workflows  Applied spark streaming for real time data transforming  Created multiple dashboards in tableau for multiple business needs  Installed and configured Hive and written Hive UDFs and used piggy bank a repository of UDFs for Pig Latin  Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access  Exported the analysed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Using Tableau  Implemented Composite server for the data virtualization needs and created multiple views for restricted data access using a REST API  Devised and led the implementation of next generation architecture for more efficient data ingestion and processing  Created and implemented various shell scripts for automating the jobs  Implemented Apache Sentry to restrict the access on the hive tables on a group level  Employed AVRO format for the entire data ingestion forecaster operation and less space utilization  Experienced in managing and reviewing Hadoop log files  Worked in an Agile environment and used rally tools to maintain the user stories and tasks  Worked with Enterprise data support teams to install Hadoop updates patches version upgrades as required and fixed problems which were raised after the upgrades  Implemented test scripts to support testdriven development and continuous integration  Used Spark for Parallel data processing and better performances  Tools Map Reduce HDFS Hive pig Impala Cassandra spark Scala solr Java SQL Tableau PIG Zookeeper Sqoop Teradata CentOS Pentaho           ITC InfoTech      Data Engineer   City          India              062014      112015     Worked on configuration and monitoring Hadoop cluster using Cloudera distribution  Involved in migrating data from on prem Cloudera cluster to AWS EC2 instances deployed on EMR cluster and developed ETL pipeline to extract logs and store in AWS S3 Data Lake and further processed it using PySpark  Moved files between HDFS and AWS S3 and worked with S3 bucket in AWS on a regular basis  Responsible for developing data pipelines using Flume Sqoop and Pig to extract the data from weblogs and store them in HDFS  Migrated data between various data sources like Teradata Oracle and MySQL to HDFS by using Sqoop  Used HCatalog to access Hive table metadata from MapReduce and Pig code  Developed a data pipeline using Kafka and Storm for streaming data and to store it into HDFS  Used Informatica Powercenter for cleaning managing and integrating data from different sources for ETL and loaded into a single warehouse repository  Used Impala to read write and query the Hadoop data in HDFS from HBase and constructed Impala scripts to reduce query response time  Analysed data stored in S3 buckets using SQL PySpark and stored the processes data in Redshift and validated data sets by implementing Spark components  Performed ETL operations using Python SQL on many data sets to obtain metrics  Prepared data according to analyst requirements on the extracted data using Pandas and NumPy modules in Python  Involved in designing and developing automation test scripts using Python  Involved in writing multiple python scripts to extract data from different API’s  Created HBase tables using Shell to load large sets of data from different databases  Involved in scheduling Time based Oozie workflow engine to run multiple Hive and Pig jobs  Developed flow XML files using Apache NiFi to process and ingest data into HDFS  Worked on performance tuning of Apache NiFi workflow to optimize the data ingestion speeds  Responsible for collecting and aggregating large amounts of log data using Flume and staging it into HDFS for further analysis  Worked on integration of Apache Storm with Kafka to perform web analytics and upload streaming data from Kafka to HBase and Hive  Responsible for developing data pipelines using Apache Kafka by implementing Kafka producers and consumers  Used Hive optimization techniques like partitioning and bucketing to provide better performance with HiveQL queries  Loaded large amounts of data to HBase using MapReduce jobs  Worked on developing UDFs to work with Hive and wrote our tests in Scala  Used zookeeper to maintain configurations across clusters and for better synchronization grouping and reliable distributed coordination  Worked with Kerberos and Apache sentry for security and authorization on Hadoop  Used Git for version control  Tools Cloudera CDH4 Hadoop 2x HDFS MapReduce Yarn Sqoop Hive AWS EC2 S3 Redshift Impala Spark Pig SQL HBase Kafka Zookeeper Flume Oozie HCatalog NiFi Storm Informatica Python MySQL Scala Teradata Oracle Git          Skills       Technical Skills  HadoopSpark Ecosystem  Hadoop MapReduce Hiveimpala YARN Kafka Flume Sqoop Oozie Zookeeper Spark  Databases Oracle My Sql SQL Server PostgreSQL HBase Snowflake Cassandra Mongo DB  Cloud computing  Amazon Web Services AWS Amazon Redshift MS Azure Azure blob storage Azure Data Factory Azure Synapse  Google cloud PlatformBig Query Big Table Dataproc    BI Tools Business Objects XI Tableau 91 Power BI  Query Languages SQL PLSQL TSQL  Scripting Languages Unix Python  Operating Systems Linux Windows Ubuntu Unix  SDLC Methodology Agile Scrum Waterfall UML         Work History       ALCON     Sr Data Engineer   Fort Worth     TX    102020      Current     Involved in requirements gathering analysis design development change management deployment  Involved in development of real time streaming applications using PySpark Apache Flink Kafka Hive on distributed Hadoop Cluster  Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Milib  Extracted data from heterogeneous sources and performed complex business logic on network data to normalize raw data which can be utilized by BI teams to detect anomalies  Designed and developed Flink pipelines to consume streaming data from Kafka and applied business logic to massage and transform and serialize raw data  Developed a common Flink module for serializing and deserializing AVRO data by applying schema  Developed Spark streaming pipeline to batch real time data detect anomalies by applying business logic and write anomalies to Hbase table  Implemented layered architecture for Hadoop to modularize design  Developed framework scripts to enable quick development  Designed reusable shell scripts for Hive Sqoop Flink and PIG jobs  Standardize error handling logging and metadata management processes  Indexed processed data and created dashboards and alerts in splunk to be utilized action by support teams  Responsible for operations and support of Big data Analytics platform Splunk and Tableau visualization  Managed developed and designed dashboard control panel for customers and Administrators using Tableau PostgresQL and REST API calls  Designed and Developed applications using Apache Spark Scala Python Nifi S3 AWS EMR on AWS cloud to format cleanse validate create schema and build data stores on S3  Developed CICD pipelines to automate build and deploy to Dev QA and production environments  Supported production jobs and developed several automated processes to handle errors and notifications  Also tuned performance of slow jobs by improving design and configuration changes of PySpark jobs  Created standard report Subscriptions and Data Driven Report Subscriptions  Tools Hadoop Map Reduce Spark Spark MLLib Tableau SQL Excel PIG Hive AWS PostgresQL Python PySpark Flink SQL Server 2012 TSQL CICD Git XML Tableau           USAA     Sr GCP Data Engineer   San Antonio     TX    012018      092020     Developed multi cloud strategies in better using GCP for its PAAS and Azure for its SAAS  Involved in loading and transforming large sets of the structured semistructured dataset and analysing them by running Hive queries  Developed custom python program including CICD rules for Google cloud data catalog for metadata management  Design and develop spark job with Scala to implement end to end data pipeline for batch processing  Do fact dimensional modeling and proposed solution to load it  Processing data with Scala spark spark SQL and load in hive partition tables in parquet file format  Develop spark job with partitioned RDD like hash range custom for faster processing  Develop and deploy the outcome using spark and Scala code in Hadoop cluster running on GCP  Develop near real time data pipeline using flume Kafka and spark stream to ingest client data from their web log server and apply transformation  Performs data analysis and design and creates and maintains large complex logical and physical data models and metadata repositories using ERWIN and MB MDR  Assist service developers in finding relevant content in the existing reference models  Like Access Excel CV Oracle flat files using connectors tasks and transformations provided by AWS Data Pipeline  Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries  Worked on developing Pyspark script to encrypting the raw data by using Hashing algorithms concepts on client specified columns  Responsible for Design Development and testing of the database and Developed Stored Procedures Views and Triggers  Developed Pythonbased API RESTful Web Service to track revenue and perform revenue analysis  Compiling and validating data from all departments and Presenting to Director Operation  Develop SQOOP script and SQOOP job to ingest data from client provided database in batch fashion on incremental basis  Use DISTCP to load files from S3 to HDFS and Processing cleansing and filtering data using Scala Spark Spark SQL HIVE Impala Query and Load in Hive tables for data scientists to apply their ML algorithms and generate recommendations as part of data lake processing layer  Build data pipelines in airflow in CP for ETL related jobs using different airflow operators both old and newer operators  Created Big Query authorized views for row level security or exposing the data to other teams  Good knowledge in using cloud shell for various tasks and deploying services           T  Mobile     Big Data Engineer   Bellevue     WA    062016      122017     Implemented a generic ETL framework with high availability for bringing related data for Hadoop  Cassandra from various FRANCHI sources using spark  Experienced in using Platfora a data visualization tool specific for Hadoop and created various Lens and Viz boards for a realtime visualization from hive tables  Queried and analysed data from Cassandra for quick searching sorting and grouping through COL  Implemented various Data Modelling techniques for Cassandra  Joined various tables in Cassandra using spark and Scala and ran analytics on top of them  Participated in various upgrades and troubleshooting activities across enterprises  Knowledge in performance troubleshooting and tuning Hadoop clusters  Applied Spark advanced procedures like text analytics and processing using inmemory processing  Implemented Apache Drill on Hadoop to join data from SQL and No SQL databases and store it in Hadoop  Created architecture stack blueprint for data access with NoSQL Database Cassandra  Brought data from various sources into Hadoop and Cassandra using Kafka  Experienced in using Tidal enterprise scheduler and Ooze Operational Services for coordinating the cluster and scheduling workflows  Applied spark streaming for real time data transforming  Created multiple dashboards in tableau for multiple business needs  Installed and configured Hive and written Hive UDFs and used piggy bank a repository of UDFs for Pig Latin  Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access  Exported the analysed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Using Tableau  Implemented Composite server for the data virtualization needs and created multiple views for restricted data access using a REST API  Devised and led the implementation of next generation architecture for more efficient data ingestion and processing  Created and implemented various shell scripts for automating the jobs  Implemented Apache Sentry to restrict the access on the hive tables on a group level  Employed AVRO format for the entire data ingestion forecaster operation and less space utilization  Experienced in managing and reviewing Hadoop log files  Worked in an Agile environment and used rally tools to maintain the user stories and tasks  Worked with Enterprise data support teams to install Hadoop updates patches version upgrades as required and fixed problems which were raised after the upgrades  Implemented test scripts to support testdriven development and continuous integration  Used Spark for Parallel data processing and better performances  Tools Map Reduce HDFS Hive pig Impala Cassandra spark Scala solr Java SQL Tableau PIG Zookeeper Sqoop Teradata CentOS Pentaho           ITC InfoTech     Data Engineer   Pune         062014      112015     Worked on configuration and monitoring Hadoop cluster using Cloudera distribution  Involved in migrating data from on prem Cloudera cluster to AWS EC2 instances deployed on EMR cluster and developed ETL pipeline to extract logs and store in AWS S3 Data Lake and further processed it using PySpark  Moved files between HDFS and AWS S3 and worked with S3 bucket in AWS on a regular basis  Responsible for developing data pipelines using Flume Sqoop and Pig to extract the data from weblogs and store them in HDFS  Migrated data between various data sources like Teradata Oracle and MySQL to HDFS by using Sqoop  Used HCatalog to access Hive table metadata from MapReduce and Pig code  Developed a data pipeline using Kafka and Storm for streaming data and to store it into HDFS  Used Informatica Powercenter for cleaning managing and integrating data from different sources for ETL and loaded into a single warehouse repository  Used Impala to read write and query the Hadoop data in HDFS from HBase and constructed Impala scripts to reduce query response time  Analysed data stored in S3 buckets using SQL PySpark and stored the processes data in Redshift and validated data sets by implementing Spark components  Performed ETL operations using Python SQL on many data sets to obtain metrics  Prepared data according to analyst requirements on the extracted data using Pandas and NumPy modules in Python  Involved in designing and developing automation test scripts using Python  Involved in writing multiple python scripts to extract data from different API’s  Created HBase tables using Shell to load large sets of data from different databases  Involved in scheduling Time based Oozie workflow engine to run multiple Hive and Pig jobs  Developed flow XML files using Apache NiFi to process and ingest data into HDFS  Worked on performance tuning of Apache NiFi workflow to optimize the data ingestion speeds  Responsible for collecting and aggregating large amounts of log data using Flume and staging it into HDFS for further analysis  Worked on integration of Apache Storm with Kafka to perform web analytics and upload streaming data from Kafka to HBase and Hive  Responsible for developing data pipelines using Apache Kafka by implementing Kafka producers and consumers  Used Hive optimization techniques like partitioning and bucketing to provide better performance with HiveQL queries  Loaded large amounts of data to HBase using MapReduce jobs  Worked on developing UDFs to work with Hive and wrote our tests in Scala  Used zookeeper to maintain configurations across clusters and for better synchronization grouping and reliable distributed coordination  Worked with Kerberos and Apache sentry for security and authorization on Hadoop  Used Git for version control  Tools Cloudera CDH4 Hadoop 2x HDFS MapReduce Yarn Sqoop Hive AWS EC2 S3 Redshift Impala Spark Pig SQL HBase Kafka Zookeeper Flume Oozie HCatalog NiFi Storm Informatica Python MySQL Scala Teradata Oracle Git</data><data key="id">217835661821555841839767419203542304004</data></node>
<node id="n1308" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/aws-data-engineer-7289f190da885bd0d8ec5349e6934932</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases         Skills          ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                      Experience       AWS Data Engineer       012022   to   022022     Deloitte    –    Gilbert     AZ             Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Used Spark SQL for Scala  amp Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process  Configured computing networking and security systems within cloud environment  Implemented cloud policies managed technology requests and maintained service availability           Data Engineer       012016   to   112019     Cognizant Technology Solutions    –    Hatboro     PA             Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Big Data Engineer  Hadoop Developer       102013   to   122015     Novogradac  Co Llp    –    Long Beach     CA           AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Python Developer        092012   to   102013     Fiserv    –    City     STATE             AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes          Education and Training       Post Graduate      Data Engineering      Expected in   022022     Purdue University      West Lafayette     IN     GPA                Post Graduate      Data Science And Business Analytics      Expected in   092021     University of Texas At Austin      Austin     TX     GPA                Bachelor of Arts     Business Administration And Management     Expected in   122009     Califonia State University       Fullerton CA          GPA</data><data key="id">239182503578527980007051234933087188268</data></node>
<node id="n1309" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary      Resultsdriven IT management professional with 20 years of experience in diverse industries including Infrastructure and data center management Expertise includes team leadership technical architecture training  recruiting mentoring  development disaster recovery planning asset management inventory control VAR experience contract to hire experience and information protection analysis Knowledge of Enterprise Project Lifecycle methodology with a emphasis in Infrastructure engineering with strong background in project management product support including breakfix to the board level More than 15 years in IT project management skilled in installation configuration migration and implementation of rack systems cable management cable testing server power and cooling infrastructure including retrofitting commissioning and decommissioning  data centers from the ground up Dynamic resourceful and extremely driven individual with a deep passion for creating and delivering programs and solutions that empower a team company and customer to meet and exceed desired expectations               Highlights         Enterprise platforms  Experienced in infrastructure design  Forecasting specialist  Knowledge of Product Lifecycle Management PLM  Supplier interface  Performance criteria tracking  Endtoend product lifecycles  Collaborative  Inventory tracking  Vendor management         Project tracking  Hardware and software upgrade planning  Product requirements documentation  Selfdirected  Budgeting and resource management  MS Visio  Decisive  Cost reduction  Colo experience and management  Staffing augmentation hiring and training                     Education       Training Technologies    Rancho Cucamonga     CA      Expected in   1997     –      –               A CNA certification          GPA            Coursework in Computer Networking and Information Technology         Certifications       A  certified 9E8DTT2557  Network  certification  Microsoft Certified Professional 1267994 Windows NT 40  Microsoft Certified Professional 1269364 Windows 2000   7073 NT Server 40  7067 Windows 2000 Professional  70270 Windows SQL 70  Administration Microsoft Networking Essentials 7058   Hewlett Packard authorized technician  SD0436  Compaq authorized technician  Dell authorized technician  25933  Network Analysis with Sniffer  Interconnecting Cisco Network Devices  Light Brigade fiber certification           Accomplishments       Responsible for managing the infrastructure project commissioning of a 428000 Sq Ft data center from a empty building to completion including assembling the infrastructure team required to install the equipment racks cabling power successfully ahead of schedule  Responsible for managing the decommissioning project of a DR data center successfully transferring the entire contents to another DR data center including the inventory and assets  Successfully Increased core system availability to 100 by developing standards in house breakfix ability redundant hot swap equipment allowing immediate replacement within minutes instead of hours and implementing best practices  Successfully transitioned a mainframe data center to a standard open system data center including the high volume printers successfully without error  under budget and ahead of schedule  Reduced the incidence of IT issues by 50 globally by designing a training improvement program for the infrastructure engineers increasing their skill sets and knowledge with industry certifications related to their daily tasks  Reduced costs and delays by internally completing breakfix on the hardware instead of contracting out the breakfix function The quality of the repairs also increased because of pride of ownership and the expectation that it be repaired correctly the first time                          Experience       Apex Systems      Senior Data Center Lab Engineer   Middleton     WI                   072016      Current       Insight Global contract position      Install and perform repairs to hardware and peripheral equipment following design and installation specifications including copperfiber cabling for connectivity This includes Dell HP servers and blades Cisco switches and firewalls HP switches  EMC arrays F5 and related KVM devices etc     Provided feet and hands support for commissioningdecommissioning breakfix to the board level cabling installation and testing and rack installation for the Barrington Chicago Milwaukee and Wauwatosa data centers as required     Conduct computer diagnostics firmware upgrades to investigate and resolve problems and provide technical assistance and support to product developers and programmers       Design maintain and audit inventory according to present and forth coming projects      Provide space power and cooling direction  and installation according to project hardware and software requirements      Provide quarterly data center hardware audits using Visio to maintain FDA standards and requirements                Accenture      Senior Data Center Engineer   Fort Harrison     MT                   102015      072016      Apex contract position   Provided daily commissioning and decommissioning of various types of equipment into racks based on HP Service Manager customer tickets including required copperfiber cables including cable management  Provided vendor escorts to install andor provide breakfix of customer equipment  Provided breakfix for servers switches firewalls and other related equipment to the board level and beyond  Provided hands and feet services for offsite engineers to troubleshoot install repair upgrade or anything required to return their equipment to operational status  Maintained the cleanliness of the data center including the floors and both the interior and exterior of each rack being installed in the data center  Set up and configure servers and blade servers in the lab racks prior to going live on the data center floor according to owner requirements  Participate in bridge calls requiring assistance for failed equipment andor connections until corrected as the onsite engineer  Maintain and monitor the power and cooling systems and related alarms      ​          Cincinnati Bell      IBM Global Remote Data Center Manager   West Chester     OH                   022015      062015      Compunnel contract position   Manage IBM Colo for Baxter pharmaceutical customer daily request for connectivity and hardware installation remotely  Project management to both decommission old customer equipment and to commission customers design in the COLO for new racks equipment and connectivity  Managed customer split between Baxter pharmaceutical and Baxalta pharmaceutical and ensure that the connectivity and equipment is moved and reinstalled in the COLO ensuring to separate companies  Worked directly with customer network administration to complete customer request for network connectivity and hardware trouble shooting including issuing the proper TCPIP and VMWare scopes  Provided mentoring and trouble shooting experience with onsite remote hands engineers to complete customer new orders for cable connectivity and hardware installation and trouble shooting       ​          Engility Corporation      Infrastructure Manager   Fort Meade     MD                   052009      062014     Directed and managed the infrastructure and engineers to bring the data online from a empty building to a fully operational 428000 Sq Ft production data center   Managed daytoday activities of 12 infrastructure technicians and DR data center which included handling daily ITSM tickets for  power server switch and router installations decommissioning and connectivity issues  Conducted performance reviews of engineers and scheduling of PTO ECC and on call 24X7 coverage  Directed engineers course of action resolving Internet connectivity general software and hardware issues  Directed handson management of racking stacking decommissions inventory asset management cable tracing cable clean up cable testing and new circuit installations including Roadm and dark fiber  Redesigned recruiting process requiring stringent technical knowledge  and experience and training for seven data centers including the training for engineers colo space and remote hands  This involved the following Windows operating systems from XP thru Windows 7 Server 2003 Cisco Nexus 7000 6500 3750s and 2800 series Net scout Netbotz 500 Compaq RF code HP and Dell servers Liebert CRACs and PDUs Terminal server using Cisco 2600 and Cisco Airports for wireless  Participated in the weekly change control and risk meetings collaborating with all levels of management from director to CTO  Hired trained and mentored 75 new infrastructure engineers  Managed the 75 person local data center infrastructure team allocating resources to ongoing projects and enforcing deadlines  Collaborated with the global team to resolve IT support cases in the data centers           Sentinel Technologies Inc      Self employed   Bloomington     IL                   032008      052009     Assisted residential and small business owners with daily hardware and software support  This included breakfix reimaging network connectivity wireless networks cabling and network installation and maintenance  This involved the following Windows from 98 thru Vista Microsoft Office 97 thru 2007 Linksys and DLink routers and switches phone support           Granite Telecommunications Llc      Premise Technician   Chicago     IL                   092007      032008     Installed UVerse into customers homes and businesses on a daily basis which involved making cross connects at the VRAD and then completing the connection using a balun at the customers NID  Cat5coax was then installed throughout the home or business which was terminated tested and certified and connected to a television andor computer  This involved the following Windows from 98 thru Vista Microsoft Office 97 thru 2007 Linksys and DLink routers and switches including trouble tickets           Amerisourcebergen Corporation  Corporate      System Integration Manager   Dothan     AL                   102006      042007      CompuCom contract position   Managed contract account for GE Healthcare including budget profit margin hiring recruiting and mentoring  This included the projects for each of my teams in cooperation with GE managers which consisted of 45 technicians ranging from tier one to designer  Responsible for the daily operation of GE end users and responded to help desk tickets to meet SOP and SLAs which ranged from windowsUNIX server issues and everything in between  This involved Windows XP and Server 2000 and 2003 UNIX 9 10 and Linux on HP Compaq Dell and Sun servers               ​     ​          ZurichFarmers Insurance      Facility Data Center Manager   City     STATE                   062006      092006      Teksystems contract position   Installed server racks and servers into the data center and maintained and monitored their connectivity  Also maintained the Liebert 750KVA UPS and Liebert CRAC units using Liebert sitescan 30  This included the Liebert PDUs and halon fire suppression system on a 247 oncall basis  This involved Windows 2000 Server UNIX and HP AUX           Wisconsin Department Of Corrections      SIS Project Manager   City     STATE                   042006      052006      Teksystems contract position   Managed the project to transfer all ownership and technical support of the Department of Corrections computer systems to the Department of Engineering which included transferring eight thousand and five hundred Microsoft Outlook accounts and six hundred and fifty print servers  This involved Outlook 2003 and process management           JP MorganChase      Data Center Engineer   City     STATE                   2006      042006      Teksystems contract position   Installed and deinstalled servers on a daily basis and installed either the Windows server OS or the UNIX OS depending upon the requirement using a script  This also involved breakfix on Dell Compaq and HP servers to the board level and maintenance and monitoring of the Liebert power and cooling systems  Collaborated with the global team to decommission the data center and transfer the assets           Allergan Pharmaceutical      NOC Manager   City     STATE                   2005      042005      Compucom contract position   Managed the daily operation of 55 tier one and tier two technicians that responded to help desk tickets for end user assistance ranging from file and folder issues to server connectivity issues  This also involved managing the SLAs SOPs and SarbanesOxley audits to ensure FDA regulation compliance  Handled the recruiting hiring and mentoring of the staff which included vendor management           Publicis Groupe Leo Burnett USA Inc      Data Center Manager   City     STATE                   012002      2005     Directed and managed the transition from a main frame data center to a traditional open systems data center including printing services  Managed three global data centers and participated in the building and project management of a fourth data center spanning Chicago Michigan New York and Europe  Directed the management of 9 analysts 2 networktelecom technicians and 3 electricians which included training and mentoring  I also managed and was responsible for calculating the cooling and power needs of the data centers using Liebert UPS and CRACs  This involved the daily breakfix of all hardware within the data centers and the maintenance and management of the AS390 mainframe on a daily basis including transitioning from the mainframe to open systems solely  This involved Dell Compaq and HP Sun HP AUX servers and Panduit racks using Windows NT 40 and 2000 Novell 411  Managed help desk ticket and phone support queue          Skills       15 years of experience in recruiting hiring training and mentoring infrastructure engineers  10 years of experience managing the daily functioning of 247 data centers  18 years of experience performing breakfix to the board level on servers desktops laptops and most printers  10 years of experience upgrading commissioning and decommissioning global data centers</data><data key="id">65679956705363143962195955010725210777</data><data key="url">https://www.livecareer.com/resume-search/r/senior-data-center-lab-engineer-99a816296ff7460daf6f5c4230dd5baf</data></node>
<node id="n1310" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                                 609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor   Home   555 4321000        Cell           resumesampleexamplecom                  Summary      · Seasoned data engineer professional adept at understanding mandates developing plans leading and implementing enterprisewide solutions Complex problemsolver with an innovative approach A proven track record with experience in all facets of data engineering from ETL to data consumption  Extensive experience in Hadoop and cloud computing platforms with focus on automated CICD Strong programming skills in design and implementation using JavaJ2EE Scala SQL and other scripting languages Excellent leadership management qualities with excellent communication and interpersonal skills        Skills          · Design and Create scalable platforms and products in data realm including ETL in hadoop ecosystem data services and SQLNoSql databases  · Create and review high level design documentation architecture diagrams  · Design and create automated CICD for various data platform both onprem and cloud ecosystems    · Lead and participate in all aspects of SDLC collaborating with product owners engineering devops and delivery teams to align and deliver products  · Test software products to ensure that the software products developed by the engineering team meet the companys quality and standards  · Guide and mentor engineering team on technical matters including design architecture coding practices                      Experience      022020   to   Current     Lead Data Engineer      Transamerica Life Insurance Company    –    Charlotte     NC            · Managing 3 scrum teams of ETL services and devOps across data platforms  · Oversaw and develop the Largescale real data processing pipelines to handle transaction and real time data using kafka and spark to various databases as part of Data Management Platform team  · Leading migration from hbase to aerospike and solr to elasticsearch which including 30TB historical data migration using spark  · Collaborate with devOps network infra teams to create data plaform architecture and establish project plans and timeline  · Design and create seamless CICD pipelines and provide solutions to all application both onprem and cloud applications And also manage build engineering team  · Managing entitlement of the applications and tools extensively working with security risk and architecture teams to define and implement RBACs across data platform  · Working closely with security infrastructure and vendors to remediate software and infrastructure vulnerabilities including in house and third party applications part of data platform   Recipient of Values Champions Award for 2021 at Early Warning         112012   to   022020     Senior Data Engineer      Change Healthcare    –    Kennesaw     GA            · Developed Batch and streaming spark application in scala to load data kafka hbase and solr on cloudera based Hadoop platform in DMP  · Explored with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Data Frame PairRDDs Spark YARN  · Developed ETL pipelines using Streamsets to move data from kenisiskafka to various datastores  · Extensive experience in designing and implementation of continuous integration continuous delivery continuous deployment through Chef ansible bamboo gitlab and harness  · Setup full CICD pipelines so that each commit a developer makes will go through standard process of software lifecycle and gets tested well enough before it can make it to the production  · Created Streamsets pipelines to consume data from Amazon Kinesis and Redhsift for data processing         042011   to   092012     Software Engineer      General Motors    –    West Chester     OH            · Involved in analyzing system specifications designing and development for multiple J2EE wholesale applications  · Performed a shakeout test to the code migrated to UAT for the new customer setups defects and for the enhancements  · Performed acceptance testing and conducted functional testing for the customer using WebMethods and UNIX environment  · Extensively worked on PM Online application project from scratch  · Conducted full lifecycle software development from planning to deployment and maintenance  · Reviewed and modified unit and integration tests to improve software quality and reliability  · Performed backend testing using SQL queries to validate the data in the backend database Used SQL to validate backend database changes deletes and update         122007   to   032011     Software Developer      Walt Disney Co    –    Cincinnati     OH            · Delivered code to meet functional or technical specifications  · Designed frontend and backend solutions for testdriven development  · Participated in code review meetings providing input on bugs inefficiencies and potential solutions to emergent issues  · Modified existing software systems to enhance performance and add new features  · Performed regression and performance tests for updated systems         Education and Training      Expected in        Bachelor of Science     Electrical Engineering     University of South Alabama      Mobile     AL     GPA</data><data key="id">94356944266639456107307305550356935516</data><data key="url">https://www.livecareer.com/resume-search/r/lead-data-engineer-7dc7cc010ca540828e6322bd6605e6e6</data></node>
<node id="n1311" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/senior-data-engineer-5e86ce6ea1a64e939ce3a7158956de13</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Practical Database Engineer possessing indepth knowledge of data manipulation techniques and computer programming paired with expertise in integrating and implementing new software packages and new products into system Offering 12year background managing various aspects of development design and delivery of database solutions Techsavvy and independent professional bringing outstanding communication and organizational abilities Hardworking and reliable Data Engineer with strong ability in building Data pipelines Highly organized proactive and punctual with teamoriented mentality        Skills           Requirements Gathering  Analysis and Modeling  Data Warehousing  SQL Reporting  Business Intelligence      Data Management  Microsoft SQL  Database Analysis  SQL Tuning  Critical Thinking                       Experience       Senior Data Engineer       122017      Current     Splunk    –    Dallas     GA            Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Worked as part of project teams to coordinate database development and determine project scopes and limitations  Trained nontechnical users and answered technical support questions  Collected outlined and refined requirements led design processes and oversaw project progress  Created conceptual logical and physical data models for use in different business areas  Wrote and coded logical and physical database descriptions specifying identifiers of database to management systems  Applied Conceptual Logical and Physical  DimensionalRelational model designs to ETL tasks  Managed endtoend operations of ETL data pipelines maintaining uptime of 95  Assisted in User Acceptance Testing for accountingmarketing users verifying ETL jobs complied with assigned parameters achieving desired results  Worked successfully with diverse group of coworkers to accomplish goals and address issues related to our products and services  Worked closely with team members to deliver project requirements develop solutions and meet deadlines           Senior Database Consultant       022013      122017     AmazonCom Inc    –    Lewisville     TX            Created Informatica mappingsETL’s using  Informatica Power Center  to load the data from Oracle MySQL databases to SQL Server 2016 databases hosted on AWS cloud  Performed the data manipulations using various  Informatica  Transformations like Expression Lookup Update Strategy Router and SQL transformation  Designed  Informatica  workflows with many sessions with Event with task Event raise task and Email task Scheduled the created workflowsjobs using  Informatica Scheduler   Created multiple  TSQL  objects mainly Stored procedures and Functions for new and existing software application requirements  Created multiple Linked Servers for ease of the users to execute commands on remote servers  Developed new code and finetuned existing Stored procedures to improve performance while utilizing the SQL Server Profiler and Database engine tuning wizard  Created SSIS packages to migrate data from legacy systems such as Oracle MySQL HP Vertica SQL Server flat files to centralized IT destination Created SSIS packages utilizing different SSIS transformations like  Script component Merge Join Look Up  and implemented error handling and logging  Performed unit testing and QA testing at various levels of the ETL’s and actively involved in team code reviews  Developed Report Models using report builder and distributed reports in multiple formats using SQL Server Reporting Services SSRS in Business intelligence development studio BIDS and SQL Server Data ToolsSSDT Created  Parameterized Cascaded Drilldown Crosstab and Drillthrough Reports  using SSRS 2008 R22012  Created Ad hoc Queries in TSQL Stored Procedures and Views to store the data of third parties and use them in SSRS reports to generate reports on the fly Worked with multivalued parameters for parameterized reports in SSRS  Developed VBA scripts for periodic Financial Daily Weekly reports generated directly from Excel utilizing Power Pivot options in Excel  Managed report delivery based on  Time driven and Data driven subscriptions  and  report security  for providing SSRS report  server level folder level and item level  permissions to various users across the organization based on role based access controlRBAC  Worked on resolving any performance issues with SSIS packages and SSRS reports and fine tune them for better performance  Worked on cleaning up users on SQL server and audit the access level Revoke Inappropriate access setting up AD groups and grant access based on the created groups  Worked on database  performance  and  maintenance  duties  such as  tuning   backup   restoration   Provide OnCall support for Production job failures Resolve and close the time critical Incidents in an appropriate way Perform root cause analysis create Problem report and work on any subsequent code changes to stop them from reoccurring           Database Consultant       082011      022013     AmazonCom Inc    –    Lithia Springs     GA            Involved in gathering business requirements definition and design of the data sourcing and data flows data quality analysis working in conjunction with the data warehouse architect on the development of logical data models  Using  TSQL  created complex  Stored Procedures Functions Cursors Tables and Views  and used other SQL joins and statements for applications in SQL Server 20052008  Developed standalone  SSIS  packages to extract data from different sources like  SQL Server Flat Files Excel Oracle and Sybase  transform and load the data onto required databases  Using  SSIS  transformations filtered bad data from different sources using  Derived Column Lookup Fuzzy lookup and Conditional split   Involved in creation of Technical Specs Design Documents Implementation Documents and Unit testingTest case test plan documents and maintained Issue logs  Using Script Component in  SSIS  wrote  C  NET  code to generate dynamic file names and create a text files Debugged and verified the values getting stored in  variables  in runtime using  C  code  Created dynamic  SSIS  packages through Variables and Script task components C  and VBNET and scheduled the packages using SQL Server Agent to process and load the data  Scheduled the package based on the Enterprise Calendar through SQL Server Agent Created several onetime and recurring jobs for package scheduling  Created XML file for package configurations and implemented parentchild package configuration in  SSIS  packages Implemented error and event handling precedence Constraints Break Points data grid and Logging in  SSIS  packages  Using  SSRS  developed multiple types of reports including Sub Reports Parameterized and Drill Down Reports using global variables expressions and functions based on the requirements provided  Deployed  SSRS  rdl reports on to the report server and created time driven subscriptions on the deployed reports  Created cubes with multiple fact measures groups and multiple dimension hierarchies based on the reporting needs using  SSAS   Modified existing dimensions and created new dimensions as per new business requirements  Created GUI interfaces using CNET and NET Framework to simplify the database access to end business users Enhanced existing GUI interfaces based on new requirements and improved efficiency  Resolved and closed production incident tickets generated because of failure of Daily Jobs          Education and Training       Master of Science       Electrical Engineering       Expected in   012011                California State University  Sacramento      Sacramento     CA     GPA        Status   </data><data key="id">130969556747859829302611690656487220649</data></node>
<node id="n1312" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/system-services-representative-data-center-engineer-08ef2cab178d43c78255173d2136d97d</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Experience      012015   to   Present     System Services Representative Data Center Engineer      Bridger Steel    –    Evansville     WY             Hired and trained by IBM to perform multiple tasks including hardware breakfix warranty repair in a multivendor environment  I am responsible for setting up coordinating and monitoring the operation of server equipment at two Delta Airline datacenters located at the airport  I run diagnostic tests to detect machine malfunctions  Independently handle high impact critical ticketsincidents  I use my connectivity skills to connect to the servers management port with SSH or Telnet  Other duties include but not limited to the following Rack stack cable configure and provision servers  Perform installation of wiring patch panel cables network switches hubs and KVMs  Deploy Cisco Layer 23 networking equipment  Installupgradereplacedeinstall servers devices and network components  Resolve issues  Execute planned changes  Use ServiceNow to follow change management requests until completion  Be accountable for ensuring a high level of client satisfaction with service delivery  Perform required site inspections  Perform audits  Problem analysis and remediation          012012   to   012014     IT Manager      City Of Atlanta    –    City     STATE             Hired by City of Atlanta to ensure reliability and availability of City of Atlanta data centers  Provided ongoing identification diagnosis and resolution of issues for users and City departments  Collaborated with internal teams and vendors at all technical levels to troubleshoot and resolve issues  Managed Windows Update Services for enterprise wide patches and security updates  Provided Exchange 2003 and 2010 administration  Upgraded maintained and installed servers and switch equipment  Provided server searches for open records act requests and litigation discovery searches  Involved in domain migration  Installed and maintained the security system for new and existing Windows servers  Proactively monitored production systems with a sense of urgency when issues arose  Worked with enterprise network loadbalancers Juniper Palo Alto and Cisco devices  Experience working with multiple server hardware platforms including IBM Dell Sun EMC and HP  Performed data center security monitoring  Successful citywide migration of Windows XP to Windows 7  Successfully implemented security software for City of Atlanta open records requests          012003   to   012012     Remote System x Server Technical Support Specialist      IBM    –    City     STATE             Hired by IBM to ensure reliability and availability of servers for IBM customers  This role participates in remote technical support of IBM hardware and software products andor systems and include the following  Provided remote troubleshooting and analysis assistance for installation or reinstallation usage and configuration questions  Provided answers for general usage and operation questions  Provided problem determination  problem source Identification  Reviewed diagnostic information to assist in isolation of a problem cause which could include assistance interpreting traces and dumps  Identify known defects and fixes to resolve problems  Identify suspected defects and engage development teams to assist in resolution  Helped with questions regarding product documentation related to the supported products  Interpreted online manuals regarding IBM code and application interfaces  Collaborated with other support centers and business units to provide seamless problem resolution  Demonstrated proficiency in the hardware and software platform supported by maintaining applicable technical certifications  Provided technical support service delivery within established guidelines demonstrating soft skills and technical skills that contribute to client satisfaction  Demonstrated excellent oral and written communication skills  Acquired industry certification and skills training  Exceeded customer satisfaction and case resolution metrics  Supported product lines including eSeries xSeries Intellistation Blade Center AIX iDataPlex fiber switches QLOGIC Emulex Cisco Brocade Quantum tape backup libraries  Provided additional support for FastT DASD fiber channel cabling ServeRAID Manager and management processors          Education      Expected in   2011     Associates of Applied Science Degree     Cyber Security     Chattahoochee Technical College      Marietta     GA     GPA       Cyber Security        Expected in        Security Certified Professional Network Certified Professional Server Certified Professional A Certified Professional Cisco Certified Technician                           GPA               Expected in        Blade eSeries xSeries AIX NAS SAN Certified          IBM University                GPA               Summary     Committed to ongoing professional development with CompTIA A Network Server and Security certifications  Also extensive academic training in network administration tracking intrusion detection firewall configuration OS administration cloud computing High level of technical proficiency with network utilities Master level of providing upperlevel support to management Master level of providing remote troubleshooting support to ensure continual operations of critical customer network systems Master experience in fastpaced high volume call environment with 12 years experience supporting IBM xSeries Blade Center Lenovo EMC AS400 DASD fiber NAS SAN RAID network appliances and tape library products Exceptional root cause analysis skills        Highlights         MS Server 2008 MS Server 2012 MS Windows 7 MS Windows 10 VMware Redhat Enterprise UNIX Networking  TCPIP SSL TLS SSH Telnet FTP HTTPS DHCP DNS WPA2 Ping Tracert TACACS Kerberos RADIUS RAS NAS IDSIPS Firewalls ToolsApplications  Solarwinds WireShark Snort Tera Term PuTTY MS Windows Security Templates MS Windows Security Update Service Microsoft Exchange 20032010 MS Windows Active Directory LDAP Terminal Services MS Windows Access Control MegaRAID Microsoft Office Lotus Notes Visual Basic 2010                       Skills     A Certified Active Directory AIX tape backup cables cable cabling change management Cisco Cisco Certified excellent oral hardware client customer satisfaction DASD delivery Dell DHCP diagnosis documentation DNS Firewalls FTP HP hubs IBM IBM hardware IDS LDAP litigation Lotus Notes Access Exchange Microsoft Exchange 2003 Microsoft Office Windows 7 Windows MS Windows 7 MS Windows Windows XP migration Enterprise NAS Network Networking Problem analysis problem resolution processors RAS Redhat SAN SSH servers SSL Sun switches switch TCPIP technical support Technician Telnet troubleshoot troubleshooting UNIX upgrade Visual Basic wiring written communication skills</data><data key="id">303008830723647812981083487132193910900</data></node>
<node id="n1313" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary       Strong IT experience in AWS cloud computing Bigdata  Data warehousing  Strong Business Domain Knowledge in Sales Segmentation  Territory Planning  Strong Coding  SQL experience in Python and Redshift  Having an experience in Agile and deliver the results based on stories and task assigned on the sprint  Implementation Knowledge in Serverless Architecture using AWS Cloud Computing  Having Good Hands on experience in GIT as well as CICD pipeline Integration  Having Good Hands on Experience in building Data Model based on requirements         Skills           Programming Languages Java Python Unix Unix Shell Scripting  Cloud Technologies Amazon Web Services AWS  Version Control AWS CodeCommit GIT GitHub Repositories      Continuous IntegrationContinuous DeliveryCICD AWS CodeDeploy AWS CodePipeline  Databases and SQL Redshift Oracle Netezza NoSQL DDB  Elasticsearch  Big Data Technologies Hadoop Hive Ozzie AirFlow Spark PySpark Scoop Flume                       Experience       Senior Data Engineer       012018   to   Current     Splunk    –    Bloomington     MN             Collaborated with product owners to gather requirement to help build a solution for Sales Segmentation  Identified key use cases and associated reference architectures for market segments and industry verticals  Worked as part of project teams to coordinate database and pipeline development and determine project scopes and limitations  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts  Developed and managed enterprisewide big data environments  Specified user access levels for each database segment to protect database integrity and company information  Developed and implemented security initiatives to protect important company data  Established hardware requirements and devised storage capacity solutions and choose wisely on the AWS services by outweigh the requirement and business usecase           Big Data Engineer       122015   to   122017     Nike Inc    –    Gurnee     IL             Perform as a Big Data Developer and work in various phases like Data Ingestion Data Integration and Transformation  Create a Sqoop import command and pull the data both tables and all tables in DBs from MySQL DB to HDFS as part of Data Ingestion  Creates a MapReduce Programs as well as Spark programs to parse analyze and implement the solutions based on its customer need  Perform operations to filter the records from DB boundary query incremental update or insert in the Sqoop import commands  Perform operations to insert the data directly to Hive tables Change the delimiters and end line character and Change the formats like textFile AvroDataFile ORC file into HDFS systems using Sqoop Import commands  Perform Sqoop Eval function to evaluate the data in MYSQL  Perform Sqoop Export function to export the data from HDFS and loaded it into MYSQL and understand the delimiter of the file define the no of mapper to perform etc  Create a Flume config file to ingest the data from various source systems like Spool Directory NetCat Exec Sequence etc and loaded it into Avro HDFS etc  Define a proper channelsMemory File etc and loaded all the data comes from Source to Sinks  Used the interceptor to modify the data comes from Source like Filtering Regex Include the Timestamp Search and Replace etc  Create a MapReduce Program using Eclipse IDE and execute it through jar file  Create a Mapper Class and using Map method generate the key value pair as nodewithdate system utilization by writing in Context application  Create a Reducer Class and using reduce method generate the final output of system utilization per nodewithdate  Implements custom based writable class to get the two values in the mapOutputValue class  Implements custom based partitioner class to assign a appropriate data to the right reducerBasically try to achieve multilevel group by function with MR limitations  Implements Pattern Matching Logic for semistructured Data and routed the good and bad records separately  Create a custom based input format as well as record reader to parse the XML file  Handson in creating the Broadcast Variables  Handson in SparkSQL DataFrame creation DF via class object in Scala Convert DF to RDD TempTable Schema creation and So on           ETL Developer       092009   to   042015     Allegis Group    –    Jacksonville     FL             Created and implemented complex business intelligence solutions  Created conceptual logical and physical data models for use in different business areas  Developed and managed enterprisewide data analytics environments  Identified protected and leveraged existing data  Monitored multiple databases to keep track of all company inventory  Assisted maintenance team with completion of preventive maintenance and unscheduled service needs  Monitored installations to ensure compliance with local codes and industry best practices          Education and Training       Master of Science     Information Technology     Expected in        Amity University      Noida India          GPA                Bachelor of Engineering          Expected in   052008     College of Engineering Guindy Anna University      Chennai India          GPA</data><data key="id">55652676729314852703085888775946776785</data><data key="url">https://www.livecareer.com/resume-search/r/senior-data-engineer-03773a6bfb094914960c12c4167839e5</data></node>
<node id="n1314" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/aws-data-engineer-f1007ddd5382d9514a28eef265cea9d0</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary       Dynamic and motivated IT professional with around 15 years of experience as principal software developer having expertise in designing data intensive applications using Spark Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API and worked explicitly on Scala   Handled ingestion of data from different data sources into HDFS using Sqoop and perform transformations using Hive Map Reduce and then loading data into HDFS  Experience of Partitions bucketing concepts in Scala and worked on fine tuning the sparkscala and sql queries to optimize performance   Experience with different file formats like Avro  parquet  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow  Experienced with using most common Operators in Airflow  Python Operator Bash Operator  Handson experience in handling database issues and connections with SQL databases such as Netezza Oracle Redshift and PostgreSQL   Experience in designing and creating RDBMS Tables Views User Created Data Types Stored Procedures  Expert in designing ETL data flows using creating mappingsworkflows to extract data from Netezza  Oracle Servers  Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on code versioning automation tools like GIT  Experienced with JSON based RESTful web services  Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases             Skills        ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                    Education and Training       Purdue University    West Lafayette     IN      Expected in   022022     –      –       Post Graduate         Data Engineering           GPA                    University of Texas At Austin    Austin     TX      Expected in   092021     –      –       Post Graduate         Data Science And Business Analytics           GPA                    Califonia State University     Fullerton CA           Expected in   122009     –      –       Bachelor of Arts        Business Administration And Management          GPA                     Experience       Deloitte      AWS Data Engineer   Rosslyn     CA                   012022      022022     Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon RDS  Used Spark SQL for Scala Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like S3  HDFS into Spark RDD and perform computations using sparkscalasparksql to generate the output response  Creating an automated alert notifications to identify and notify the idle EMR and EC2 clusters in our application regions to reduce the cost for EC2 and EMR resources  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Developed Grafana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Environment AWS EMR S3 RDS Redshift Lambda Apache Spark HIVE SQOOP Map Reduce Python  Assessed organization technology infrastructure and managed cloud migration process  Implemented cloud policies managed technology requests and maintained service availability           Splunk      Data Engineer   Memphis     TN                   012016      112019     Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           General Dynamics      Big Data Engineer  Hadoop Developer   Bossier City     MA                   102013      122015   AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Fiserv      Python Developer    City     STATE                   092012      102013     AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes</data><data key="id">311813237829986595668673300748081977825</data></node>
<node id="n1315" labels=":CV"><data key="labels">:CV</data><data key="resume">JC     Jessica    Claire                                        100 Montgomery St 10th Floor           555 4321000                 resumesampleexamplecom                         Summary       7 years of experience as a BI developer with a proven track record in Business Intelligence BI Data Warehouse DWH and Data Analytics related consulting projects  Proven ability to identify business needs and develop valuable solutions to drive accuracy and process efficiency  Experienced in developing implementing documenting monitoring and maintaining the data warehouse extracts transformations and ETL process in various industries like Financial Health Care and Retail Industry with  Sales Marketing Inventory Management Supply Chain and Finance  domains    Delivered BI reporting solutions in Power BI Reporting Services SSRS  Expertise in  Data Warehousing Architectures  including  ETL  design  Staging   Transformations   Deltachange Data  capture  StarSchemas  Cubes and  History  loading  Skilled in writing  TSQL Queries  Dynamicqueries subqueries and joins for generating  Stored Procedures Triggers Userdefined Functions  Views and Cursors  Experience with all phases of software development life cycle SDLC and Agile methodologies  Have delivered as a team member team lead and as an independent consultant on medium to large scale projects         Skills           DataBusinessSystems  Tools MS SQL Server IntegrationSSIS Analysis ServicesSSAS Metl  Databases SQL Server 20122016Netezza 72 Oracle 11g Azure  Development Skills TSQL DAX MDX C      BI Reporting Tools Power BI Reporting Services Services SSRS  Data Quality and Standards  Agile Methodology  Programming Languages   TSQL C VB Scripting                       Education        Expected in   052013   Bachelor of Science       Information Technology    GMR Institute of Technology     Rajam India           GPA               Experience        052020   to   Current   Sr Data AnalystSupport Engineer    Honeywell         Gladstone     MO            Client  Citi Bank Irving Tx   Working as Level2 Support Engineer for Data Pipelines the Retail Customer Applications and Azure Data Staging  Analyzed large amounts of data to identify trends and find patterns signals and hidden stories within data  Developing and supporting DevOps Repos and code pipelines for all ETLs Tasks and Jobs  Developed  Standard  ETL  integrations  to extract CommunityLoanLocation data from Mainframes system and implemented Business Rules to confirm the data  Experience in creating and managing SSAS tabular models creating dimension and fact tables  Responsible for data administration tasks  maintenance plans   performance tuning   backup and security  for MS SQL Server systems  Primarily involved in data migration using SQL SQL Azure Azure storage and azure data factory SSIS Powershell  Integrated Custom Visuals based on business requirements using Power BI desktop  Developed complex SQL queries using stored procedures common table expressions CTEs temporary table to support Power BI and SSRS reports  Developed complex calculated measures using Data Analysis Expression language DAX  Embedded Power BI reports on the Salesforce portal page and also managed access of reports and data for individual users using Roles  Provided continued maintenance and development of bug fixes for the existing and new Power BI Reports            082018   to   052020   Sr BI Data Developer    Butler Technical Group         Opa Locka     FL            Led the development of Data Mart for supply chainmarketing departments along with analysis of existing data warehouse for performance improvements  Worked on all phases of data warehouse development life cycle from gathering requirements implementation testing training and support  Interviewed Business Users of the Data Warehouse and the Business Analysts to understand and troubleshoot the existing bottlenecks and problem areas  Developed applications and designed processes for transformation and data management from companywide databases  Developed automated data dictionary for Enterprise Data Warehouse using SSRS and SQL  Completed a thorough analysis of the issues with the cube processing and query processing to identify bottleneck for the existing facts and dimensions  Designed and developed ETL packages to facilitate incremental load process  Created aggregates partitions attribute relationships and user defined hierarchies with the goal to reduce the cube processing time and query processing time  Created stored procedures and performed index analysis for slow performing SSRS reports to bring down the report rendering time  Developed Enterprise wide cascading reports in SSRS that were used throughout the agency to monitor the performance of the cube logging of the nightly ETL loads and also to monitor most active users of the cube  Created robust and highperforming ETL mappings using  SSIS  and  Metl   Migrated existing selfservice reports and adhoc reports to Power BI  Developed custom calculated measures using DAX in Power BI to satisfy business needs  Assisted users in accessing databases troubleshooting malfunctions and removing systembased barriers to task completion            112016   to   072018   Sr ETL Developer    Kairos Technologies Inc         City     STATE            Client  Cvs Caremark Irving Tx      Worked on ambitious project to develop an Enterprise Wide Drug plan Record system data warehouse connecting Insurance companies and Drug Manufacturers for CVS Caremark  Worked with Business Intelligence Manager to plan BI strategies create BI road map and budget plans to deliver on organizations Business Intelligence needs  Assessed the impact of current business processes on users and stakeholders  Developed multiple dashboards analytical reports KPIs and Interactive reports based on business requirements using SSRS to support executive business needs  Transferred data from various data sourcesbusiness systems including MS Excel MS Access Flat filesCOSMOS etc to SQL Server using SSIS  Implemented Query Optimization and performance tuning for slow performing SQL queries  Involved in writing complex TSQL queries and stored procedures to perform data transformations and to support SSRS Reports  Involved in creating User Security and Roles in reporting services at both parent level and report level  Supported maintenance of Data Quality Services DQS knowledge base and development of Master Data Services database            072013   to   042016   Business Data Analyst    Tata Consultancy Services         City     STATE            Client  Sun Trust Bank US   Designed Implemented and aided EndtoEnd data migration project using Microsoft SSIS  Assisted in creation of internal applications for marketing and customer relations departments including financial reporting tools and reports  Designed SSIS Packages to extract data from various data sources such as Excel spreadsheet and flat files and load data into destination databases for further Data Analysis and Reporting  Performed rigorous business case analyses and proposed various process improvements  Used Excel and Access applications for visualizing the data functionality  Led an initiative to convert all excel files which were used in inhouse timeexpense reporting wherein I automated the process to find discrepancies in timesheets by creating tables views using stored procedures    Client   Marks and SpencerUK   Improved business direction by prioritizing customers and implementing changes based on collected feedback  Analyzed open orders backlog and sales data to provide sales team with insights  Created tables and define their relations by using foreign key and primary key relationship  Involved in creating database objects using stored procedures Triggers Functions Views and TSQL Joins using TSQL  Performed normalization of database to reduce redundancy and achieve maximum system performance  Wrote custom TSQL stored procedures and triggers to improve performance preserve referential integrity and provide additional application functionality  Worked as ETL Developer for an EndtoEnd implementation of Operation Data Warehousing Project  Responsible for data integration and transformation using SSIS  Formulated and documented the physical ETL process design based on business requirements and system specifications with strong ETL design skills  Created source to target mappings transformations lookups aggregations expressions  Involved in scheduling reports creating snapshot reports and subscription for the reports Using SSRS</data><data key="id">71493749037436587054693787398602463147</data><data key="url">https://www.livecareer.com/resume-search/r/sr-data-analyst-support-engineer-0690548375b14b17a0ecaf5aa825cc34</data></node>
<node id="n1316"><data key="name">pig</data></node>
<node id="n1317"><data key="name">sqoop</data></node>
<node id="n1318"><data key="name">oozie</data></node>
<node id="n1319"><data key="name">s3emrlambda</data></node>
<node id="n1320"><data key="name">ml use cases</data></node>
<node id="n1321"><data key="name">scheduling jobs</data></node>
<node id="n1322"><data key="name">data ingestion</data></node>
<node id="n1323"><data key="name">pipeline design</data></node>
<node id="n1324"><data key="name">distcp</data></node>
<node id="n1325"><data key="name">aster</data></node>
<node id="n1326"><data key="name">flume</data></node>
<node id="n1327"><data key="name">zookeeper</data></node>
<node id="n1328"><data key="name">ranger</data></node>
<node id="n1329"><data key="name">storm</data></node>
<node id="n1330"><data key="name">amazon aws</data></node>
<node id="n1331"><data key="name">api design</data></node>
<node id="n1332"><data key="name">mdm</data></node>
<node id="n1333"><data key="name">tidal</data></node>
<node id="n1334"><data key="name">uc4</data></node>
<node id="n1335"><data key="name">autosys</data></node>
<node id="n1336"><data key="name">opcon</data></node>
<node id="n1337"><data key="name">data pipeline</data></node>
<node id="n1338"><data key="name">cloud storage</data></node>
<node id="n1339"><data key="name">mpp</data></node>
<node id="n1340"><data key="name">uat</data></node>
<node id="n1341"><data key="name">load testing</data></node>
<node id="n1342"><data key="name">performance testing</data></node>
<node id="n1343"><data key="name">latin</data></node>
<node id="n1344"><data key="name">hiveql</data></node>
<node id="n1345"><data key="name">bitbucket</data></node>
<node id="n1346"><data key="name">xmldat</data></node>
<node id="n1347"><data key="name">linear regression</data></node>
<node id="n1348"><data key="name">naive bayes</data></node>
<node id="n1349"><data key="name">logistic regression</data></node>
<node id="n1350"><data key="name">kmean</data></node>
<node id="n1351"><data key="name">n grams</data></node>
<node id="n1352"><data key="name">distance gradient descent</data></node>
<node id="n1353"><data key="name">shiny</data></node>
<node id="n1354"><data key="name">dplyr</data></node>
<node id="n1355"><data key="name">caret</data></node>
<node id="n1356"><data key="name">impala</data></node>
<node id="n1357"><data key="name">bo xi</data></node>
<node id="n1358"><data key="name">webi</data></node>
<node id="n1359"><data key="name">os x</data></node>
<node id="n1360"><data key="name">red hat enterprise</data></node>
<node id="n1361"><data key="name">wins</data></node>
<node id="n1362"><data key="name">dfs</data></node>
<node id="n1363"><data key="name">data mart</data></node>
<node id="n1364"><data key="name">yarn</data></node>
<node id="n1365"><data key="name">mahout</data></node>
<node id="n1366"><data key="name">sparklib</data></node>
<node id="n1367"><data key="name">cosmos</data></node>
<node id="n1368"><data key="name">pycharm</data></node>
<node id="n1369"><data key="name">boto3</data></node>
<node id="n1370"><data key="name">map reduce</data></node>
<node id="n1371"><data key="name">snowflakes</data></node>
<node id="n1372"><data key="name">snowsql</data></node>
<node id="n1373"><data key="name">datadog</data></node>
<node id="n1374"><data key="name">cloudtrail</data></node>
<node id="n1375"><data key="name">pilot testing</data></node>
<node id="n1376"><data key="name">requirement gathering</data></node>
<node id="n1377"><data key="name">scripting</data></node>
<node id="n1378"><data key="name">cloudera</data></node>
<node id="n1379"><data key="name">mesos</data></node>
<node id="n1380"><data key="name">visual source safe</data></node>
<node id="n1381"><data key="name">clearcase</data></node>
<node id="n1382"><data key="name">ldap</data></node>
<node id="n1383"><data key="name">red hat</data></node>
<node id="n1384"><data key="name">subversion</data></node>
<node id="n1385"><data key="name">photoshop</data></node>
<node id="n1386"><data key="name">illustrator</data></node>
<node id="n1387"><data key="name">indesign</data></node>
<node id="n1388"><data key="name">dreamweaver</data></node>
<node id="n1389"><data key="name">coldfusion</data></node>
<node id="n1390"><data key="name">gcs</data></node>
<node id="n1391"><data key="name">paas</data></node>
<node id="n1392"><data key="name">saas</data></node>
<node id="n1393"><data key="name">apache drill</data></node>
<node id="n1394"><data key="name">apache sentry</data></node>
<node id="n1395"><data key="name">apache storm</data></node>
<node id="n1396"><data key="name">query analyzer</data></node>
<node id="n1397"><data key="name">macos</data></node>
<node id="n1398"><data key="name">kerberos</data></node>
<node id="n1399"><data key="name">database modelling</data></node>
<node id="n1400"><data key="name">dimension modeling</data></node>
<node id="n1401"><data key="name">er modeling</data></node>
<node id="n1402"><data key="name">star schema</data></node>
<node id="n1403"><data key="name">databrick</data></node>
<node id="n1404"><data key="name">cloud watch</data></node>
<node id="n1405"><data key="name">solr</data></node>
<node id="n1406"><data key="name">data frame</data></node>
<node id="n1407"><data key="name">pairrdds</data></node>
<node id="n1408"><data key="name">acceptance testing</data></node>
<node id="n1409"><data key="name">data management</data></node>
<node id="n1410"><data key="name">database analysis</data></node>
<node id="n1411"><data key="name">unit testing</data></node>
<node id="n1412"><data key="name">etl’s</data></node>
<node id="n1413"><data key="name">flat files</data></node>
<node id="n1414"><data key="name">version control</data></node>
<node id="n1415"><data key="name">codecommit</data></node>
<node id="n1416"><data key="name">codedeploy</data></node>
<node id="n1417"><data key="name">netezza</data></node>
<node id="n1418"><data key="name">ddb</data></node>
<node id="n1419"><data key="name">ozzie</data></node>
<node id="n1420"><data key="name">reporting skills</data></node>
<node id="n1421"><data key="name">sparkscala</data></node>
<node id="n1422"><data key="name">hortonworks</data></node>
<node id="n1423"><data key="name">toad</data></node>
<node id="n1424"><data key="name">aws s3</data></node>
<node id="n1425"><data key="name">amazon emr</data></node>
<node id="n1426"><data key="name">aws emr</data></node>
<node id="n1427"><data key="name">amazon sagemaker</data></node>
<node id="n1428"><data key="name">apache pig</data></node>
<node id="n1429"><data key="name">azure data factory</data></node>
<node id="n1430"><data key="name">azure synapse</data></node>
<node id="n1431"><data key="name">azure data lake</data></node>
<node id="n1432"><data key="name">aws glue</data></node>
<node id="n1433"><data key="name">apache oozie</data></node>
<node id="n1434" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary       7 years of IT Industry experience in in designing and developing data mininganalytics solutions data centric integration developing and maintaining Business analytics   Adept in data query data migration data analysis predictive modeling machine learning data mining and data visualization implementations with extensive use of SQL Python R Java and Unix Shell scripting with platform of Toad Oracle developer Jupyter Notebook Pycharm RStudio Tableau Hadoop with Spark  Experience in predictive analytic procedures used in supervised learning Classification Regression Decision trees Random Forest SVM Neural Networks unsupervised learning ClusteringkMeans and PCA and reinforcement learning  Solid theory background for machine learning data mining text mining graph mining statistics modeling NLP and deep learning  Expert in Natural Language Processing like POS Tagging Parsing Named Entity Recognition Relationship extraction Information       Retrieval Sentimental Classification Machine Translation etc  Solid knowledge of deep learning algorithms like CNN RNN LSTM GRU etc for text mining and image processing  Professional in writing complex SQL queries on Oracle MS SQL server Teradata and MySQL using a lot of subqueries joins aggregate functions analytical functions and temporary tables etcWorked on Big Data Analytics Hadoop ecosystems Hadoop Sqoop Hive Pig Mapreduce and Spark for big data migration cleaning transformation processing query and analysis  Familiar with software lifecycle includes requirement collectingdocumentation development and testing for Unit Smoke  integration system nonfunctional testing regarding performance scalability usability enduration load and volume testing and regression testing multivariate testing AB testing and system maintenance  Work with business domain experts and application developers to identify data relevant analysis  mining and develop new predictive  analytical modeling methods andor tools in Financial like loan and foreign exchange Product Customer Sales domains etc  Experience in data aggregation and reduction techniques of large data sets with high performance and parallel computing for high performance analytical projects  Involved in diagnosing and resolving predictive  analytical model performance issues monitoring analytical system performance  ·       and implementing efficiency improvements  Conversant with MS SQL Oracle PLSQL and RDBMS Contributed in data definitions for new database filetable development andor changes to existing ones as needed for analysis and mining purpose  Experienced Oracle PLSQL Developer for designing developing debugging  maintaining and administrating database in Oracle RDBMS Solid experience in PLSQL and SQL programming and performance tuning   Familiarity with Oracle data warehousing features such as star  snowflake data modeling schemas materialized views bitmap indexes Index Organized Tables external tables etc and OLTP system using Btree index Hash Join etc  Experienced in frontend developing using Java Javascript C and backend developing using C  Professional in integrating and maintaining code using version control tools PVCS SVN CVS  Solid experience and knowledge in ETL and Data warehousing conceptsData Processing experience in designing implementing transformation processes using ETL tool   Involved in all aspects of ETL requirement gathering with standard interfaces to be used by operational sources data cleaning data loading strategies ETL mappings designing documentation and ETL jobs performance testing                    Using Unix bashcorn shell scripting to do backend process operation system resources  checking job scheduling batch data loading performance tuning and reporting     Conversant with Project Management deliverables and SDLC phases – Waterfall and Agile   A selfstarter team player excellent communicator prolific researcher  Expert technical documentation skills Strong interpersonal and communication skills both written and oral ability to communicate with people in a wide variety of areas and at various levels from technical specialists to senior management          Skills                        Roles  Data Scientist Data Analyst Business System Analyst Oracle PLSQL developer   Data Visualization  D3js Tableau R visualization packages Microsoft Excel    Data Analytics ToolsProgramming                 Python numpy scipy pandas scikitlearn gensim keras Rcaret weka ggplot  MATLAB Microsoft SQL Server Oracle PLSQL     Machine Learning Algorithms  classification regression clustering feature engineering    Big Data Tools  Hadoop MapReduce SQOOP Pig Hive NOSQL Cassandra Spark    Others Deep Learning NLP Topic Modeling Sklearn Graph Mining Text Mining C C  Java Javascript ASP Shell Scripting  					 				 			 		        Experience       Data Scientist       082011      092016     Lockheed Martin Corporation    –    Yuma     AZ                              Actively develop predictive models and strategies for effective fraud detection for credit and  customer banking activities using Kmeans clustering using Python    Assisted senior data scientist to do text mining on customer reviewcomment data using topic modeling  and sentimental classification using deep learning algorithms like CNN RNN LSTM GRU to  remediate according financial products using Python    Assisted senior quantitative analyst in assessing risk management of financial derivative products like foreign exchange products bonds funds etc using machine learning techniques for providing appropriate investment recommendations using Collaborative filtering recommender system using Python    Mentored sophisticated organizations on large scale customer data and analytics using advanced  machine learning and statistical models relying for issuing loan using Random Forest using R    Performed kMeans clustering in order to understand customer backgrounds and segment the  customers based on the customer transaction behavior information for customized product  offering customized and priority service to improve existing profitable relationships and to avoid customer churn etc using R    Worked on Interactive Dashboards for building story and presenting to business using Tableau     Implementing Hadoop to provision big data analytics platforms for customer data Used  MapReduce Sqoop Hive and Spark to migrate and analyze large callqualitydata datasets from  multiple Data sources like integrated funds transfer system like FedWire CHIPS SWIFT for  securities treasury or derivatives and webbased cash management systems eGifts GiftsWEB GiftsWEB EDD for fraud detection and risk management for accounts based on positive pay and Automated Cash Handling balance reporting etc    Installed and configured Hadoop cluster in Test and Production environments Moving data from Oracle 9i database to HDFS and viceversa using SQOOP Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Developed multiple MapReduce jobs in java for data cleaning and preprocessing Writing Pig scripts to transform raw data from several data sources into forming baseline data  Solved performance issues in Hive and Pig scripts with understanding of Joins Group and aggregation and how does it translate to MapReduce jobs Developed Oozie workflow for scheduling and coordinating the ETL process Using Spark for further data analysismining     Experience in using Sequence files RCFile AVRO and HAR file formats  Work with Data Analytics team to develop time series and optimization    Involved in development and maintenance of Oracle database using PLSQL and backend  development using CC for intranet management system for Employee Management System  EMS and Agent Payout System APS              Business System Analyst       042009      082011     Accenture    –    Overland Park     KS            Project Summary   This project is in Application service group for Mercury system in Canon USA Inc  which mainly in charge of the new item request item disclosure between companies item data import from other Canon Americas companies to Canon USA Canon Americas Master inquiry Model tree maintenance model configurations and cameravideo merchandise Maintenance for Canon Americas systems includes S21 for Canon USA merchandise master S98 for Panama CCI21 for Canada S85 for Mexico Chili Brazil and Argentina Ideal for Latin Americas countries       HardwareSoftware       Windows VistaNTXP7 Linux Oracle 11g10g9i8i SQLPLUS  Oracle SQL developer Toad  Microsoft SQL Server management studio 2008Microsoft Visual Studio 60ODBCJDBC  Microsoft IIS 511 Putty Cygwin Winmerge VPN ITG project management system PQedit IIS AutosysPCXware 510 MS Word Excel Access Project Visio      Responsibilities     Operational support for Canon Americas Mercury system includes data adjustmentresearch batch data loading system migration Technical and functional specification documentation reports business process alignment workflow stuck and reconciliation etc      BreakFix any issues or bugs collected from client and development regarding setup performance functionality and workflow stuck etc   System Enhancement regarding functionality and performance etc  Reproduce and review existing oracle 9i schema objects includes tables temporary tables views materialized views indexes triggers procedures functions packages based on customer requirement and system upgrade using Toad and Oracle SQL developer tools        Review and analyze ASP code for UMC Mercury application web development for data research and system feature fix and enhancement using Visual Interdev 60        Query realtime data regarding Canon Americas new item request item status inquiry item data disclosure and import model configuration and maintenance warranty maintenance and model configuration inquiry  etc  using complex SQL queries on Oracle 9i Canon mercury database      Using Unix bashcorn shell scripting to do backend process operation system resources checking job scheduling batch data loading performance tuning and reporting  Maintain scheduled day and night batch jobs for mercury system using AutosysPCXware and Unix box and check the MQ series using PQ edit      Implement client session action module service instance level endtoend application tracing using SQL trace with TKPROF and Explain plan to check execution plans for highload and Top SQL statement  Using Cygwin FTP Putty with Unix Bash shell to make a tunnel for Oracle database connection       Using Tortoise SVN for code checkout update and release–comparison etc       Tracking and documenting tickets for development and reproduction Using ITG ticket tracking system                        Assisted QA and build team to be involved in unit smoke integration system UAT nonfunctional testing regarding performance scalability usability  enduration load and volume testing and  regression testing and maintenance using SOUP UI and Seapine QA Wizard Pro for product release          Data loading using Impexp data pump and external tables from Americas Mercury system to S21CUSA merchandise master to RossCUSA retails system and Global Mercury system                Oracle PLSQL Developer       2008      042009     Cognizant Technology Solutions    –    Novi     MI            Project Summary   NYU Langone Medical Center a worldclass patientcentered integrated  academic medical center is one of the nation’s premier centers for excellence in healthcare biomedical research and medical education The project is to develop new oracle database objects on online Health Information Managment system on FindWdev instanceserver for 29 NYU medical school departments using in clinical education research and foundational areas etc to be used as Oracle staging area to store the loaded data from NYU Medical Dash DWH from different source systems to provide further data to be loaded into DWH for historical record Decision support and Datamart for reports  and Cube for UI display      HardwareSoftware     Unix Oracle 11g Oracle developer 11g Oracle EBS 11 ERP R12 IBM DataStage 80 Designer Director Manager Parallel Extender Oracle Enterprise Manager MS SQL server management studio 2008 Toad for Oracle 90 TSQL  PLSQL XML Erwin Microsoft Visio Autosys IBM Data stage 8 Oracle reports 11g   Responsibilities                      Independently develop Oracle database objects includes tables views materialized views indexes triggers functions procedures packages etc     Cooperated with BA SME to collect and document database design requirements and do data modeling with DB architect using Erwin and Microsoft Visio     Assisted DBA for job scheduling data loading and performance tuning using OEM SQL tuningaccess advisor hints  explain plan  SQL trace and V performance views under Unix     Write complicated queries using a lot of aggregate functions joins analytical functions subqueries  etc to provide realtime data from Oracle DB  for client and UI development supporting  Checking execution plan using explain plan together with SQL trace with TKPROF using trcsess under unix to realized endtoend application tracing      Add optimization hints into high–load and top SQL statements to change the optimization goal access method join method join order and parallelization etc    Designed and developed ETL processes using DataStage designer to load data from Oracle to staging database and from staging to the target Data Warehouse    Worked with Datastage Manager for importing metadata from repository new job Categories and creating new data elements  Used DataStage stages namely Hash file Sequential file Transformer Aggregate Sort Datasets Join Lookup Change Capture Funnel Peek Row Generator stages in accomplishing the ETL Coding  Job scheduling using Autosys  Coorporated with QA team for debugging unit system functional UI regression testing for new ISO release production    Working on Linux system for batch data loading job scheduling and system resource checking etc    Assisted backend developer for reviewing and debugging C program for Health information management systems                  Involved in web development of online Health information management systems using JAVA  Reviewed and reproduced online JAVA reports of Health information management system checked DB references in it for intelligent Decision Support System            Education       Master of Science       Computer Science       Expected in   2016                University of California      Los Angeles     CA     GPA        Status          GPA378   Courses                Statistics Programming Databases and Knowledge Bases Graphs and Network Flows  Language and Thought Current Topics in Computer TheoryMachine Learning Algorithm Computer Science ClassicsBasic Data Science Data Mining and Big Data Analytics System Security            Master of Science       Electrical Engineering       Expected in   2010                University of Bridgeport      Bridgeport     CT     GPA        Status          GPA362   Courses Computer Networks Database Management Systems Data and Computer Communications Data Structures          Bachelor of Science       Telecommunications Engineering       Expected in   2007                Jilin University      Changchun     Jilin     GPA        Status         GPA 350</data><data key="id">57144578075288773773635457122564211508</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-2a3791654fd140e2ad9f230d470f1043</data></node>
<node id="n1435" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-001599f915214ef19135cfa5398e06b8</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Professional Summary      Highly driven Data scientist with 5 years of experience who brings formidable solutions to diverse complex business problems using machine learning data mining and statistical analysis  Familiar with gathering cleaning and organizing data for use by technical and nontechnical personnel Advanced understanding of statistical algebraic and other analytical techniques  Organized motivated and diligent with significant background in PythonMachine LearningDeep learning Experienced working with vast data sets to break down information gather relevant points and solve advanced business problems Skilled in predictive modeling data mining and hypothetical testing        Skills           Machine learningStatistical AnalysisData Mining  · Python R SQL  · Keras NumPy Pandas Scikitlearn NLTK Spacy TensorFlow  Development Tools Anaconda PyCharm Spyder Visual Studio RStudio      Theoretical Competencies AGILE Scrum Algorithms Data Structures Databases SDLC  Version Control Git Bitbucket Perforce  Web TechnologiesAWS GCP Azure                       Work History       Data Scientist       062019   to   122020     Lockheed Martin Corporation    –    Durham     NC             1MedConnect   Jul 20  Dec 20    Developed and implemented a Forecasting algorithm to predict sales trx total count and nrx individual count of medicine purchasedCreated data visualization graphics translating complex data sets into comprehensive visual representations  Forecasted sales and improved accuracy – MAPE and RMSE by 30 by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns  Increased accuracy helped business plan better with respect to budgeting and sales and operations planning Tuned model parameters p d q for ARIMA using walk forward validation techniques for optimal model performance    2Imperial Bank  Nov 19  Jul 20    Clustered the customers owning the credit cards to optimize the targeting and offerings to increase the customer satisfaction and which in turn enhances revenue  Provided Comprehensive analysis and recommended solutions by applying advanced analytical methods to assess factors impacting growth and profitability across product and service offering by running the model against unstructured dataChat Conversations and identified the topic of chat  Generated word frequency for 1gram 2gram and 3gram words and used regex to identify the frequency of the main topicsissues Performed clustering on the TFIDF to segment the data    3Market Place Intelligence  Jun 19  Nov 19    Collaborated with senior personnel to define and meet data modeling standards for deep dive projects  Translated benefits of machine learning technology for nontechnical audiences including costbenefit analyses  Evaluated processes and data identifying productivity gains and sales growth within digitization segment           Data Scientist       112015   to   062019     Lockheed Martin Corporation    –    El Segundo     CA             1Client Samsung  Apr 17  Jun 19    Developed voice enabled features for Samsung Health Internet and Gallery applications Performed NLP tasks and techniques such as parsing text pre processing named entity recognition Modelling and Testing Train set preparation model building for CNN based Intent classifier and Slot tagger  Applied statistical and algebraic techniques to interpret key points from gathered data Best epoch selection by plotting cost functions  Coached developed and motivated team members providing coaching and mentoring to junior data scientists on SAS and data mining techniques           Software Engineer       112015   to   042017     Cgi Group Inc    –    Norfolk     VA             Adjusted design parameters to boost performance and incorporate new features  Checked client code for bugs and weaknesses and fixed the issues accordingly  Collaborated with business unit team members to design new applications system to enhance client requirements for mobile computing capabilities          Education       Master of Science     Computer Science     Expected in   012021     University Of Alabama At Birmingham      Birmingham     AL     GPA                Bachelor of Science     Electronics And Communication Engineering     Expected in   072015     SVPCET      Anantapur          GPA         Graduated with 324 GPA           Board of Intermediate Education     Mathematics Physics And Chemistry     Expected in   062011     Board Of Intermediate      Tirupathi          GPA         Graduated with 926100 percentage           High School Diploma          Expected in   052009     Infant Jesus English Medium School      Chittoor          GPA         Completed with 888100 percentage          Accomplishments       Received Best Employee of the month Award from Deloitte for Documenting and Resolving issues which led to a optimal solution to client  Received APPRECIATION Award from client SAMSUNG for colloborating with team in the development of Bixby Project  Received PAT on the BACK Award from Tech Mahindra for on time quality delivery work         Certifications       Completed the certification of “Neural Networks and Deep Learning” “Improving Deep Neural Networks” Hyperparameter tuning Regularization and Optimization “Convolutional Neural Networks” in Coursera by Andrew NG  Completed the certification of “Machine Learning” in Coursera by Carlos and Emily  Completed the certification of Google Cloud Platform Big Data and Machine Learning Fundamentals in coursera</data><data key="id">147256625688504397034381753044625660336</data></node>
<node id="n1436" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Professional Summary      Data Scientist with over 4 years of successful experience in Machine Learning NLP and Predictive Modeling Recognized consistently for performance excellence and contributions to success in business industry Strengths in math and programming skill backed by training in data science            Skills         Database MySQL Postgres SQL MongoDB  Programming Language Python and R  Machine Learning  Data Mining  Natural Language Processing  Statistical analysis      AB Testing  Project Management  Agile framework understanding  BigData HBase Hadoop  Hiveand Spark  Operating Systems LINUX WINDOW  Environment AWS AZURE Databricks                     Education       Colorado State University    Fort Collins     CO      Expected in   052019     –      –       Master of Science        Applied Statistics Specialization Data Science          GPA            GPA 343 400   Professional development completed in Data Science           Addis Ababa University    Addis Ababa           Expected in   062011     –      –       Master of Science        Biochemistry  Biophysics          GPA            GPA 350 400          Addis Ababa University    Addis     Ababa      Expected in   082003     –      –       Bachelor of Science        Medical Laboratory Technology          GPA                   Certifications       Introduction of TensorFlow for AIMachine Learning and Deep Learning Training  Nov 2019           Work History       Yahoo      Data Scientist   New York     NY                   062019      Current     Transformed raw data into MySQL with custommade ETL application to prepare unruly data for machine learning  Performed data cleaning features scaling featurization features engineering used Pandas NumPy SciPy Matplotlib Seaborn Scikitlearn in Python at various stages for developing machine learning model and AB Testing multivariate to measure impact on new initiatives  Used spark’s machine learning library to build and evaluate different models  Developed endtoend machine learning prototypes and scaled them to run in Aws sagemaker notebooks  Collaborated with internal stakeholders identifying and gathering analytical requirements for customer product and projects needs  Provided comprehensive analysis and recommend solutions to address complex business problems and issues using data from internal and external sources and applied advanced analytical methods to assess factors impacting growth and profitability across product and service offerings           Epic Games      Capstone Project    Colorado Springs     CO                   092017      062019     Built Recommendation system using cosine similarity to choose similar songs added to given playlist  Customer Churn Prediction Projects  Pick up top model based on recall score Evaluated models using Cross validation recall used to measure performance and used ROC curves and AUC for feature selection  Predict product categories for ibotta datasets Built machine learning classifier for many datasets and applied data mining to improve text quality by autocorrecting grammatical issues  Analysis on NBA Real PlusMinus for 20142015 Regular Seasons Applied multiple regression to examine relationship between players’ performances and numerical variables  Potato Virus Y detection using machine learning algorithms Pick up top model for detection of potato Y virus in potato’s seed from Xgboost Random Forest SVM Decision Tree kNN and Logitboost on chemical fingerprinting datasets           Houston Methodist      Research Scientist   Jacksonville     FL                   112015      052019     Use data visualization tools to help tell story about findings from data to people who are not necessarily dataliterate  Performed data collection data cleaning and analysis and recoding of existing vendor laboratory computer software  Ability to collaborate effectively with mentor and others in lab on new research projects  Excellent interpersonal skills crossgroup and crossculture collaboration  Lead Quality Assurance Data Management in different instruments evaluation and analysis of QAQC data identifying trends and shift and taking corrective actions           Swedish Medical Center      Medical Laboratory Scientist   City     STATE                   112011      102015     Collect store retrieve analyze and present data  Issues and regularly interfaces with hospital staff and administrators regarding their finding and collaborate with teams to integrate systems  Help clinicians and other staff troubleshoot technology issues and regularly interface with hospital staff and administrators regarding their findings and collaborate with teams to integrate systems  Led quality control diagnostic testing documenting all variances in results and blood product performance</data><data key="id">86124719202524122377094036987621447706</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-b4624bc712c04044834adfff057941ab</data></node>
<node id="n1437" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-0701abab38a940ccbd523ad6b6ba8810</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary       Data Scientist in 4 years of broadbased experience in building data processing data mining algorithms business need analysis and informative reporting solutions Good understanding of data visualizations using R Python and creating dashboards also working with AWS cloud Dedicated and hardworking with passion for Big Data         Skills           Operating system    Windows  MacOS    Database Server     MySQL SQL server    Languages     R Python SQL    ML Algorithm      Linear Regression  Logistic Regression  Decision Trees  Supervised Learning and Unsupervised Learning Classification SVM Random Forests Naive Bayes KNN K Means CNN    Other Software  Tools     AWS services Git Docker hub Hadoop Spark scikitlearn NumPy Pandas Matplotlib SciPy                       Experience       Data Scientist       082021   to   Current     Hanes Brands    –    Gaithersburg     MD             Wrote and executed various MySQL database queries from Pychram and MySQL Db package  Hands on experience with Docker and Git for version control and code sharing with team members  Conducted business analysis to understand business needs and requirements to translate into conceptual designs Proficient in performing data analysis on various IDEs like VSC and PyCharm  Building predictive models using various machine learning tools to predict the possibility of equipment failure  Developing algorithms using Natural Language Processing  Creating charts and graphs for company’s outcome reporting purpose  Knowledge of Hadoop ecosystem and different frameworks inside it – HDFS YARN MapReduce Apache Pig and realtime processing Framework Apache Spark  Involved in understanding the applications using all AWS services like EC2 S3 bucket           Data Scientist       062018   to   122020     Hanes Brands    –         MD             Designing of Queries compiling of data and generating reports in MS Excel and MS Access  Data collection and entry as needed and problem solving  Created various types of data visualizations using Rggplot2 Python Matplotlib and Power BI  Writing and executing SQL queries  Data collection and entry as needed and problem solving  For getting company insights created dashboard with Tableau           Entry Level Data Scientist       052017   to   052018     200ok Solutions    –         STATE             Implemented ML algorithms to evaluate and solve diverse company problems  Developing and automating the data manipulation process for above using store proceduresviews in SQL Server Also Creating data visualization and reports for requested projects  Using various packages like NumPy SciPy Pandas Natural Language Processing Toolkit matplotlib to build the model          Education and Training       Master of Science     Data Science      Expected in   122022     New Jersey Institute of Technology      Newark     NJ     GPA                Bachelors of Engineering      Information Communication Technology     Expected in   052018     LJ Institute of Engineering and Technology      Ahmedabad Gujarat           GPA               Activities and Honors              Skills       Operating system    Windows  MacOS    Database Server     MySQL SQL server    Languages     R Python SQL    ML Algorithm    Linear Regression  Logistic Regression  Decision Trees  Supervised Learning and Unsupervised Learning Classification SVM Random Forests Naive Bayes KNN K Means CNN    Other Software  Tools     AWS services Git Docker hub Hadoop Spark scikitlearn NumPy Pandas Matplotlib SciPy         Work History       Data Scientist     082021   to   Current     Thinksoft Technologies   –   Tampa     FL      Wrote and executed various MySQL database queries from Pychram and MySQL Db package  Hands on experience with Docker and Git for version control and code sharing with team members  Conducted business analysis to understand business needs and requirements to translate into conceptual designs Proficient in performing data analysis on various IDEs like VSC and PyCharm  Building predictive models using various machine learning tools to predict the possibility of equipment failure  Developing algorithms using Natural Language Processing  Creating charts and graphs for company’s outcome reporting purpose  Knowledge of Hadoop ecosystem and different frameworks inside it – HDFS YARN MapReduce Apache Pig and realtime processing Framework Apache Spark  Involved in understanding the applications using all AWS services like EC2 S3 bucket           Data Scientist     062018   to   122020     Teleysia Networks PvtLtd   –        India      Designing of Queries compiling of data and generating reports in MS Excel and MS Access  Data collection and entry as needed and problem solving  Created various types of data visualizations using Rggplot2 Python Matplotlib and Power BI  Writing and executing SQL queries  Data collection and entry as needed and problem solving  For getting company insights created dashboard with Tableau           Entry Level Data Scientist     052017   to   052018     200ok Solutions   –        India      Implemented ML algorithms to evaluate and solve diverse company problems  Developing and automating the data manipulation process for above using store proceduresviews in SQL Server Also Creating data visualization and reports for requested projects  Using various packages like NumPy SciPy Pandas Natural Language Processing Toolkit matplotlib to build the model</data><data key="id">110719289910414187169856379639115720786</data></node>
<node id="n1438" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/graduate-assistant-data-scientist-30e747ef925940eb909e346892bc157e</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Experience       Graduate Assistant Data Scientist       011      011     Pace University    –                     Information System IS Analyze data using SPSS for faculty development  Part of the research team on conference proceedings like AMCIS ICIS and ECIS on geographic information system GIS spatial analysis and location analysis  Assist undergraduate students understanding SQL MYSQL etc  Data Visualization using tableau pivot tables D3js etc           Business Analyst  eCARGO       011      011     Information System    –                     The project involves responsibility for the product planning and execution throughout the Product Lifecycle including gathering and prioritizing product and customer requirements defining the product vision and working closely with operations sales client delivery technology and other stakeholders to implement new products ensure revenue and customer satisfaction goals are met  The Product Managers job also includes ensuring that the product supports the companys overall strategy and goals  Responsibilities Reviewed  gathered business requirements with the business and created business analysis process  Created detailed Functional Requirement Specification FRS and NonFunctional Requirements for Individual Market Portal Project based on the approved scope document  Facilitated JAD sessions and workshops required to understand workflows business needs and storyboards  Conducted Scrum meetings on regular basis  Kept the Product Owner  Project Manager informed about project status and issues that may impact client relations  Attended client meetings and assist with determination of project requirements  Organized meetings team celebrations between team members and clients  Recorded minutes at meetings  kept detailed project notes and records  Worked with cross functional team like UX design development QA marketing and different Line of Businesses  Generated Use Case diagrams Activity diagrams Business Object to depict process flows and PowerPoint presentations  Performed User Acceptance Testing UAT for various web based and database related applications  Work on defining the sprint scope and oversee the BA schedule and deliverables  Work with IT to design and develop data collection and management tools to manage the information  Certifications and Training PH1252x Data Science Visualization Certificate ID          June XXX8 ExiLearn Business Analyst          Aug XXX8 Communicated with the client to elicit analyze and validate the requirements  Created System Requirements Specification SRS Business Requirements Specification and Document  Created Wireframes using Mockup Plus and InVision  Created Use case activity and sequence diagrams using drawio Prepared Gantt Chart Requirement Traceability Metrix using Microsoft Excel           Academic Projects       011      011     1Store One Stop Shop    –                     Objected at creating an application to integrate all utilities like electricity internet travel and mobile recharge Scraped data from all utility websites and REST APIs provided by them The framework used is Ionic 20 with Angular 2 Worked with users and stakeholders to analyze and validate requirements Managed project through status meetings weekly reports identifying risks and tracking issues  Refreshable Braille Display for Mathematical Equations          Feb15May16  Developed a hardware tool that could help visually impaired to read and understand mathematical equation using braille pins and  tactile displays  Identified the solutions that could help visually impaired to read and understand mathematical equation Responsible for specifications implementations and analytics Prepared business models flowcharts and diagrams          Education       Masters       Computer Science       Expected in                   Pace University Seidenberg School of CSIS                GPA        Status         Computer Science GPA 384         Algorithms and computing theory Mobile Web Content and Development Web Development and Content Management system Human Factors and Usability Metrics              Expected in                                   GPA        Status                  Bachelors       Computer Engineering       Expected in                   University of Mumbai                GPA        Status         Computer Engineering GPA 321         Analysis of algorithm Software Engineering Computer graphics Artificial Intelligence Distributed databases Data Warehouse and mining Cryptography and system security              Expected in                                   GPA        Status                 Summary     To leverage my knowledge and expertise to growth of organization and self Master of Science in Computer Science with graduate assistantship and GPA of 384 Strong communication skills Expert in Algorithms and Computing Theory Master in Artificial Intelligence Demonstrated efficiency in team projects as well as handled projects independently Highly organized with the ability to manage multiple projects and meet deadlines        Highlights           Mac Windows Linux Ubuntu  Programming Languages SQL Python MySQL Relational databases HTML5 CSS3 JavaScript XML XHTML jQuery JSON D3 ThreeJS  WebGL SVG images HTML Canvas Ionic 20 AngularJS React  Applications Jira InVision Axure Blueprint Mockup Plus Agile Scrum Microsoft Excel Pivot Tables VSLOOKUP macros  VBA Tableau Visual Studio Android Studio Photoshop Gimp Quincy Eclipse NetBeans GitHub                         Skills     Photoshop Agile API Artificial Intelligence BA Blueprint Business Analyst business analysis Canvas hardware draw Cryptography CSS3 client clients client relations customer satisfaction data collection Data Visualization Data Warehouse Databases database delivery Eclipse XML Functional Gimp GIS Computer graphics UX HTML HTML5 JavaScript jQuery JSON Linux notes Mac macros marketing Market meetings Microsoft Excel PowerPoint presentations Windows MYSQL Operating Systems Pivot Tables product planning Product Manager Programming Python QA read Relational databases Requirement research sales Scrum Software Engineering Specification SPSS SQL strategy Tableau utilities vision VBA Visual Studio Web Development and Content websites Web Content and Development workshops XHTML       Additional Information       Extracurricular Activities Event Organizer for colleges cultural and technical festival Participant in the CodeZilla Competition held by my Undergraduate School in XXX4 Honors and Awards Pace University Scholarship worth 6000</data><data key="id">233404906853350828820150465750701247563</data></node>
<node id="n1439" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-consultant-26d8093a10f14c83bb8218b443e4f969</data><data key="resume">Jessica    Claire                      San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Professional Summary     Data scientist with 1 years of experience in deClairevering datadriven insights in large data management enterprise fastpaced hedge fund and media entertainment conglomerate passionate and skilled at solving business problems with machine learning models and data analytics       Skills           Regression GLM Ridge Lasso KNN  Classification Logistic Regression Decision Trees Random Forest XGB SVM NaiveBayes  Clustering KMeans Hierarchical DBSCAN  Statistics  AB Testing Hypothesis Testing Bayesian Inference ProbabiClairety  Programming  Python SQL R Hive Spark Git Scala Java      Ad  Automate  Clustering  Credit  CRM  ETL  Java  Marketing  ModeClaireng  Predict  Programming  Promotion  Python  QuaClairety  Sales  SQL  Statistics  VaClairedation                       Work History      092019   to   Current     Data Scientist Consultant      Deloitte    –    San Juan     PR             Implemented and deployed marketing propensity model using XGBoost for customer acquisition on upcoming movies identified top 70 firsttime purchasers with highest product interest scores  Constructed adhoc SQL queries and performed AB test for quantifying retention and churn campaign promotion upClairefts collaborated with CRM teams to identify key metrics and evaluate testing results  Designed and developed ETL process using SQL and R to automate data vaClairedation process for identifying and tracking data quaClairety issues  Created user defined function using Kmeans in Snowflake warehouse to segment customer by user behavior and demographic information  Developed and pubClaireshed several interactive and scalable Rshiny visuaClairezation dashboards to increase visibiClairety on various KPIs          062019   to   082019     Data Scientist Intern      Ascend Learning    –    Nashua     NH             Identified and analyzed problems of cascade effect and data misrepresentation when using credit card transaction to predict companies’ sales provided solutions that reduced outofsample MAE error by 75  Partnered with investment team from different sectors to increase size of modeClaireng data using clustering sampClaireng and rule based method effectively improved data reClaireabiClairety and reduced geobias from various alternative data sources          052018   to   092018     Data Scientist Intern          –    Orlando     FL             Designed developed and deployed automated streamClairened procedure in Python for parsing test performance data and building visuaClairezation platform using Pyplot and Tableau for multiple drive performance comparison improving tasks efficiency by 30  UtiClairezed Hive platform to develop an automated pipeClairene for data query cleaning and transformation          Education      Expected in   052019     Master of Science     Statistical Science     Duke University      Durham     NC     GPA       GPA 37        Expected in   082016     Bachelor of Arts          University Of CaClairefornia Berkeley      Berkeley     CA     GPA       GPA 392        Work History      092019   to   Current     Data Scientist Consultant       Warner Bros Entertainment Inc Insight Global and Horkus Solutions   –   Burbank     CA      Implemented and deployed marketing propensity model using XGBoost for customer acquisition on upcoming movies identified top 70 firsttime purchasers with highest product interest scores  Constructed adhoc SQL queries and performed AB test for quantifying retention and churn campaign promotion upClairefts collaborated with CRM teams to identify key metrics and evaluate testing results  Designed and developed ETL process using SQL and R to automate data vaClairedation process for identifying and tracking data quaClairety issues  Created user defined function using Kmeans in Snowflake warehouse to segment customer by user behavior and demographic information  Developed and pubClaireshed several interactive and scalable Rshiny visuaClairezation dashboards to increase visibiClairety on various KPIs          062019   to   082019     Data Scientist Intern       Point72 Asset Management LP   –   New York     NY      Identified and analyzed problems of cascade effect and data misrepresentation when using credit card transaction to predict companies’ sales provided solutions that reduced outofsample MAE error by 75  Partnered with investment team from different sectors to increase size of modeClaireng data using clustering sampClaireng and rule based method effectively improved data reClaireabiClairety and reduced geobias from various alternative data sources          052018   to   092018     Data Scientist Intern          –   Sunnyvale     CA      Designed developed and deployed automated streamClairened procedure in Python for parsing test performance data and building visuaClairezation platform using Pyplot and Tableau for multiple drive performance comparison improving tasks efficiency by 30  UtiClairezed Hive platform to develop an automated pipeClairene for data query cleaning and transformation          Skills      Regression GLM Ridge Lasso KNN  Classification Logistic Regression Decision Trees Random Forest XGB SVM NaiveBayes  Clustering KMeans Hierarchical DBSCAN  Statistics  AB Testing Hypothesis Testing Bayesian Inference ProbabiClairety  Programming  Python SQL R Hive Spark Git Scala Java  Ad automate Clustering credit CRM ETL Java marketing modeClaireng predict Programming promotion Python quaClairety sales SQL Statistics vaClairedation</data><data key="id">190995324271360738531549311082018797149</data></node>
<node id="n1440" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     12 years of machine learningdata science experience with a proven track record of developing and        implementing large scale algorithms that have significantly impacted business revenues and user experience        10 years of experience applying R Python SAS and SQL for algorithm development data modeling statistical        learnings and data visualization        Ranked a top 1 competitor on Kaggle worlds largest community of data scientist with over 300k members        Handson experience applying several MLStatistical algorithms to realworld problems Neural Networks        Gradient Boosted Trees Random Forest Clustering Generalized Linear Models Simulation models and        Gaussian Mixture Models        Strong experience building endtoend machine learning platform using Java and big data technologies        Cassandra Spark and Hive        Excellent skills at programming in JavaOOP       Highlights           R          Neural Networks  Java          Gradient Boosted Trees  Python          Random Forest  NoSQL Cassandra          Generalized Linear Models  Spark Hive Pig          Optimization  SQL CQL          Gaussian Mixture Models                         Experience       Senior Data Scientist       042017      Current     Factset Research Systems Inc    –    Reston     VA          Conceptualized architected and implemented a deep learning model to accurately predict demand for Nvidias products Algorithm applies Deep neural networks along with several real time social feeds econometric indicators product momentum to forecast demand         Data Science Lead       062014      042016     Bristol Myers Squibb    –    Minneapolis     MN            Developed and owned the core machine learning feature of the product a patentpending behavioral analytics engine for predicting crosscloud application security threats  Architected and implemented the machine learning platform from the ground up in Java Cassandra Hive and Spark              Analytic platform monitors multiple cloud applications AWS Salesforce Box etc and predicts security threats and breaches  Implemented the endtoend platform for performing user behavior analytics using unsupervised machine         learning  Designed and implemented algorithms for realtime decisioning         Implemented process to capture 500 behavioral profile features         Implemented several high IV features for ML algorithm consumption Maximum Geodistance between users         loginday Fast Geolocation Hopping etc         Owned the feature endtoend and worked directly with cofounderCTO to drive product enhancements           Principal Data Scientist       122012      052014     Lumeris    –    Saint Louis     MO            Lead several big data machine learning initiatives involving the design development and deployment of advanced machine learning algorithms that impacted PayPals core products allowing business to grow and scale to over 150 million customers and process over 3 billion online transactions annually  The developed solutions have directly helped PayPal lower losses improve user experience and increase revenue simultaneously  Few key PayPal core products worked on are included below  PayPal ATO Models         Designed and developed Neural Network models to prevent account takeover activity at the time of user         authentication  Performed feature selection from over 2000 features and applied best practices in Neural Network         model development  Model accurately captured 81 of incoming fraud activity and resulted in 48bps of reduction         of losses for PayPal in 2013 freeing up capital to grow and expand margins  PayPal Here Mobile Card Reader         Architected the very first antifraud ML algorithm that enabled PayPal here product to expand and scale to over       million merchants  Designed many new geolocation specific features that showed high IV contribution to the       implemented model  PayPal Wallet       Spearheaded the development of ensemble models for predicting stolen financial usage at the time of transaction       completion  Developed Gradient Boosted Trees using GBM package in R  Implemented new features using IP       IPGEO Velocity etc  to improve algorithm performance  Designed several tools in R to visualize GBT model and       developed process to extract model parameters for deployment  Partnered with business and product teams to       execute and influence business decision around the solution implementation           Lead Analytic Scientist       052007      112012     Cushman  Wakefield Inc    –    Stockton     CA            Managed the design development and deployment of several predictive modeling solutions that underlie ISO Risk Analyzer portfolio of products which have contributed to significant revenues for the company  Few highlights are listed below  Lead the development of an expert system to accurately predict a consumers likelihood of committing a traffic violationDeveloped Logistic models for predicting violation probability and Poisson models for predict violation frequencyIncorporated intelligence from 1500 features from 7 different data sources Claritas Crime Traffic Patterns Business Info Weather etc TweedieCompound Poisson Distribution Model Developed and implemented several Tweedie distribution models to examine environmental factor for predicting personal auto risks for calculating insurance premiumsDeveloped separate frequency and severity models to improve model accuracyCaptured interaction effects between various environment variables such as traffic generator traffic pattens population density weather etcRisk Analyzer Credit Scoring Module Designed and developed first generation credit scoring algorithms to predict a consumers accident likelihood using credit payment behaviorPerformed feature reduction on close to 800 credit attributes and applied Multivariate Adaptive Regression Splines MARS to detect 2nd and 3rd order interactions effectsCaptured nonlinearity using splines and piecewise regressionWorked as the analytic manager for the movie advertisement testing product a high revenue growth initiative for the company           Senior Research Analyst       092004      042007     Nbc Universal    –    Boston     MA            Managed 5 statisticians to build predictive models for rating movie trailerTV spots content on their effectiveness to generate interest   Provided analytic insights and advice to maximize the impact and reach of clients resources  The product served 4 of the top hollywood movie studios and generated over 13M in annualized revenues for the company  Movie TrailerTV Spots Advertisement Testing Product Predictive models for rating movie trailers TV spots print ads etc  Team worked on over 100 movies for most of the motion picture studiosmajors and assisted in building         customized statistical models for several minimajors  Provided strategic insights and advice to Hollywood         Studios to maximize the impact and reach of clients marketing resources  Box Office Forecasting Models         Developed forecasting models to predict weekend box office revenues to help studios estimate accurately a films         potential at the day of the release  Model used several audience tracking data to accurately predict weekend box         office receipts  Fox Movie Sequel Study Client  20th Century Fox          Developed a series of ordinal logistic regression models to compare characteristics and attributes describing         several Fox movies  Based on the study results 20th Century Fox decided to produce a sequel to Die Hard in         2007 which was one of the biggest box office hits of that year           Researcher       012002      012004     University Of Missouri    –    City     STATE            Managed home healthcare initiative for Missouris Sinclair School that involved developing a AI system based GPS to optimize hospital resources for saving cost  The GPS system incorporated new algorithms for solving multipletraveling salesman problem in a dynamic setting  Researched and applied spatial clustering algorithms in the context of constraint optimization problem          Education       Master of Science       Industrial EngineeringOperations Research       Expected in   2004                University of Missouri      Columbia     MO     GPA        Status         Industrial EngineeringOperations Research Thesis  Dynamic Stochastic Vehicle Routing Model in Home Healthcare Scheduling                Metallurgical Engineering       Expected in   2002                Indian Institute of Technology                GPA        Status         Metallurgical Engineering BHU          Varanasi UP IN Thesis  Monte Carlo Simulation Modeling of NonUniform Stress in Structured Materials Secured All India IITJEE rank in the top 06  ranked among 250000 applicants        Skills     ads AI big data clustering content Credit Client clients expert system Fast features financial Forecasting GPS Home Healthcare insurance IP ISO Java machine learning machine         learning marketing MARS Materials Office 2000 Modeling monitors Network Networks Neural NoSQL Optimization predict Testing Product Python realtime Routing Scheduling Simulation SQL strategic Structured TV</data><data key="id">12582526467359889087064736236944181337</data><data key="url">https://www.livecareer.com/resume-search/r/senior-data-scientist-298f59e78c104fc0bcfa7b8e3d3f43d6</data></node>
<node id="n1441" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-tech-director-dba-of-citeseerx-a1a1abff4a8e4074ad01b0cc6b661bd7</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Career Overview     Ten years work experience under LinuxUnix environment Latex and MS Excel    · Five years work experience of buildingmaintaining production MySQL databases and Apache Solr debugging      and optimizing ETL work flows based on scholarly big data    · Five years work experience of search engine architecture and infrastructure deploying and implementing web      application features    · Five years work experience of designing coding and testing LAMP website powered by MySQL databases      and Apache Solr using frameworks such as Django and Spring           1          Update on February 10 2017   · Five years programming experience with Python familiar with load balancing virtual environment firewall     eg iptables and file systems   · Three years work experience of managing software projects on open source software platforms eg GitHub   · Two years experience of analyzing logs using MapReduce Deep Learning architectures of RNN and CNN     on video data experience with Amazon AWS Microsoft Azure Cloud Google Cloud and Google Analytics     Experience with NLP tools Bash Java R Ruby on Rails RESTful API   · Backgrounds in Physics Math and Statistics Familiar with ML NLP ANN IR and genetic algorithms       Qualifications           Guest services  Inventory control procedures  Merchandising expertise      Loss prevention  Cash register operations  Product promotions                       Technical Skills                Accomplishments              Work Experience      062013   to   Present     Data Scientist Tech Director  DBA of CiteSeerX      State Of Louisiana    –    Ville Platte     LA     USA        I started with the web crawling module of CiteSeerX in 2011 then expanded to the full architecture around 2013  My job duties include administrating the MySQL database and Apache Solr index servers hacking the source    code PythonJavaPerl to fix security vulnerabilities developing new web application features managing 100    terabytes production and research data maintaining 30 physical and virtual servers to facilitate production    and research and developing software to improve web crawling information classification and extraction  By    the end of 2014 I was able to run the entire search engine single handed  In 2015 I proposed infrastructure    and software solutions to overcome scalability bottlenecks and blueprinted the next generation of CiteSeerX  By    the end of 2016 I had scaled the data collection from 3 million to over 10 million documents  Currently the    system can keep running for several months without major issues  The 200 page system document wrote by    me significantly flattens learning curve for new admins  I used to assist 3 professors to build private cloud and    GPU infrastructure  I also have experience of working on a Hadoop cluster and programming with MapReduce  Postdoctoral Scholar          June 2011  present          062006   to   052011     Research Assistant      Decatur Public Schools    –    Decatur     IL     USA        Utilize astronomical big data compiled from archives of space and groundbased telescopes such as the Hub    ble Space Telescope and the Sloan Digital Sky Survey to investigate important correlations between physical    parameters of Active Galactic Nuclei and quasars  Publish 7 peer reviewed journal articles          082004   to   052006     Teaching Assistant      Astronomy  Astrophysics Pennsylvania State University    –    City     STATE     USA        Lecture nonscience college students on astronomical fundamentals          Education and Training      Expected in   August 2011     PhD     Astronomy and Astrophysics     Pennsylvania State University      University Park     PA     GPA       Astronomy and Astrophysics        Expected in        PhD     Computational Science                     GPA       Computational Science        Expected in   July 2004     BS     Physics and Astronomy     University of Science and Technology of China          Hefei                GPA       Physics and Astronomy        Interests     Entity Recognition in Scientific Document          Ongoing   Leader          Research · Recognize and extract semantic domain knowledge entities from scientific documents   Video Compression with ANN          Ongoing   Coleader          Research · Perform nearlossless video compression using artificial neural network models   Migrating CiteSeerX to a Private Cloud          Published in 2014   Leader          System · Migrate CiteSeerX production servers to a private cloud with virtualization techniques   Document Classification in Digital Libraries          Published in 2014 and 2016   Coleader          Research · Automatically and accurately classify PDF documents with ML and structural features PUBLICATIONS   · See httpfanchynawixsitecomJessicaClairepubs for all publications OTHER INFORMATION   · PC members of 5 conferencesworkshops   · Reviewers for 14 toptier conferencesjournalstransactions including WWW SIGIR and TKDE   · Collaborated with people from UNT Microsoft AllenAI and Internet Archive           2          Update on February 10 2017       Skills     Apache AI big data conferences content data collection Database features Hub Java managing MySQL NLP next search engines page PDF Perl programming proposals publications Python research scientific servers developing software teaching typing articles       Additional Information       HONORS AND AWARDS   Best paper nomination in the 8th International Conference on Knowledge Capture          2015   Best application paper in the 26th Annual Conference on Innovative Applications of Artificial Intelligence 2014   Best paper nomination in the IEEE International Conference on Cloud Engineering          2014   Zaccheus Daniel Fund          2009   Zaccheus Daniel Fund          2007   Stephen B Brumbach Fellowship          2006   USTC Excellent Graduate Student Award          2004 SELECTED PROJECTS   Entity Recognition in Scientific Document          Ongoing   Leader          Research · Recognize and extract semantic domain knowledge entities from scientific documents   Video Compression with ANN          Ongoing   Coleader          Research · Perform nearlossless video compression using artificial neural network models   Migrating CiteSeerX to a Private Cloud          Published in 2014   Leader          System · Migrate CiteSeerX production servers to a private cloud with virtualization techniques   Document Classification in Digital Libraries          Published in 2014 and 2016   Coleader          Research · Automatically and accurately classify PDF documents with ML and structural features PUBLICATIONS   · See httpfanchynawixsitecomJessicaClairepubs for all publications OTHER INFORMATION   · PC members of 5 conferencesworkshops   · Reviewers for 14 toptier conferencesjournalstransactions including WWW SIGIR and TKDE   · Collaborated with people from UNT Microsoft AllenAI and Internet Archive           2          Update on February 10 2017</data><data key="id">258326464797710858453372203041426919293</data></node>
<node id="n1442" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary     Analytical processoriented data aficionado and problem solver with indepth knowledge of web development product management databases big data capture curation manipulation and visualization           Education       Fowler College of Business San Diego State University    San Diego     CA      Expected in   May 2019     –      –       Master of Science        Information systems          GPA           Information systems GPA 351                        Expected in        –      –       Statistical Analysis Marketing Business Analytics Decision Support SystemsData Mining Visualization and Analysis Big Data Project Management Enterprise Data Management                  GPA                    Jawaharlal Nehru Technological University               Expected in   May 2014     –      –       Bachelor of Technology        Electronics and Communication Engineering          GPA           Electronics and Communication Engineering GPA 35        Highlights         R JavaScript including D3js MySQL Python Java HTML5CSS3 Bootstrap Frameworks jQuery Nodejs  Project management methodologies Waterfall AgileScrum Project management applications Atlassian JIRA Service Desk  Ecommerce Platforms BigCommerce Shopify  Business IntelligenceData Analysis and visualization tools MicroStrategy Tableau RShiny SAS Visual Analytics MS Excel Including formulas pivot tables and macros R Studio QlikView MS Office Visio  Analytical ModelsTechniques Data Mining Sequence Mining Bayesian Models Sentiment Analysis Principal Component Analysis Multivariate Linear Regression Clustering Time series modelling graph theory                         Experience       Artificial Intelligence Lab San Diego State University      Research Assistant Data Scientist   City     STATE                   092017      Present     Established a streamlined work flow and enforced process and code quality improvements  Analysed and visualized data relating to Border security  Deception Detection project using Excel RShiny Tableau CPS fusion Pro software for polygraphs Python and R  Applied various data analytics and statistical modelling techniques like Clustering Principal Component Analysis and Classification to identify patterns and relations to achieve optimization  Developed a standalone website for the AI lab using Squarespace and was instrumental in incorporating the Sona systems cloudbased subject pool software to implement and promote research activities at SDSU           San Diego State University      Graduate AssistantData analyst   City     STATE                   122017      052018     Assisted with the research relating to USA business schoolscore curriculum by collecting and extracting data model building  Analysed data using Excel PIVOT tables for sorting and used R for multiple regression to come up with predictive analytics reports with considerable assumptions  Created visually impactful dashboards in Excel and Tableau for data reporting  Academic ProjectsGraduate Student           Lister Technologies      Software Developer and Data Analyst   City          India              102014      052017     Project1 Rosetta Stone Web Application Solved tasks related to front end user interface as well as Java and managed pages by making using of Liferay content management system concepts  Designed pages and created content from scratch using HTML and CSS  Mentored new team members from the USA through webinars about optimal usage and core concepts of Liferay  Involved in Proof of Concept PoC prepared Unit Test Plan when required and engaged in test and support activities during sprint release  Developed marketing materials for existing and new product launches and was crucial in gathering and analysing market data and outline strategic recommendations  Collaborated with cross functional teams to ensure smooth working of Adobe Dynamic Tag Manager and Eloqua Oracle marketing automation platform  Project2 My AimHigh Hybrid Mobile Application Assisted in managing the entire software life cycle from planning through development testing and launch phase  Researched extensively and developed a strategy to include aspects of revenue market opportunity and customer fit  Designed responsive pages using Bootstrap and solved numerous JavaScript functionality bugs to improve code efficiency  Analysed product and client data to suggest improvements  Project3 Automation of tasks for Modmed Services Designed and developed application using Nodejs and MySQL to automate download process for a USbased insurance firm which saved a total of 210 manhours per week  Project4 BigCommerce RD and Data Mining Did extensive market research about BigCommerce and optimal ways to integrate plugins as per the customer requirements and documented the different plugin integration  Created interactive dashboards to improve usability  Edited core themes of BigCommerce to enhance customer satisfaction using stencil editor and Nodejs  Project5 Hewlett Packard Enterprise  reskinning and Data Analysis Designed website with fully responsive pages using advanced technologies like UIkit Version 30 LESS scripts HTML5 and CSS3  Researched extensively on the product data to come up with the order in which the goods are to be placed  Collaborated with Product Specific analytics team and helped them to understand the hidden patterns using Excel and R          Accomplishments       Certifications SAP R Tableau         Interests     Lister Technologies Chennai India Saving Environment Role Environment Leader lead a team of 200 people Formed and led driving change challenge team a group of 200 people and worked towards transforming office into a 100 recycling office by closely working with Paperman one of the states efficient recycling agencywwwpapermanin during the year 201516 This project was instrumental in making Lister Technologies an ecofriendly office till date Promoting Girl Child Education RoleVillage Representative lead a team of 5 people Participated in MYGAON village challenge organised as part of Lister foundation and won the challenge Received 1000 from the Lister foundation during July 2016 Used this fund towards providing quality education to girl children in a village near the Chennai city Thaiyur Village Kancheepuram District Tamil Nadu by closely working with an NGO named GramaValarchi Was instrumental in promoting Thaiyur village to other parts of India throughmygaonorg and involved in fundraising activities        Skills     Academic Adobe Agile AI automate Automation Big Data Business Intelligence Clustering Concept content content management CSS CSS3 client customer satisfaction Data Analysis Data Management Data Mining Data Analysis and visualization Decision Support Ecommerce editor functional Hewlett Packard HTML HTML5 insurance Java JavaScript jQuery macros managing market research Market Research and Analysis marketing market marketing materials MS Excel Excel MS Office MicroStrategy MySQL Enterprise optimization Oracle PIVOT tables Project Management Python quality reporting research San SAS Scrum scripts sorting Statistical Analysis strategy strategic supply chain Tableau user interface Visio website       Additional Information       AWARDS AND ACHIEVEMENTS Recipient of scholarship for being nominated as Intercultural Ambassador at SDSU to represent India for the year 2018 Presented a technical paper at the National level conference on Global Climatic Changes and Its Impact received a cash prize for excellence from Hon Chief Minister of NEW DELHI Smt Sheila Dixit for excellence in fact collection prediction and presentation during 2008 Stood first among 200 orators for presenting a technical paper on RED TACTON  Communication via human flesh at JNTU University Anantapur and Osmania University Hyderabad during Undergrad Received Gold medal and letter of appreciation from the college management for topping the Undergrad class of 201014 with 8525 marks COMMUNITY SERVICE Lister Technologies Chennai India Saving Environment Role Environment Leader lead a team of 200 people Formed and led driving change challenge team a group of 200 people and worked towards transforming office into a 100 recycling office by closely working with Paperman one of the states efficient recycling agencywwwpapermanin during the year 201516 This project was instrumental in making Lister Technologies an ecofriendly office till date Promoting Girl Child Education RoleVillage Representative lead a team of 5 people Participated in MYGAON village challenge organised as part of Lister foundation and won the challenge Received 1000 from the Lister foundation during July 2016 Used this fund towards providing quality education to girl children in a village near the Chennai city Thaiyur Village Kancheepuram District Tamil Nadu by closely working with an NGO named GramaValarchi Was instrumental in promoting Thaiyur village to other parts of India throughmygaonorg and involved in fundraising activities</data><data key="id">19313998424525899019818547169657820675</data><data key="url">https://www.livecareer.com/resume-search/r/research-assistant-data-scientist-a0ac60734008486993ec47865153be1d</data></node>
<node id="n1443" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-project-manager-ceo-73538fa67f974e7588dcc366bcbc951f</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary     I am a data scientistengineeranalyst with a passion for statisticsmachine learningdeep learning and focused on providing insightful datadriven solutions I have 6 years experience in data analytics and data engineering in various fields of manufacturer finance and infrastructureMy current research and development interests include data engineering machine learning deep learning NLP and experimentation       Skills         Data Scientist Data Analyst Business System Analyst Oracle PLSQL developer Data Visualization D3js Tableau R visualization packages Microsoft Excel Data Analytics ToolsProgramming Python numpy scipy pandas scikitlearn gensim keras R caret weka and ggplot SQL Microsoft SQL Server MYSQL Oracle PLSQL Hadoop Hive and MapReduce Machine Learning Algorithms classification regression clustering feature engineering Big Data Tools Hadoop MapReduce SQOOP Pig Hive NOSQL Cassandra Spark Others Deep Learning NLP Topic Modeling SkLearn Graph Mining Text Mining C C PHP JavaScript ASP Shell Scripting                       Experience       Data Scientist  Project Manager CEO       072017   to   Current     Apex Systems    –    Albuquerque     NM             Develop Tableau map report to show new born puppykitten data distribution in US for better visualization for customers on pet online purchase website  Optimized Recommendation System of our ECommerce website by deeply integrating with users background information users subconscious and personality information through hisher social network and users pet information to better predict and recommend petpet productpet service to users based on Collaborative Filtering using Python  Optimized pet supplies website product search engine for users searching by a customized sentence instead of key word by using NLP Tokenization Normalization POS Tagging Parsing Named Entity Recognition and Terminology extraction topic modeling automatic summarization  Analyze the users tagging comments retweet and like history of Pet Social Forum on our website for a whole week to predict hot pet topictrend for coming week  Abstract the pet topic related dataset from twitters dataset do text mining on Twitters Pet dataset using topic modeling and sentimental classification by integrating users background information using deep learning algorithms like CNNRNNLSTMGRU compared with Random ForestSVM to predict hot topictrend among pet owners to further adjust our website sales strategy to recommend popular petpet productservice to customers  Assisted in development of wwwpettigpetcom website of PC version and responsive mobile version using PHP MYSQL HTML CSS and JavaScript based on framework of Thinkphp  Assisted in development of Mobile PettingPet service app on IOS using Swift and Android using Angular  Recruiting assigning and training personnel to fill roles as needed on project team handling project execution from preplanning progress through implementation and evaluation  Design plan and manage the PettingPet project by establishing comprehensive work plan by preparing the business development technical requirement documents to specify the project scope objectives and criteria for evaluation  Identifying sequencing assigning monitoring and tracking individual activities tasks and project performance as required  Reviewing work progress including quality and quantity with team members at regular intervals ensuring project deliverables are completed intime and tostandard  Overseeing and managing project progress in a manner that complies with company policies and procedures           Data Engineer       092016   to   092017         –    Waukesha     WI             Develop Facebook DataSwarm pipelines using Python and Hiveql for hardware data processingload between MySQL database Hive Data warehouse Scuba system and ODS system liking refactoring cablesreboots pipeline to have less tasks setting up automatic email alert for SAS dashboard grab ODS uptime to Hive developing pipelines to parse NVME metrics etc  Daily Hardware Health Dashboard reporting using Line chart bar chart histogram Bullet Graph Heat map Map creating data sets groups bins annotations and mark labels filters trend lines writing Tableau functionformulas etc  using Tableau for hardware health data maintenance like Server reboot dashboard hardware component failure dashboard SASSATA NVME HDD Flash DriveIPMI OOBCables health dashboard for data centers in Department of Infrastructure in Facebook  Create Thermostat dashboard for rack level sensors  failures data using Line chart bar chart histogram by creating sets groups filters and writing Tableau formulas connect server and publish the dashboard with scheduling set up  Machine learning application in hardware product validation cycle  Data AnalysisPrediction on Infrastructure Hardware data using Statistical Method and Machine Learning algorithms and deep learning algorithms using Python and R  Identifybreakfix any issues or bugs involved in data pipelines of data from FB infrastructure hardware health regarding setup performance functionality and workflow stuck etc           Business System Analyst       052010   to   092014     American Dental Partners Inc    –    City     STATE             Operational support for Canon Americas Mercury system includes data adjustmentresearch batch data loading system migration technical and functional specification documentation reports business process alignment workflow stuck and reconciliation etc  BreakFix any issues or bugs collected from client and development regarding setup performance functionality and workflow stuck etc  System Enhancement regarding functionality and performance etc  Review and analyze ASP code for UMC Mercury application web development for data research and system feature fix and enhancement using Visual Interdev 60  Query realtime data regarding Canon Americas new item request item status inquiry item data disclosure and import model configuration and maintenance warranty maintenance and model configuration inquiry etc  Using complex SQL queries on Oracle 9i Canon mercury database  Data loading using Impexp data pump and external tables from Americas Mercury system to S21CUSA merchandise master to RossCUSA retails system and Global Mercury system          Education and Training       Master of Science     Computer Science     Expected in   2016     University of California      Los Angeles Los Angeles     CA     GPA       Computer Science GPA 378 CoursesStatistics Programming Databases and Knowledge Bases Graphs and Network Flows Language and Thought Current Topics in Computer Theory Computer Science Classics Basic Data Science Data Mining and Big Data Analytics System Security         Master of Science     Electrical Engineering     Expected in   2010     University of Bridgeport      Bridgeport     CT     GPA       Electrical Engineering Electrical Engineering 362 Computer Networks Database Management Systems Data and Computer Communications Data Structures         Bachelor of Science     Telecommunications Engineering     Expected in   2007     Jilin University      Changchun     Jilin     GPA       Telecommunications Engineering GPA 350        Skills     Big Data business process C Cables clustering Data Analyst Data Analysis data processing Data Mining Data Visualization Data warehouse Database Management ECommerce Electrical Engineering 362 PHP JavaScript Machine Learning Microsoft SQL MYSQL NOSQL Oracle 9 Oracle PLSQL Python sales SAS scheduling Shell Scripting SQL System Analyst Tableau validation web development       Activities and Honors</data><data key="id">201638520764581607760023457305043594550</data></node>
<node id="n1444" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/principal-data-scientist-5e2b05e23fab40a8b8196e0999593b73</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Professional Summary     To help an organization improve upon and create a truly transformational Customer Experience with endtoend design analytics and operational excellence       Core Qualifications         Advanced Microsoft Excel and Access JavaScript and Jscript skills Highly proficient in statistical analysis using applications Minitab and R Studio Github Other Technical Skills Familiarity with Java Visual Basic C SQL XML HTML                       Experience       Principal Data Scientist       082017   to   Current     Lumeris    –    Arkansas     KY             Primary analyst tasked with yielding strategic insights to drive key corporate initiatives based through customer feedback tools such as NPS product based surveys and operational data  Common predictive analytical techniques such as multivariate data models hypothesis testing using chisquares ANOVA ttests and ztests cluster and conjoint analyses are used to conclude necessary actions  Corporate NPS analysis has driven corporate initiatives to improve strategic partner relationships product quality and usability implementation services technical support and contracting licensing programs  Performed financial linkage analysis with customer satisfaction data to highlight the importance of customer satisfaction to bottom line top line figures through OLS and logistical regression  Revenue and new sales growth models were also developed to show projections of enhanced financials by focusing on improved customer experience  Developed predictive analytics models using Bayesian probability theory that helped to anticipate future customer escalations using many disparate data sources  Performed analysis on linkages between employee satisfaction sentiments and customer satisfaction scores to understand if areas of high employee satisfaction correlate to high customer satisfaction  Helped product teams conclude top areas to enhance improve or maintain to maximize customer adoption rates and likelihood for recommendation  Project managed a data science initiative of evaluating all Customer Experience metrics to understand biggest drivers of customer renewals and retentions  Led a series of recorded RProgramming training sessions for business intelligence analysts across all divisions within the Global Business Operations division           Survey Service Bureau Manager       042017   to   072017     Stilt    –    San Francisco     CA             Helped to establish the framework of a new Voice of Customer department by creating all processes guidelines and training documentation for Survey Service Bureau within CA Technologies Inc  These documents outlined the protocols for the design creation distribution and reporting of all corporate surveys sent through CA Technologies Inc  Managed the rollout of more than 50 survey projects including the Customer Relationship NPS Technical Support Satisfaction Services Satisfaction Partner Relationship Satisfaction Student Education Evaluation CA Communities and CA World Symposium Feedback managed through Confirmit  Outlined the guidelines for consulting with various departments throughout CA Technologies to help them formulate their key business problems and objectives to help design the survey structure determine appropriate conditional skip logic assign appropriate scalesmeasurements to help them get the right data to solve their problems  Reviewed sample selection strategies to reduce bias concerns  Managed data collections process from extraction through all cleansing steps  Trained and managed a team of analysts through handson sessions and documented processes on areas such as consulting best practices survey design project management survey programming data management statistical data analysis  Developed several project management tools used to outline all survey bureau tasks and timelines to efficiently and effectively complete survey projects  Facilitated relationship between survey software vendors and CA Technologies  Built survey programs and action oriented alert mechanisms using jScript coding           Senior Operations Specialist       082017   to   042017     Sanmina Sci    –    Manchester     NH             Customer satisfaction ambassador delegated to representing the voice of the customer through market research projects  Supervised a team of three responsible for disseminating a weekly executive management report based on data mining Web 20 consumer generated media  Developed data mining process that dissects consumer discussions to track sentiments towards Canon and competitor models including features purchase drivers brand perceptions safety and quality assurance issues and potential legal liabilities  Analyzed data to discover trends in customer sentiment towards Canon products using Microsoft Excel and Access  Summarized data into a weekly consumer insight report that has lead to changes in Canon strategies such as implementation of instant rebate programs a recall of the EOS 1D Mark III the detection of four separate class action movements and several product service announcements  Consulted executive management with a detailed study outlining advantages of Web 20 platforms in yielding customer insights with recommendations for potential future Canon projects such as a private online community and ideagoras for product innovation and a wiki knowledgebase to enhance online support  Analyzed survey data regarding customer experiences with the Canon websiteas well as repair and phone services to track customer satisfaction through multivariate statistical and commentary analysis  Developed Six Sigma control charts designed to identify a decline in service performance  Created key driver quadrant charts to prioritize primary customer satisfaction drivers  Helped improve customer satisfaction by 5 by changing focus from turnaround time to repair quality and communication  Presented PowerPoint presentations to summarize market research data to management upon requests           Business Administration Associate       082017   to   082017     Canon USA Inc    –    City     STATE             Extranet administrator responsible for the management of Consumer Imaging Group CIG marketing content user access management and ordering functionality  Organized and outlayed thousands of marketing product announcements images notices price lists and promotions for dealer display  Administered and assigned user privileges based upon contractual agreements to enhance security and maintain site integrity  Ecommerce Project Coordinator responsible for overseeing development and completion of EDI projects by ensuring that all trading partners are in accordance with Canons technical logistical and legal policies  Worked extensively with logistics and IT departments to ensure Supply Chain Vendor Compliance with issues such as EDI technical mappings logistical routing procedures RFID implementation and standard business practices  Analyzed current processes to help facilitate changes necessary for initiatives such as the systematic restructuring of the CIG sales force customer orders impacted by business unit consolidation and contract administration for all CIG customers  Generated daily sales reports which help to track sales volume of all CIG business units and the sales quota percentage as dictated by CIG sales management           ProgrammerAnalyst       011   to   082017     Datacor Inc    –    City     STATE             Developed and maintained an ERP application responsible for enabling customer order entry invoice tracking AR and AP payment processing and inventory control management  Provided customer support to meet daytoday operations in areas such as sales reporting resolving unbalanced general ledger postings troubleshooting inventory discrepancies  Trained customers extensively with onsite demonstrations that combined PowerPoint presentations with practical handson instruction  Performed analysis for all stages of the Systems Development Life Cycle such as inquiring customers of their needs and desires fielding customer specifications for systems enhancements detailing desired features in documentation making necessary coding adjustments testing and debugging enhancements installation of enhancements user training and support          Education       Bachelors of Science     Management Information Systems     Expected in   2000     Seton Hall University                GPA       Management Information Systems         Masters of Business Administration     Market Research     Expected in   2008     Stony Brook University                GPA       Market Research        Professional Affiliations              Skills     analyst AP business intelligence Business Operations C charts chi CA consulting content contract administration Customer satisfaction customer support data analysis data management data mining debugging documentation drivers Ecommerce EDI ERP executive management XML Extranet features financials financial focus general ledger HTML Imaging innovation instruction inventory inventory control Java JavaScript Jscript legal logic logistics Mark III market research marketing Access Microsoft Excel PowerPoint presentations Minitab order entry payment processing policies processes marketing product coding Programming project management protocols quality quality assurance reporting routing safety sales sales management sales reports sales reporting Six Sigma SQL statistical analysis strategic Supply Chain survey design surveys Symposium Systems Development technical support user training phone troubleshooting Visual Basic website</data><data key="id">317149744153318857246030286097031836162</data></node>
<node id="n1445" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/associate-data-scientist-c873aee03e6048789ee0f7938e91183c</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Career Overview     Data Science professional with 5 years of cross functional experience working on advanced analytics projects with marketing finance and strategy teams within retail and healthcare expertise in predictive modeling           Qualifications       R   Python   Apache Spark   Hive   SQL   Advanced Excel   Tableau Machine Learning Skills Linear Regression Logistic Regression Clustering Ensemble methods Deep Learning Naïve Bayes Random Forest Decision tree Text mining Time Series modeling NLP                     Education and Training       University of Cincinnati Carl H Lindner College of Business    Cincinnati     Ohio      Expected in   June 2016     –      –       Master of Science        Business Analytics          GPA           Business Analytics 3940         Indian Institute of Technology Delhi    New Delhi           Expected in   August 2010     –      –       Bachelor of Science        Mechanical Engineering          GPA           Mechanical Engineering          Accomplishments              Work Experience       Cox Communications Inc      Associate Data Scientist                           011      Present     Working on big data solutions for Sams club marketing and merchandising team           Maximus Inc      Novartis AG          Analyst                                      Marketing Science          21st October13  7th July15           Analytics advisory to global business strategy team and brand directors on forecasting and strategy projects for contact lens and lens care solutions portfolio with Alcon Vision Care  Capacity planning Developed model for entire contact lens portfolio with utilization estimated using MonteCarlo     simulation method leading to approval of 6 additional production lines costing 300MM to meet future demand   Product crosssell strategy Utilized transactional data from customers to determine most common pairs of contact lens    and lens solution to help crossselling strategy for Alcon model based on apriori algorithm developed in R   Forecast and valuation Built forecasts and valuation models for portfolio of existing and inline products Nielsen MSci          Business Analyst          30th January13  3rd October13           Redesigned standard operating procedures for universe estimation improving productivity by 60 transferred most of the procedures from excel to SAS  Automated VBA based tool to detect outliers in data           WNS Global Services      Senior Associate                                      24th March11  24th January13             Portfolio valuation and risk estimation Incorporated risk adjusted valuation in sales forecast models to estimate risk associated with each product development this helped accelerate the ideation process for products    Time series forecast Automated tool built for established drugs which was replicated for 6 other countries   Customer segmentation and retention analysis Built a model using clustering algorithm in R to segment physicians Model to predict churn rate for certain segments based on Rx data   Project management Trained and led 2 associates for projects in GSK emerging market team          Skills     Apache big data Business Analyst business strategy Capacity planning Clustering costing forecasting Machine Learning marketing market merchandising Excel modeling NLP predict product development Project management Python selling sales SAS simulation SQL strategy Tableau valuation Vision VBA</data><data key="id">322736539040472959018295158511611566430</data></node>
<node id="n1446" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-intern-c3dc13dfae6547a9ade6967112f3bc22</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Profile     Detailedoriented Senior Software Developerprogrammer with 9  years experience devising innovative and tailored solutions to achieve onschedule and high client satisfaction Provide cradletograve oversight of software project management and rollout of Enterprise applications       Relevant Professional Experience       Data Scientist Intern       062016   to   082016     Ascend Learning    –    San Antonio     TX             Created a predictive model for effective field goal percentage EFG used to understand the effect of player actions both offensively and defensively on shot quality  Analysis summary and visualization of shot qualitycontestedness  Created web application to display expected field goal and shot contentedness resultTechnologies used R RShiny RStudio Postgres           Additional Professional Experience       Senior Net Developer       012016   to   Current     CHICKASAW NATION INDUSTRIES    –    Norman     Oklahoma             Develop solutions in alignment with the Enterprise architecture while contributing and advocating the use of Enterprise frameworks  Defines applications and websites objectives by analyzing user requirements envisioning system features and functionality  Researching and understanding new web technologies to provide technical leadership in developing service applications and analyzing business requirements  Designs and develops user interfaces to Internetintranet applications by setting expectations and features priorities throughout development life cycle  Technologies used C ASPNet HTML CSS SQL Server SVN JQuery JavaScript CSS AngularJS Bootstrap Entity framework TFS WEBAPI2 Waterfall Methodology           Software Developer II       2015   to   092015     CHESAPEAKE ENERGY    –    Oklahoma City     Oklahoma             Reduced department cost 10 by reducing amount of time spent working of previous released projects by ensuring new code integrated with existing productReduced server load by 26 by converting numerous serverside components to clientside  Leveraged advanced knowledge of ASPNET and AngularJS to optimize Sharepoint created pages eliminating 13 of code while enhancing overall functionality and speed  Technologies used C MVC NET Test unit SQL Server AngularJS Bootstrap HMTL CSS PowerShell TFS SSIS REST SQLite Agile Methodology         ​          Senior Software Developer       012013   to   112014     NIC LLC    –    Oklahoma City     Oklahoma             Led programming tasks including enhancements maintenance and support for clients applications and interfaces  Developed new procedures for requirements gathering needs analysis testing scripting and documentation to strengthen quality and functionality of businesscritical applications  Managed 6member developer team  Technologies used WAMP stack HTML CSS Oracle SVN JQuery JavaScript Java Agile  Waterfall Methodology           Senior Software Developer       102007   to   112012     PAYCOM PAYROLL LLC    –    Oklahoma City     Oklahoma             Design and implementation of applications collaborating with project managers and quality assurance team to ensure ontime completion and quality of projects  Provided technical leadership to junior developers  Focused teams on business objectives and tracked progress to ensure project milestones were completed on time on budget and with the desired results  Reduced client cost by 32 by Improving client experience with the ability of managing multiple client products and employees from a single location  Created module that allows clients ability to send and receive synchronous feedback on employment eligibility and verification by connecting to the Department of Homeland Security EVerify  Created an Executive dashboard that displays client toplevel metrics helping executives run their companies efficiently This increased new business sales 270K in 2011  Technologies used LAMP stack HTML CSS MYSQL Subversion JQuery JavaScript SOAP XML C Visual Agile  Waterfall Methodology          Other Skills     JAVA CC   Python AMPL Matlab WEKA MongoDB data analysis optimization Git HIVE HIVEQL Tableau       Education       Master of Science     DATA SCIENCE  ANALYTICS     Expected in   2017     University of Oklahoma      Norman     Oklahoma     GPA                Bachelor of Science     Computer Engineering     Expected in   2007     University Of Oklahoma      Norman     Oklahoma     GPA               Affiliations       Engineering Student Life  member 2005 Date   National Society of Black Engineers  member 2005 Date   Gallogly College Of Engineering Graduate Student Community  member 2015  Date   Graduate Student Life  member 2015  Date        Certifications       R Programming  John Hopkins UniversityCoursera   Introduction to Big Data  University of California San DiegoCoursera   Hadoop Platform and Application Framework  University of California San DiegoCoursera   Getting and Cleaning Data  University of California San DiegoCoursera   Machine Learning Foundations  University of WashingtonCoursera</data><data key="id">298719385438324182462482549788079225263</data></node>
<node id="n1447" labels=":CV"><data key="labels">:CV</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                                   Summary     Qualitydriven AIML Product Manager with 4year background in Data Science enterprise BI and building machine Learning product with a proven track record of ontime delivery Excellent customer interaction skills technical knowledge and presentation skills Good understanding of product management and firm grasp of Data Science principles 17 years of combined experience in Application Engineering presale activities product management and business development of Engineering and simulation technology softwares       Skills           People skills  Customer service  Business development  Requirements gathering  Scope development  Communication  Critical thinking  Agile methodology  Story oversight  Product planning  JIRAConfluence  Balsamiq software  AutoML framework  Keras LSTM  Statistical Modelling Regression Classification clustering Multivariate Forecasting      MLOps  Explainable AI  PDP ICE LIME SHAP  Hypothesis Testing DOE  Exploratory Data Analysis  Feature Engineering  Python R Language SQL SAS  Spyder RStudio MySQL Workbench VirtualBox  SAS Enterprise Miner  Ubuntu                       Experience        052019   to   Current   Data Scientist Product Manager    Snapchat         AZ     State            Designed implemented and released 3 versions of AIML desktop product Altair Knowledge Studio in 2021 in an Agile framework  Sphereheaded opensource integration features of AIML product  Increased the customer base from 19 to 35 an increase of 84  Wrote detailed specification for 4 explainable AI algorithms PDP ICE LIME SHAP for native implementation in Knowledge Studio  Wrote production grade Python code for integrating 5 key features Keras LSTM ARIMA scikitlearn libraries GLM and XGBOOST  Followed industry innovations and emerging trends through scientific articles conference papers or selfdirected research  Worked alongside PM leadership to identify product requirements and updated on product release status monthly  Translated business goals and customer needs into prioritized product requirements and use cases  Created epics and wrote user stories for different user personas  Communicated feature requirements to the developers worked closely with the developers reviewed developed features provided feedback to dev make sure the features meet MVP criteria  Recommended product changes to enhance customer interest and maximize sales  Outlined new feature plans created epics wrote detailed specifications produced wireframes of UI and made user facing decisions  Collaborated with design team to enhance the user experience and modernize the product UI  Collaborated with Data Scientists to review new feature specifications and gathered feedback on released features  Collaborated with legal team to understand the licensing and various other aspects of opensource libraries and code  Communicated changes in feature scope and timelines empathetically to the relevant stake holders            052018   to   052019   Applied Data Scientist    Atrium Health         Fort Mill     SC            Worked with presales sales and marketing teams to promote the new features and gathered feedback  Modernized products based on consumer feedback and market analysis to increase sales and expand product offering  Demonstrated selfreliance and leadership by meeting and exceeding workflow needs  Provided consultation to Altair Customers to help drive software usage Developed AutoML algorithm as a turnkey solution to do Machine Learning such as forecasting classification and regression using R on SaaS platform – Altair’s cloudnative enterprise level business analytics solution  Operationalized AutoML on the SaaS platform by working closely with DevOps team  Operationalized anomaly detection models on SaaS platform to understand remaining useful life RUL of a bearing  Tested validated and reformulated models to foster accurate prediction of outcomes  Built and deployed predictive models to understand the reliability of aircraft engines  Demonstrated Altairs Data Science suite of products to customers and helped them evaluate success criteria  Presented the results of AutoML to business stakeholders            062016   to   052018   Senior Application Specialist    Atrium Health         Belmont     NC            Worked with client database engines like Hadoop and HIVE to update custom dashboards on SaaS based BI application in almost realtime to meet customers’ business needs  Increased customer engagement by inperson troubleshooting sessions  Evaluated and led rootcause analysis for production issues and empathetically managed user concerns  Wrote user manuals and other documentation for rollout in customer training sessions  Performed advanced usability testing to improve software robustness  Documented decisions to allow for broader company learning and iteration opportunities            062007   to   062016   Senior Application Specialist    Altair Engineering Inc         City     STATE            Worked closely with account managers and helped them cross technical side of sales cycle  Analyzed user needs and software requirements by involving in product design life cycle of key customers to drive software usage  Resolved customer issues by establishing workarounds and solutions to debug and report defects to product manager which minimized software down time  Coached SMEs across Altair offices globally to meet prescribed business goals and to improve customer retention  Published paper on optimizing design process by coupling simulation software and design exploration at an international conference  Travelled to clients places internationally to better understand the process and client requirements  Established and maintained key relationships with business stakeholders to promote future opportunities          Education and Training        Expected in   122018   Master of Science       Predictive    Northwestern University Analytics  University of Mysore     Evanston     IL      GPA                 Expected in      Bachelor of Engineering       Mechanical                    GPA</data><data key="id">6157402954840963105064484742886609274</data><data key="url">https://www.livecareer.com/resume-search/r/data-scientist-product-manager-367fc1f489de4dc0b8b7aa6b3bffa89a</data></node>
<node id="n1448" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/associate-scientist-i-3e53a6897c764772b86223f4b9861d1f</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Overview     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market       Summary of Skills           Cell Culture  Culturing of cancer cells MDAMB231 T47D U251 BCWM1 Raji  Colony forming assay Clonogenic Assay  Culturing of Tumor initiating stem cells  Isolation of PBMCs buffy coat from whole blood samples  Molecular  Cellular Biology  Immunological Techniques  Sanger Sequencing using Big Dye Terminator v31 method  Plasmid DNA preparation Mini through Giga scale  Rolling Circle Amplification using TempliPhi Amplification  PCR Product purification using ExoSAPIT  Western Blotting  Electrophoresis Agarose SDSPAGE  MTT Assay  Single and Multicolor FACS  Transfection Plasmid siRNA  ELISA  Protein Nucleic Acid Quantification  Genotyping PCR Jessica Claire Proficient in Graphpad Prism Microsoft Office Exposure to bioinformatics software such as Rasmol Swiss pdb Ligand Scout Marvinsketch DS Visualizer and Argus Lab                         Education       Master of Science       Molecular Microbiology  Immunology       Expected in   May 2013                University of Southern California USC Keck School of Medicine      Los Angeles     CA     GPA        Status         Molecular Microbiology  Immunology         Bachelor of Technology       Biotechnology       Expected in   June 2011                Dr D Y Patil University      Mumbai          GPA        Status         Biotechnology         Molecular Biology of Cancer Infection  Host Responses Introduction to histology Pathology Cell Biology Biochemistry Biostatistics Structure and Management of Clinical Trials              Expected in                                   GPA        Status                  Immunology Microbiology Molecular Biology Pharmacology Animal  Plant Cell Culture Genetic Engineering IPR Biosafety Bioethics and Entrepreneurship              Expected in                                   GPA        Status                 ThesisDissertation              Professional Experience       Associate Scientist I       052014      Present     Planet Pharma    –    Oceanside     CA            Perform bench procedures in the process of isolating plasmid DNA  Midi Maxi Mega and Giga scale Work with both CLIMS program and Excel to track orders Provide troubleshooting help and quality customer service for customers both through email and over the phone Perform molecular biology techniques such as colony isolation culture inoculation and running sequencing reactions Process sequencing template including Preparation amplification and PCR purification Manage inventory and replenish consumable sequencing supplies to perform routine maintenance of DNA sequencers           Product Surveillance Coordinator       102013      012014     St Jude Medical    –    City     STATE            Retrospectively analysed medical device complaints  reclassified them as per new complaint guidelines from FDA Ensuring all adverse event medical device reports MDRseMDRs are submitted to FDA per 21 CFR 803  Gained knowledge of medical device GCP Used SAP to get product details and follow product life cycle also used Microsoft Sharepoint for collaborative work Evaluate files for inconsistencies and missing information          Fellowships and Awards              Publications     Chen TC Cho HY Wang W Claire M Sharma N Hofman FM Schönthal AH 2014 A novel temozolomide perillyl alcohol conjugate exhibits superior activity against breast cancer cells in vitro and intracranial triplenegative tumor growth in vivo Mol Cancer Ther 20141311811193 I coauthored two abstracts on Genetic basis of Rhabdomyosarcoma M Claire 2011 and on Chromosomal abnormalities in follicular thyroid carcinoma A V Kanugovi 2011which were selected for publication in the European Society of Human Genetics Conference 2011 Cocurricular  Extracurricular Highlights Completed short term certification course in Responsible Conduct of Research conducted at USC  2013 Presented a poster of research findings in the annual Microbiology and Immunology meet at USC The event was host to all department researchers and students 2012        Papers and Lectures              Memberships and Affiliations              Accomplishments       Inventory management for lab supplies  Research Experience Dr  AxelH Schönthals laboratory at Keck School of Medicine of USC  Masters Thesis          Jan 2012 ­ June 2013 The project involved preclinical trials of a novel anticancer drug candidate on breast cancer cell lines  The research focused on discovering the mechanism of action potency and mechanism of uptake and other characteristics of the compound  The longterm aim is treating breast cancers in the breast as well as those that have metastasized to the brain  This project helped me grow as a researcher and think outside the box  It got me accustomed to working individually while also taught me the value of a team  Cytogenetic Analysis of Acute Lymphoblastic Leukemia ALL at MGM hospital India  Undergraduate Thesis Jan 2011 ­ June 2011 The project was under the guidance of Dr  Bani  B  Ganguly towards the partial fulfilment of Bachelors degree  The project involved karyotyping of bone marrow samples of ALL patients followed by additional cytological analysis of the sample  Karyotyping was done following Gbanding of chromosomes in order to determine chromosomal aberrations         Skills     Acid Biochemistry Biology Biostatistics Cancer Cell Culture Clinical Trials customer service DNA ELISA email GCP Immunology inventory MB Excel Microsoft Office Microsoft Sharepoint 31 Midi Molecular Biology PAGE Pathology PCR Pharmacology Prism quality safety SAP phone troubleshooting Visualizer Western Blotting</data><data key="id">134865543431201283840878417070887163255</data></node>
<node id="n1449" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/assistant-research-scientist-ii-3e4df92f2ca34c5eb200060582d59671</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Professional Summary      ASCP board certified Histotechnologist with extensive GLP lab experience in necropsy fixation processing embedding microtoming and special stains with additional extensive experience as an Assistant Administrator for the PathData System organizing data and specimens as a Study Coordinator and identifying and entering gross data in necropsy as a Pathologist Assistant                 Skills       Can perform necropsy on various rodent and nonrodent animals and experienced in several specialized techniques ie EM sampling freezing in liquid nitrogen harvesting spinal fluid etc  Experienced in various trimming processing paraffin and OCT embedding microtoming cryostat microtoming and staining techniques  Experienced with preparation and use of HE staining as well as a variety of special staining techniques  Experienced with quality assurance review of microslides  Can operate maintain and troubleshoot various histology lab equipment  Experienced with gross data entry weighing tissue checkoff study setup troubleshooting training of staff and report setup and generation in PathData  Experience as Pathologist Assistant identifying and entering gross findings at necropsy  Complies with safety regulations SOPs GLPs and study protocols and has assisted in the writing and review of SOPs  Experienced at serving as Study Coordinator preparing and maintaining materials and data for studies  Experienced with inventory shipping and receiving and archival of specimens  Trained new personnel and personnel from other departments in necropsy and histology techinques  Acted as Fire Marshall and trained and certified in CPR         Professional Experience       Assistant Research Scientist II       062015   to   Current     Rutgers University    –    New Brunswick     NJ           Responsible for necropsy of rodents and nonrodents overall preparation of microslides for Pathologist review operating and maintaining lab equipment serving as a PathData Assistant Administrator preparing and generating reports for studies organizing histology specimens and study files for archival complying with SOP and GLP regulations assisting in the training of personnel and conducting inventory of lab supplies         Assistant Research Scientist I       102010   to   052015     Bristol MyersSquibb Company    –    City     STATE           Responsible for necropsy of rodents and nonrodents overall preparation of microslides for Pathologist review operating and maintaining lab equipment organizing histology specimens and study files for archival complying with SOP and GLP regulations assisting in the training of personnel and conducting inventory of lab supplies         Laboratory Animal Care Technician       092007   to   102010     Bioanalytical Systems INC    –    City     STATE           Responsible for health dosing bleeding clinical observations and necropsy of animals  As a Study Manager responsible for running and collection of data for studies  As Backup Necropsy Coordinator responsible for setup and cleanup of necropsies  Proficient in PathData and ToxData systems        Academic Background       High School Diploma          Expected in   2000     Sullivan High School      Sullivan     Indiana     GPA         Member of National  Beta  Club    Academic Honors Diploma    ​  ​          Bachelor of Science     Biology     Expected in   2004     Oakland City University      Oakland City     Indiana     GPA        20022003 Scholar Athlete  Member of Student Council  ​  ​         Awards and Professional Affiliations        Awards    Star Awards    2011 Supporting the Successful DSE MTV FDA Audit  PDS TissueBase Evaluation   2013 2016  Teamwork and Support of the PET Committee    2015 – Provantis 9402 User Acceptance Test Validation     ​   Extra Helpful Hand Awards Awarded 20102014    July 2011   February December 2012    March June September October 2013    January 2014       Professional Affiliations     2008Current               Member AALAS    2008Current               Member NSH   2014Current               HTLASCP Certified         ​         Presentations       CoAuthor of Gross Pathology Data Entry in Preclinical Toxicologic Studies in a GLP Environment a workshop presented at the 41st Annual NSH SymposiumConvention in Washington DC August 2015</data><data key="id">318784460999398815883461010528100620230</data></node>
<node id="n1450"><data key="name">data query</data></node>
<node id="n1451"><data key="name">predictive</data></node>
<node id="n1452"><data key="name">predictive analytic procedures</data></node>
<node id="n1453"><data key="name">supervised learning</data></node>
<node id="n1454"><data key="name">random forest</data></node>
<node id="n1455"><data key="name">unsupervised learning</data></node>
<node id="n1456"><data key="name">text mining</data></node>
<node id="n1457"><data key="name">graph mining</data></node>
<node id="n1458"><data key="name">pos</data></node>
<node id="n1459"><data key="name">tagging</data></node>
<node id="n1460"><data key="name">parsing</data></node>
<node id="n1461"><data key="name">named entity recognition</data></node>
<node id="n1462"><data key="name">relationship extraction</data></node>
<node id="n1463"><data key="name">information       retrieval</data></node>
<node id="n1464"><data key="name">sentimental classification</data></node>
<node id="n1465"><data key="name">machine translation</data></node>
<node id="n1466"><data key="name">rnn</data></node>
<node id="n1467"><data key="name">lstm</data></node>
<node id="n1468"><data key="name">gru</data></node>
<node id="n1469"><data key="name">image processing</data></node>
<node id="n1470"><data key="name">migration</data></node>
<node id="n1471"><data key="name">cleaning</data></node>
<node id="n1472"><data key="name">nonfunctional testing</data></node>
<node id="n1473"><data key="name">enduration load and volume testing</data></node>
<node id="n1474"><data key="name">multivariate testing</data></node>
<node id="n1475"><data key="name">predictive analytical modeling</data></node>
<node id="n1476"><data key="name">reduction techniques</data></node>
<node id="n1477"><data key="name">plsql developer</data></node>
<node id="n1478"><data key="name">pvcs</data></node>
<node id="n1479"><data key="name">cvs</data></node>
<node id="n1480"><data key="name">bashcorn</data></node>
<node id="n1481"><data key="name">scipy</data></node>
<node id="n1482"><data key="name">gensim</data></node>
<node id="n1483"><data key="name">rcaret</data></node>
<node id="n1484"><data key="name">weka</data></node>
<node id="n1485"><data key="name">feature engineering</data></node>
<node id="n1486"><data key="name">asp</data></node>
<node id="n1487"><data key="name">kmeans clustering</data></node>
<node id="n1488"><data key="name">customer churn</data></node>
<node id="n1489"><data key="name">fedwire</data></node>
<node id="n1490"><data key="name">chips</data></node>
<node id="n1491"><data key="name">rcfile</data></node>
<node id="n1492"><data key="name">avro</data></node>
<node id="n1493"><data key="name">har</data></node>
<node id="n1494"><data key="name">cygwin</data></node>
<node id="n1495"><data key="name">volume testing</data></node>
<node id="n1496"><data key="name">soup ui</data></node>
<node id="n1497"><data key="name">seapine qa wizard pro</data></node>
<node id="n1498"><data key="name">hypothetical testing</data></node>
<node id="n1499"><data key="name">anaconda</data></node>
<node id="n1500"><data key="name">spyder</data></node>
<node id="n1501"><data key="name">forecasting algorithm</data></node>
<node id="n1502"><data key="name">tfidf</data></node>
<node id="n1503"><data key="name">parsing text</data></node>
<node id="n1504"><data key="name">algebraic techniques</data></node>
<node id="n1505"><data key="name">epoch selection</data></node>
<node id="n1506"><data key="name">plotting cost functions</data></node>
<node id="n1507"><data key="name">“neural networks</data></node>
<node id="n1508"><data key="name">deep learning”</data></node>
<node id="n1509"><data key="name">deep neural networks”</data></node>
<node id="n1510"><data key="name">hyperparameter tuning</data></node>
<node id="n1511"><data key="name">regularization</data></node>
<node id="n1512"><data key="name">“convolutional neural networks”</data></node>
<node id="n1513"><data key="name">“machine learning”</data></node>
<node id="n1514"><data key="name">spark’s</data></node>
<node id="n1515"><data key="name">analytical requirements</data></node>
<node id="n1516"><data key="name">comprehensive analysis</data></node>
<node id="n1517"><data key="name">recommend solutions</data></node>
<node id="n1518"><data key="name">cross validation recall</data></node>
<node id="n1519"><data key="name">roc curves</data></node>
<node id="n1520"><data key="name">auc</data></node>
<node id="n1521"><data key="name">feature selection</data></node>
<node id="n1522"><data key="name">logitboost</data></node>
<node id="n1523"><data key="name">shift</data></node>
<node id="n1524"><data key="name">diagnostic testing</data></node>
<node id="n1525"><data key="name">business need analysis</data></node>
<node id="n1526"><data key="name">ml algorithm</data></node>
<node id="n1527"><data key="name">k means</data></node>
<node id="n1528"><data key="name">pychram</data></node>
<node id="n1529"><data key="name">ml algorithms</data></node>
<node id="n1530"><data key="name">research team</data></node>
<node id="n1531"><data key="name">amcis</data></node>
<node id="n1532"><data key="name">icis</data></node>
<node id="n1533"><data key="name">ecis</data></node>
<node id="n1534"><data key="name">geographic information system</data></node>
<node id="n1535"><data key="name">spatial analysis</data></node>
<node id="n1536"><data key="name">css3</data></node>
<node id="n1537"><data key="name">xhtml</data></node>
<node id="n1538"><data key="name">threejs</data></node>
<node id="n1539"><data key="name">webgl</data></node>
<node id="n1540"><data key="name">angularjs</data></node>
<node id="n1541"><data key="name">vslookup</data></node>
<node id="n1542"><data key="name">gimp</data></node>
<node id="n1543"><data key="name">quincy</data></node>
<node id="n1544"><data key="name">xgb</data></node>
<node id="n1545"><data key="name">naivebayes</data></node>
<node id="n1546"><data key="name">kmeans</data></node>
<node id="n1547"><data key="name">dbscan</data></node>
<node id="n1548"><data key="name">bayesian</data></node>
<node id="n1549"><data key="name">inference</data></node>
<node id="n1550"><data key="name">glm</data></node>
<node id="n1551"><data key="name">ridge</data></node>
<node id="n1552"><data key="name">lasso</data></node>
<node id="n1553"><data key="name">hierarchical</data></node>
<node id="n1554"><data key="name">hypothesis</data></node>
<node id="n1555"><data key="name">probabiclairety</data></node>
<node id="n1556"><data key="name">quaclairety</data></node>
<node id="n1557"><data key="name">statistical learnings</data></node>
<node id="n1558"><data key="name">gradient boosted trees</data></node>
<node id="n1559"><data key="name">random forest clustering</data></node>
<node id="n1560"><data key="name">simulation models</data></node>
<node id="n1561"><data key="name">gaussian mixture models</data></node>
<node id="n1562"><data key="name">cql</data></node>
<node id="n1563"><data key="name">logistic models</data></node>
<node id="n1564"><data key="name">poisson models</data></node>
<node id="n1565"><data key="name">credit scoring algorithms</data></node>
<node id="n1566"><data key="name">feature reduction</data></node>
<node id="n1567"><data key="name">restful api</data></node>
<node id="n1568"><data key="name">tokenization</data></node>
<node id="n1569"><data key="name">customer satisfaction</data></node>
<node id="n1570"><data key="name">logistical regression</data></node>
<node id="n1571"><data key="name">predictive analytics</data></node>
<node id="n1572"><data key="name">bayesian probability theory</data></node>
<node id="n1573"><data key="name">ensemble methods</data></node>
<node id="n1574"><data key="name">naïve bayes</data></node>
<node id="n1575"><data key="name">apriori algorithm</data></node>
<node id="n1576"><data key="name">tfs</data></node>
<node id="n1577"><data key="name">webapi2</data></node>
<node id="n1578"><data key="name">ampl</data></node>
<node id="n1579"><data key="name">multivariate</data></node>
<node id="n1580"><data key="name">explainable ai</data></node>
<node id="n1581"><data key="name">pdp</data></node>
<node id="n1582"><data key="name">ice</data></node>
<node id="n1583"><data key="name">lime</data></node>
<node id="n1584"><data key="name">shap</data></node>
<node id="n1585"><data key="name">doe</data></node>
<node id="n1586"><data key="name">arima</data></node>
<node id="n1587" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">12 Apr 2024</data><data key="target_group">Target audience: researchers from academia and industry, working with sensitive text data</data><data key="data">Keeping things private Exploring opensource large language models for sensitive text data Target audienceThe main target audience are researchers from both academia and industry  working with sensitive text data eg  patient records  HR files who want to explore opensource large language models In addition  anyone wanting to learn about opensource large language models is welcomePrerequisitesFor the lecture in the morning  there are no prerequisites in terms of background or skills  and no equipment is neededFor the handson workshop in the afternoon  a basic understanding of Python and the command line is required Google Colab notebooks are provided so no programming from scratch  but participants need to at least understand the given Python code and be able to tweak it Advanced programming skills  however  are not required In case of doubt  contact the workshop teacher Pieter Fivez PieterfivezuantwerpenbeFor the workshop  make sure to bring a fully charged laptop with Python installed on it version 39 or higher Access to Google Colab free or paying is required  tooThis course is the very first edition of the TEXTUA Invites series  in which TEXTUA University of Antwerp invites national and international experts to tackle diverse challenges within the field of text mining This course is coorganized by the Flanders AI Academy VAIAThe lecture is given by international NLP expert dr Enrique Manjavacas  and the workshop is given by text mining expert and TEXTUA coordinator dr Pieter Fivez The opening and introduction is given by TEXTUA director prof dr Walter DaelemansTEXTUA is a core facility of the University of Antwerp  directed by prof dr Walter Daelemans and coordinated by dr Pieter Fivez  which provides scalable text mining solutions to researchers from any scientific discipline It offers a diverse collection of services for a broad range of textual data  including automatically transcribed speech and written text in images TEXTUA bundles the unique existing expertise in digital text analysis at the University of Antwerp with special emphasis on explainable Artificial Intelligence  More info  Share this course   httpswwwuantwerpenbeenresearchfacilitiestextuaeducationalactivitiestextuainvitesenrique 15 Apr 2024 1000  1600 lecture amp workshopAntwerpTEXTUA UAntwerp VAIA</data><data key="sub_title">15 Apr 2024 10:00 - 16:00</data><data key="start_time">15 Apr 2024 10:00 - 16:00</data><data key="language">English</data><data key="location_detail">City Campus of the University of Antwerp, Building C, room C.002 (Entrance via Prinsstraat 13, Antwerp)</data><data key="price">€50-€150 (see Practical information below)</data><data key="intro">More info  Share this course  </data><data key="date">15/04/2024</data><data key="details">https://www.uantwerpen.be/en/research-facilities/textua/educational-activities/textua-invites-enrique/</data><data key="full_body">Target audienceThe main target audience are researchers from both academia and industry  working with sensitive text data (e.g.  patient records  HR files) who want to explore open-source large language models. In addition  anyone wanting to learn about open-source large language models is welcome!PrerequisitesFor the lecture in the morning  there are no prerequisites in terms of background or skills  and no equipment is needed.For the hands-on workshop in the afternoon  a basic understanding of Python and the command line is required. Google Colab notebooks are provided (so no programming from scratch)  but participants need to at least understand the given Python code and be able to tweak it. Advanced programming skills  however  are not required. In case of doubt  contact the workshop teacher Pieter Fivez (Pieter.fivez@uantwerpen.be)For the workshop  make sure to bring a fully charged laptop with Python installed on it (version 3.9 or higher). Access to Google Colab (free or paying) is required  too.This course is the very first edition of the 'TEXTUA Invites' series  in which TEXTUA (University of Antwerp) invites national and international experts to tackle diverse challenges within the field of text mining. This course is co-organized by the Flanders AI Academy VAIA.The lecture is given by international NLP expert dr. Enrique Manjavacas  and the workshop is given by text mining expert and TEXTUA coordinator dr. Pieter Fivez. The opening and introduction is given by TEXTUA director prof. dr. Walter Daelemans.TEXTUA is a core facility of the University of Antwerp  directed by prof. dr. Walter Daelemans and coordinated by dr. Pieter Fivez  which provides scalable text mining solutions to researchers from any scientific discipline. It offers a diverse collection of services for a broad range of textual data  including automatically transcribed speech and written text in images. TEXTUA bundles the unique existing expertise in digital text analysis at the University of Antwerp with special emphasis on explainable Artificial Intelligence. </data><data key="url">https://www.vaia.be/en/courses/keeping-things-private-exploring-open-source-large-language-models-for-sensitive-text-data</data><data key="title">Keeping things private: Exploring open-source large language models for sensitive text data</data><data key="constraints">for the lecture there are no prerequisites; for the workshop a laptop and a basic understanding of Python and the command line are required</data><data key="course_info">lecture &amp;amp; workshop-Antwerp-TEXTUA; UAntwerp; VAIA</data></node>
<node id="n1588" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: This course targets professionals and researchers from all areas that are involved in predictive modeling based on large and/or high-dimensional databases.</data><data key="data">Machine Learning with Python Tackle the analytical part of data mining projects                            DescriptionMany modern digital applications increasingly rely on machine learning as a means to derive predictive strength from highdimensional data sets Compared to traditional statistics  the absence of a focus on scientific hypotheses  and the need for easily leveraging detailed signals in the data require a different set of models  tools  and analytical reflexesThis course aims to bring participants to the level where they can independently tackle the analytical part of data mining projects This means that the most common types of projects will be addressed  regressiontype with continuous outcomes  classification with categorical outcomes  and clustering For each of these  the practical use of a set of standard methods will be shown  like Random Forests  Gradient Boosting Machines  Support Vector Machines  kNearestNeighbors  Kmeans  Furthermore  throughout the course  concepts will be highlighted that are of concern in every statistical learning applications  like the curse of dimensionality  model capacity  overfitting and regularization  and practical strategies will be offered to deal with them  introducing techniques such as the Lasso and ridge regression  crossvalidation  bagging and boosting Instructions will also be given on a selection of specific techniques that are often of interest  such as modern visualization of highdimensional data  model calibration  outlier detection using isolation forests  explanation of blackbox models  Finally  the last lecture will introduce the idea of deep learning as a powerful tool for data analysis  discussing when and how to practically use it  and when to shy away from itTarget audienceThis course targets professionals and investigators from all areas that are involved in predictive modeling based on large andor highdimensional databasesFeesThe participation fee is 1470 EUR for participants from the private sector Reduced prices apply to students and staff from nonprofit  social profit  and government organizations An exam fee of 35 EUR will be appliedIndustry  private sector  profession € 1470Non profit  government  higher education staff € 1105Doctoral students  unemployed € 600If two or more employees from the same company enrol simultaneously for this course a reduction of 20 on the course fee is taken into account starting from the second enrolmentRegistrationMore information and registration on our BetaAcademy website Tackle the analytical part of data mining projects httpsbetaacademyugentbeenprogramshortandlongrunninginitiatives202320242024m12pymodule12machinelearningwith Tackle the analytical part of data mining projects courseGhentUGent</data><data key="start_time">15 Apr 2024 - 3 Jun 2024</data><data key="price">€600 - €1470</data><data key="sub_title">Tackle the analytical part of data mining projects</data><data key="location_detail">Krijgslaan 281, 9000 Gent</data><data key="full_body">Tackle the analytical part of data mining projects                            DescriptionMany modern digital applications increasingly rely on machine learning as a means to derive predictive strength from high-dimensional data sets. Compared to traditional statistics  the absence of a focus on scientific hypotheses  and the need for easily leveraging detailed signals in the data require a different set of models  tools  and analytical reflexes.This course aims to bring participants to the level where they can independently tackle the analytical part of data mining projects. This means that the most common types of projects will be addressed - regression-type with continuous outcomes  classification with categorical outcomes  and clustering. For each of these  the practical use of a set of standard methods will be shown  like Random Forests  Gradient Boosting Machines  Support Vector Machines  k-Nearest-Neighbors  K-means ... Furthermore  throughout the course  concepts will be highlighted that are of concern in every statistical learning applications  like the curse of dimensionality  model capacity  overfitting and regularization  and practical strategies will be offered to deal with them  introducing techniques such as the Lasso and ridge regression  cross-validation  bagging and boosting. Instructions will also be given on a selection of specific techniques that are often of interest  such as modern visualization of high-dimensional data  model calibration  outlier detection using isolation forests  explanation of black-box models ... Finally  the last lecture will introduce the idea of deep learning as a powerful tool for data analysis  discussing when and how to practically use it  and when to shy away from it.Target audienceThis course targets professionals and investigators from all areas that are involved in predictive modeling based on large and/or high-dimensional databases.FeesThe participation fee is 1470 EUR for participants from the private sector. Reduced prices apply to students and staff from non-profit  social profit  and government organizations. An exam fee of 35 EUR will be applied.Industry  private sector  profession*: € 1470Non profit  government  higher education staff: € 1105(Doctoral) students  unemployed: € 600*If two or more employees from the same company enrol simultaneously for this course a reduction of 20% on the course fee is taken into account starting from the second enrolment.RegistrationMore information and registration on our Beta-Academy website.</data><data key="intro">Tackle the analytical part of data mining projects</data><data key="language">English</data><data key="course_info">course-Ghent-UGent</data><data key="details">https://beta-academy.ugent.be/en/program/short-and-long-running-initiatives/2023-2024-2024m12py-module-12-machine-learning-with</data><data key="title">Machine Learning with Python</data><data key="url">https://www.vaia.be/en/courses/module-12-machine-learning-with-python</data><data key="constraints">Participants are expected to be familiar with basic statistical modeling (as for instance taught in Module 4 of this program), and to have had a first experience programming in Python (as for instance taught in Module 5 of this program).</data><data key="date">15/04/2024</data></node>
<node id="n1589" labels=":Course"><data key="labels">:Course</data><data key="subscription_limit">10 Apr 2024</data><data key="annotated_by">manual</data><data key="start_time">17 Apr 2024 - 25 Apr 2024</data><data key="price">€240-€800</data><data key="data">Current Trends in AI Advanced GenAI 17 April 2024  12301530hDr Thomas WintersThese days  we are surrounded by creative text and image generators like GPT and diffusion models that seem to be able to generate anything we want But how do we ensure that these types of AI truly aid us in overcoming our unique challenges This talk sheds light on several techniques for controlling such generative models We look at several powerful prompt engineering techniques – the art of enhancing our communication with AI – and useful ways of connecting these generators to other systems We dive into the world of autoregressive text generators  learn their inner mechanisms and which training phases they went through to get to the current stateoftheart text generators These insights help us understand why certain prompt engineering techniques such as fewshot prompting  roleprompting and chainofthought prompting are able to outperform simpler prompting methods We also briefly look at several other techniques to overcome the limitations of such models  such as retrievalaugmented generation and function calling Similarly  we uncover the workings of diffusion models and show several techniques to gain more control over the generated images We show how even some of AIs classic hard problems  such as humour generation  become even more within reach thanks to these large language models and their prompt engineering techniquesKnowledge Graphs 17 April 2024  16001900hProf Pieter Bonte and prof Anastasia DimouKnowledge graphs have become the ultimate technology for unlocking the full potential of your data  illuminating the connections between entities  attributes  and relationships with unparalleled clarity Sharing  exchanging data  harvesting insights  driving innovation  fostering integration  and revolutionizing data exploration  knowledge graphs pave the way for transformative discoveries and understandingIn this seminar  the following topic will be tackledIntroduction to knowledge graphsSemantic Web basics RDFFrom raw data to knowledge graphs with R2RMLUnlocking insights with querying through SPARQLMLOps 25 April 2024  12301530hProf Mathias Verbeke and drs Lara LuysOne of the main challenges for industry today is to get machine learning models out of the proofofconcept phase and into production This is not an easy task since data in production is not static  due to which the model performance can degrade over time Machine learning operations or MLOps is a paradigm that aims to address this problem Being a contraction of Machine Learning and DevOps  MLOps focuses on developing and maintaining machine learning models in production This includes training  evaluating as well as monitoring the model In this seminar  the MLOps pipeline and the underlying principles will be explained  illustrated by means of a number of tools that can be used in the MLOps processAIEdge 25 April 2024  16001900hProf Hans Hallez and drs Gregory De RuyterEdge Computing has been proven to be an optimised way to delegate computation from the cloud towards the devices where the sensing takes place Edge computing on embedded devices mostly limits itself towards compression  filtering or other basic analysis Recent trends also show that devices near the edge of the sensor network are capable of machine learning algorithms In this session  we will explore different techniques to bring machine learning towards the edge network and deploy these algorithms First  we will give an overview of what embedded devices are  and how we can perform machine learning at these devices both in inference and in training Second  we will give a handson demonstration as an inspiration of how machine learning at the edge can be implemented The world of artificial intelligence evolves at an astonishing speed For those working with AI on a daily basis staying up to date with the newest techniques within various domains is a challenge In this seminar series academic experts will bring you up to speed with the hottest topics Advanced GenAI Knowledge Graphs MLOps and AIEdge All seminars include a handson component httpspuckuleuvenbenlopleidingcurrenttrendsinaikwdjog3exxqm4807 Bring AIprofessionals up to speed with the latest developments in the field lezingenreeksBruggeKU Leuven Postuniversitair Centrum VAIA</data><data key="language">English</data><data key="sub_title">Bring AI-professionals up to speed with the latest developments in the field</data><data key="target_group">Target audience: AI-professionals who wish to stay up to date - AI engineers, R&amp;amp;D managers, IT developers, functional analysts, etc.</data><data key="details">https://puc.kuleuven.be/nl/opleiding/current_trends_in_ai-kwdjog3exxqm4807</data><data key="full_body">Advanced GenAI (17 April 2024  12.30-15.30h)Dr. Thomas WintersThese days  we are surrounded by creative text and image generators like GPT and diffusion models that seem to be able to generate anything we want. But how do we ensure that these types of AI truly aid us in overcoming our unique challenges? This talk sheds light on several techniques for controlling such generative models. We look at several powerful prompt engineering techniques – the art of enhancing our communication with AI – and useful ways of connecting these generators to other systems. We dive into the world of autoregressive text generators  learn their inner mechanisms and which training phases they went through to get to the current state-of-the-art text generators. These insights help us understand why certain prompt engineering techniques (such as few-shot prompting  role-prompting and chain-of-thought prompting) are able to outperform simpler prompting methods. We also briefly look at several other techniques to overcome the limitations of such models  such as retrieval-augmented generation and function calling. Similarly  we uncover the workings of diffusion models and show several techniques to gain more control over the generated images. We show how even some of AI's classic hard problems  such as humour generation  become even more within reach thanks to these large language models and their prompt engineering techniques.Knowledge Graphs (17 April 2024  16.00-19.00h)Prof. Pieter Bonte and prof. Anastasia DimouKnowledge graphs have become the ultimate technology for unlocking the full potential of your data  illuminating the connections between entities  attributes  and relationships with unparalleled clarity. Sharing &amp; exchanging data  harvesting insights  driving innovation  fostering integration  and revolutionizing data exploration  knowledge graphs pave the way for transformative discoveries and understanding.In this seminar  the following topic will be tackled:Introduction to knowledge graphsSemantic Web basics: RDFFrom raw data to knowledge graphs with [R2]RMLUnlocking insights with querying through SPARQLMLOps (25 April 2024  12.30-15.30h)Prof. Mathias Verbeke and drs. Lara LuysOne of the main challenges for industry today is to get machine learning models out of the proof-of-concept phase and into production. This is not an easy task since data in production is not static  due to which the model performance can degrade over time. Machine learning operations or MLOps is a paradigm that aims to address this problem. Being a contraction of Machine Learning and DevOps  MLOps focuses on developing and maintaining machine learning models in production. This includes training  evaluating as well as monitoring the model. In this seminar  the MLOps pipeline and the underlying principles will be explained  illustrated by means of a number of tools that can be used in the MLOps process.AI@Edge (25 April 2024  16.00-19.00h)Prof. Hans Hallez and drs. Gregory De RuyterEdge Computing has been proven to be an optimised way to delegate computation from the cloud towards the devices where the sensing takes place. Edge computing on embedded devices mostly limits itself towards compression  filtering or other basic analysis. Recent trends also show that devices near the edge of the sensor network are capable of machine learning algorithms. In this session  we will explore different techniques to bring machine learning towards the edge network and deploy these algorithms. First  we will give an overview of what embedded devices are  and how we can perform machine learning at these devices both in inference and in training. Second  we will give a hands-on demonstration as an inspiration of how machine learning at the edge can be implemented.</data><data key="intro">The world of artificial intelligence evolves at an astonishing speed. For those working with AI on a daily basis, staying up to date with the newest techniques within various domains is a challenge. In this seminar series, academic experts will bring you up to speed with the hottest topics: Advanced GenAI, Knowledge Graphs, MLOps and AI@Edge. All seminars include a hands-on component.</data><data key="location_detail">Spoorwegstraat 12, 8200 Brugge</data><data key="constraints">NA</data><data key="course_info">lezingenreeks-Brugge-KU Leuven Postuniversitair Centrum; VAIA</data><data key="date">17/04/2024</data><data key="url">https://www.vaia.be/en/courses/current-trends-in-ai-3</data><data key="title">Current Trends in AI</data></node>
<node id="n1590" labels=":Course"><data key="labels">:Course</data><data key="data">Digital Ethics As our reliance on technology continues to grow  the importance of digital ethics cannot be overstated In todays world  organisations in every sector must be aware of the ethical implications of their actions in the digital realm From the handling of personal data to the development of new technologies  the decisions we make have the potential to impact individuals  society  and the environment in profound ways By participating in this course  you will gain the knowledge and skills necessary to navigate the complex ethical landscape of the digital world and make informed decisions that align with your organisations valuesOur course will cover a range of important topics  including ethics frameworks and tools  regulation and legal issues  developing ethical technology  technology assessment  ethical dilemmas  and ethics in data and visualisationThis course is designed for professionals working in a wide range of organisations and industries Whether you are employed in government  industry  academia  or the nonprofit sector  you will benefit from the comprehensive coverage of key topics in Digital Ethics The course is particularly relevant for those working in fields such as technology  interaction design  data analysis  policymaking  law  and research However  anyone with an interest in Digital Ethics and the impact of technology on society will find value in this courseSession 1 Ethics frameworks – Thursday 18 April 2024During this session  we will explore the various frameworks that can be used to justify ethical decisions  including virtue ethics  consequence ethics  and duty ethics The participants will learn how to think ethically  and how these basic ethics frameworks can be applied to various scenarios in which digital technologies play a central role We will also explore the concept of responsibility  including the different types of responsibility moral  legal  causal or role responsibility that can arise in the digital realm and discuss the responsibility gap This session will provide attendees with a solid foundation in the basic principles of ethics and their relation to digital technologySession 2 Building Ethical Foundations  Thursday 25 April 2024This session aims to create a communal learning environment among the participants  both online and onsite During this session  participants will introduce themselves  share their professional backgrounds and discuss their expectations for the course  providing invaluable insight for the instructors to tailor the subsequent sessions Additionally  participants will reflect on the ethical aspects of their jobs  exploring potential ethical dilemmas within their specific work contexts such as government  industry  academia  or nonprofit sectors This early engagement is designed to foster a deeper understanding and awareness of the ethical issues to be explored throughout the course  setting the stage for a more insightful reflection during the closing sessionSession 3 Tools and methods for ethical decision making  Thursday 2 May 2024In this session we will introduce tools and methods that can be used to make ethical decisions in different contexts  such as in industry  government  academia and research Participants will learn about practical frameworks and tools such as the guidance ethics approach  the AI Blindspots and the Data Ethics Decision Aid This session will provide attendees with the basic tools and methods in the practice of ethical decisionmaking in the digital realmSession 4 Technology assessment in practice – Thursday 16 May 2024This session will provide insight into the impact and effects of technology  and will introduce tools and methods for assessing the risks of innovation and averting undesirable consequences of new technologies The session will look into the challenges of achieving positive or negative impacts with technology due to the interplay of technology  society  organisations and individuals A dedicated focus will be on the impact of AI technologies in a government context as well as on evaluating the results of social corporate responsibility actions in the context of a company This session will provide attendees with practical examples of how to effectively assess the risks and potential consequences of technologySession 5 Designing ethical technology – Thursday 23 May 2024In this session  we will explore different approaches to designing technology that takes into account the needs and values of human beings We will explore humanitycentered design including design thinking  inclusive design  valuecentered design  and participatory design or cocreation We will then discuss how technologies are often designed in a way that threatens our digital wellbeing  and which strategies can be used to improve digital wellbeing This session will provide attendees with the tools and knowledge they need to create ethical technology that is designed with people at its coreSession 6 Ethical topics and dilemmas – Thursday 30 May 2024This session will explore the application of behaviorist principles in digital technologies  particularly in the realm of surveillance It will highlight how these technologies not only observe but actively influence and modify user behavior  raising important ethical questions about privacy  autonomy  and the power dynamics between users and technology providers The session will emphasize the need for ethical guidelines to address the challenges posed by digital surveillance This session further examines the various functions fairness and equality hold in the AI governance debate Drawing from political philosophical theories of justice and practical usecases  a relational perspective on AI is developed to understand better the social and technical dynamics that give rise to digital injustice This session will provide attendees with an understanding of the ethical challenges that can arise in the digital world  and the tools and frameworks that can be used to address themSession 7 Legal and policy issues – Thursday 6 June 2024This session will explore a number of legal issues that arise in the digital realm This session will first provide attendees with an understanding of appropriate legal frameworks related to the free flow of information  personal data protection  and the principles  rights and obligations of the EUs General Data Protection Regulation GDPR This session will also provide a comprehensive review of the impact AI may have on individual and collective human rights across various sectors The participants will learn about the technical legal instruments at their disposal to combat AI discrimination  including legal tools related to direct and indirect discrimination  as well as discrimination by association Moreover  participants will be invited to critically reflect on the resilience of ethics in the context of AI This approach to AI will consider highlevel legal instruments within the European Union  such as the EU Charter on Fundamental Rights  the proposed AI Act  and the set of equality directives A comparative law approach will also be mobilized to provide a better understanding of addressing AI challenges with traditional legal instrumentsSession 8 Ethics in data and visualisation – Thursday 13 June 2024In this session  we will discuss the ethical aspects of handling and visualising of data We will start by examining how data are constructed and therefore are inherently biased  rather than representing an objective truth We cross over to understand the value of visualising data  acknowledging its potential risk to yet further objectify this constructed  situated nature of data We then dive into how some data visualisations can be unintentionally perceived as unethical  whether through potentially misleading design choices  the use of dubious data  the omission of uncertainties  or the disregard of cognitive biases This session aims to provide practical skills and reflective insights for handling and visualising data in ways that are more transparent  equitable and responsibleSession 9 Closing and reflection – Thursday 20 June 2024The seventh and final session will be a closing session focused on reflection In this session  participants will have the opportunity to reflect on how the topics covered in the previous sessions apply to their own professional situations This session will provide a space for attendees to share their thoughts  insights  and experiences  and to discuss how they plan to incorporate the knowledge and skills they have gained into their work This will be a valuable opportunity for participants to consolidate their learning and to develop a personal plan for applying the principles of Digital Ethics in their professional lives More info  Share this course   httpswwwkuleuvenbedigisoceducationandtraininghybridcourseondigitalethics 18 Apr 2024  20 Jun 2024 coursehybridKU Leuven</data><data key="annotated_by">manual</data><data key="sub_title">18 Apr 2024 - 20 Jun 2024</data><data key="start_time">18 Apr 2024 - 20 Jun 2024</data><data key="price">€ 1500</data><data key="subscription_limit">31 Mar 2024</data><data key="language">English</data><data key="location_detail">Hybrid</data><data key="intro">More info  Share this course  </data><data key="date">18/04/2024</data><data key="full_body">As our reliance on technology continues to grow  the importance of digital ethics cannot be overstated. In today's world  organisations in every sector must be aware of the ethical implications of their actions in the digital realm. From the handling of personal data to the development of new technologies  the decisions we make have the potential to impact individuals  society  and the environment in profound ways. By participating in this course  you will gain the knowledge and skills necessary to navigate the complex ethical landscape of the digital world and make informed decisions that align with your organisation's values.Our course will cover a range of important topics  including ethics frameworks and tools  regulation and legal issues  developing ethical technology  technology assessment  ethical dilemmas  and ethics in data and visualisation.This course is designed for professionals working in a wide range of organisations and industries. Whether you are employed in government  industry  academia  or the non-profit sector  you will benefit from the comprehensive coverage of key topics in Digital Ethics. The course is particularly relevant for those working in fields such as technology  interaction design  data analysis  policymaking  law  and research. However  anyone with an interest in Digital Ethics and the impact of technology on society will find value in this course.Session 1: Ethics frameworks – Thursday 18 April 2024During this session  we will explore the various frameworks that can be used to justify ethical decisions  including virtue ethics  consequence ethics  and duty ethics. The participants will learn how to 'think ethically'  and how these basic ethics frameworks can be applied to various scenarios in which digital technologies play a central role. We will also explore the concept of responsibility  including the different types of responsibility (moral  legal  causal or role responsibility) that can arise in the digital realm and discuss the responsibility gap. This session will provide attendees with a solid foundation in the basic principles of ethics and their relation to digital technology.Session 2: Building Ethical Foundations - Thursday 25 April 2024This session aims to create a communal learning environment among the participants  both online and on-site. During this session  participants will introduce themselves  share their professional backgrounds and discuss their expectations for the course  providing invaluable insight for the instructors to tailor the subsequent sessions. Additionally  participants will reflect on the ethical aspects of their jobs  exploring potential ethical dilemmas within their specific work contexts such as government  industry  academia  or non-profit sectors. This early engagement is designed to foster a deeper understanding and awareness of the ethical issues to be explored throughout the course  setting the stage for a more insightful reflection during the closing session.Session 3: Tools and methods for ethical decision making - Thursday 2 May 2024In this session we will introduce tools and methods that can be used to make ethical decisions in different contexts  such as in industry  government  academia and research. Participants will learn about practical frameworks and tools such as the guidance ethics approach  the AI Blindspots and the Data Ethics Decision Aid. This session will provide attendees with the basic tools and methods in the practice of ethical decision-making in the digital realm.Session 4: Technology assessment in practice – Thursday 16 May 2024This session will provide insight into the impact and effects of technology  and will introduce tools and methods for assessing the risks of innovation and averting undesirable consequences of new technologies. The session will look into the challenges of achieving (positive or negative) impacts with technology due to the interplay of technology  society  organisations and individuals. A dedicated focus will be on the impact of AI technologies in a government context as well as on evaluating the results of social corporate responsibility actions in the context of a company. This session will provide attendees with practical examples of how to effectively assess the risks and potential consequences of technology.Session 5: Designing ethical technology – Thursday 23 May 2024In this session  we will explore different approaches to designing technology that takes into account the needs and values of human beings. We will explore humanity-centered design (including design thinking)  inclusive design  value-centered design  and participatory design (or co-creation). We will then discuss how technologies are often designed in a way that threatens our digital well-being  and which strategies can be used to improve digital well-being. This session will provide attendees with the tools and knowledge they need to create ethical technology that is designed with people at its core.Session 6: Ethical topics and dilemmas – Thursday 30 May 2024This session will explore the application of behaviorist principles in digital technologies  particularly in the realm of surveillance. It will highlight how these technologies not only observe but actively influence and modify user behavior  raising important ethical questions about privacy  autonomy  and the power dynamics between users and technology providers. The session will emphasize the need for ethical guidelines to address the challenges posed by digital surveillance. This session further examines the various functions fairness and equality hold in the AI governance debate. Drawing from political philosophical theories of justice and practical use-cases  a relational perspective on AI is developed to understand better the social and technical dynamics that give rise to digital injustice. This session will provide attendees with an understanding of the ethical challenges that can arise in the digital world  and the tools and frameworks that can be used to address them.Session 7: Legal and policy issues – Thursday 6 June 2024This session will explore a number of legal issues that arise in the digital realm. This session will first provide attendees with an understanding of appropriate legal frameworks related to the free flow of information  (personal) data protection  and the principles  rights and obligations of the EU's General Data Protection Regulation (GDPR). This session will also provide a comprehensive review of the impact AI may have on individual and collective human rights across various sectors. The participants will learn about the technical legal instruments at their disposal to combat AI discrimination  including legal tools related to direct and indirect discrimination  as well as discrimination by association. Moreover  participants will be invited to critically reflect on the resilience of ethics in the context of AI. This approach to AI will consider high-level legal instruments within the European Union  such as the EU Charter on Fundamental Rights  the proposed AI Act  and the set of equality directives. A comparative law approach will also be mobilized to provide a better understanding of addressing AI challenges with traditional legal instruments.Session 8: Ethics in data and visualisation – Thursday 13 June 2024In this session  we will discuss the ethical aspects of handling and visualising of data. We will start by examining how data are 'constructed' and therefore are inherently 'biased'  rather than representing an objective truth. We cross over to understand the value of visualising data  acknowledging its potential risk to yet further objectify this constructed  situated nature of data. We then dive into how some data visualisations can be (unintentionally) perceived as unethical  whether through potentially misleading design choices  the use of dubious data  the omission of uncertainties  or the disregard of cognitive biases. This session aims to provide practical skills and reflective insights for handling and visualising data in ways that are more transparent  equitable and responsible.Session 9: Closing and reflection – Thursday 20 June 2024The seventh and final session will be a closing session focused on reflection. In this session  participants will have the opportunity to reflect on how the topics covered in the previous sessions apply to their own professional situations. This session will provide a space for attendees to share their thoughts  insights  and experiences  and to discuss how they plan to incorporate the knowledge and skills they have gained into their work. This will be a valuable opportunity for participants to consolidate their learning and to develop a personal plan for applying the principles of Digital Ethics in their professional lives.</data><data key="target_group">Target audience: professionals looking to expand their knowledge and skills in the field of Digital Ethics</data><data key="course_info">course-hybrid-KU Leuven</data><data key="details">https://www.kuleuven.be/digisoc/education-and-training/hybrid-course-on-digital-ethics</data><data key="url">https://www.vaia.be/en/courses/digital-ethics-2024</data><data key="title">Digital Ethics</data><data key="constraints">NA</data></node>
<node id="n1591" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: business professionals</data><data key="data">AI amp Digital Technologies AI technology can have a huge impact on organisations and society in general This info session offers valuable insights to equip business leaders with the knowledge they need to navigate the AI landscape successfully However  managers should realise that the potential of this technology is closely tied to data requirements  which may pose limitations for some businesses This session aims to demystify AI for business leaders to enable them to uncover opportunities to improve operations increase customer experience and more We’ll unravel the complexities of AI and empower your organisation to harness its power responsibly and effectively Your speaker Professor Philippe Baecke is an expert in the power of AI big data and analytics in all areas of marketing httpswwwvlerickcomeneventsonlineinfosessionaianddigital Business value with artificial intelligence exploring opportunities amp overcoming limitations webinaronlineVlerick Business School</data><data key="annotated_by">manual</data><data key="start_time">19 Apr 2024 12:30 - 13:30</data><data key="location_detail">Online</data><data key="sub_title">Business value with artificial intelligence: exploring opportunities &amp;amp; overcoming limitations</data><data key="subscription_limit">NA</data><data key="intro">This session aims to demystify AI for business leaders to enable them to uncover opportunities to improve operations, increase customer experience, and more. We’ll unravel the complexities of AI and empower your organisation to harness its power responsibly and effectively. Your speaker, Professor Philippe Baecke is an expert in the power of AI, big data and analytics in all areas of marketing.</data><data key="language">English</data><data key="price">free</data><data key="date">19/04/2024</data><data key="details">https://www.vlerick.com/en/events/online-info-session-ai-and-digital/</data><data key="full_body">AI technology can have a huge impact on organisations and society in general. This info session offers valuable insights to equip business leaders with the knowledge they need to navigate the AI landscape successfully. However  managers should realise that the potential of this technology is closely tied to data requirements  which may pose limitations for some businesses.</data><data key="url">https://www.vaia.be/en/courses/ai-digital-technologies</data><data key="title">AI &amp;amp; Digital Technologies</data><data key="constraints">NA</data><data key="course_info">webinar-online-Vlerick Business School</data></node>
<node id="n1592" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: PhD students, postdocs, academics, healthcare professionals</data><data key="data">How to setup a Health Data Sharing Initiative Part I before lunch Navigating the Roadmap of SettingUp a Health Data Sharing InitiativeDuring this session  the participants will be introduced into the highlevel roadmap of settingup a data sharing initiative We will delve into the initial stages of ideation  missionvision definition  stakeholder identification  and engagement strategies necessary to establish a solid foundation required to successfully kickstart the initiativesAfterwards  we will introduce them into the process of finetuning the idea  governance principles  data collection  harmonization  quality assessment  and finally  transforming data into insightful visualizations  ensuring a holistic approach towards initiative setupPart II afternoon Practical Examples and HandsOn ExercisesSome basic concepts of data handling and analyses will be explained through simple handson exercises For example  participants will engage in practical exercises involving transforming data from patientlevel to aggregated  while also addressing concerns such as handling missing data and ensuring data quality Next to this  we will explore how insights can be visualized through a dashboard interfaceImportant note  these exercises will be simple and will not require coding experience or expertise in data science On Monday 22 April the Research Group in Biomedical Data Sciencesof UHasselt is organising a handson workshop called How to set up a health data sharing initiative in which participants will be through the various stages of successfully setting up and implementing this type of initiative We aim for peertopeer knowledge leveraging via a number of pioneers in the field as well as demonstrating a number of crucial concepts via simple exercises eg data missingness data aggregation and the visualisation of data via a dashboard inspired by the COVID19 in MS Global Data Sharing Initiative GDSI httpswwwuhasseltbeeninstitutenenbiomedenimmunologybiomedicaldatasciencesworkshophowtosetupahealthdatasharinginitiative Workshop workshopDiepenbeekHasselt University</data><data key="annotated_by">manual</data><data key="start_time">22 Apr 2024 09:30 - 16:30</data><data key="price">free</data><data key="sub_title">Workshop</data><data key="subscription_limit">NA</data><data key="intro">On Monday 22 April, the Research Group in Biomedical Data Sciencesof UHasselt is organising a hands-on workshop called "How to set up a health data sharing initiative?", in which participants will be through the various stages of successfully setting up and implementing this type of initiative. We aim for peer-to-peer knowledge leveraging via a number of pioneers in the field as well as demonstrating a number of crucial concepts via simple exercises (e.g. data missingness, data aggregation) and the visualisation of data via a dashboard, inspired by the COVID-19 in MS Global Data Sharing Initiative (GDSI).</data><data key="language">English</data><data key="location_detail">Embuild - Wetenschapspark 33, 3590 Diepenbeek</data><data key="date">22/04/2024</data><data key="details">https://www.uhasselt.be/en/instituten-en/biomed-en/immunology/biomedical-data-sciences/workshop-how-to-set-up-a-health-data-sharing-initiative</data><data key="full_body">Part I (before lunch): Navigating the Roadmap of Setting-Up a Health Data Sharing InitiativeDuring this session  the participants will be introduced into the high-level roadmap of setting-up a data sharing initiative. We will delve into the initial stages of ideation  mission-vision definition  stakeholder identification  and engagement strategies necessary to establish a solid foundation required to successfully kick-start the initiatives.Afterwards  we will introduce them into the process of fine-tuning the idea  governance principles  data collection  harmonization  quality assessment  and finally  transforming data into insightful visualizations  ensuring a holistic approach towards initiative setup.Part II (afternoon): Practical Examples and Hands-On ExercisesSome basic concepts of data handling and analyses will be explained through simple hands-on exercises. For example  participants will engage in practical exercises involving transforming data from patient-level to aggregated  while also addressing concerns such as handling missing data and ensuring data quality. Next to this  we will explore how insights can be visualized through a dashboard interface.Important note - these exercises will be simple and will not require coding experience or expertise in data science.</data><data key="url">https://www.vaia.be/en/courses/workshop-how-to-set-up-a-health-data-sharing-initiative</data><data key="title">How to set-up a Health Data Sharing Initiative</data><data key="constraints">basic awareness and engagement in health data sharing initiatives (no coding experience required); limited experience in executing large-scale collaborative projects</data><data key="course_info">workshop-Diepenbeek-Hasselt University</data></node>
<node id="n1593" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: health data enthusiasts</data><data key="data">Big Data for Health amp Care The Arisal of Data Spaces In recent years  a series of health data spaces have arised However  at the moment  the strategic oversight is lost and many questions remain For exampleWhich data space initiatives are out thereHow do they relate to each otherHow will they work togetherin synergy to maximize the impactHow will this affect the local data management at the level of the individual data partnersWith this symposium  we aspire to facilitate an indepth exploration of data spaces and research infrastructures  elucidate challenges and best practices for largescale projects  and engage in a panel debate on the feasibility of implementing and integrating various data spaces at different levels  all underpinning the advancement of healthcare data sharing and researchThe event plans to highlight different Belgian data spaces and to delve into profound discussions during panel debates  exploring complex topics like challenges and future perspectives in health data sharing and analysis As iHD will comoderate the event  the bridge towards Europe is guaranteedTarget Audience The symposium is tailored for health data enthusiasts from Belgium and neighboring countries While expecting diverse attendees  both beginners and experts  we are aiming for accessibility without compromising depthIn our sessions  we explore a wide array of dimensions  ranging from geographical nuances to various data types and the complexities of transmural care We delve into the intricacies of firstline care  hospital settings  and innovative mobile healthcare mHealth Furthermore  our exploration extends to discovering the transformative potential inherent in different data types  encompassing everything from imaging to omics and harmonized clinical data JoinTheJourney All speakers are asked to touch upon the following questions in preparation of their talkWhat are their “why” and expected clinical impactWhich sociotechnical challenges are they trying to addressWhat are their main achievements so farWhat excites them for the future10001010  Meeting Start WelcomeLiesbet M Peeters UHasselt  Dipak Kalra iHD10101120  Session 1 Borderless Insights  Unveiling Opportunities  Challenges in Scaling RWD across Limburg  Flanders  Belgium and EuropeModerator Liesbet M Peeters UHasselt andor Dipak Kalra iHDKeynote 1 Setting the scene  introducing the European Health Data Space and explaining the overall challenges of scalingup health data and how they are currently handled at the European levelDipak Kalra iHDHealth Campus province of LimburgPiet Stinissen UHasseltDepartment of CareKoenraad Jacob Department of CareBelgian Health Data Agency HDA and Data Governance at SciensanoSofie De Broe HDA  SciensanoPanel Debate session 111201140  Coffee Break11401250  Session 2 Breaking Barries  Navigating RWD Challenges from First Line to Hybrid Digital Care in Transmural Integrated SettingsModerator Liesbet M Peeters UHasselt andor Dipak Kalra iHDKeynote 2 Enhancing Hybrid Digital Healthcare  Principles of Quality Assurance in Digital InnovationsChristophe Maes iHDFirst line General practioners  Intego and Population Health ManagementBert Vaes IntegoUnlocking Belgiums health data reuse potential the RWD4BE iniativeIngrid Maes InovigateRemote monitoring mHealthHans De Clercq BytesfliesPanel Debate Session 212501350  Lunch Break13501500  Session 3 Data Diversity Unveiled  Exploring Omics  Clinical  and Brainrelated Data in RealWorld ScenariosModerator Liesbet M Peeters UHasselt andor Dipak Kalra iHDKeynote 3 Assuring Research Excellence Strategies for Assessing Data Quality in European ProjectsJens Declerck iHDOHDSI BelgiumAnnelies Verbiest UZAEBRAINS BelgiumWim Vanduffel KU LeuvenELIXIR BelgiumFrederik Coppens VIBPanel Debate Session 315001520  Coffee Break15201630  Session 4 Synergizing Tomorrow  Unveiling Strategies to Maximise Collaboration and ProgressArisal of Data Spaces Why I am excited and worriedPanel DebateModerator Liesbet M Peeters UHasselt andor Dipak Kalra iHDPiet Stinissen Health Campus Limburg  Sofie De Broe Health Data Agency  Bert Vaes Intego  Hans De Clercq Byteflies  BeMedTech  Annelies Verbiest OHDSI  Frederik Coppens ELIXIRExample of questions that will be addressedHow can we ensure strategic oversightHow can we maximise synergyHow to cocreate a joint strategyWhats next  aspirations for the futureClosing remarksLiesbet M Peeters UHasselt  Dipak Kalra iHD16301730  Networking Reception   The Research Group in Biomedical Data Sciences of UHasselt and The European Institute for Innovation through Health Data iHD are delighted to invite you to the “Big Data in Health and Care The Arisal of Data Spaces” Symposium The term data space has gained a lot of traction lately specifically with the upcoming European Health Data Space legislation The definition of data space is still under discussion However we and others eg NCP Flanders describe it as a  decentralized infrastructure for trustworthy data sharing and exchange  in data ecosystems based on commonly agreed principles httpswwwuhasseltbeeninstitutenenbiomedenimmunologybiomedicaldatasciencesbigdataforhealthcarethearisalofdataspaces Symposium symposiumHasseltHasselt University</data><data key="start_time">23 Apr 2024 10:00 - 17:30</data><data key="location_detail">Corda Campus - Kempische Steenweg 293/16, 3500 Hasselt</data><data key="price">free</data><data key="sub_title">Symposium</data><data key="full_body">In recent years  a series of health data spaces have arised. However  at the moment  the strategic oversight is lost and many questions remain. For example:Which data space initiatives are out there?How do they relate to each other?How will they work together/in synergy to maximize the impact?How will this affect the local data management at the level of the individual data partners?With this symposium  we aspire to facilitate an in-depth exploration of data spaces and research infrastructures  elucidate challenges and best practices for large-scale projects  and engage in a panel debate on the feasibility of implementing and integrating various data spaces at different levels  all underpinning the advancement of healthcare data sharing and research.The event plans to highlight different Belgian data spaces and to delve into profound discussions during panel debates  exploring complex topics like challenges and future perspectives in health data sharing and analysis. As i~HD will co-moderate the event  the bridge towards Europe is guaranteed.Target Audience: The symposium is tailored for health data enthusiasts from Belgium and neighboring countries. While expecting diverse attendees  both beginners and experts  we are aiming for accessibility without compromising depth.In our sessions  we explore a wide array of dimensions  ranging from geographical nuances to various data types and the complexities of transmural care. We delve into the intricacies of first-line care  hospital settings  and innovative mobile healthcare (mHealth). Furthermore  our exploration extends to discovering the transformative potential inherent in different data types  encompassing everything from imaging to -omics and harmonized clinical data. #JoinTheJourney All speakers are asked to touch upon the following questions in preparation of their talk:What are their “why” and expected clinical impact?Which socio-technical challenges are they trying to address?What are their main achievements so far?What excites them for the future?10:00-10:10 | Meeting Start (Welcome)Liesbet M. Peeters (UHasselt) &amp; Dipak Kalra (i~HD)10:10-11:20 | Session 1: Borderless Insights - Unveiling Opportunities &amp; Challenges in Scaling RWD across Limburg  Flanders  Belgium and EuropeModerator: Liesbet M. Peeters (UHasselt) and/or Dipak Kalra (i~HD)Keynote 1: Setting the scene - introducing the European Health Data Space and explaining the overall challenges of scaling-up health data and how they are currently handled at the European levelDipak Kalra (i~HD)Health Campus (province of Limburg)Piet Stinissen (UHasselt)Department of CareKoenraad Jacob (Department of Care)Belgian Health Data Agency (HDA) and Data Governance at SciensanoSofie De Broe (HDA &amp; Sciensano)Panel Debate session 111:20-11:40 | Coffee Break11:40-12:50 | Session 2: Breaking Barries - Navigating RWD Challenges from First Line to Hybrid Digital Care in Transmural Integrated SettingsModerator: Liesbet M. Peeters (UHasselt) and/or Dipak Kalra (i~HD)Keynote 2: Enhancing Hybrid Digital Healthcare - Principles of Quality Assurance in Digital InnovationsChristophe Maes (i~HD)First line (General practioners)  Intego and Population Health ManagementBert Vaes (Intego)Unlocking Belgium's health data reuse potential: the RWD4BE iniativeIngrid Maes (Inovigate)Remote monitoring (mHealth)Hans De Clercq (Bytesflies)Panel Debate Session 212:50-13:50 | Lunch Break13:50-15:00 | Session 3: Data Diversity Unveiled - Exploring -Omics  Clinical  and Brain-related Data in Real-World ScenariosModerator: Liesbet M. Peeters (UHasselt) and/or Dipak Kalra (i~HD)Keynote 3: Assuring Research Excellence Strategies for Assessing Data Quality in European ProjectsJens Declerck (i~HD)OHDSI BelgiumAnnelies Verbiest (UZA)EBRAINS BelgiumWim Vanduffel (KU Leuven)ELIXIR BelgiumFrederik Coppens (VIB)Panel Debate Session 315:00-15:20 | Coffee Break15:20-16:30 | Session 4: Synergizing Tomorrow - Unveiling Strategies to Maximise Collaboration and ProgressArisal of Data Spaces: Why I am excited and worried?Panel DebateModerator: Liesbet M. Peeters (UHasselt) and/or Dipak Kalra (i~HD)Piet Stinissen (Health Campus Limburg)  Sofie De Broe (Health Data Agency)  Bert Vaes (Intego)  Hans De Clercq (Byteflies)  (BeMedTech)  Annelies Verbiest (OHDSI)  Frederik Coppens (ELIXIR)Example of questions that will be addressed:How can we ensure strategic oversight?How can we maximise synergy?How to co-create a joint strategy?What's next - aspirations for the future?Closing remarksLiesbet M. Peeters (UHasselt) &amp; Dipak Kalra (i~HD)16:30-17:30 | Networking Reception</data><data key="intro">  The Research Group in Biomedical Data Sciences of UHasselt and The European Institute for Innovation through Health Data (i~HD) are delighted to invite you to the “Big Data in Health and Care: The Arisal of Data Spaces” Symposium. The term 'data space' has gained a lot of traction lately, specifically with the upcoming 'European Health Data Space' legislation. The definition of data space is still under discussion. However, we and others (e.g. NCP Flanders) describe it as "a  decentralized infrastructure for trustworthy data sharing and exchange  in data ecosystems, based on commonly agreed principles."</data><data key="language">English</data><data key="course_info">symposium-Hasselt-Hasselt University</data><data key="constraints">Registration is mandatory</data><data key="details">https://www.uhasselt.be/en/instituten-en/biomed-en/immunology/biomedical-data-sciences/big-data-for-health-care-the-arisal-of-data-spaces</data><data key="url">https://www.vaia.be/en/courses/big-data-for-health-care-the-arisal-of-data-spaces</data><data key="title">Big Data for Health &amp;amp; Care: The Arisal of Data Spaces</data><data key="date">23/04/2024</data></node>
<node id="n1594" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: CEOs/General directors of medium to large SMEs</data><data key="data">Defining Responsible AI strategies This course will be given by Johan Loeckx at the FARI Test  Experience Center Cantersteen 16  1000  Brussels on Wednesday 24 April 2024 from 900 to 1600 – lunch not includedThe training will provide CEOs with toolkits that allow them to design a highlevel strategy for AI The toolkits are designed to understand their current position  explore opportunities  the impact on their organisation  and most importantly  draw a roadmap of innovation actionsPurposeIntroduce toolkits to define AI strategies that boost your businessTarget audienceThis executive masterclass is intended for CEOsGeneral directors of medium to large SMEs  who wish to transform their business with AIThis training is for you if…– Your competitors investing in AI– You need to scale– You are confronted with a lack of talent– You wish to create new revenue streams from your data– You encounter too much variability in your processes– You need to develop new capabilities to stay competitive– You lack highquality insights to improve your bottom line  quality  or efficiency– You need better customer experienceAs FARI is supported by the European Resilience  Recovery Fund RRF  we can continue to offer free access to participants from public administrations public institutions and research institutionsWe can also provide a 50 discount to participants from the following target groupsEducational institutionsNGOs and nonprofit organisationsBrussels based companiesBrussels citizens More info  Share this course   httpswwwfaribrusselseducationdefiningresponsibleaistrategies 24 Apr 2024 0900  1600 executive masterclassBrusselsFARI</data><data key="sub_title">24 Apr 2024 09:00 - 16:00</data><data key="start_time">24 Apr 2024 09:00 - 16:00</data><data key="location_detail">Be Central, FARI Experience Centre - Cantersteen 16, 1000 Bruxelles</data><data key="price">€250 excl. VAT</data><data key="intro">More info  Share this course  </data><data key="details">https://www.fari.brussels/education/defining-responsible-ai-strategies</data><data key="full_body">This course will be given by Johan Loeckx at the FARI Test &amp; Experience Center (Cantersteen 16  1000  Brussels) on Wednesday 24 April 2024 from 9:00 to 16:00 – lunch not included.The training will provide CEOs with toolkits that allow them to design a high-level strategy for AI. The toolkits are designed to understand their current position  explore opportunities  the impact on their organisation  and most importantly  draw a roadmap of innovation actions.PurposeIntroduce toolkits to define AI strategies that boost your business.Target audienceThis executive masterclass is intended for CEOs/General directors of medium to large SMEs  who wish to transform their business with AIThis training is for you if…– Your competitors investing in AI;– You need to scale;– You are confronted with a lack of talent;– You wish to create new revenue streams from your data;– You encounter too much variability in your processes;– You need to develop new capabilities to stay competitive;– You lack high-quality insights to improve your bottom line  quality  or efficiency;– You need better customer experience.As FARI is supported by the European Resilience &amp; Recovery Fund (RRF)  we can continue to offer free access to participants from public administrations public institutions and research institutions.We can also provide a 50% discount to participants from the following target groups:Educational institutionsNGOs and non-profit organisationsBrussels based companiesBrussels citizens</data><data key="language">English</data><data key="course_info">executive masterclass-Brussels-FARI</data><data key="title">Defining Responsible AI strategies</data><data key="url">https://www.vaia.be/en/courses/defining-responsible-ai-strategies</data><data key="constraints">NA</data><data key="date">24/04/2024</data></node>
<node id="n1595" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: system administrators, network administrators, SOC analysts, security engineers, and individuals in similar roles</data><data key="data">Efficient use of a network protocol analyzer in cyber threats Many companies rely on a Security Information and Event Management system SIEM to consolidate data from various applications and equipment  allowing them to maintain an overview of their security landscape To investigate alerts generated by their SIEM system more thoroughly  they turn to a Network Protocol Analyzer NPA An NPA captures  analyzes  and visualizes network traffic and can have a broad range of applicationsHowever  the procedures and configurations of NPAs vary according to the specific issue at hand cybersecurity  troubleshooting  etc  which often leads to incorrect or inefficient usage in practice During this workshop  we will build the knowledge and skills necessary to use Network Protocol Analyzers correctly and efficiently  particularly in the context of cybersecurity threats We will work with reallife scenarios and gain practical insights into the underlying workings of NPAs The NPA that we will delve deeper into during the workshop is the opensource NPA  WiresharkThis session is part of the series Cybersecurity excellenceSession 1 Tackling cybersecurity challenges a complex security puzzle  28 February 2024  Vincent Naessens KU LeuvenSession 2 ‘Privacy by design a technical approach to privacy risk  26 March 2024  Kim Wuyts PwCSession 3 Efficient use of a network protocol analyzer in cyber threats workshop  24 April 2024  Tom Cordemans KU LeuvenSession 4 Hacking and protecting embedded devices workshop  29 May 2024  Jorn Lapon KU LeuvenSession 5 EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems  28 August 2024  Vincent Naessens KU LeuvenSession 6 Cyberattack response  25 September 2024  Tom Bauwens Eubelius  Kalman Tiboldi TVHSession 7 Postquantum cryptography  23 October 2024  Eric Michiels IBM In an increasingly technologydriven world cybersecurity stands as the cornerstone of digital resilience In this programme we will explore the full spectrum of cybersecurity from prevention to response while gaining both immediate handson skills and a foresight for the future of cybersecurity This programme brings together academic researchers and industrial experts and thus provides a blend of lectures and use cases and practical testimonials httpspuckuleuvenbenlopleidingefficientuseofanetworkprotocolanalyzerincyberthreatsworkshopxdojwgde0jla3bpz Session 3  Cybersecurity excellence series 2024 workshopGhentKU Leuven Postuniversitair Centrum</data><data key="annotated_by">manual</data><data key="start_time">24 Apr 2024 14:00 - 17:00</data><data key="price">€250</data><data key="sub_title">Session 3 | Cybersecurity excellence series 2024</data><data key="subscription_limit">NA</data><data key="intro">In an increasingly technology-driven world, cybersecurity stands as the cornerstone of digital resilience. In this programme, we will explore the full spectrum of cybersecurity, from prevention to response, while gaining both immediate, hands-on skills and a foresight for the future of cybersecurity. This programme brings together academic researchers and industrial experts, and thus provides a blend of lectures and use cases and practical testimonials.</data><data key="language">English</data><data key="location_detail">KU Leuven, Campus Rabot - Gebroeders de Smetstraat 1, 9000 Ghent</data><data key="date">24/04/2024</data><data key="details">https://puc.kuleuven.be/nl/opleiding/efficient_use_of_a_network_protocol_analyzer_in_cyber_threats_workshop-xdojwgde0jla3bpz</data><data key="full_body">Many companies rely on a Security Information and Event Management system (SIEM) to consolidate data from various applications and equipment  allowing them to maintain an overview of their security landscape. To investigate alerts generated by their SIEM system more thoroughly  they turn to a Network Protocol Analyzer (NPA). An NPA captures  analyzes  and visualizes network traffic and can have a broad range of applications.However  the procedures and configurations of NPAs vary according to the specific issue at hand (cybersecurity  troubleshooting  etc.)  which often leads to incorrect or inefficient usage in practice. During this workshop  we will build the knowledge and skills necessary to use Network Protocol Analyzers correctly and efficiently  particularly in the context of cybersecurity threats. We will work with real-life scenarios and gain practical insights into the underlying workings of NPAs. The NPA that we will delve deeper into during the workshop is the open-source NPA  Wireshark.This session is part of the series Cybersecurity excellence:Session 1: Tackling cybersecurity challenges: a complex security puzzle | 28 February 2024 - Vincent Naessens (KU Leuven)Session 2: ‘Privacy by design': a technical approach to privacy risk | 26 March 2024 - Kim Wuyts (PwC)Session 3: Efficient use of a 'network protocol analyzer' in cyber threats (workshop) | 24 April 2024 - Tom Cordemans (KU Leuven)Session 4: Hacking and protecting embedded devices (workshop) | 29 May 2024 - Jorn Lapon (KU Leuven)Session 5: EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems | 28 August 2024 - Vincent Naessens (KU Leuven)Session 6: Cyberattack response | 25 September 2024 - Tom Bauwens (Eubelius)  Kalman Tiboldi (TVH)Session 7: Post-quantum cryptography | 23 October 2024 - Eric Michiels (IBM)</data><data key="url">https://www.vaia.be/en/courses/efficient-use-of-a-network-protocol-analyzer-in-cyber-threats</data><data key="title">Efficient use of a network protocol analyzer in cyber threats</data><data key="constraints">a solid background in network protocols and network analysis is a prerequisite (CompTIA Network+, CCNA, etc.)</data><data key="course_info">workshop-Ghent-KU Leuven Postuniversitair Centrum</data></node>
<node id="n1596" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="sub_title">The CFO of the future</data><data key="start_time">25 Apr 2024 14:00 - 21:00</data><data key="price">€445</data><data key="target_group">Target audience: CFOs and future CFOs, professionals</data><data key="data">2024 Digital Finance Conference Come along and dive into the topics that will define tomorrow’s CFO  includingThe digital CFO revolutionWinning the war for talentNavigating climate changeMastering the art and science of negotiation with investorsAnd moreJoin us to take part in expert session with leading CFOs  technology experts and worldfamous academicsThe event rounds up with an inspirational keynote talk from Hod Lipson – the groundbreaking Columbia University Professor in Robotics and AI His accolades include being named as Esquire magazine’s Best and Brightest – and his lab has been listed as one of the 25 Most Awesome Labs in the US by Popular Science Forbes has listed him as one of the Top 7 Data Scientists in the World – and his TED talk is the most viewed on AIHis talk will be on The Next AI – a deep dive into what’s driving AI  how to use it and how to anticipate its futureAfterwards  there will be networking over dinner with CFOs  finance leaders and company representatives It promises to be an intimate  relaxed and thoughtprovoking finale to the day Join the fourth edition of our Digital Finance Conference – and join the revolution in financial leadership To succeed today’s finance leaders need so much more than financial  knowhow They need to be able to see the big picture to anticipate and  respond to trends and phenomena like GenAI climate change investor  negotiations and the war for talent Join us to delve into these subjects – and look towards a future where success isn’t simply achieved it’s sustained httpswwwvlerickcomenevents2024digitalfinanceconference The CFO of the future eventGhentVlerick Business School</data><data key="language">English</data><data key="location_detail">Ghent</data><data key="details">https://www.vlerick.com/en/events/2024-digital-finance-conference/</data><data key="full_body">Come along and dive into the topics that will define tomorrow’s CFO  including:The digital CFO revolutionWinning the war for talentNavigating climate changeMastering the art and science of negotiation with investorsAnd more!Join us to take part in expert session with leading CFOs  technology experts and world-famous academics.The event rounds up with an inspirational keynote talk from Hod Lipson – the ground-breaking Columbia University Professor in Robotics and AI. His accolades include being named as Esquire magazine’s Best and Brightest – and his lab has been listed as one of the 25 Most Awesome Labs in the US by Popular Science. Forbes has listed him as one of the Top 7 Data Scientists in the World – and his TED talk is the most viewed on AI.His talk will be on The Next AI – a deep dive into what’s driving AI  how to use it and how to anticipate its future.Afterwards  there will be networking over dinner with CFOs  finance leaders and company representatives. It promises to be an intimate  relaxed and thought-provoking finale to the day.</data><data key="intro">Join the fourth edition of our Digital Finance Conference – and join the revolution in financial leadership. To succeed, today’s finance leaders need so much more than financial  know-how. They need to be able to see the big picture, to anticipate and  respond to trends and phenomena like (Gen)AI, climate change, investor  negotiations and the war for talent. Join us to delve into these subjects – and look towards a future where success isn’t simply achieved, it’s sustained.</data><data key="subscription_limit">NA</data><data key="course_info">event-Ghent-Vlerick Business School</data><data key="title">2024 Digital Finance Conference</data><data key="url">https://www.vaia.be/en/courses/2024-digital-finance-conference</data><data key="constraints">NA</data><data key="date">25/04/2024</data></node>
<node id="n1597" labels=":Course"><data key="labels">:Course</data><data key="data">HumanRobot Collaboration Robots are becoming an integral part of our economy  society and planet  assisting us in various tasks However  the rapid growth in robotics usage has also given rise to relevant concerns regarding sustainability At present  urgent issues such as rareearth material usage  ewaste  energy efficiency and ethical concerns hinders the sustainable development of robotics As we increasingly rely on robotics  it becomes increasingly critical and urgent to ensure that these technological advancements are sustainable and ecofriendly Since robotics is the integration of many technologies  we will discuss in this Forum sustainable robotics technologies ranging from material science  material processing and metal  electronics with sensors and processing  actuators  energy  power and batteries and control and artificial intelligenceProgramme will be announced soonOrganised byGeneral Chair  Prof An Jacobs VUB  Program Chair Dr Shirley Elprama VUBProgram Committee members Prof Bernardo Innocenti ULB  Prof Tamas Heidegger Obuda University  Hungary More info  Share this course   httpswwwbriasbeenbriasforumonhumanrobotcollaboration 26 Apr 2024 0900  1700 seminaronline amp BrusselsBrussels Institute of Advanced Studies BrIAS FARI</data><data key="annotated_by">manual</data><data key="sub_title">26 Apr 2024 09:00 - 17:00</data><data key="start_time">26 Apr 2024 09:00 - 17:00</data><data key="location_detail">Online &amp;amp; Cantersteen 16, 1000 Brussels</data><data key="price">free</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: academics and researchers</data><data key="intro">More info  Share this course  </data><data key="details">https://www.brias.be/en/brias-forum-on-human-robot-collaboration</data><data key="full_body">Robots are becoming an integral part of our economy  society and planet  assisting us in various tasks. However  the rapid growth in robotics usage has also given rise to relevant concerns regarding sustainability. At present  urgent issues such as rare-earth material usage  e-waste  energy efficiency and ethical concerns hinders the sustainable development of robotics. As we increasingly rely on robotics  it becomes increasingly critical and urgent to ensure that these technological advancements are sustainable and eco-friendly. Since robotics is the integration of many technologies  we will discuss in this Forum sustainable robotics technologies ranging from material science  material processing and metal  electronics with sensors and processing  actuators  energy  power and batteries and control and artificial intelligence.Programme will be announced soonOrganised byGeneral Chair : Prof. An Jacobs (VUB)  Program Chair: Dr. Shirley Elprama (VUB)Program Committee members: Prof. Bernardo Innocenti (ULB)  Prof. Tamas Heidegger (Obuda University  Hungary)</data><data key="language">English</data><data key="constraints">NA</data><data key="course_info">seminar-online &amp;amp; Brussels-Brussels Institute of Advanced Studies (BrIAS); FARI</data><data key="date">26/04/2024</data><data key="url">https://www.vaia.be/en/courses/human-robot-collaboration</data><data key="title">Human-Robot Collaboration</data></node>
<node id="n1598" labels=":Course"><data key="labels">:Course</data><data key="data">Supervised machine learning with tensor network kernel machines In this talk Kim Batselier will introduce tensor network kernel machines These models are able to learn nonlinear patterns from data for both regression and classification tasks and are described by an exponential amount of model parameters Livedemos will show that such models can be learned efficiently and at the same time achieve stateofthe art performance on validation data In this talk Kim Batselier will introduce tensor network kernel machines These models are able to learn nonlinear patterns from data for both regression and classification tasks and are described by an exponential amount of model parameters Livedemos will show that such models can be learned efficiently and at the same time achieve stateofthe art performance on validation data httpshomesesatkuleuvenbesistawwwbdmbacktotherootsindexphpseminars Seminar by Kim Batselier Delft University of Technology seminarHeverlee LeuvenKU Leuven ESAT STADIUS</data><data key="annotated_by">manual</data><data key="start_time">30 Apr 2024 17:00 - 18:00</data><data key="price">free</data><data key="sub_title">Seminar by Kim Batselier (Delft University of Technology)</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: researchers and academics with an interest in system theory, algebraic geometry, polynomial optimization, numerical linear algebra, system identification</data><data key="intro">In this talk Kim Batselier will introduce tensor network kernel machines. These models are able to learn nonlinear patterns from data for both regression and classification tasks and are described by an exponential amount of model parameters. Live-demos will show that such models can be learned efficiently and at the same time achieve state-of-the art performance on validation data.</data><data key="language">English</data><data key="location_detail">KU Leuven Thermotechnisch Instituut Aula van de Tweede Hoofdwet (01.02)</data><data key="date">30/04/2024</data><data key="details">https://homes.esat.kuleuven.be/~sistawww/bdm/backtotheroots/index.php/seminars</data><data key="full_body">In this talk Kim Batselier will introduce tensor network kernel machines. These models are able to learn nonlinear patterns from data for both regression and classification tasks and are described by an exponential amount of model parameters. Live-demos will show that such models can be learned efficiently and at the same time achieve state-of-the art performance on validation data.</data><data key="url">https://www.vaia.be/en/courses/supervised-machine-learning-with-tensor-network-kernel-machines</data><data key="title">Supervised machine learning with tensor network kernel machines</data><data key="constraints">NA</data><data key="course_info">seminar-Heverlee; Leuven-KU Leuven ESAT STADIUS</data></node>
<node id="n1599" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="target_group">Target audience: managers</data><data key="sub_title">Using artificial intelligence and big data to support decision-making</data><data key="subscription_limit">NA</data><data key="data">Creating Business Value with AI and Big Data This programme blends essential business  analytics and technology knowledge to empower you to take the lead on AI and big data Over six thoughtprovoking days  you’ll learn approaches that you can implement immediately to create real impact in your organisation With a combination of oncampus and online learning  you’ll discover how to enhance your company’s decision making – and develop a strategy that you can implement immediatelyThis programme is offered in a blended format  covering both oncampus and online sessionsModule 1 Data and Analytics StrategyDevelop a data and analytics strategy that supports your organisation’s digital transformationGet inspired by stateoftheart AI and big data applications that create value in multiple industries Identify datadriven innovations for your organisationManage analytical teams and projectsModule 2 Artificial Intelligence and Big Data AnalyticsDiscover machine learningExplore decision automationUnderstand network analysisLearn about natural Language ProcessingDive into deep reinforcement learning Module 3 Operationalising AI and Big DataDiscover how to collect and integrate multiple data sourcesLearn to integrate big data technology with existing data infrastructureUnderstand how to leverage highvelocity dataDemonstrate the value of data governanceModule 4 Cultivating Artificial Intelligence and Big DataDiscover how to win customer buyin by dealing with ethics and privacy in a big data worldLearn to facilitate collaboration around AI and big data projects with internal stakeholdersExplore how to promote data savviness within your organisation Your personal datadriven project Across a period of two months you’ll work on a personal datadriven idea  innovation  project or strategy You’ll receive valuable feedback from your peers on your project – and you’ll get support from expert Vlerick facultyOncampus day This day will give you an opportunity to share the challenges and successes of your project with your fellow participants – and get their feedback and support Identify new business opportunities associated with advanced data and analytics technologiesDevelop a data and analytics strategy for your organisation Understand the principles of artificial intelligence techniques and recognise the business advantages and limitations of AIDiscover tools to operationalise your data and analytics strategyCommunicate and collaborate better with technical colleagues including data scientists and engineers Champion AI and big data projects Chief Information Officers  Chief Data Officers and Chief Digital OfficersManagers across all functions who want to leverage AI and big dataIT managers who have to adapt their infrastructure to new technologiesProject managers leading AI and big data projectsInnovation managers  tech entrepreneurs  data stewards and analytical translatorsBusiness analysts  data scientists  data engineers and IT specialists who are moving into a managerial role or want to expand their business knowledgeConsultants who want to stay uptodate with the latest data and analytics technologies  Machine learning natural language processing robotics cloud and modern data platforms… They all have immense potential for your organisation But how do you make them work for you Over this six day programme you’ll live and learn AI and big data You’ll gain a strategic overview and you’ll develop a longterm data and analytics strategy that you can put into action straight away You’ll have the confidence to leap into your organisation with a clear vision of how to create value  httpswwwvlerickcomenprogrammesprogrammesindigitaltransformationcreatingbusinessvaluewithaiandbigdata Using artificial intelligence and big data to support decisionmaking courseBrussels amp onlineVlerick Business School</data><data key="start_time">2 May 2024 - 1 Oct 2024</data><data key="price">€ 7,195 (excl. VAT)</data><data key="intro">Machine learning, natural language processing, robotics, cloud and modern data platforms… They all have immense potential for your organisation. But how do you make them work for you? Over this six day programme, you’ll live and learn AI and big data. You’ll gain a strategic overview and you’ll develop a long-term data and analytics strategy that you can put into action straight away. You’ll have the confidence to leap into your organisation with a clear vision of how to create value. </data><data key="language">English</data><data key="location_detail">Brussels Campus (Bolwerklaan 21/bus 32, 1210 Brussels) &amp;amp; Online</data><data key="date">2/05/2024</data><data key="details">https://www.vlerick.com/en/programmes/programmes-in-digital-transformation/creating-business-value-with-ai-and-big-data/</data><data key="full_body">This programme blends essential business  analytics and technology knowledge to empower you to take the lead on AI and big data. Over six thought-provoking days  you’ll learn approaches that you can implement immediately to create real impact in your organisation. With a combination of on-campus and online learning  you’ll discover how to enhance your company’s decision making – and develop a strategy that you can implement immediately.This programme is offered in a blended format  covering both on-campus and online sessions.Module 1: Data and Analytics StrategyDevelop a data and analytics strategy that supports your organisation’s digital transformationGet inspired by state-of-the-art AI and big data applications that create value in multiple industries Identify data-driven innovations for your organisationManage analytical teams and projectsModule 2: Artificial Intelligence and Big Data AnalyticsDiscover machine learningExplore decision automationUnderstand network analysisLearn about natural Language ProcessingDive into deep (reinforcement) learning Module 3: Operationalising AI and Big DataDiscover how to collect and integrate multiple data sourcesLearn to integrate big data technology with existing data infrastructureUnderstand how to leverage high-velocity dataDemonstrate the value of data governanceModule 4: Cultivating Artificial Intelligence and Big DataDiscover how to win customer buy-in by dealing with ethics and privacy in a big data worldLearn to facilitate collaboration around AI and big data projects with internal stakeholdersExplore how to promote data savviness within your organisation Your personal data-driven project Across a period of two months you’ll work on a personal data-driven idea  innovation  project or strategy. You’ll receive valuable feedback from your peers on your project – and you’ll get support from expert Vlerick faculty.On-campus day This day will give you an opportunity to share the challenges and successes of your project with your fellow participants – and get their feedback and support. Identify new business opportunities associated with advanced data and analytics technologiesDevelop a data and analytics strategy for your organisation Understand the principles of artificial intelligence techniques and recognise the business advantages and limitations of AIDiscover tools to operationalise your data and analytics strategyCommunicate and collaborate better with technical colleagues including data scientists and engineers Champion AI and big data projects Chief Information Officers  Chief Data Officers and Chief Digital OfficersManagers across all functions who want to leverage AI and big dataIT managers who have to adapt their infrastructure to new technologiesProject managers leading AI and big data projectsInnovation managers  tech entrepreneurs  data stewards and analytical translatorsBusiness analysts  data scientists  data engineers and IT specialists who are moving into a managerial role or want to expand their business knowledgeConsultants who want to stay up-to-date with the latest data and analytics technologies </data><data key="url">https://www.vaia.be/en/courses/creating-business-value-with-ai-and-big-data-may2024</data><data key="title">Creating Business Value with AI and Big Data</data><data key="constraints">NA</data><data key="course_info">course-Brussels &amp;amp; online-Vlerick Business School</data></node>
<node id="n1600" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: academics and researchers</data><data key="data">War amp Peace and Robotics Programme will be announced soonOrganised byGeneral Chairs  Prof Mauro Birattari ULB  Prof Emanuele Garone ULBProgram Chair Prof Geert De Cubber Royal Military Academy  BelgiumProgram Committee members Alain Vande Wouver Universite’ de Mons  Andres Cotorruelo ULB  Junior Fellow More info  Share this course   httpswwwbriasbeenbriasforumonwarpeaceandrobotics 2 May 2024  3 May 2024 seminaronline amp BrusselsBrussels Institute of Advanced Studies BrIAS FARI</data><data key="sub_title">2 May 2024 - 3 May 2024</data><data key="start_time">2 May 2024 - 3 May 2024</data><data key="language">English</data><data key="location_detail">Online &amp;amp; USquare - Av. de la Couronne 227, 1050 Ixelles</data><data key="price">free</data><data key="intro">More info  Share this course  </data><data key="date">2/05/2024</data><data key="details">https://www.brias.be/en/brias-forum-on-war-peace-and-robotics</data><data key="full_body">Programme will be announced soonOrganised byGeneral Chairs : Prof. Mauro Birattari (ULB)  Prof. Emanuele Garone (ULB)Program Chair: Prof. Geert De Cubber (Royal Military Academy  Belgium)Program Committee members: Alain Vande Wouver (Universite’ de Mons)  Andres Cotorruelo (ULB - Junior Fellow)</data><data key="url">https://www.vaia.be/en/courses/war-peace-and-robotics</data><data key="title">War &amp;amp; Peace, and Robotics</data><data key="constraints">NA</data><data key="course_info">seminar-online &amp;amp; Brussels-Brussels Institute of Advanced Studies (BrIAS); FARI</data></node>
<node id="n1601" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: business professionals</data><data key="data">Digital Strategy amp AI Over six modules  this programme equips you with insights and tools to drive your organisations digital and AI strategyModule 1 Introduction to Digital Strategy and AIDive into what digital and AI strategy is and why its importantExplore digitalisation and how it can transform your organisationModule 2 The Context of Digital and AI StrategyUnderstand the building blocks of an effective digital and AI strategyExplore how digitalisation can impact an organisation’s strategy  performance and boost its sustainabilityModule 3 Digitalisation of WorkDiscover how AI and digitalisation impact how work gets doneLearn how to digitalise workModule 4 Digitalisation of Products  Processes and Business ModelsIdentify digital and AI innovation initiatives that enhance performance and support strategic prioritiesLearn how to leverage digital technology to transform processes  products and services  and business modelsModule 5 Organising for Digital  AI Strategy  and InitiativesDiscover how to strategise and organise for implementing digital strategy and AI strategy effectivelyModule 6 Evaluating Digital Strategy  AI and InitiativesDiscover how to sustain a competitive advantage through the use of digital technologies and artificial intelligenceExplore ways to measure and evaluate the success of digital and AI initiativesGain a thorough understanding of how digital and generative AI are different and drive performanceIdentify digital and AI initiatives to transform work  products  processes  and business modelsDevelop a thorough knowledge of how to prioritise digital and AI strategy initiatives and evaluate success Executives  general managers or managing directors who want to build or stress test a digital and AI strategyStrategy  Innovation  Marketing  IT or other directors or managers involved in driving digital and AI strategyProfessionals and entrepreneurs who are fascinated by the impact of digital and AI technology on business and want to capitalise on digital disruptionProfessionals who are keen to stay ahead in today’s digital global marketplace Digital strategy and AI is changing how we do business – and no industry is immune to its power This programme for business professionals gives you a comprehensive understanding of how digital technology and artificial intelligence can support your strategic priorities You’ll live and learn the key principles of digital strategy and AI – and gain the knowledge skills and insights to take the leap into giving your organisation a sustainable competitive edge httpswwwvlerickcomenprogrammesprogrammesindigitaltransformationdigitalstrategyandai Discover how to outperform your competition courseGhentVlerick Business School</data><data key="start_time">13 May 2024 - 15 May 2024</data><data key="price">€3,995 (excl. VAT)</data><data key="location_detail">Reep 1, 9000 Ghent</data><data key="sub_title">Discover how to outperform your competition</data><data key="full_body">Over six modules  this programme equips you with insights and tools to drive your organisation's digital and AI strategy.Module 1: Introduction to Digital Strategy and AIDive into what digital and AI strategy is and why it's importantExplore digitalisation and how it can transform your organisationModule 2: The Context of Digital and AI StrategyUnderstand the building blocks of an effective digital and AI strategyExplore how digitalisation can impact an organisation’s strategy  performance and boost its sustainabilityModule 3: Digitalisation of WorkDiscover how AI and digitalisation impact how work gets doneLearn how to digitalise workModule 4: Digitalisation of Products  Processes and Business ModelsIdentify digital and AI innovation initiatives that enhance performance and support strategic prioritiesLearn how to leverage digital technology to transform processes  products and services  and business modelsModule 5: Organising for Digital &amp; AI Strategy  and InitiativesDiscover how to strategise and organise for implementing digital strategy and AI strategy effectivelyModule 6: Evaluating Digital Strategy &amp; AI and InitiativesDiscover how to sustain a competitive advantage through the use of digital technologies and artificial intelligenceExplore ways to measure and evaluate the success of digital and AI initiativesGain a thorough understanding of how digital and (generative) AI are different and drive performanceIdentify digital and AI initiatives to transform work  products  processes  and business modelsDevelop a thorough knowledge of how to prioritise digital and AI strategy initiatives and evaluate success Executives  general managers or managing directors who want to build or stress test a digital and AI strategyStrategy  Innovation  Marketing  IT or other directors or managers involved in driving digital and AI strategyProfessionals and entrepreneurs who are fascinated by the impact of digital and AI technology on business and want to capitalise on digital disruptionProfessionals who are keen to stay ahead in today’s digital global marketplace</data><data key="intro">Digital strategy and AI is changing how we do business – and no industry is immune to its power. This programme for business professionals, gives you a comprehensive understanding of how digital technology and artificial intelligence can support your strategic priorities. You’ll live and learn the key principles of digital strategy and AI – and gain the knowledge, skills and insights to take the leap into giving your organisation a sustainable, competitive edge.</data><data key="language">English</data><data key="course_info">course-Ghent-Vlerick Business School</data><data key="constraints">3 years of working experience</data><data key="title">Digital Strategy &amp;amp; AI</data><data key="url">https://www.vaia.be/en/courses/digital-strategy-ai</data><data key="date">13/05/2024</data><data key="details">https://www.vlerick.com/en/programmes/programmes-in-digital-transformation/digital-strategy-and-ai/</data></node>
<node id="n1602" labels=":Course"><data key="labels">:Course</data><data key="data">Khovanskii Bases for Semimixed Systems of Polynomial Equations In this talk  I will present an efficient approach for counting roots of polynomial systems  where each polynomial is a general linear combination of fixed  prescribed polynomials Our tools primarily rely on the theory of Khovanskii bases  combined with toric geometryI will demonstrate the application of this approach to the problem of counting the number of approximate stationary states for coupled Duffing oscillators We have derived a Khovanskii basis for the corresponding polynomial system and determined the number of its complex solutions for an arbitrary degree of nonlinearity in the Duffing equation and an arbitrary number of oscillators This is the joint work with Viktoriia Borovik  Mateusz Michalek  Javier del Pino  and Oded Zilberberg In this talk I will present an efficient approach for counting roots of polynomial systems where each polynomial is a general linear combination of fixed prescribed polynomials Our tools primarily rely on the theory of Khovanskii bases combined with toric geometryI will demonstrate the application of this approach to the problem of counting the number of approximate stationary states for coupled Duffing oscillators We have derived a Khovanskii basis for the corresponding polynomial system and determined the number of its complex solutions for an arbitrary degree of nonlinearity in the Duffing equation and an arbitrary number of oscillators This is the joint work with Viktoriia Borovik Mateusz Michalek Javier del Pino and Oded Zilberberg httpshomesesatkuleuvenbesistawwwbdmbacktotherootsindexphpseminars Seminar by Paul Breiding University of Osnabrück seminarHeverlee LeuvenKU Leuven ESAT STADIUS</data><data key="annotated_by">manual</data><data key="start_time">14 May 2024 17:00 - 18:00</data><data key="price">free</data><data key="sub_title">Seminar by Paul Breiding, University of Osnabrück</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: researchers and academics with an interest in system theory, algebraic geometry, polynomial optimization, numerical linear algebra, system identification</data><data key="intro">In this talk, I will present an efficient approach for counting roots of polynomial systems, where each polynomial is a general linear combination of fixed, prescribed polynomials. Our tools primarily rely on the theory of Khovanskii bases, combined with toric geometry.I will demonstrate the application of this approach to the problem of counting the number of approximate stationary states for coupled Duffing oscillators. We have derived a Khovanskii basis for the corresponding polynomial system and determined the number of its complex solutions for an arbitrary degree of nonlinearity in the Duffing equation and an arbitrary number of oscillators. This is the joint work with Viktoriia Borovik, Mateusz Michalek, Javier del Pino, and Oded Zilberberg.</data><data key="language">English</data><data key="location_detail">KU Leuven, Department of Electrical Engineering (ESAT), Aula R (ELEC 00.54)</data><data key="date">14/05/2024</data><data key="details">https://homes.esat.kuleuven.be/~sistawww/bdm/backtotheroots/index.php/seminars</data><data key="full_body">In this talk  I will present an efficient approach for counting roots of polynomial systems  where each polynomial is a general linear combination of fixed  prescribed polynomials. Our tools primarily rely on the theory of Khovanskii bases  combined with toric geometry.I will demonstrate the application of this approach to the problem of counting the number of approximate stationary states for coupled Duffing oscillators. We have derived a Khovanskii basis for the corresponding polynomial system and determined the number of its complex solutions for an arbitrary degree of nonlinearity in the Duffing equation and an arbitrary number of oscillators. This is the joint work with Viktoriia Borovik  Mateusz Michalek  Javier del Pino  and Oded Zilberberg.</data><data key="url">https://www.vaia.be/en/courses/khovanskii-bases-for-semimixed-systems-of-polynomial-equations</data><data key="title">Khovanskii Bases for Semimixed Systems of Polynomial Equations</data><data key="constraints">NA</data><data key="course_info">seminar-Heverlee; Leuven-KU Leuven ESAT STADIUS</data></node>
<node id="n1603" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="target_group">Target audience: anyone interested</data><data key="start_time">16 May 2024 14:00 - 23:30</data><data key="price">€290</data><data key="subscription_limit">NA</data><data key="data">Spring Fest GenAI for Business Welcome to Spring Fest GenAI for Business  a celebration of the dynamic growth and achievements of our Postgraduate Studies in Big Data and Analytics in Business and Management  which was initiated in the academic year 20172018 Since then  our program has fostered a vibrant alumni network  and now its time to reunite and expand our connections within the thriving Big Data and Analytics communityAt this Spring Fest  we invite you to join us for an insightful afternoon program Our distinguished keynotes will provide a broader perspective on the application of GenAI in the world of business Theyll delve into the innovative possibilities and emerging trends that GenAI offers  equipping you with valuable insights to navigate this transformative landscapeFollowing the keynotes  well transition into a captivating networking event  where youll have the opportunity to reunite with former participants  forge new connections  and exchange knowledge and experiences with professionals from a diverse range of companies and organizations This event is a unique opportunity to engage  learn  and grow within the realm of Big Data and AnalyticsAfternoon Program GenAI for Business1400 Welcome Reception1430 Opening1440 GenAI Introduction and Academic PerspectiveProf dr Jochen De Weerdt  prof dr Seppe vanden Broucke  KU Leuven1530 Sustainable Value Creation with LLMs beyond Early AdoptionTim Leers  RD and machine Learning Engineer Dataroots1630 Coffee Break1700 Trends in GenAI and the Hugging Face EcosystemNiels Rogge  Hugging Face  ML61800 Practical Use Cases in BusinessTBDEvening Program1900Welcome with aperitif1945 Start dinnerOpening by Prof dr Jochen De Weerdt  KU LeuvenCoffee  dessert buffet2330 Closing At this Spring Fest we invite you to join us for an insightful afternoon program Our distinguished keynotes will provide a broader perspective on the application of GenAI in the world of business Theyll delve into the innovative possibilities and emerging trends that GenAI offers equipping you with valuable insights to navigate this transformative landscape  Following the keynotes well transition into a captivating networking event where youll have the opportunity to reunite with forme httpspuckuleuvenbenlopleidingykzo8gyer6lr3j5v GenAI BigData DataAnalytics seminarLeuvenKU Leuven Postuniversitair Centrum</data><data key="language">English</data><data key="location_detail">Faculty Club - Groot Begijnhof 14, 3000 Leuven</data><data key="sub_title">#GenAI #BigData #DataAnalytics</data><data key="date">16/05/2024</data><data key="full_body">Welcome to 'Spring Fest: GenAI for Business' - a celebration of the dynamic growth and achievements of our Postgraduate Studies in Big Data and Analytics in Business and Management  which was initiated in the academic year 2017-2018. Since then  our program has fostered a vibrant alumni network  and now it's time to reunite and expand our connections within the thriving Big Data and Analytics community.At this Spring Fest  we invite you to join us for an insightful afternoon program. Our distinguished keynotes will provide a broader perspective on the application of GenAI in the world of business. They'll delve into the innovative possibilities and emerging trends that GenAI offers  equipping you with valuable insights to navigate this transformative landscape.Following the keynotes  we'll transition into a captivating networking event  where you'll have the opportunity to reunite with former participants  forge new connections  and exchange knowledge and experiences with professionals from a diverse range of companies and organizations. This event is a unique opportunity to engage  learn  and grow within the realm of Big Data and Analytics.Afternoon Program: GenAI for Business14.00 Welcome Reception14.30 Opening14.40 GenAI: Introduction and Academic PerspectiveProf. dr. Jochen De Weerdt &amp; prof. dr. Seppe vanden Broucke  KU Leuven15.30 Sustainable Value Creation with LLMs beyond Early AdoptionTim Leers  R&amp;D and machine Learning Engineer Dataroots16.30 Coffee Break17.00 Trends in GenAI and the Hugging Face EcosystemNiels Rogge  Hugging Face &amp; ML618.00 Practical Use Cases in BusinessTBDEvening Program19.00Welcome with aperitif19.45 Start dinnerOpening by Prof. dr. Jochen De Weerdt  KU LeuvenCoffee &amp; dessert buffet23.30 Closing</data><data key="intro">At this Spring Fest, we invite you to join us for an insightful afternoon program. Our distinguished keynotes will provide a broader perspective on the application of GenAI in the world of business. They'll delve into the innovative possibilities and emerging trends that GenAI offers, equipping you with valuable insights to navigate this transformative landscape.  Following the keynotes, we'll transition into a captivating networking event, where you'll have the opportunity to reunite with forme</data><data key="title">Spring Fest: GenAI for Business</data><data key="url">https://www.vaia.be/en/courses/spring-fest-genai-for-business</data><data key="constraints">NVT</data><data key="course_info">seminar-Leuven-KU Leuven Postuniversitair Centrum</data><data key="details">https://puc.kuleuven.be/nl/opleiding/ykzo8gyer6lr3j5v</data></node>
<node id="n1604" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="sub_title">Structural equation modeling</data><data key="start_time">20 May 2024 - 21 May 2024</data><data key="price">€340 - €750</data><data key="data">Structural equation modeling with lavaan Introduction to the theory and application of structural equation modeling and handling missing data  nonnormal data  categorical data  etc                            Structural equation modeling SEM is a general statistical modeling technique to study the relationships among observed and latent variables It spans a wide range of multivariate methods including path analysis  mediation analysis  confirmatory factor analysis  growth curve modeling  and many more Many applications of SEM can be found in the social  economic  behavioral and health sciences  but the technology is increasingly used in disciplines like biology  neuroscience and operation research SEM is often used to test theories or hypotheses that can be represented by a path diagram In a path diagram  observed variables are depicted by boxes  while latent variables hypothetical constructs measured by multiple indicators are depicted by circles Hypothesized possibly causal effects among these variables are represented by singleheaded arrows If you had ever found yourself drawing a path diagram in order to get a better overview of the complex interrelations among some key variables in your data  this course is for youThe first day of the course provides an introduction to the theory and application of structural equation modeling  and illustrates how to use the opensource R package lavaan’ see httpslavaanorg to conduct an SEM analysis On the second day  we discuss several special topics that are often needed by applied users handling missing data  nonnormal data  categorical data  longitudinal data  multilevel data  etc With the exception of a short practical session at the end of the first day  the two days are mostly lectures  to maximize the amount of information that we can teach However  doityourself practicals with written feedback and solutions will be made available and illustrate all the topics that are covered in this courseTarget audienceThis course targets everyone with an interest in testing theories or models that involve relationships between both observed and latent variables The audience for this course can include both novices with little or no previous experience with SEM  as well as existing users who wish to refresh or update their theoretical and practical understanding of structural equation modelingFeesThe participation fee is 750 EUR for participants from the private sector Reduced prices apply to students and staff from nonprofit  social profit  and government organizations An exam fee of 35 EUR will be appliedIndustry  private sector  profession € 750Non profit  government  higher education staff € 565Doctoral students  unemployed € 340If two or more employees from the same company enrol simultaneously for this course a reduction of 20 on the course fee is taken into account starting from the second enrolmentRegistrationMore information and registration on our BetaAcademy website Introduction to the theory and application of structural equation modeling and handling missing data nonnormal data categorical data etc httpsbetaacademyugentbeenprogramshortandlongrunninginitiatives202320242024m13emmodule13structuralequation Structural equation modeling courseGhentUGent</data><data key="language">English</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: This course targets everyone with an interest in testing theories or models that involve relationships between both observed and latent variables. The audience for this course can include both novices with little or no previous experience with SEM, as well as existing users who wish to refresh or update their theoretical and practical understanding of structural equation modeling.</data><data key="date">20/05/2024</data><data key="intro">Introduction to the theory and application of structural equation modeling and handling missing data, nonnormal data, categorical data, etc.</data><data key="location_detail">Henri Dunantlaan 2, 9000, Gent</data><data key="course_info">course-Ghent-UGent</data><data key="full_body">Introduction to the theory and application of structural equation modeling and handling missing data  nonnormal data  categorical data  etc.                            Structural equation modeling (SEM) is a general statistical modeling technique to study the relationships among observed and latent variables. It spans a wide range of multivariate methods including path analysis  mediation analysis  confirmatory factor analysis  growth curve modeling  and many more. Many applications of SEM can be found in the social  economic  behavioral and health sciences  but the technology is increasingly used in disciplines like biology  neuroscience and operation research. SEM is often used to test theories or hypotheses that can be represented by a path diagram. In a path diagram  observed variables are depicted by boxes  while latent variables (hypothetical constructs measured by multiple indicators) are depicted by circles. Hypothesized (possibly causal) effects among these variables are represented by single-headed arrows. If you had ever found yourself drawing a path diagram in order to get a better overview of the complex interrelations among some key variables in your data  this course is for you.The first day of the course provides an introduction to the theory and application of structural equation modeling  and illustrates how to use the open-source R package `lavaan’ (see https://lavaan.org) to conduct an SEM analysis. On the second day  we discuss several special topics that are often needed by applied users (handling missing data  nonnormal data  categorical data  longitudinal data  multilevel data  etc.). With the exception of a short practical session at the end of the first day  the two days are mostly lectures  to maximize the amount of information that we can teach. However  do-it-yourself practicals (with written feedback and solutions) will be made available and illustrate all the topics that are covered in this course.Target audienceThis course targets everyone with an interest in testing theories or models that involve relationships between both observed and latent variables. The audience for this course can include both novices with little or no previous experience with SEM  as well as existing users who wish to refresh or update their theoretical and practical understanding of structural equation modeling.FeesThe participation fee is 750 EUR for participants from the private sector. Reduced prices apply to students and staff from non-profit  social profit  and government organizations. An exam fee of 35 EUR will be applied.Industry  private sector  profession*: € 750Non profit  government  higher education staff: € 565(Doctoral) students  unemployed: € 340*If two or more employees from the same company enrol simultaneously for this course a reduction of 20% on the course fee is taken into account starting from the second enrolment.RegistrationMore information and registration on our Beta-Academy website.</data><data key="url">https://www.vaia.be/en/courses/structural-equation-modeling-with-lavaan</data><data key="title">Structural equation modeling with lavaan</data><data key="constraints">Participants should have a solid understanding of regression analysis and basic statistics (hypothesis testing, p-values, etc.). Some knowledge of (exploratory) factor analysis (or PCA) is recommended, but not required. Because lavaan is an R package, some experience with R (reading in a dataset, fitting a regression model) is recommended, but not required.</data><data key="details">https://beta-academy.ugent.be/en/program/short-and-long-running-initiatives/2023-2024-2024m13em-module-13-structural-equation</data></node>
<node id="n1605" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: technical profiles in IT or (business) engineering and people working in R&amp;amp;D, or business profiles in charge of streamlining business processes.</data><data key="data">A NoCode Approach to Decision Support and Knowledge Capturing In their daytoday operations  companies typically make many small but important decisions  such as which employee should handle an incoming requestwhich service should be recommended to a specific customerhow should a specific product be configured to meet a customers requirement whether a specific customer is entitled to a discount  etc Decision support systems can help employees make these decisions faster  more reliably and more consistently  reducing costs and increasing confidence in the process To develop such systems  blackbox AI methods such as deep learning are not wellsuited  because they lack the required transparency and explainability A more promising approach are knowledgebased AI systems  which derive their intelligence not from data  but from logical models of rules  regulations and expert knowledgeIn this workshop  you will get to know the stateoftheart IDP knowledgebase system  developed at KU Leuven It is currently being used by companies in the manufacturing httpswwwyoutubecomwatchvkpPe0GRjifE  financial httpsyoutubeZzsy4MtdWBE and legal sectors to provide various decision support  both to their customers and inhouse This system is based on methods and techniques that were developed for declarative programming systems such as Prolog  but it offers a number of important innovations 1 instead of focusing only on querying  the IDP system provides a wide range of different functionalities that can be used to implement rich interactive interfaces that support users in different workflows  2 instead of relying on programmers  the system allows its end users to maintain the underlying knowledge base  which means that we can cut out the middle man of IT staff and allow the business experts themselves to update the system  thereby not only reducing cost and turnaround time but also putting the ownership of the decision knowledge in the right handsThe workshop starts with a brief introduction to Knowledgebased AI and presents existing applications in the financial and manufacturing sectors We then proceed with a handson tutorial  using a freely available web interface that participants access from their own laptops In this workshop you will get to know the stateoftheart IDP  knowledgebase system that has been developed at KU Leuven and is  currently used by companies in various sectors You will be able to build basic applications using IDP and you will be able to identify potential use cases for this technology httpspuckuleuvenbenlopleiding6kjpeqerb3gbx7zo Workshop workshopBruggeKU Leuven Postuniversitair Centrum VAIA</data><data key="start_time">24 May 2024 09:00 - 12:30</data><data key="price">€250</data><data key="sub_title">Workshop</data><data key="intro">In this workshop, you will get to know the state-of-the-art IDP  knowledge-base system, that has been developed at KU Leuven and is  currently used by companies in various sectors. You will be able to build basic applications using IDP, and you will be able to identify potential use cases for this technology.</data><data key="language">English</data><data key="location_detail">KU Leuven - Brugge | Spoorwegstraat 12, 8200 Brugge</data><data key="date">24/05/2024</data><data key="details">https://puc.kuleuven.be/nl/opleiding/6kjpeqerb3gbx7zo</data><data key="full_body">In their day-to-day operations  companies typically make many small but important decisions  such as: which employee should handle an incoming requestwhich service should be recommended to a specific customerhow should a specific product be configured to meet a customer's requirement whether a specific customer is entitled to a discount  etc. Decision support systems can help employees make these decisions faster  more reliably and more consistently  reducing costs and increasing confidence in the process. To develop such systems  black-box AI methods (such as deep learning) are not well-suited  because they lack the required transparency and explainability. A more promising approach are knowledge-based AI systems  which derive their 'intelligence' not from data  but from logical models of rules  regulations and expert knowledge.In this workshop  you will get to know the state-of-the-art IDP knowledge-base system  developed at KU Leuven. It is currently being used by companies in the manufacturing (https://www.youtube.com/watch?v=kpPe0GRjifE)  financial (https://youtu.be/Zzsy4MtdWBE) and legal sectors to provide various decision support  both to their customers and in-house. This system is based on methods and techniques that were developed for declarative programming systems such as Prolog  but it offers a number of important innovations: (1) instead of focusing only on querying  the IDP system provides a wide range of different functionalities that can be used to implement rich interactive interfaces that support users in different work-flows  (2) instead of relying on programmers  the system allows its end users to maintain the underlying knowledge base  which means that we can cut out the "middle man" of IT staff and allow the business experts themselves to update the system  thereby not only reducing cost and turn-around time but also putting the ownership of the decision knowledge in the right hands.The workshop starts with a brief introduction to Knowledge-based AI and presents existing applications in the financial and manufacturing sectors. We then proceed with a hands-on tutorial  using a freely available web interface that participants access from their own laptops.</data><data key="url">https://www.vaia.be/en/courses/a-no-code-approach-to-decision-support-and-knowledge-capturing</data><data key="title">A No-Code Approach to Decision Support and Knowledge Capturing</data><data key="constraints">NA</data><data key="course_info">workshop-Brugge-KU Leuven Postuniversitair Centrum; VAIA</data></node>
<node id="n1606" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: researchers and academics with an interest in system theory, algebraic geometry, polynomial optimization, numerical linear algebra, system identification</data><data key="data">Chebyshev varieties Chebyshev varieties are algebraic varieties parametrized by Chebyshev polynomials They arise naturally when solving polynomial equations expressed in the Chebyshev basis More precisely  when passing from monomials to Chebyshev polynomials  Chebyshev varieties replace toric varieties I will introduce these objects  discuss their defining equations and present key properties Via examples  I will motivate their use in practical computations This is joint work with Zaïneb BelAfia and Chiara Meroni Chebyshev varieties are algebraic varieties parametrized by Chebyshev polynomials They arise naturally when solving polynomial equations expressed in the Chebyshev basis More precisely when passing from monomials to Chebyshev polynomials Chebyshev varieties replace toric varieties I will introduce these objects discuss their defining equations and present key properties Via examples I will motivate their use in practical computations This is joint work with Zaïneb BelAfia and Chiara Meroni httpshomesesatkuleuvenbesistawwwbdmbacktotherootsindexphpseminars Seminar by Simon Telen Max Planck Institute seminarHeverlee LeuvenKU Leuven ESAT STADIUS</data><data key="start_time">28 May 2024 17:00 - 18:00</data><data key="language">English</data><data key="price">free</data><data key="sub_title">Seminar by Simon Telen, Max Planck Institute</data><data key="details">https://homes.esat.kuleuven.be/~sistawww/bdm/backtotheroots/index.php/seminars</data><data key="full_body">Chebyshev varieties are algebraic varieties parametrized by Chebyshev polynomials. They arise naturally when solving polynomial equations expressed in the Chebyshev basis. More precisely  when passing from monomials to Chebyshev polynomials  Chebyshev varieties replace toric varieties. I will introduce these objects  discuss their defining equations and present key properties. Via examples  I will motivate their use in practical computations. This is joint work with Zaïneb Bel-Afia and Chiara Meroni.</data><data key="intro">Chebyshev varieties are algebraic varieties parametrized by Chebyshev polynomials. They arise naturally when solving polynomial equations expressed in the Chebyshev basis. More precisely, when passing from monomials to Chebyshev polynomials, Chebyshev varieties replace toric varieties. I will introduce these objects, discuss their defining equations and present key properties. Via examples, I will motivate their use in practical computations. This is joint work with Zaïneb Bel-Afia and Chiara Meroni.</data><data key="location_detail">KU Leuven, Department of Electrical Engineering (ESAT), Aula C (ELEC B91.300)</data><data key="constraints">NA</data><data key="course_info">seminar-Heverlee; Leuven-KU Leuven ESAT STADIUS</data><data key="date">28/05/2024</data><data key="url">https://www.vaia.be/en/courses/chebyshev-varieties</data><data key="title">Chebyshev varieties</data></node>
<node id="n1607" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: developers of embedded systems and IoT</data><data key="data">Hacking and protecting embedded devices The Internet of Things IoT presents significant opportunities for businesses  but it also introduces unique and new security challenges ‘Smart’ devices connected to the internet serve as potential entry points for cybercriminals and can render your company exceptionally vulnerable Enhanced cybersecurity for embedded devices and systems is therefore essential In this workshop  you will gain practical insights into security issues related to embedded systems We will delve into seven typical IoT hacks  providing you with handson knowledge of common vulnerabilities in embedded devices  contemporary attacks  and security technologies Additionally  you will become familiar with security guidelines OWASP for designing  developing  and maintaining new embedded systems By the end of the workshop  you will have the expertise to detect common vulnerabilities and enhance the security of embedded devicesThis session is part of the series Cybersecurity excellenceSession 1 Tackling cybersecurity challenges a complex security puzzle  28 February 2024  Vincent Naessens KU LeuvenSession 2 ‘Privacy by design a technical approach to privacy risk  26 March 2024  Kim Wuyts PwCSession 3 Efficient use of a network protocol analyzer in cyber threats workshop  24 April 2024  Tom Cordemans KU LeuvenSession 4 Hacking and protecting embedded devices workshop  29 May 2024  Jorn Lapon KU LeuvenSession 5 EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems  28 August 2024  Vincent Naessens KU LeuvenSession 6 Cyberattack response  25 September 2024  Tom Bauwens Eubelius  Kalman Tiboldi TVHSession 7 Postquantum cryptography  23 October 2024  Eric Michiels IBM In an increasingly technologydriven world cybersecurity stands as the cornerstone of digital resilience In this programme we will explore the full spectrum of cybersecurity from prevention to response while gaining both immediate handson skills and a foresight for the future of cybersecurity This programme brings together academic researchers and industrial experts and thus provides a blend of lectures and use cases and practical testimonials httpspuckuleuvenbenlopleidinghackingandprotectingembeddeddevicesworkshopj5no0lj0prgwmp9b Session 4  Cybersecurity excellence series 2024 workshopGhentKU Leuven Postuniversitair Centrum</data><data key="annotated_by">manual</data><data key="start_time">29 May 2024 14:00 - 17:00</data><data key="price">€250</data><data key="sub_title">Session 4 | Cybersecurity excellence series 2024</data><data key="subscription_limit">NA</data><data key="intro">In an increasingly technology-driven world, cybersecurity stands as the cornerstone of digital resilience. In this programme, we will explore the full spectrum of cybersecurity, from prevention to response, while gaining both immediate, hands-on skills and a foresight for the future of cybersecurity. This programme brings together academic researchers and industrial experts, and thus provides a blend of lectures and use cases and practical testimonials.</data><data key="language">English</data><data key="location_detail">KU Leuven, Campus Rabot - Gebroeders de Smetstraat 1, 9000 Ghent</data><data key="date">29/05/2024</data><data key="details">https://puc.kuleuven.be/nl/opleiding/hacking_and_protecting_embedded_devices_workshop-j5no0lj0prgwmp9b</data><data key="full_body">The Internet of Things (IoT) presents significant opportunities for businesses  but it also introduces unique and new security challenges. ‘Smart’ devices connected to the internet serve as potential entry points for cybercriminals and can render your company exceptionally vulnerable. Enhanced cybersecurity for embedded devices and systems is therefore essential. In this workshop  you will gain practical insights into security issues related to embedded systems. We will delve into seven typical IoT hacks  providing you with hands-on knowledge of common vulnerabilities in embedded devices  contemporary attacks  and security technologies. Additionally  you will become familiar with security guidelines (OWASP) for designing  developing  and maintaining new embedded systems. By the end of the workshop  you will have the expertise to detect common vulnerabilities and enhance the security of embedded devices.This session is part of the series Cybersecurity excellence:Session 1: Tackling cybersecurity challenges: a complex security puzzle | 28 February 2024 - Vincent Naessens (KU Leuven)Session 2: ‘Privacy by design': a technical approach to privacy risk | 26 March 2024 - Kim Wuyts (PwC)Session 3: Efficient use of a 'network protocol analyzer' in cyber threats (workshop) | 24 April 2024 - Tom Cordemans (KU Leuven)Session 4: Hacking and protecting embedded devices (workshop) | 29 May 2024 - Jorn Lapon (KU Leuven)Session 5: EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems | 28 August 2024 - Vincent Naessens (KU Leuven)Session 6: Cyberattack response | 25 September 2024 - Tom Bauwens (Eubelius)  Kalman Tiboldi (TVH)Session 7: Post-quantum cryptography | 23 October 2024 - Eric Michiels (IBM)</data><data key="url">https://www.vaia.be/en/courses/hacking-and-protecting-embedded-devices</data><data key="title">Hacking and protecting embedded devices</data><data key="constraints">NA</data><data key="course_info">workshop-Ghent-KU Leuven Postuniversitair Centrum</data></node>
<node id="n1608" labels=":Course"><data key="labels">:Course</data><data key="data">Digital Marketing In a careful blend of two online and three oncampus modules  you’ll develop the knowledge and skills to make informed decisions and create strategies that are resilient and adaptable in a constantly changing digital environmentModule 1 Introduction to digital marketing onlineBe inspired by new trends and technologies – and understand their impact on customer behaviourDiscover how to create a digital marketing strategy which tells a consistent story across all channelsModule 2 Acquiring customers via digital channelsExplore the benefits of outbound digital marketing and the value of digital ad platformsUnderstand the power of content marketing – and the role that generative AI can playDiscover how to improve website user experience through AB testingModule 3 Customer conversion and retentionDive into CRMs and customer data platforms – and how they can power segmentation and personalisationDiscover how to automate personalised communicationsUse big data and AI to boost customer loyaltyExplore how to enhance brand advocacy through word of mouth and influencer marketingModule 4 Online simulation onlineGain handson experience of handling common digital marketing challengesDevelop your decisionmaking skillsModule 5 Reviewing your digital marketing strategyDiscover how to measure effectivenessDevelop a clear view of the competitive advantage of omnichannel marketingEmbrace marketing technologies and discover how they can power lead generation  increase conversion  and enhance loyalty and advocacyDevelop an integrated  competitive  differentiating digital marketing strategy that’s aligned to your contextLeverage AI and generative AI to drive efficiencyLearn how to develop greater customer insight and articulate a single customer view to improve personalisation of marketing communicationsDiscover how to balance marketing budget across multiple channelsApply everything you’ve learned in a handson  riskfree learning experienceMarketers from B2B and B2C organisations who want to develop a strategic understanding of digital marketing – and support their teams to implement itAdvertising managers  brand managers  CRM specialists  channel marketers and ecommerce specialists who need to create a digital marketing strategyAgency owners  content marketers  marcomms specialistsOwners of startups and scaleups who want to increase their customer reach through digital channels If  your digital marketing efforts are focused on channels it may be time  to reboot your strategy This programme sets out a holistic powerful  and integrated approach to digital marketing that can transform your  results You’ll learn how to develop a robust strategy allocate budgets  improve your customer experience keep up with latest trends and grow  your business All of this is underpinned with a clear understanding of  AI big data and other technologies httpswwwvlerickcomenprogrammesprogrammesinmarketingsalesdigitalmarketing Learn to create futureproof technologyenabled digital marketing strategies courseonline amp GhentVlerick Business School</data><data key="annotated_by">manual</data><data key="start_time">6 Jun 2024 - 18 Jun 2024</data><data key="price">€4,395 (excl. btw)</data><data key="location_detail">Online &amp;amp; Campus Gent</data><data key="language">English</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: marketers, marcomms specialists, advertising managers, brand managers</data><data key="details">https://www.vlerick.com/en/programmes/programmes-in-marketing-sales/digital-marketing/</data><data key="full_body">In a careful blend of two online and three on-campus modules  you’ll develop the knowledge and skills to make informed decisions and create strategies that are resilient and adaptable in a constantly changing digital environment.Module 1: Introduction to digital marketing (online)Be inspired by new trends and technologies – and understand their impact on customer behaviourDiscover how to create a digital marketing strategy which tells a consistent story across all channelsModule 2: Acquiring customers via digital channelsExplore the benefits of outbound digital marketing and the value of digital ad platformsUnderstand the power of content marketing – and the role that generative AI can playDiscover how to improve website user experience through A/B testingModule 3: Customer conversion and retentionDive into CRMs and customer data platforms – and how they can power segmentation and personalisationDiscover how to automate personalised communicationsUse big data and AI to boost customer loyaltyExplore how to enhance brand advocacy through word of mouth and influencer marketingModule 4: Online simulation (online)Gain hands-on experience of handling common digital marketing challengesDevelop your decision-making skillsModule 5: Reviewing your digital marketing strategyDiscover how to measure effectivenessDevelop a clear view of the competitive advantage of omnichannel marketingEmbrace marketing technologies and discover how they can power lead generation  increase conversion  and enhance loyalty and advocacyDevelop an integrated  competitive  differentiating digital marketing strategy that’s aligned to your contextLeverage AI and generative AI to drive efficiencyLearn how to develop greater customer insight and articulate a single customer view to improve personalisation of marketing communicationsDiscover how to balance marketing budget across multiple channelsApply everything you’ve learned in a hands-on  risk-free learning experienceMarketers from B2B and B2C organisations who want to develop a strategic understanding of digital marketing – and support their teams to implement itAdvertising managers  brand managers  CRM specialists  channel marketers and e-commerce specialists who need to create a digital marketing strategyAgency owners  content marketers  marcomms specialistsOwners of start-ups and scale-ups who want to increase their customer reach through digital channels</data><data key="intro">If  your digital marketing efforts are focused on channels, it may be time  to reboot your strategy. This programme sets out a holistic, powerful  and integrated approach to digital marketing, that can transform your  results. You’ll learn how to develop a robust strategy, allocate budgets,  improve your customer experience, keep up with latest trends and grow  your business. All of this is underpinned with a clear understanding of  AI, big data and other technologies.</data><data key="sub_title">Learn to create future-proof, technology-enabled digital marketing strategies</data><data key="constraints">NA</data><data key="course_info">course-online &amp;amp; Ghent-Vlerick Business School</data><data key="date">6/06/2024</data><data key="url">https://www.vaia.be/en/courses/digital-marketing</data><data key="title">Digital Marketing</data></node>
<node id="n1609" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: postgraduate students with a master degree, junior/senior researchers from various disciplines, and policy analysts, lawyers and legal experts, civil servants, members of civil society organisations and AI practitioners.</data><data key="data">Law Ethics and Policy of Artificial Intelligence In 2021  KU Leuven organised the first edition of the Summer School on the Law  Ethics and Policy of Artificial Intelligence AI Given the programmes overwhelming success  three editions have taken place thus far The fourth edition will take place from 1 to 10 July 2024The Summer School aims to provide a comprehensive overview of the various legal  ethical and policyrelated issues around AI and algorithmdriven processes more broadly As these technologies have a growing impact on all domains of our lives  it becomes increasingly important to map  understand and assess the challenges and opportunities they raise This requires an interdisciplinary approach  which is why we are collaborating across faculties and departments to organise this Summer School The programmes goal is to offer participants the latest insights on AI from various perspectives  and in particular the fields of law  ethics and policy The lectures are provided by renowned academics  policymakers from EU and international institutions as well as practitioners  allowing participants to grasp not only the theoretical but also the practical implications of the use of AI in these fields The Summer Schools intended audience concerns postgraduate students who already obtained a masters degree and juniorsenior researchers from various disciplines  as well as policy analysts  lawyers and legal experts  civil servants  members of civil society organisations  AI practitioners  and other professionals with an interest in broadening their understanding of AI and its impact on society Participants receive a certificate of attendance with the equivalence of 3 ECTS credits Those interested also have an opportunity to share their research with fellow participants through a research presentation session Thus far  the Summer School has taken place as a hybrid event  enabling participants to join both on campus and online Whether physically or virtually  we look forward to welcoming you to LeuvenCourse overviewThe Summer School covers a range of topics that will enable participants to better grasp the various legal  ethical and policy implications of the development and use of AI in society All lectures are taught in EnglishThe schedule of the Summer Schools 2023 edition can be found hereMost lectures focus on horizontal domains of interest  covering topics such asPhilosophy of AIEthics of AIAI and Data Protection LawAI and Competition LawAI and Intellectual Property LawAI and Consumer ProtectionAI and Liability LawAI and Labour LawAI and FairnesThe European Commissions proposals for an AI Regulation  the Digital Services Act  the Digital Markets ActAI governance and the role of European  International institutionsThough Artificial Intelligence is a general purpose technology  many of its legal  ethical and policyrelated implications are context or sectorspecific Therefore  the programme also explores several vertical domains These cover inter aliaAI and Law EnforcementAI and Public ServicesAI and WarfareAI and HealthcareAI and EducationAI and Legal TechAt the end of the course  participants who attended 90 of the classes and passed the exam receive a certificate of attendance with an equivalence of 3 ECTS credits Those interested will also have the possibility to present their own research during a research presentation session Intended AudienceThe Summer School’s intended audience concerns postgraduate students who already obtained a masters degree and PhD researchers from various disciplines  as well as more senior researchers  policy analysts  lawyers and legal experts  civil servants  members of civil society organisations  AI practitioners  and other professionals with an interest in broadening their understanding of AI and its impact on societyProgramme Format  LocationIn the past  the Summer School was organised as a hybrid event  enabling participants to join either on campus or online The lectures take place at the KU Leuven Faculty of LawCollege De Valk  DV3Faculty of Law  KU LeuvenTiensestraat 413000 LeuvenBelgiumThe application process for the Summer Schools 2024 edition will run from 19 February to 27 March 2024If you would like us to keep you posted  you can register your interest hereApplication processIn their application form  applicants are asked to provideA cover letter of max 500 words explaining their background and motivation for enrolling to the Summer SchoolA CV in PDF format andAt least one letter of recommendation in PDF format including the signatorys name  title and signature Depending on your profile  the recommendations can be academic or professional in natureThe results of the selection process will be communicated to applicants by midAprilPlaces in the programme are limited to ensure the quality and depth of the interactions and discussions English proficiency is required though no certificate of language proficiency will be asked In addition to the quality of applications  the selection committee also aims to ensure a diverse cohort of participantsTuition feesIn 2024  the Summer School will once again be offered in a hybrid format  In the application form  applicants will be able to indicate their preferred participation format or indicate that they would be happy to participate in either format Here below we list the tuition fees in each case1 Tuition fees for the on campus programmeFor postgraduate  PhD students €825For senior researchers  civil servants and employees of civil society organisations €995For other professionals €1250This fee includes all the classes and course materials  lunches  coffee breaks  social activities and a closing dinner It also includes the certificate of attendance with the equivalence of 3 ECTS credits Note that this fee does not include accommodation For information about accommodation in Leuven  please consult the KU Leuven housing website2 Tuition fees for the online programmeFor postgraduate  PhD students €595For senior researchers  civil servants and employees of civil society organisations €750For other professionals €995This fee includes all the classes and course materials  access to the online learning platform  and a certificate of attendance with the equivalence of 3 ECTS credits  We reserve the right to only offer the programme on campus  depending on the amount of online applications received The general terms and conditions for enrolling in certified continuing education programmes at KU Leuven can be found here Please note the following cancellation policyUntil 24 May 2024 50 reimbursement of the tuition feeFrom 25 May 2024 no reimbursement of the tuition feeScholarshipsThis year  we expect to be able to offer three scholarships in the form of a tuition fee waiver  aimed at ensuring that meritorious applicants who would otherwise not be able to afford the course  can nevertheless participate In their application form  those applying for a scholarship will be able to upload an additional letter in which they set out the reasons why they believe to be eligible for a tuition fee waiverIn addition  successful applicants from LDC countries will receive a tuition fee discount of €150 Get a comprehensive overview of the various legal ethical and policyrelated issues around AI and algorithmdriven processes more broadly As these technologies have a growing impact on all domains of our lives it becomes increasingly important to map understand and assess the challenges and opportunities they raise The programmes goal is to offer participants the latest insights on AI from various perspectives and in particular the fields of law ethics and policy httpswwwlawkuleuvenbeaisummerschooldescriptionai Summer School Law ethics and policy of AI summer schoolLeuvenKU Leuven</data><data key="annotated_by">manual</data><data key="start_time">1 Jul 2024 - 10 Jul 2024</data><data key="price">€595-1250</data><data key="subscription_limit">27 Mar 2024</data><data key="language">English</data><data key="location_detail">online or at KU Leuven Faculty of Law and Criminology - College De Valk DV3 (Tiensestraat 41, 3000 Leuven)</data><data key="sub_title">Summer School Law, ethics and policy of AI</data><data key="date">1/07/2024</data><data key="full_body">In 2021  KU Leuven organised the first edition of the Summer School on the Law  Ethics and Policy of Artificial Intelligence (AI). Given the programme's overwhelming success  three editions have taken place thus far. The fourth edition will take place from 1 to 10 July 2024.The Summer School aims to provide a comprehensive overview of the various legal  ethical and policy-related issues around AI and algorithm-driven processes more broadly. As these technologies have a growing impact on all domains of our lives  it becomes increasingly important to map  understand and assess the challenges and opportunities they raise. This requires an interdisciplinary approach  which is why we are collaborating across faculties and departments to organise this Summer School. The programme's goal is to offer participants the latest insights on AI from various perspectives  and in particular the fields of law  ethics and policy. The lectures are provided by renowned academics  policy-makers from EU and international institutions as well as practitioners  allowing participants to grasp not only the theoretical but also the practical implications of the use of AI in these fields. The Summer School's intended audience concerns postgraduate students who already obtained a master's degree and junior/senior researchers from various disciplines  as well as policy analysts  lawyers and legal experts  civil servants  members of civil society organisations  AI practitioners  and other professionals with an interest in broadening their understanding of AI and its impact on society. Participants receive a certificate of attendance with the equivalence of 3 ECTS credits. Those interested also have an opportunity to share their research with fellow participants through a research presentation session. Thus far  the Summer School has taken place as a hybrid event  enabling participants to join both on campus and online. Whether physically or virtually  we look forward to welcoming you to Leuven!Course overviewThe Summer School covers a range of topics that will enable participants to better grasp the various legal  ethical and policy implications of the development and use of AI in society. All lectures are taught in English.The schedule of the Summer School's 2023 edition can be found here.Most lectures focus on horizontal domains of interest  covering topics such as:Philosophy of AIEthics of AIAI and Data Protection LawAI and Competition LawAI and Intellectual Property LawAI and Consumer ProtectionAI and Liability LawAI and Labour LawAI and FairnesThe European Commission's proposals for an AI Regulation  the Digital Services Act &amp; the Digital Markets ActAI governance and the role of European &amp; International institutionsThough Artificial Intelligence is a general purpose technology  many of its legal  ethical and policy-related implications are context- or sector-specific. Therefore  the programme also explores several vertical domains. These cover inter alia:AI and Law EnforcementAI and Public ServicesAI and WarfareAI and HealthcareAI and EducationAI and Legal TechAt the end of the course  participants who attended 90% of the classes and passed the exam receive a certificate of attendance with an equivalence of 3 ECTS credits. Those interested will also have the possibility to present their own research during a research presentation session. Intended AudienceThe Summer School’s intended audience concerns postgraduate students who already obtained a master's degree and PhD researchers from various disciplines  as well as more senior researchers  policy analysts  lawyers and legal experts  civil servants  members of civil society organisations  AI practitioners  and other professionals with an interest in broadening their understanding of AI and its impact on society.Programme Format &amp; LocationIn the past  the Summer School was organised as a hybrid event  enabling participants to join either on campus or online. The lectures take place at the KU Leuven Faculty of Law:College De Valk - DV3Faculty of Law  KU LeuvenTiensestraat 413000 LeuvenBelgiumThe application process for the Summer School's 2024 edition will run from 19 February to 27 March 2024.If you would like us to keep you posted  you can register your interest here.Application processIn their application form  applicants are asked to provide:A cover letter of max. 500 words explaining their background and motivation for enrolling to the Summer School;A CV in PDF format; andAt least one letter of recommendation in PDF format (including the signatory's name  title and signature). Depending on your profile  the recommendation(s) can be academic or professional in nature.The results of the selection process will be communicated to applicants by mid-April.Places in the programme are limited to ensure the quality and depth of the interactions and discussions. English proficiency is required (though no certificate of language proficiency will be asked). In addition to the quality of applications  the selection committee also aims to ensure a diverse cohort of participants.Tuition feesIn 2024  the Summer School will once again be offered in a hybrid format (*). In the application form  applicants will be able to indicate their preferred participation format (or indicate that they would be happy to participate in either format). Here below we list the tuition fees in each case:1) Tuition fees for the on campus programme:For postgraduate / PhD students: €825For senior researchers  civil servants and employees of civil society organisations: €995For other professionals: €1250This fee includes all the classes and course materials  lunches  coffee breaks  social activities and a closing dinner. It also includes the certificate of attendance with the equivalence of 3 ECTS credits. Note that this fee does not include accommodation. For information about accommodation in Leuven  please consult the KU Leuven housing website.2) Tuition fees for the online programme:For postgraduate / PhD students: €595For senior researchers  civil servants and employees of civil society organisations: €750For other professionals: €995This fee includes all the classes and course materials  access to the online learning platform  and a certificate of attendance with the equivalence of 3 ECTS credits. (*) We reserve the right to only offer the programme on campus  depending on the amount of online applications received. The general terms and conditions for enrolling in certified continuing education programmes at KU Leuven can be found here. Please note the following cancellation policy:Until 24 May 2024: 50% reimbursement of the tuition fee.From 25 May 2024: no reimbursement of the tuition fee.ScholarshipsThis year  we expect to be able to offer three scholarships in the form of a tuition fee waiver  aimed at ensuring that meritorious applicants who would otherwise not be able to afford the course  can nevertheless participate. In their application form  those applying for a scholarship will be able to upload an additional letter in which they set out the reasons why they believe to be eligible for a tuition fee waiver.In addition  successful applicants from LDC countries will receive a tuition fee discount of €150.</data><data key="intro">Get a comprehensive overview of the various legal, ethical and policy-related issues around AI and algorithm-driven processes more broadly. As these technologies have a growing impact on all domains of our lives, it becomes increasingly important to map, understand and assess the challenges and opportunities they raise. The programme's goal is to offer participants the latest insights on AI from various perspectives, and in particular the fields of law, ethics and policy.</data><data key="course_info">summer school-Leuven-KU Leuven</data><data key="url">https://www.vaia.be/en/courses/law-ethics-and-policy-of-artificial-intelligence-2024</data><data key="title">Law, Ethics and Policy of Artificial Intelligence</data><data key="constraints">NA</data><data key="details">https://www.law.kuleuven.be/ai-summer-school/description-ai</data></node>
<node id="n1610" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="target_group">Target audience: Researchers</data><data key="sub_title">12 Aug 2024 - 16 Aug 2024</data><data key="start_time">12 Aug 2024 - 16 Aug 2024</data><data key="price">max. €125</data><data key="subscription_limit">30 Jun 2024</data><data key="language">English</data><data key="location_detail">Gent</data><data key="data">The Society for Imprecise Probabilities Theories and Applications Summer School Our summer school has the aim of introducing doctoral students – and other interested researchers – to the basics of imprecise probabilities  from its foundations in applied mathematics and logic to its applications in quantum physics  economy and AI Emphasis will be put on why and how imprecision is adopted in  relevant to  or even necessary in each of these fieldsOn the last day  we’ll provide the participants with a number of research projects on the subjects of the lectures  which they will work on in smaller groups supervised by the lecturers This will allow the participants to work together with and learn from fellow students and researchers  who presumably have expertise in different research fields More info  Share this course   httpsschool24siptaorgregistration 12 Aug 2024  16 Aug 2024 Summer SchoolGentUGent amp VAIA</data><data key="intro">More info  Share this course  </data><data key="date">12/08/2024</data><data key="details">https://school24.sipta.org/registration/</data><data key="full_body">Our summer school has the aim of introducing doctoral students – and other interested researchers – to the basics of imprecise probabilities  from its foundations in (applied) mathematics and logic to its applications in quantum physics  economy and AI. Emphasis will be put on why and how imprecision is adopted in  relevant to  or even necessary in each of these fields.On the last day  we’ll provide the participants with a number of research projects on the subjects of the lectures  which they will work on in smaller groups supervised by the lecturers. This will allow the participants to work together with and learn from fellow students and researchers  who presumably have expertise in different research fields.</data><data key="url">https://www.vaia.be/en/courses/sipta-the-society-for-imprecise-probabilities-theories-and-applications-summer-school</data><data key="title">The Society for Imprecise Probabilities: Theories and Applications Summer School</data><data key="constraints">NA</data><data key="course_info">Summer School-Gent-UGent &amp;amp; VAIA</data></node>
<node id="n1611" labels=":Course"><data key="labels">:Course</data><data key="data">Digital Leadership Over five consecutive days on campus  you’ll dive into the world of digital transformation  develop the critical skills to evolve your leadership role and share experiences with your fellow participantsModule 1 Understand the digital difference Explore the challenges and opportunities facing today’s digital leadersPlay the interactive Digital Leadership Game and discover the capabilities you need to be successfulModule 2 Vigilant leadership Discover the Vigilant leader – alert and ready to respond to new technology  market disruption and changing customer behaviourDive into a testimonial and case study from a Vigilant leaderExplore multiple theories and complete various exercisesModule 3 Voyager leadership Discover the Voyager leader – an entrepreneur who connects diverse  creative people to make ideas tangibleDive into a testimonial and case study from a Voyager leaderExplore multiple theories and complete various exercisesModule 4 Vested leadershipDiscover the Vested leader – who goes beyond vision to create organisational mechanisms to mobilise skills and resourcesDive into a testimonial and case study from a Vested leaderExplore multiple theories and complete various exercisesModule 5 Visionary leadershipDiscover the Visionary leader – who turns inspiration  ideas and experiments into strategic business goalsDive into a testimonial and case study from a Visionary leaderExplore multiple theories and complete various exercisesGain a solid understanding of the key challenges that can help or hinder digital transformationTackle the leadership challenges of transforming into a sustainable digital businessLearn how to align people  working practices  culture and structureDiscover bestpractice frameworks  pragmatic tools  reallife case studies and actionable insightsGet away from the everyday concerns of the workplace and really focus on digital transformation for your businessSenior executives  middle managers and ambitious professionals who sponsor  lead or are about to lead digital transformation projectsProfessionals who act as change agents in their organisationsEntrepreneurs who want to design a sustainable digital transformation roadmap for their businessesProfessionals from a range of different sectors and specialisms  To make digital transformation a success in today’s complex world companies need to be sustainable agile alert to opportunities – and act faster than their competitors This programme dives into the challenges and opportunities facing leaders going through digital transformation Using the 4V digital transformation leadership model you’ll explore the types of leadership that can contribute to your digital journey – and understand what digital transformation means for you httpswwwvlerickcomnlopleidingenopleidingenindigitaletransformatiedigitalleadership Take charge of your organisation’s digital transformation courseBrusselsVlerick Business School</data><data key="annotated_by">manual</data><data key="start_time">26 Aug 2024 - 30 Aug 2024</data><data key="price">€7,695 (excl. VAT)</data><data key="location_detail">Bolwerklaan 21/bus 32, 1210 Brussels</data><data key="sub_title">Take charge of your organisation’s digital transformation</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: business professionals</data><data key="full_body">Over five consecutive days on campus  you’ll dive into the world of digital transformation  develop the critical skills to evolve your leadership role and share experiences with your fellow participants.Module 1: Understand the digital difference Explore the challenges and opportunities facing today’s digital leadersPlay the interactive Digital Leadership Game and discover the capabilities you need to be successfulModule 2: Vigilant leadership Discover the Vigilant leader – alert and ready to respond to new technology  market disruption and changing customer behaviourDive into a testimonial and case study from a Vigilant leaderExplore multiple theories and complete various exercisesModule 3: Voyager leadership Discover the Voyager leader – an entrepreneur who connects diverse  creative people to make ideas tangibleDive into a testimonial and case study from a Voyager leaderExplore multiple theories and complete various exercisesModule 4: Vested leadershipDiscover the Vested leader – who goes beyond vision to create organisational mechanisms to mobilise skills and resourcesDive into a testimonial and case study from a Vested leaderExplore multiple theories and complete various exercisesModule 5: Visionary leadershipDiscover the Visionary leader – who turns inspiration  ideas and experiments into strategic business goalsDive into a testimonial and case study from a Visionary leaderExplore multiple theories and complete various exercisesGain a solid understanding of the key challenges that can help or hinder digital transformationTackle the leadership challenges of transforming into a sustainable digital businessLearn how to align people  working practices  culture and structureDiscover best-practice frameworks  pragmatic tools  real-life case studies and actionable insightsGet away from the everyday concerns of the workplace and really focus on digital transformation for your businessSenior executives  middle managers and ambitious professionals who sponsor  lead or are about to lead digital transformation projectsProfessionals who act as change agents in their organisationsEntrepreneurs who want to design a sustainable digital transformation roadmap for their businessesProfessionals from a range of different sectors and specialisms </data><data key="intro">To make digital transformation a success in today’s complex world, companies need to be sustainable, agile, alert to opportunities – and act faster than their competitors. This programme dives into the challenges and opportunities facing leaders going through digital transformation. Using the 4V digital transformation leadership model, you’ll explore the types of leadership that can contribute to your digital journey – and understand what digital transformation means for you.</data><data key="language">English</data><data key="course_info">course-Brussels-Vlerick Business School</data><data key="date">26/08/2024</data><data key="details">https://www.vlerick.com/nl/opleidingen/opleidingen-in-digitale-transformatie/digital-leadership/</data><data key="constraints">3 years of working experience</data><data key="url">https://www.vaia.be/en/courses/digital-leadership</data><data key="title">Digital Leadership</data></node>
<node id="n1612" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: developers, integrators, and operators deploying embedded systems</data><data key="data">EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems In this session  we will delve into the critical landscape of cybersecurity standards and regulations within the European Union  specifically tailored for IoT ecosystems and Industrial Control Systems The overview encompasses the diverse realm of IoT systems  including smart devices found in retail outlets Additionally  we address Industrial Control Systems crucial to sectors such as rail and energy  emphasizing the importance of compliance with EU cybersecurity standards to fortify the security posture of these interconnected systems This session is part of the series Cybersecurity excellenceSession 1 Tackling cybersecurity challenges a complex security puzzle  28 February 2024  Vincent Naessens KU LeuvenSession 2 ‘Privacy by design a technical approach to privacy risk  26 March 2024  Kim Wuyts PwCSession 3 Efficient use of a network protocol analyzer in cyber threats workshop  24 April 2024  Tom Cordemans KU LeuvenSession 4 Hacking and protecting embedded devices workshop  29 May 2024  Jorn Lapon KU LeuvenSession 5 EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems  28 August 2024  Vincent Naessens KU LeuvenSession 6 Cyberattack response  25 September 2024  Tom Bauwens Eubelius  Kalman Tiboldi TVHSession 7 Postquantum cryptography  23 October 2024  Eric Michiels IBM In an increasingly technologydriven world cybersecurity stands as the cornerstone of digital resilience In this programme we will explore the full spectrum of cybersecurity from prevention to response while gaining both immediate handson skills and a foresight for the future of cybersecurity This programme brings together academic researchers and industrial experts and thus provides a blend of lectures and use cases and practical testimonials httpspuckuleuvenbenlopleidingeucybersecuritystandardsandregulationforiotecosystemsandindustrialcontrolsystemsknpzmqvzpbg06w9v Session 5  Cybersecurity excellence series 2024 trainingGhentKU Leuven Postuniversitair Centrum</data><data key="annotated_by">manual</data><data key="start_time">28 Aug 2024 14:00 - 17:00</data><data key="price">€250</data><data key="sub_title">Session 5 | Cybersecurity excellence series 2024</data><data key="subscription_limit">NA</data><data key="intro">In an increasingly technology-driven world, cybersecurity stands as the cornerstone of digital resilience. In this programme, we will explore the full spectrum of cybersecurity, from prevention to response, while gaining both immediate, hands-on skills and a foresight for the future of cybersecurity. This programme brings together academic researchers and industrial experts, and thus provides a blend of lectures and use cases and practical testimonials.</data><data key="language">English</data><data key="location_detail">KU Leuven, Campus Rabot - Gebroeders de Smetstraat 1, 9000 Ghent</data><data key="date">28/08/2024</data><data key="details">https://puc.kuleuven.be/nl/opleiding/eu_cybersecurity_standards_and_regulation_for_iot_ecosystems_and_industrial_control_systems-knpzmqvzpbg06w9v</data><data key="full_body">In this session  we will delve into the critical landscape of cybersecurity standards and regulations within the European Union  specifically tailored for IoT ecosystems and Industrial Control Systems. The overview encompasses the diverse realm of IoT systems  including smart devices found in retail outlets. Additionally  we address Industrial Control Systems crucial to sectors such as rail and energy  emphasizing the importance of compliance with EU cybersecurity standards to fortify the security posture of these interconnected systems. This session is part of the series Cybersecurity excellence:Session 1: Tackling cybersecurity challenges: a complex security puzzle | 28 February 2024 - Vincent Naessens (KU Leuven)Session 2: ‘Privacy by design': a technical approach to privacy risk | 26 March 2024 - Kim Wuyts (PwC)Session 3: Efficient use of a 'network protocol analyzer' in cyber threats (workshop) | 24 April 2024 - Tom Cordemans (KU Leuven)Session 4: Hacking and protecting embedded devices (workshop) | 29 May 2024 - Jorn Lapon (KU Leuven)Session 5: EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems | 28 August 2024 - Vincent Naessens (KU Leuven)Session 6: Cyberattack response | 25 September 2024 - Tom Bauwens (Eubelius)  Kalman Tiboldi (TVH)Session 7: Post-quantum cryptography | 23 October 2024 - Eric Michiels (IBM)</data><data key="url">https://www.vaia.be/en/courses/eu-cybersecurity-standards-and-regulation-for-iot-ecosystems-and-industrial-control-systems</data><data key="title">EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems</data><data key="constraints">NA</data><data key="course_info">training-Ghent-KU Leuven Postuniversitair Centrum</data></node>
<node id="n1613" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">google colab</data></node>
<node id="n1614" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">notebooks</data></node>
<node id="n1615" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">text mining</data></node>
<node id="n1616" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">textual data</data></node>
<node id="n1617" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">predictive</data></node>
<node id="n1618" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">categorical</data></node>
<node id="n1619" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">gradient boosting machines</data></node>
<node id="n1620" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">support vector machines</data></node>
<node id="n1621" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">knearestneighbors</data></node>
<node id="n1622" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">kmeans</data></node>
<node id="n1623" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">dimensionality model capacity overfitting</data></node>
<node id="n1624" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">regularization</data></node>
<node id="n1625" labels=":Skill"><data key="labels">:Skill</data><data key="name">lasso</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n1626" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">ridge regression</data></node>
<node id="n1627" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">crossvalidation</data></node>
<node id="n1628" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">bagging</data></node>
<node id="n1629" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">boosting</data></node>
<node id="n1630" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">visualization</data></node>
<node id="n1631" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">highdimensional data</data></node>
<node id="n1632" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">model calibration</data></node>
<node id="n1633" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">outlier detection</data></node>
<node id="n1634" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">explanation</data></node>
<node id="n1635" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">data harvesting</data></node>
<node id="n1636" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">knowledge graphs</data></node>
<node id="n1637" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">edge network</data></node>
<node id="n1638" labels=":Skill"><data key="labels">:Skill</data><data key="name">genai</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n1639" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">aiedge</data></node>
<node id="n1640" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">digital ethics</data></node>
<node id="n1641" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">personal data</data></node>
<node id="n1642" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">ethics frameworks</data></node>
<node id="n1643" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">ethical foundations</data></node>
<node id="n1644" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">ethical decisionmaking</data></node>
<node id="n1645" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">technology assessment</data></node>
<node id="n1646" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">designing ethical technology</data></node>
<node id="n1647" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">behaviorist principles</data></node>
<node id="n1648" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">visualising data</data></node>
<node id="n1649" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">data handling</data></node>
<node id="n1650" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">transforming data</data></node>
<node id="n1651" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">missing data</data></node>
<node id="n1652" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">dashboard</data></node>
<node id="n1653" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">individual data</data></node>
<node id="n1654" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">data sharing</data></node>
<node id="n1655" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">data spaces</data></node>
<node id="n1656" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">ai strategies</data></node>
<node id="n1657" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">network protocol analyzers</data></node>
<node id="n1658" labels=":Skill"><data key="labels">:Skill</data><data key="name">npa</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n1659" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">cybersecurity challenges</data></node>
<node id="n1660" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">‘privacy by design</data></node>
<node id="n1661" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">eu cybersecurity standards</data></node>
<node id="n1662" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">cyberattack</data></node>
<node id="n1663" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">robotics</data></node>
<node id="n1664" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">sensors</data></node>
<node id="n1665" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">processing actuators</data></node>
<node id="n1666" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">energy power</data></node>
<node id="n1667" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">batteries</data></node>
<node id="n1668" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">supervised</data></node>
<node id="n1669" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">tensor network kernel machines</data></node>
<node id="n1670" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">nonlinear patterns</data></node>
<node id="n1671" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">existing data</data></node>
<node id="n1672" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">analytics strategy</data></node>
<node id="n1673" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">digital strategy</data></node>
<node id="n1674" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">polynomial systems</data></node>
<node id="n1675" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">structural equation modeling</data></node>
<node id="n1676" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">lavaan</data></node>
<node id="n1677" labels=":Skill"><data key="labels">:Skill</data><data key="name">sem</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n1678" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">multivariate methods</data></node>
<node id="n1679" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">path analysis</data></node>
<node id="n1680" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">mediation analysis</data></node>
<node id="n1681" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">confirmatory factor analysis</data></node>
<node id="n1682" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">growth curve modeling</data></node>
<node id="n1683" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">categorical data</data></node>
<node id="n1684" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">longitudinal data</data></node>
<node id="n1685" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">multilevel data</data></node>
<node id="n1686" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">latent variables</data></node>
<node id="n1687" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">decision support systems</data></node>
<node id="n1688" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">transparency</data></node>
<node id="n1689" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">explainability</data></node>
<node id="n1690" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">declarative programming systems</data></node>
<node id="n1691" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">prolog</data></node>
<node id="n1692" labels=":Skill"><data key="labels">:Skill</data><data key="name">idp</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n1693" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">chebyshev varieties</data></node>
<node id="n1694" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">algebraic varieties</data></node>
<node id="n1695" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">polynomial equations</data></node>
<node id="n1696" labels=":Skill"><data key="labels">:Skill</data><data key="name">owasp</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n1697" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">tackling cybersecurity</data></node>
<node id="n1698" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">network protocol analyzer</data></node>
<node id="n1699" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">hacking and protecting</data></node>
<node id="n1700" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">eu cybersecurity</data></node>
<node id="n1701" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">cyberattack response</data></node>
<node id="n1702" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">postquantum cryptography</data></node>
<node id="n1703" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">digital marketing</data></node>
<node id="n1704" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">customer experience</data></node>
<node id="n1705" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">digital marketing strategies</data></node>
<node id="n1706" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">law ethics</data></node>
<node id="n1707" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">quantum physics</data></node>
<node id="n1708" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">digital leadership</data></node>
<node id="n1709" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">digital transformation</data></node>
<node id="n1710" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=96f36bc0f484008f&amp;bb=AbDN_Sau9azpIrnPUsr1yV0fwQxUB_X_Xih96UVN2--G2yptid26CpAqqctVsdfYs8LNV6Bm6tmrcW3nH5CbEj-AEZrP6Q1ZsdJ17Q3Q9GM%3D&amp;xkcb=SoDl67M3CNkev1Rylh0LbzkdCdPP&amp;fccid=11caadcdc98800d4&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay52000  78000 a yearJob typeFulltime LocationRemote BenefitsPulled from the full job description401k401k matchingADD insuranceDisability insuranceFamily leaveHealth insuranceHealth savings accountShow morechevron down Full job description    Type of Requisition   Regular       Clearance Level Must Currently Possess   None       Clearance Level Must Be Able to Obtain   None       Suitability        Public TrustOther Required        Job Family   Data Analysis       Job Qualifications       Skills   Data Analysis Data Analytics Datasets Statistics      Certifications       Experience   3  years of related experience      US Citizenship Required   No       Job Description       GDIT is looking for a Data Scientist with deep data analytics skills The candidate needs to be an advanced practitioner of SQL R Python and be familiar with working with data in an Azure Cloud environment using DataBricks and other cloud analytics tools      Supporting CDC in a fastpaced environment that requires the ability to work with big data sets and turn complex analysis around in a few days There may be opportunities to support MMWR and other publications        Responsibilities     The candidate will provide highlevel expertise to develop study questions and designs conduct analyses and interpret findings based on electronic case reporting data and pharmacy hospital and healthcare claims data  Support use of SAS Viya in quality evaluation  Experience in Iqvia and other public health data sources  Coauthor and write papers for publications in peerreview journals and reports present papersresearchfindings to the scientific community at professional meetings and conferences  Maintain requisite knowledge for managing and analyzing large data sets using Python SAS SQL or R  Trainsdemonstrates new analytics tools and software languages for DHISPEB staff Perform work for review in timely intervals as defined by Health Scientist Coordinator  Reviews deliverables to ensure time frames and quality standards are met and then submits deliverables within established time frames  Support CDC clearance process for scientific products which includes reviewing materials produced by others  Draft data analysis plans for ad hoc or standard reports  Develop and document Python SAS R or SQL programs views and databases  Conduct exploratory or studyspecific analyses  Verify results and validate with other known information sources  Summarize and visualize results for example develop graphs for dashboards showing data metrics  Draft final reports methodological white papers or articles for publication  Track technical assistance being provided to ensure queries are answered promptly and consistently  Produce data extracts and cleaning scripts  Develop data management and analysis materials including graphics to convey analyses  Publish analysis and QC scripts to central repository or shared space like GitHub for use by others within PEB and across the agency upon review and acceptance by CDC  Lead the analysis and writing for at least one peer review publication  Oversee the development of presentations based on hisher analyses andor the analyses of staffcontractors to be shared with internal partners Community of Practice members and other partners  Oversee the development of data visualizations and dashboard showcasing the portfolio of CDH’s managed data sources for sharing with internal and external audiences The candidate will provide highlevel expertise to develop study questions and designs conduct analyses and interpret findings based on electronic case reporting data and pharmacy hospital and healthcare claims data  Coauthor and write papers for publications in peerreview journals and reports present papersresearchfindings to the scientific community at professional meetings and conferences  Maintain requisite knowledge for managing and analyzing large data sets using Python SAS SQL or R  Trainsdemonstrates new analytics tools and software languages for DHISPEB staff Perform work for review in timely intervals as defined by Health Scientist Coordinator  Reviews deliverables to ensure time frames and quality standards are met and then submits deliverables within established time frames  Support CDC clearance process for scientific products which includes reviewing materials produced by others       WHAT YOU’LL NEED     Bachelors Science in Computer Science or related field or Masters in Public Health  4 or more Years of data analysis experience with emphasis on Big data  3 Years of specific data analysis tools Python R SAS PySpark Tableau Server PostgreSQL SQL Django ArcGIS       Qualifications     Mid level understanding of data analysis including the use of SAS R SQL and Python for developing database views and programs  Experience with large ‘big data’ datasets  Ability to draft data analysis plans  Selfsufficient needs little supervision  Sound interpersonal communication telephone email and business etiquette  Strong attention to detail and organizational skills  Experience in facilitating quick turnaround for complex documents  Experience in assisting in the preparation of documents for public audiences  Experience in the use of MS Teams and Microsoft Office software ie Word Excel PowerPoint Outlook  Must have the ability to obtain a National Agency Check with Inquiries NACI clearance  Other valuable experience BIG DATA Data and Systems Analysis Strong R and Python Experience Data ETL experience WorkflowProcess Redesign Automating Data Pipelines Mapping SQL Database Tables StatisticalPredictive Modeling       Desired Skills and Experience     Previous CDC and or public health experience highly desirable  Superior multitasking and organization skills Must manage multiple simultaneous projects and prioritize assignments and tasks accordingly remaining flexible to changing priorities and new initiatives       Attributes for Success     Demonstrated proactive approaches to problem solving with strong decisionmaking capabilities  Forward looking thinker who actively seeks opportunities and proposes solutions  Analyze organizational goals and objectives operating environment and business processes to determine most efficient methods of accomplishing work       LOCATION Remote        CLEARANCE     Ability to obtain a Public Trust candidate must pass a public trust background investigation and meet the residency requirement for having resided in the US for at least 3 three out of the last 5 five years      GDIT IS YOUR PLACE      Fullflex work week to own your priorities at work and at home 401K with company match Comprehensive health and wellness packages Internal mobility team dedicated to helping you own your career Professional growth opportunities including paid education and certifications Cuttingedge technology you can learn from Rest and recharge with paid vacation and holidays    The likely salary range for this position is 52000  78000 This is not however a guarantee of compensation or salary Rather salary will be set based on experience geographic location and possibly contractual requirements and could fall outside of this range       Scheduled Weekly Hours   40       Travel Required   Less than 10       Telecommuting Options   Remote       Work Location   Any Location  Remote       Additional Work Locations        Total Rewards at GDIT   Our benefits package for all USbased employees includes a variety of medical plan options some with Health Savings Accounts dental plan options a vision plan and a 401k plan offering the ability to contribute both pre and posttax dollars up to the IRS annual limits and receive a company match To encourage worklife balance GDIT offers employees full flex work weeks where possible and a variety of paid time off plans including vacation sick and personal time holidays paid parental military bereavement and jury duty leave GDIT typically provides new employees with 15 days of paid leave per calendar year to be used for vacations personal business and illness and an additional 10 paid holidays per year Paid leave and paid holidays are prorated based on the employee’s date of hire The GDIT Paid Family Leave program provides a total of up to 160 hours of paid leave in a rolling 12 month period for eligible employees To ensure our employees are able to protect their income other offerings such as short and longterm disability benefits life accidental death and dismemberment personal accident critical illness and business travel and accident insurance are provided or available We regularly review our Total Rewards package to ensure our offerings are competitive and reflect what our employees have told us they value most   We are GDIT A global technology and professional services company that delivers consulting technology and mission services to every major agency across the US government defense and intelligence community Our 30000 experts extract the power of technology to create immediate value and deliver solutions at the edge of innovation We operate across 30 countries worldwide offering leading capabilities in digital modernization AIML Cloud Cyber and application development Together with our clients we strive to create a safer smarter world by harnessing the power of deep expertise and advanced technology   We connect people with the most impactful client missions creating an unparalleled work experience that allows them to see their impact every day We create opportunities for our people to lead and learn simultaneously From securing our nation’s most sensitive systems to enabling digital transformation and cloud adoption our people are the ones who make change real   GDIT is an Equal OpportunityAffirmative Action employer All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability or veteran status or any other protected class  </data></node>
<node id="n1711" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=fbad3eb27783d4e8&amp;bb=AbDN_Sau9azpIrnPUsr1yYb_68sueHn4lzpEZ-MpwkC4z9K13nx_ZKamXsQ6NmrvxsWiwS4v8xnuybnX10xt1BAQG9esaU9QZsNwm9NujQY%3D&amp;xkcb=SoBR67M3CNkev1Rylh0KbzkdCdPP&amp;fccid=fe2d21eef233e94a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid CPA licenseYesNoSkillsDo you have experience in Financial modelingYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePayUp to 128800 a yearJob typeFulltime LocationSeattle WA BenefitsPulled from the full job descriptionHealth insurance Full job description 3 years of tax finance or a related analytical field experience Bachelors degree in finance accounting business economics or a related analytical field eg engineering math computer science  Amazon seeks a highly effective Senior Financial Analyst to be a key member in the AGI Finance team Amazons Artificial General Intelligence AGI Team mission is to build the world’s best Artificial General Intelligence that will enable every Amazon business to deliver more value to its customers and benefits humanity This is an exciting opportunity to join one of the most innovative creative and fastest growing businesses at Amazon This role will partner with the teams developing the core AI functionality of Alexa    This candidate will be highly analytical detail oriented and have a demonstrated ability to effectively manage financial responsibilities and decision making in a rapidly evolving business This role requires a selfstarter with strong modeling skills a keen attention to detail and the ability to manage multiple projects effectively This role also has regular interaction with senior executives various business units and multiple stakeholders across Amazon and requires strong interpersonal and communication skills This role supports a technical business team and will contribute to the future direction through data analysis and clear articulation of what really matters in rapidly evolving environments    Key job responsibilities    Drive standardization and automation of planning and reporting processes to enable quicker and more effective decision making Own financial planning insights and operational cost efficiency strategy for some of the largest teams in Alexa Ad hoc analysis including creating maintaining and expanding the scope of business and financial models that highlight opportunities to deliver results Partner with the business to make tradeoff decisions regarding resourcing and support levels including ownership of review meetings Analyze operating expenses to provide insight to business partners including costsaving and process improvement recommendations  We are open to hiring candidates to work out of one of the following locations    Seattle WA USA     CPA or MBA Experience working with stakeholders Experience in identifying leading and executing opportunities to improve automate standardize or simplify finance or business tools and processes Experience in financial modeling PL management or analysis  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus    Our compensation reflects the cost of labor across several US geographic markets The base pay for this position ranges from 56900year in our lowest geographic market up to 128800year in our highest geographic market Pay is based on a number of factors including market location and may vary depending on jobrelated knowledge skills and experience Amazon is a total compensation company Dependent on the position offered equity signon payments and other forms of compensation may be provided as part of a total compensation package in addition to a full range of medical financial andor other benefits For more information please visit httpswwwaboutamazoncomworkplaceemployeebenefits This position will remain posted until filled Applicants should apply via our internal or external career site </data></node>
<node id="n1712" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=41a5c177b94ae9fd&amp;bb=AbDN_Sau9azpIrnPUsr1ybrZepEI9zxhl_v1Zs_64xawn0tIA80NsvyxqfxLhCGtUoLAZiIeOhARrU-G-zyGwDMdmDWJpHT0yoJ9OUG-3ieSSxiOVaQxBA%3D%3D&amp;xkcb=SoDM67M3CNkev1Rylh0JbzkdCdPP&amp;fccid=18a767da5af06fb4&amp;cmp=Founding-Ventures&amp;ti=Ai%2Fml+Engineer&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay150000  300000 a yearJob typeParttimeFulltimeShift and scheduleMonday to Friday LocationRemote BenefitsPulled from the full job descriptionFlexible schedule Full job descriptionFounding Teams is a new AI Tech Incubator and talent platform We are supporting the next generation of AI startup founders with the resources they need including engineering product sales marketing and operations staff to create and launch their products The ideal candidate will have a passion for nextgeneration AI tech startups and working with great global startup talent Job Title  Lead Engineer Company Stealth AI Startup Remote 1620 hours per week Flexible hours  Yes Job Description  Build prototype AI ML models and tools to help us understand our customers and create personalised customer recommendations across multiple use cases and productize solutions to scale  Deeply understand customers their behaviours and pain points and develop a diversity of AI models addressing an array of customers’ needs  Translate business needs into AIML problems and create innovative solutions to advance our business goals  Determine the types and amount of data needed and work with data engineer to identify data sources and ingest into data lake  Structure standardise and annotate data into processable formats for ML enrich data with necessary attributes to allow sophisticated personalization  Help shape the way our data science team does work  researching and making key decisions about what we build how we build it and which tools are best for solving our problems  Work alongside software and data engineers to implement data processing and visualisation systems that make data readily available and simplify how insights are communicated  Evaluate the performance of AI models and make tradeoffs against quality metrics Investigate and resolve performance issues in a timely manner Requirements  Bachelor’s or Master’s degree in Mathematics ML statistics Computer Science SoftwareData Engineering or a related field  Strong mathematical background in probability statistics and optimization algorithms  Experience in building machine learning models and deploying them to production to make real decisions then measuring the impact of these decisions  Deep understanding of and have applied various machine learning techniques for solving realworld problems  Expertise with advanced programming skills in Python Java or any of the major languages to build robust algorithms  Proficient with SQL and can work “full stack” to integrate solutions with our data ecosystem  Confident in taking ownership of projects from start to finish and enjoy the process of turning nebulous ideas into reality  Excellent communication skills  A selfstarter who drives projects and builds strong relationships with stakeholders and teams to tackle large crossfunctional efforts  Thrive with minimal guidance and process  Worked in both small teamsincubators and large corporations Job Types Fulltime Parttime Salary 15000000  30000000 per year Schedule  Monday to Friday  Work Location Remote </data></node>
<node id="n1713" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=f34f60cf6897754d&amp;bb=AbDN_Sau9azpIrnPUsr1yW_bUScbsH3Z3fcHsmZ2GjHPlYNhB93MbSQoC-mg1IqAPMNNqJmG1dK3LZTSVpDlVeT5GVdlixVihVAkRdwDOvsx6WLcUhe9qw%3D%3D&amp;xkcb=SoB467M3CNkev1Rylh0IbzkdCdPP&amp;fccid=3ca7b0879e770c4b&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNoEducationDo you have a High school diploma or GEDYesNo Job detailsHere’s how the job details align with your profilePay70000  85000 a yearJob typeFulltime Location1710 Whitestone Expy Whitestone NY 11357 Full job description 7000000  8500000 a year  Job Description    Restaurant Depot is the leading wholesale distributor of food equipment and supplies to independent grocery retailers and foodservice operators across the United States  We seek a highly motivated technically proficient and business savvy data scientist for our growing analytics function You will have the opportunity to work on a variety of interesting data science projects that have far reaching implications for all stakeholders across our organization   Experience effectively translating between business value requirements and technical solutions  Demonstrated ability to take realworld analyticsdata science projects from start to finish including data cleaning descriptive analysis predictive modelling and visualizations  Advanced knowledge of Python with respect to the data analysis and machine learning stacks  Experience in extracting cleaning and aggregating from different data sources  Experience in building models using statistical modelling techniques and machine learning models  Experience with building diagnosing and improving time series models  Prior experience with demand forecasting andor pricing analytics is a big plus  Comfortable with using SQL to extract and transform data Advanced knowledge of querying SQL databases and writing stored procedures is a big plus  Familiarity with Docker and Docker Compose  Experience interacting with nix OS through the command line  Familiarity with numerical optimization is a plus  Familiarity with Apache Airflow is a plus  Familiarity with Plotly Dash is a plus  Bachelors Degree or higher in Analytics  Statistics  Data Science or other related subjects  Strong working knowledge of utilizing standard MS Office tools Excel Word and Power Point  Ability to multitask and adjust priorities based on workload and direction from management  Selfmotivated to document processes and teach others    Candidates that do not demonstrate all of the above are still strongly encouraged to apply    1Applicants should submit a paragraph summarizing one of their data science projects the challenges they faced and how they overcame it  2Applicants should also provide links to their GitHub profiles or portfolios   Brand Restaurant Depot    Address 1710 Whitestone Expressway Whitestone NY  11357    Property Description 590  RD EAST COAST CORP OFFICE    Property Number 590  </data></node>
<node id="n1714" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=cf53a79bcc507199&amp;bb=AbDN_Sau9azpIrnPUsr1yVNdT8FLG-F36ujTgkdD5zhHRiJMiPQNqyOCanmv7fiu9wbFfPuuTouJoNmqEGzIjnas8_OZiGJz_3cpg2LLTYWKK2wmtb5ArA%3D%3D&amp;xkcb=SoD267M3CNkev1Rylh0PbzkdCdPP&amp;fccid=5b8d10630cb111eb&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location360 Westar Boulevard Westerville OH 43082 BenefitsPulled from the full job description401kDental insuranceHealth insurancePaid time offVision insurance Full job description Position would require travel 70 of the year  Sr Business Data Analyst   Are you a passionate leader looking for autonomy and exciting career possibilities Do you take an energetic and resourceful approach to problemsolving while bringing innovative ideas and analytics to life on behalf of your team and your customers Do you enjoy effectively translating requirements into an efficient process andor system solution If so DHL Supply Chain has the opportunity for you   Job Description  To apply knowledge and analytics to develop and communicate timely accurate and actionable insight to the business through the use of modeling visualization and optimization Responsible for the reporting analyzing and predicting of operational processes performance and Key Performance Indicators Communication with site leadership operations and finance on efficiency customer requirements account specific issues and insight into to the business operations and customer     Applies hindsight insight and foresight techniques to communicate complex findings and recommendations to influence others to take action  Uses knowledge of business and data structure to discover andor anticipate problems where data can be used to solve the problem  Uses spreadsheets databases and relevant software to provide ongoing analysis of operational activities  Applies data visualization for discovery and timely insights to decrease Cycle Time to Action CTA  Assists site operations in identifying areas for improving service levels reducing operational costs and providing other operational enhancements  Supports account startup analysis andor report implementation as needed  Develop standardized and ad hoc site andor customer reporting  Streamlines andor automates internal and external reporting  May investigate and recommend new technologies and information systems  May conduct feasibility analyses on various processes and equipment to increase efficiency of operations  Partners with Finance to develop financial models to analyze productivity and payroll calculates cost benefits and business impact and proposes solutions  Develops predictive models to help drive decision making  Designs develops and implements data gathering and reporting methods and procedures for Operations  Coordinates with Operations Systems group to ensure technical issues and problems are being identified addressed and resolved in a timely manner  May coordinate with ILD group on issues related to modeling customer solutions including providing data and relevant insight for customer pursuits  Responsible for assisting finance and senior leadership in modeling yearly labor budget based on operational and profile changes    Required Education and Experience   Undergraduate degree in business logistics mathematics statistics related field or equivalent experience required  1 years of analytics experience required    Our Organization has a business casual environment and focuses on teamwork associate development training and continuous improvement We offer competitive wages excellent affordable insurance benefits including health dental vision and life 401K plan paid vacation and holidays  Our Organization is an equal opportunity employer   Brand DHL    Address 360 Westar Blvd Westerville OH  43082    Property Description 9657  Westerville OH    Property Number 9657  </data></node>
<node id="n1715" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=9e1fc02c1cadb0c7&amp;bb=elsuOR9bLelBTDHLxukK-AL8lriRrKIDJyfqIyoJmW2nESI_s1u3GGp29wjBep-ezz2MqD_XgKpJaWQxqFuqImQNcDIyJ2miwgqv14bsnI0%3D&amp;xkcb=SoCX67M3CNkbJNQ8MZ0LbzkdCdPP&amp;fccid=d13187a50c252e33&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SnowflakeYesNoEducationDo you have a Doctoral degreeYesNo LocationArden Hills MN 55112 Full job description      Work mode Hybrid              Onsite Locations Arden Hills MN US 55112               Additional Locations NA        Diversity  Innovation  Caring  Global Collaboration  Winning Spirit  High Performance         At Boston Scientific we’ll give you the opportunity to harness all that’s within you by working in teams of diverse and highperforming employees tackling some of the most important health industry challenges With access to the latest tools information and training we’ll help you in advancing your skills and career Here you’ll be supported in progressing – whatever your ambitions          Impact Statement         Responsible for executing activities related to research and development of diagnostic and monitoring capabilities including identification evaluation planning execution and implementationtechnology transfer of new technologies features and products that offer innovative medical solutions to improve the health of patients around the world          Purpose         Responsible for the initiation design development execution and implementation of scientific RD projects          Key Responsibilities             Investigates the feasibility of applying scientific AIML principles and concepts to develop new technology process and product specifications                    Maintains substantial knowledge of stateoftheart principles and theories and contributes to scientific literature and conferences                    Participates in intellectual property evaluations and development of patent applications                    Consults and interacts with external and internal customers                    May coordinate interdepartmental activities and RD efforts                    Writes and submits professional articles for technical journals             Basic Qualifications             Advanced degree required  PhD in Computer Science Biomedical or Electrical engineering plus 3 or more years of related work experience or an equivalent combination of education and work experience                    Ability to independently analyze and interpret complex data sets for the development of novel AI powered solution                    Knowledge and experience in developing machine learning andor deep learning models using timeseries data            Preferred Qualifications             Experience in developing predictive and prescriptive analytics using a multimodal data and a broad spectrum of algorithm architectures decision trees logistic regression Bayesian analysis SVM DNN LSTM transformers and efficient use of data semisupervised learning transfer learning etc                    Experience developing Deep Learning models using biosignals                    Familiarity with data processing pipelines including cloud storage and compute infrastructures eg AWS Databricks Snowflake                    Proficiency in Python R andor MATLAB                    Well versed in statistics and theoretical foundations of Machine Learning                    Team player with good communication skills – written verbal and interpersonal             Quality System Requirements             In all actions demonstrates a primary commitment to patient safety and product quality by maintaining compliance to the Quality Policy and all other documented quality processes and procedures                    Assures that appropriate resources personnel tools etc are maintained in order to assure Quality System compliance and adherence to the BSC Quality Policy                    Establishes and promotes a work environment that supports the Quality Policy and Quality System             Requisition ID 580680              Among other requirements Boston Scientific maintains specific prohibited substance testing requirements for safetysensitive positions This role is deemed safetysensitive and as such candidates will be subject to a drug test as a preemployment requirement The goal of the drug testing is to increase workplace safety in compliance with the applicable law          As a leader in medical science for more than 40 years we are committed to solving the challenges that matter most – united by a deep caring for human life Our mission to advance science for life is about transforming lives through innovative medical solutions that improve patient lives create value for our customers and support our employees and the communities in which we operate Now more than ever we have a responsibility to apply those values to everything we do – as a global business and as a global corporate citizen                So choosing a career with Boston Scientific NYSE BSX isn’t just business it’s personal And if you’re a natural problemsolver with the imagination determination and spirit to make a meaningful difference to people worldwide we encourage you to apply and look forward to connecting with you                At Boston Scientific we recognize that nurturing a diverse and inclusive workplace helps us be more innovative and it is important in our work of advancing science for life and improving patient health That is why we stand for inclusion equality and opportunity for all By embracing the richness of our unique backgrounds and perspectives we create a better more rewarding place for our employees to work and reflect the patients customers and communities we serve Boston Scientific is proud to be an equal opportunity and affirmative action employer                Boston Scientific maintains a prohibited substance free workplace Pursuant to Va Code § 224312 2000 Boston Scientific is providing notification that the unlawful manufacture sale distribution dispensation possession or use of a controlled substance or marijuana is prohibited in the workplace and that violations will result in disciplinary action up to and including termination                Please be advised that certain US based positions including without limitation field sales and service positions that call on hospitals andor health care centers require acceptable proof of COVID19 vaccination status Candidates will be notified during the interview and selection process if the roles for which they have applied require proof of vaccination as a condition of employment Boston Scientific continues to evaluate its policies and protocols regarding the COVID19 vaccine and will comply with all applicable state and federal law and healthcare credentialing requirements As employees of the Company you will be expected to meet the ongoing requirements for your roles including any new requirements should the Company’s policies or protocols change with regard to COVID19 vaccination             Nearest Major Market Minneapolis  Job Segment RD Engineer Research Scientist RD Machinist Engineering Science Research Manufacturing  </data></node>
<node id="n1716" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e7024ccbfae5205f&amp;bb=elsuOR9bLelBTDHLxukK-NgN9wGCfvmIhN_TfmRkH5FuxzgBGsoWQrB5nRIQlpuZAcXoMFLDpIu8geUGA-nNhaGXwFf32J_Y3pvUdqgpgLQ%3D&amp;xkcb=SoAj67M3CNkbJNQ8MZ0KbzkdCdPP&amp;fccid=fe2d21eef233e94a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePayFrom 127300 a yearJob typeFulltime LocationNew York NY BenefitsPulled from the full job descriptionHealth insurance Full job description 5 years of data querying languages eg SQL scripting languages eg Python or statisticalmathematical software eg R SAS Matlab etc experience 4 years of data scientist experience Bachelors degree Experience with statistical models eg multinomial logistic regression  Calling all inventors to work on exciting new opportunities in Sponsored Products Amazon is building a world class advertising business and defining and delivering a collection of selfservice performance advertising products that drive discovery and sales of merchandise Our products are strategically important to our Retail and Marketplace businesses driving longterm growth Sponsored Products SP helps merchants retail vendors and brand owners grows incremental sales of their products sold on Amazon through native advertising SP achieves this by using a combination of machine learning big data analytics ultralow latency highvolume engineering systems and quantitative product focus We are a highly motivated collaborative and funloving group with an entrepreneurial spirit and bias for action    You will join a newlyfounded team with a broad mandate to experiment and innovate with a focus on driving growth of sponsored products ad experiences across Amazon stores worldwide This broad charter gives us the flexibility to explore and apply scientific techniques to novel product problems You will have the satisfaction of seeing your work improve the experience of millions of Amazon shoppers worldwide while driving quantifiable revenue impact More importantly you will have the opportunity to broaden your technical skills and be a science leader in an environment that thrives on creativity experimentation and product innovation    We are open to hiring candidates to work out of one of the following locations    New York NY USA     2 years of data visualization using AWS QuickSight Tableau R Shiny etc experience Experience managing data pipelines Experience as a leader and mentor on a data science team  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus    Our compensation reflects the cost of labor across several US geographic markets The base pay for this position ranges from 127300year in our lowest geographic market up to 247600year in our highest geographic market Pay is based on a number of factors including market location and may vary depending on jobrelated knowledge skills and experience Amazon is a total compensation company Dependent on the position offered equity signon payments and other forms of compensation may be provided as part of a total compensation package in addition to a full range of medical financial andor other benefits For more information please visit httpswwwaboutamazoncomworkplaceemployeebenefits This position will remain posted until filled Applicants should apply via our internal or external career site </data></node>
<node id="n1717" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=deadd5ef4a4b9201&amp;bb=elsuOR9bLelBTDHLxukK-JiT-25u1szjPgi_5-PnXYS2vp4Chh6MFP7smdoHDfkfpPlbOfILRGleNP7JNqc-sx_ETIxVH30oEWLEm-xXE7Y%3D&amp;xkcb=SoC-67M3CNkbJNQ8MZ0JbzkdCdPP&amp;fccid=734cb5a01ee60f80&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid AWS Certification certificationYesNoSkillsDo you have experience in Solution architectureYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay101200  194800 a yearJob typeFulltime LocationUnited States Full job description    With over 18000 employees worldwide the Microsoft Customer Experience  Success CES organization is responsible for the strategy design and implementation of Microsoft’s endtoend customer experience Come join CES as a Cloud Solution Architect  Azure AI  Partner Success and help us build a future where customers come to us not only because we provide industryleading products and services but also because we provide a differentiated and connected customer experience      The Global Customer Success GCS organization is leading the effort to create the desired customer experience through support offer creation driving digital transformation across our tools and delivering operational excellence across CES     We are looking for a Cloud Solution Architect CSA specializing in Azure AI who is passionate about driving our partners’ transformation on Microsoft Azure This is a partnerfacing role owning the technical relationship between the partner and Microsoft helping partners to leverage their Microsoft investments through architecture implementation and operational health engagements      Microsoft’s mission is to empower every person and every organization on the planet to achieve more As employees we come together with a growth mindset innovate to empower others and collaborate to realize our shared goals Each day we build on our values of respect integrity and accountability to create a culture of inclusion where everyone can thrive at work and beyond    Responsibilities  PartnerCustomer Centricity  Understand partners overall data estate Business and IT priorities and success measures to design Data  Analytics solutions that drive business value Partner Satisfaction  Drive positive Customer Satisfaction  become a trusted advisor  Ensure that solution exhibits high levels of performance security scalability maintainability repeatability appropriate reusability and reliability upon deployment CustomerPartner Insights  Provide feedback  insights from customerspartners Business Impact  Consumption Cloud  Support growth  Develop opportunities to drive Customer Success business results  help Customers get value from their Microsoft investments  Resolution of Customer Blockers  Identify resolutions to Customer blockers by leveraging SA subject matter expertise Deliver according to MS best practices  using repeatable Intellectual Property IP  Apply technical knowledge to architect and design solutions that meet business and IT needs create Data  Analytics roadmaps drive POCs and MVPs and ensure long term technical viability of new deployments infusing key AI technologies where appropriate Technical Leadership  Be the Voice of Partner to share insights and best practices connect with Engineering team to remove key blockers and drive product improvements Learn It All  Maintain technical skills and knowledge keeping up to date with market trends and insights collaborate and share with the AI technical community while educating customers on Azure platform Accelerate customer outcomes  Share expertise contribute to IP creation  reuse to accelerate customer outcomes Business Value The ability to convey the business need and value of proposed solutions plans and risks to stakeholders and decision makers This includes the ability to persuade and inform based on facts and alignment with goals and strategy Trusted Advisership The ability to build trusted advisor status and deep relationships across stakeholders eg technical decision makers business decision makers through an understanding of customer needs and technologies   o Situational fluency Using selfawareness as a mechanism to interpret verbal and nonverbal cues to increase your ability to read the room  o Insightful listening asking insightful questions to understand the customer needs issues business environment and drivers and going beyond what customer has said   Other      Embody our culture and values    Qualifications   RequiredMinimum Qualifications    Bachelors Degree in Computer Science Information Technology Engineering Business or related field AND 4 years experience in cloudinfrastructure technologies information technology IT consultingsupport systems administration network operations software developmentsupport technology solutions practice development architecture andor consulting      o OR equivalent experience      Additional or Preferred Qualifications    Bachelors Degree in Computer Science Information Technology Engineering Business or related field AND 8 years experience in cloudinfrastructure technologies information technology IT consultingsupport systems administration network operations software developmentsupport technology solutions practice development architecture andor consulting      o OR Masters Degree in Computer Science Information Technology Engineering Business or related field AND 6 years experience in cloudinfrastructure technologies technology solutions practice development architecture andor consulting       o OR equivalent experience     4 years experience working in a customerfacing role eg internal andor external  4 years experience working on technical projects  Technical Certification in Cloud eg Azure Amazon Web Services Google security certifications  Prior Technical experience      Breadth of technical experience and knowledge in foundational security foundational AI architecture design with depth  Subject Matter Expertise in one or more of the following        Deep domain expertise in one of the Dataspecific areas such as Azure SQL Data IaaSPaaS deployments and migrations to the cloud Opensource database deployments and migrations Cloud Scale Analytics Data Governance etc OR handson experience working with the respective products at the expertise level  Experience creating Data  Analytics Proof of Concepts PoC Minimum Viable Products MVPs for customers that lead to production deployments  Landscape Knowledge of key Data  Analytics platforms such as AWS GCP Snowflake etc  Software development practices like DevOps and CICD tool chains ie Jenkins Azure Developer Services GitHub and container orchestration systems ie Docker Kubernetes Cloud Foundry Azure Kubernetes Service GitHub        Cloud Solution Architecture IC4  The typical base pay range for this role across the US is USD 101200  194800 per year There is a different range applicable to specific work locations within the San Francisco Bay area and New York City metropolitan area and the base pay range for this role in those locations is USD 130000  213200 per year         Certain roles may be eligible for benefits and other compensation Find additional benefits and pay information here httpscareersmicrosoftcomusenuscorporatepay         Microsoft will accept applications for the role until April 8 2024     Microsoft is an equal opportunity employer Consistent with applicable law all qualified applicants will receive consideration for employment without regard to age ancestry citizenship color family or medical care leave gender identity or expression genetic information immigration status marital status medical condition national origin physical or mental disability political affiliation protected veteran or military status race ethnicity religion sex including pregnancy sexual orientation or any other characteristic protected by applicable local laws regulations and ordinances If you need assistance andor a reasonable accommodation due to a disability during the application process read more about requesting accommodations  </data></node>
<node id="n1718" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=677e1daae9becc28&amp;bb=elsuOR9bLelBTDHLxukK-ARLk3at5U9TuQ1BM33QpKuvOOu3hzMEiXs50fZOxZRMa2Hu34K2wqcUYlN3QbgAsesqXWBQoMESqJIvnIfkTyo%3D&amp;xkcb=SoAK67M3CNkbJNQ8MZ0IbzkdCdPP&amp;fccid=fe2d21eef233e94a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SDLCYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePayFrom 115000 a yearJob typeFulltime LocationArlington VA BenefitsPulled from the full job descriptionHealth insurance Full job description 3 years of noninternship professional software development experience 2 years of noninternship design or architecture design patterns reliability and scaling of new and existing systems experience Experience programming with at least one software programming language   This position is also open to candidates in Washington DC New York City     Amazon is investing heavily in building a world class advertising business and we are responsible for defining and delivering a collection of selfservice performance advertising products that drive discovery and sales Our products are strategically important to our Retail and Marketplace businesses driving long term growth We deliver billions of ad impressions and millions of clicks daily and are breaking fresh ground to create worldclass products We are highly motivated collaborative and funloving with an entrepreneurial spirit and bias for action With a broad mandate to experiment and innovate we are growing at an unprecedented rate with a seemingly endless range of new opportunities    Our systems and algorithms operate on one of the worlds largest product catalogs matching shoppers with advertised products with a high relevance bar and strict latency constraints We work handinhand with Machine Learning and NLP research scientists to come up with novel solutions that deliver highly relevant ads We consistently strive to improve the customer search and detail page experiences You will drive appropriate technology choices for the business lead the way for continuous innovation and shape the future of ecommerce This is an opportunity to make a significant impact on the future of the Amazon vision    As a Software Development Engineer in Machine Learning at Amazon you will drive the technical direction of our offerings and solutions working with many different technologies across the sponsored products organization You will design code troubleshoot and support scalable machinelearning pipelines and online serving systems You will work closely with applied scientists to optimize the performance of machinelearning models and infrastructure and implement endtoend solutions What you create is also what you own    We are open to hiring candidates to work out of one of the following locations    Arlington VA USA  New York NY USA     3 years of full software development life cycle including coding standards code reviews source control management build processes testing and operations experience Bachelors degree in computer science or equivalent  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus    Our compensation reflects the cost of labor across several US geographic markets The base pay for this position ranges from 115000year in our lowest geographic market up to 223600year in our highest geographic market Pay is based on a number of factors including market location and may vary depending on jobrelated knowledge skills and experience Amazon is a total compensation company Dependent on the position offered equity signon payments and other forms of compensation may be provided as part of a total compensation package in addition to a full range of medical financial andor other benefits For more information please visit httpswwwaboutamazoncomworkplaceemployeebenefits This position will remain posted until filled Applicants should apply via our internal or external career site </data></node>
<node id="n1719" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0ed511b366f8e422&amp;bb=elsuOR9bLelBTDHLxukK-AjKex1wLBiRe0-CkAgJDwekOWB1m3bk0W-1vR8Pr-f15JOaZejqac-496IF_azmiwparpRSX7q3S5YIQyRvs80%3D&amp;xkcb=SoCE67M3CNkbJNQ8MZ0PbzkdCdPP&amp;fccid=a51fd46bede29189&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in WorkdayYesNoEducationDo you have a Bachelors degreeYesNo LocationAtlanta GA BenefitsPulled from the full job description401k401k matchingDental insuranceDisability insuranceHealth insurancePaid parental leavePaid time offShow morechevron down Full job description Who We Are  FTI Consulting is the world’s leading expertdriven consulting firm Over the last 40 years FTI Consulting experts have served as the trusted advisor to Fortune 500 companies and the world’s leading law and private equity firms when they are facing their greatest opportunities and challenges Our strong performance and continued success are a direct reflection of the ambition energy and commitment of our talented professionals across the globe to make a positive impact for our clients and communities  At FTI Consulting you’ll be inspired and empowered to make an impact on headline matters that change history Working side by side with the world’s leading experts in your field you’ll be surrounded by an open collaborative culture that embraces diversity recognition professional development and most importantly you  There’s never been a more exciting time to join FTI Consulting and this is where you will do the most exciting and fulfilling work of your career  Are you ready to make an impact    About The Role  The Senior Data Analyst Human Resources Operations will play a pivotal role in supporting the daily operational aspects of FTI’s Core Operations groups and project support This position will report to the Director HR Operations They will collaborate closely with the Global Human Resources Service Center GHRSC leadership team as well as leaders in adjacent Core Operations teams to provide highlevel support for reporting insights and global projects impacting both Core Operations groups and Business Units directly    What You’ll Do  Analyze and interpret data to help solve complex business problems Conduct analyses on performance hiring talent planning data diversity survey results and other related topics Build models to support datadriven decisions Develop original insights and recommendations to support leadership discussions and decisions Prepare clear wellwritten presentations to facilitate discussions with leaders Present methodology analysis results and recommendations to internal clients Document action plans to drive change and track metrics over time Build crossdepartmental relationships by partnering with Core Operations and business leaders Support multiple complex crossfunctional projects as a key team member Review develop and communicate policies processes and system training Provide complex and highlevel reporting suitable for toplevel leaders across a variety of functions     How You’ll Grow  We are committed to investing and supporting you in your professional development and we have developed a range of programs focused on fostering leadership growth and development opportunities We aim to promote continuous learning and individual skills development through onthejob learning selfguided professional development courses and certifications You’ll be assigned a dedicated coach to mentor guide and support you through regular coaching sessions and serve as an advocate for your professional growth  As you progress through your career at FTI Consulting we offer tailored programs for critical professional milestones to ensure you are prepared and empowered to take on your next role    What You Will Need To Succeed  Basic Qualifications  Bachelors degree in Human Resources Business Data Analytics or a related field 5 years of relevant professional experience with data management analytics and insights Advanced proficiency in Microsoft Power BI report creation DAX Power Query and data modeling as well as in Microsoft Excel Power Pivot DAX advanced formula usage etc Ability to travel to clients and FTI Offices as needed    Preferred Qualifications  Direct experience with HR functions andor systems such as Workday PeopleSoft or ServiceNow Microsoft Certified Power BI Data Analyst Associate Microsoft PL300 Ability to explore data repositories and make intuitive deductions on how they connect Ability to work in a global complex matrixed structure where cooperation is often gained through influence rather than through direct command Demonstrated ability in influencing others and building strong professional relationships Strong interpersonal and communication skills both written and oral and the ability to work effectively with a wide range of constituencies    Our Benefits  Our goal is to support the wellbeing of you and your families—physically emotionally and financially We offer comprehensive benefits such as the following     Competitive total compensation including bonus earning potential Full package of benefits plans including medical dental and vision coverage along with life and disability insurance Generous paid time off Company matched 401k retirement savings plan Potential for flexible work arrangements Generous paid parental leave and flex return support Family care benefits including backup childelder care Employee wellness platform Employee recognition programs Paid time off for volunteering in your community Corporate matching for charitable donations most important to you Make an impact in our communities through company sponsored pro bono work Professional development and certification programs Free inoffice snacks and drinks Free smartphone and cellular plan if applicable FTI Perks  Discounts at retailers and businesses Upscale offices close to public transportation     About FTI Consulting       FTI Consulting is a global business advisory firm dedicated to helping clients with their most significant opportunities and challenges With more than 8000 employees located in 31 countries our broad and diverse bench of award–winning experts advise their clients when they are facing their most significant opportunities and challenges At FTI Consulting we embrace cultivate and maintain a culture of diversity inclusion  belonging which are fundamental components to our core values FTI Consulting is publicly traded on the New York Stock Exchange and has been named the 1 Professional Services Firm on Forbes List of America’s Best Employers and the best firm to work for by Consulting Magazine For more information visit wwwfticonsultingcom and connect with us on Instagram and LinkedIn      FTI Consulting is an equal opportunity employer and does not discriminate on the basis of race color national origin ancestry citizenship status protected veteran status religion physical or mental disability marital status sex sexual orientation gender identity or expression age or any other basis protected by law ordinance or regulation    </data></node>
<node id="n1720" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=268efff1adc66373&amp;bb=elsuOR9bLelBTDHLxukK-CssYH1KQVs1e42J_KzZ07NkqmjDcUfECkf8NBHJ92azBTR_x1ibTr88HbMTOnu7rPSqdN6tdglXYcqwE7rWpc0Z2-8VqQfRyQ%3D%3D&amp;xkcb=SoAw67M3CNkbJNQ8MZ0ObzkdCdPP&amp;fccid=cd21fdfdf779d657&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in StatisticsYesNoEducationDo you have a Masters degreeYesNo LocationRemote BenefitsPulled from the full job description401k401k 3 Match401k matchingPaid parental leaveParental leaveWork from home stipend Full job description  About Us                                                       Since 2016 dbt Labs has been on a mission to help analysts create and disseminate organizational knowledge dbt Labs pioneered the practice of analytics engineering built the primary tool in the analytics engineering toolbox and has been fortunate enough to see a fantastic community coalesce to help push the boundaries of the analytics engineering workflow Today there are 30000 companies using dbt every week 100000 dbt Community members and over 4100 dbt Cloud customers You can learn more about our values here                                                         Overview  dbt Labs is seeking an experienced Senior Data Analyst to lead and drive insights and analysis connected to dbt Lab’s marketing and traffic This role will be responsible for providing regular analysis and insight to marketing leaders to inform optimizations across onsite and offsite campaigns This role will be expected to proactively identify trends and facilitate analysis to identify issues and opportunities that impact our marketing campaign effectiveness The ideal Senior Data Analyst will have an extensive background in running SQL queries designing marketing dashboards driving channel optimization analysis projects and generating actionable marketing insights  This is a critical role for the organization and requires an individual with effective communication skills outstanding business acumen and the ability to work across various functional teams and internal partner teams We are looking for individuals who are selfstarters comfortable with ambiguity demonstrate strong attention to detail and have the ability to work in a fastpaced and everchanging environment  In this role you can expect to  Work closely with business stakeholders analytics engineers and data engineers to deliver actionable insights and datadriven strategies Define and analyze metrics that inform the success of marketing programs and allow us to monitor the health of product adoption and internal tooling and processes Optimize funnels to reduce friction and drive the best possible user experience Identify trends across our various campaigns to influence growth strategy and maximize long term efficiency Execute on projects by working closely with marketing operations and finance teams to make a significant impact on gotomarket goals  You are a good fit if you have  5 years of experience working in marketing analytics Worked asynchronously as part of a fullyremote distributed team Proven to effectively collaborate across variety of technical and nontechnical functions to drive towards common marketing goals Proficiency in quantitative analysis geared towards drawing actionable insights from complex datasets Experience performing exploratory analysis with minimal direction to answer ambiguous open ended questions Experience with data querying languages eg SQL  Youll have an edge if you have  Have a Master’s degree in Data Analytics Applied Analytics Operations Research Mathematics Statistics or a related field Experience developing and scaling a dbt project Experience designing multivariate tests synthesizing test results and building frameworks to make datainformed launch decisions  Compensation  Benefits  Salary 140000  180000 USD Equity Stake Benefits In the US dbt Labs offers unlimited vacation and yes we use it 401k w3 guaranteed contribution excellent healthcare paid parental leave wellness stipend and a home office stipend For employees outside the United States dbt Labs offers a competitive benefits package Benefits  dbt Labs offers      Unlimited vacation and yes we use it 401k w3 guaranteed contribution Excellent healthcare Paid Parental Leave Wellness stipend Home office stipend and more     Equity or comparable benefits may be offered depending on the legal limitations   What to expect in the hiring process all video interviews unless accommodations are needed   Interview with Talent Acquisition Partner   Interview with Hiring Manager   Team Interviews   Final interview with one of our Values Carriers      Who we are  At dbt Labs we have developed strong opinions on how companies should practice analytics  Specifically we believe that  Code not graphical user interfaces is the best abstraction to express complex analytic logic Data analysts should adopt similar practices and tools to software developers Critical analytics infrastructure should be controlled by its users as open source software Analytic code itself — not just analytics tools — will increasingly be open source  It turns out that a lot of other people believe this too Today there are 30000 companies using dbt every week 100000 dbt Community members and 4100 companies paying for dbt Cloud Our customers include JetBlue Hubspot Vodafone New Zealand and Dunelm dbt is synonymous with the practice of analytics engineering defining an entire industry We’re backed by top investors including Andreessen Horowitz Sequoia Capital and Altimeter  dbt Labs is an equal opportunity employer Were committed to building an inclusive team that welcomes a diversity of perspectives people and backgrounds regardless of race color national origin gender sexual orientation age religion disability citizenship veteran status or any other protected status We feel strongly that whether or not your experience exactly fits the job description your passion and skills will stand out and set you apart even if your career has taken some twists and turns If you are on the fence about whether you meet our requirements we encourage you to apply anyway Please reach out to us directly at recruitingdbtlabscom if you need assistance or an accommodation  Want to learn more about our focus on Diversity Equity and Inclusion at dbt Labs Check out our DEI page  dbt Labs reserves the right to amend or withdraw the posting at any time For employees outside the United States dbt Labs offers a competitive benefits package Equity or comparable benefits may be offered depending on the legal or country limitations    </data></node>
<node id="n1721" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=4570ae74b9d5c0b5&amp;bb=elsuOR9bLelBTDHLxukK-Jbn-eVwCDe1clw7VD3L3x3dtPD5HKbF3jV27jJF4_cO9ByjOP2pKx2ApUsl4n2gUi81PpsHjAl0a9s0EPAWEqI%3D&amp;xkcb=SoCt67M3CNkbJNQ8MZ0NbzkdCdPP&amp;fccid=78bbcd26e39621f5&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in User acceptance testingYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location401 South Tryon Street Charlotte NC 28202 Full job description About this role   Wells Fargo is seeking a Senior Data Product Management Consultant within the Consumer Data Analytics and Artificial Intelligence group CDAI who will be responsible for supporting the Product Ownerscrum team to deliver product features based on business priorities This role will be responsible for defining data requirements across ingestion integration transformation and provisioning based on business requirements    The Data Consultant will be expected to bring significant technical skill in SQL Hive Drools API development etc and will be responsible for obtaining and translating business rules requirements into technical solutions The Data Consultant will work in a highly matrixed organization with the Product Owner Scrum Master management and analyst support and across SMEs in other CDP and CDAI Leads    Learn more about the career areas and lines of business at httpswwwwellsfargojobscom      In this role you will   Lead datatechnical design sessions working with SMEs to understand and capture functional and nonfunctional design and rules requirements  Identify ingestion integration transformation and provisioning needs conducting detailed review of articles such as API code data element mappings etc to confirm mutual understanding of both needs and potential deliverables  Write and story point user stories to capture ingestion integration transformation and provisioning needs and working with the Product Owner to confirm that stories are assigned to sprints in accordance with delivery expectations and priorities  Conduct source to target mapping conducting data lineage exercises in an environment with minimal metadata to identify root source of record and capture necessary transformation to the data from source of record to the point of presentationconsumption  Generate and configure rules in a rules library cross referencing existing rules to generate efficiency in new product implementation  Help resolve data related execution issuesrisks and data defect remediation activities during developmentdeployment activities  Perform data analysis to decompose data transformations and understand logic for data elements  Assist with conceptual and logical data model design and requirements definition  Raise data related execution issuesrisks to Product Owner and help identify remediation approach  Support testing activities including user acceptance testing  Support data defect remediation activities during developmentdeployment activities  Mentor less experienced Data Analysts  Required Qualifications   4 years of data product or data management experience or equivalent demonstrated through one or a combination of the following work experience training military experience education  Agile experience including story writing and working in a scrum team  Demonstrated experience working in Jira  Proven experience performing data mapping and lineage  Strong SQL  HiveQL skills  QA  UAT skills  Experience performing business requirement analysis  Desired Qualifications   Experience within the Wells Fargo Consumer data environment  or equivalent experience from another leading financial services company  Familiarity with Hadoop  HDFS  Hive  Familiarity with Teradata  Familiarity with Python or a similar programming language for data analysis  data wrangling  Familiarity with Hue Dremio or other similar data workbenches  Experience with Cloudfirst data architecture  Prior exposure to data governance documentation and standards  Ability to prioritize work meet deadlines achieve goals and work under pressure in a dynamic and complex environment  Strong analytical skills with high attention to detail and accuracy  Broad experience with Banking applications products technology architecture and systems integration experience in consumer and small business banking consumer lending and wealth and investment management  Highly developed interpersonal and communication skills proven ability to engage negotiate collaborate and influence senior technology and business executives  Job Expectations   This position offers a hybrid work schedule  Locations   401 S Tryon St Charlotte North Carolina  Required location listed above Relocation assistance is not available for this position    This position is not eligible for Visa sponsorship    Please note Based on the volume of applications received this job posting may be removed prior to the indicated close date    Posting End Date  1 Apr 2024   Job posting may come down early due to volume of applicants   We Value Diversity   At Wells Fargo we believe in diversity equity and inclusion in the workplace accordingly we welcome applications for employment from all qualified candidates regardless of race color gender national origin religion age sexual orientation gender identity gender expression genetic information individuals with disabilities pregnancy marital status status as a protected veteran or any other status protected by applicable law    Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliancedriven culture which firmly establishes those disciplines as critical to the success of our customers and company They are accountable for execution of all applicable risk programs Credit Market Financial Crimes Operational Regulatory Compliance which includes effectively following and adhering to applicable Wells Fargo policies and procedures appropriately fulfilling risk and compliance obligations timely and effective escalation and remediation of issues and making sound risk decisions There is emphasis on proactive monitoring governance risk identification and escalation as well as making sound risk decisions commensurate with the business units risk appetite and all risk and compliance program requirements    Candidates applying to job openings posted in US All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability status as a protected veteran or any other legally protected characteristic    Candidates applying to job openings posted in Canada Applications for employment are encouraged from all qualified candidates including women persons with disabilities aboriginal peoples and visible minorities Accommodation for applicants with disabilities is available upon request in connection with the recruitment process    Applicants with Disabilities   To request a medical accommodation during the application or interview process visit Disability Inclusion at Wells Fargo     Drug and Alcohol Policy   Wells Fargo maintains a drug free workplace Please see our Drug and Alcohol Policy to learn more  </data></node>
<node id="n1722" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=cc0733fb1c68f5bd&amp;bb=elsuOR9bLelBTDHLxukK-HUBcN3NBzVuKkPh_dvL6_-iz_ELxQKxTY7md9wCZjh1epiOwXKZMlzEP3CP00BiS8lQLA7W_OgDMaEZ65rUyoDb2ZcRY8lgGw%3D%3D&amp;xkcb=SoAZ67M3CNkbJNQ8MZ0MbzkdCdPP&amp;fccid=f80b1ab2ecf7173c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Supervising experienceYesNo Job detailsHere’s how the job details align with your profilePay734925  988941 a monthJob typeFulltime Location1701 North Congress Avenue Austin TX 78701 Full job description  Senior School Finance Data Analyst Data Analyst VI 00041212   Organization TEXAS EDUCATION AGENCY        Primary Location TexasAustin     Work Locations Texas Education Agency 1701 NORTH CONGRESS AVENUE Austin 78701          Job Computer and Mathematical        Employee Status Regular        Schedule Fulltime     Standard Hours Per Week 4000        Travel Yes 5  of the Time        State Job Code 0655        Salary Admin Plan B        Grade 28        Salary Pay Basis 734925  988941 Monthly        Number of Openings 1        Overtime Status Exempt        Job Posting Mar 29 2024 103426 AM        Closing Date Apr 12 2024 115900 PM        Description   MISSION The Texas Education Agency TEA will improve outcomes for all publicschool students in the state by providing leadership guidance and support to school systems   Core Values   We are Determined We are committed and intentional in the pursuit of our main purpose to improve outcomes for students We are PeopleCentered We strive to attract develop and retain the most committed talent representing the diversity of Texas each contributing to our common vision for students We are Learners We seek evidence reflect on success and failure and try new approaches in the pursuit of excellence for our students We are Servant Leaders Above all else we are public servants working to improve opportunities for students and provide support to those who serve them    POSITION OVERVIEW The Senior School Finance Data Analyst reports to the Chief School Finance Officer CSFO and is responsible for supporting the work of the State Funding Forecasting  Fiscal Analysis Division and as needed the Financial Compliance Division The position performs data analysis research and statistical modeling for funding calculations including financial forecasting and reporting The Senior Data Analyst under the direction of the CSFO supports the preparation of agency financial documents presentations and other documents such as fiscal notes and the Legislative Appropriations Request The work is highly complex and conducted under limited supervision with extensive latitude for the use of independent judgment  Flexible work location within the state of Texas may be considered for qualified candidates  Please note that a resume tailored cover letter and short answer responses are required attachments for applying to this position Incomplete applications will not be considered Additionally applicants who are strongly being considered for employment must submit to a national criminal history background check  ESSENTIAL FUNCTIONS Job duties are not limited to the essential functions mentioned below You may perform other functions as assigned  1 Modeling and Analysis  Performs advanced seniorlevel statewide school finance funding formula calculation and modeling work including analyzing the potential impact of proposed changes to the school finance system 2 Supporting and Collaborating  Supports the design development delivery and management of legislative inquiries and projections of statewide school finance formulas and the implementation of new legislative requirements collaborates with other internal teams on school finance funding calculations and systems 3 Training and Mentoring  Trains and supports the development of analysts and modelers in the department including reviewing others’ analysis and modeling providing technical advice to team members and TEA stakeholders on methodologies for costing out school finance changes 4 Coding and Documenting  Reviews and recommends updates to existing Oracle database SAS code models including Foundation School Program and Available School Fund calculation code while documenting and explaining changes to methodologies in code changes 5 Leadership Support  As directed by the Chief School Finance Officer supports the preparation review and submission of policy analysis and TEA fiscal notes       Qualifications    MINIMUM QUALIFICATIONS   Education Graduation from an accredited fouryear college or university Experience At least five 5 years of experience in data analysis research compilation andor reporting work andor school finance at the local or state level Ten 10 years of experience is preferred Substitutions An advanced degree may substitute for two years of required experience     OTHER QUALIFICATIONS   Share the belief that all Texas students can achieve at high levels and are able to succeed in college career or the military Strong analytical and critical thinking skills A problem solver with great attention to detail and the ability to work with little supervision multitask and learn quickly Experience conducting indepth analysis of complicated systems and with analyzing complex decision alternatives to make recommendations Knowledge of statistics and analyzing data sets running queries report writing and presenting findings and record keeping including security procedures for handling protecting and distributing confidential data Ability to compile review and analyze data to prepare reports to maintain accuracy and attention to detail and to communicate effectively Knowledge of data models database design development data mining and segmentation techniques Knowledge of local state and federal laws and regulations related to Texas budget education and tax laws and experience working with the Texas Governor’s Office legislators andor other state or local entities Five 5 years of statelevel experience in appropriations legislative processes tax policy school finance policy education andor other fiscal matters is preferred Five 5 years of experience in programming financial models for legislative fiscal notes including school finance models is preferred Eight 8 years of experience is highly preferred Five 5 years of experience in each of the following areas SAS programming large relational databases and advanced Microsoft Excel work is preferredTen 10 years of experience is highly preferred    As an equal opportunity employer we hire without consideration to race religion color national origin sex disability age or veteran status unless an applicant is entitled to the military employment preference   To review the Military Occupational Specialty MOS codes from each branch of the US Armed Forces to each job classification series in the State’s Position Classification Plan provided by the State Auditors Office please access the Military Crosswalk occupational specialty code Guide and click on the military “occupational category” that corresponds with the state classification in this job posting title      This position requires the applicant to meet Agency standards and criteria which may include passing a preemployment criminal background check prior to being offered employment by the Agency   No phone calls or emails please Due to the high volume of applications we do not accept telephone calls and cannot reply to all email inquiries Only candidates selected for interview will be contacted   </data></node>
<node id="n1723" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=64454dd27e56995d&amp;bb=elsuOR9bLelBTDHLxukK-PnRA-7d1laO2FuxZrEMBTQlyj6wB5bM9tL9qeotdVT09MwqrtzPpyqYH0RQhTUSA1BE6N9vZOzuY-oX9i71oSAQIjGJlCRuyA%3D%3D&amp;xkcb=SoDw67M3CNkbJNQ8MZ0DbzkdCdPP&amp;fccid=c38b7d5e0419a6a7&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid Hazmat Endorsement licenseYesNoSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay111400  175300 a yearJob typeFulltime LocationRahway NJ 07065 BenefitsPulled from the full job descriptionHealth insuranceRetirement plan Full job description Job Description   At our company we are leveraging analytics and technology as we invent for life on behalf of patients around the world We are seeking those who have a passion for using data analytics and insights to drive decision making that will allow us to tackle some of the world’s greatest health threats   Within our Human Health Digital Data  Analytics organization we are transforming to better power decisionmaking across our endtoend commercialization process from business development to late lifecycle management As we endeavor we are seeking a dynamic talent for a Business Process Analyst – Vendor Management in our Operational Excellence department Senior Specialist   In this role you will report to the Program Director and will support the development and implementation of vendor performance management framework for HHDDA The assignments will involve work with teams procurement function and vendor partners performing deep dives and analyses for expanding the preferred vendor program maintaining policies and tools and assisting in designing Vendor KPIs and scorecards   Detailed Roles  Assignments   Support the development and implementation of a comprehensive vendor performance management framework including KPIs and measurement techniques  collaborating with crossfunctional teams and vendors  Assists with the development and maintenance of comprehensive vendor management processes – design and reengineering  Work alongside the Program Lead and Program Manager to run performance and risk reviews and conduct business reviews with HHDDA vendors  Provide analyses and design initiatives for expanding the preferred vendor program at HHDDA working closely with Procurement and Internal stakeholders  Design methodologies to capture and measure the value derived from vendor management initiatives including tracking mechanisms and data analysis  Synthesize and analyze vendor data including compliance risk financial and workforce Communicatepresent the data findings in an understandable way to HHDDA leadership  Accountable for maintaining and updating policies standards and tools related to Vendor Management  Partner with Reporting  Metrics team in designing KPIs vendor scorecards and reports  Collaborate with Financial Management team in analyzing financial data assessing contract terms and conditions and managing invoices as needed    Location Rahway NJ Hybrid   Education  Skills   BABS Degree in Business Finance Economics Mathematics MIS  Min 5 years of experience in analytical  quantitative skills including but not limited to analyzing and interpreting data  Min 2 years of working knowledge with vendor management Procurement vendor performance and risk reviews KPIs and scorecards  Demonstrated capability to simplify complex informationprocesses within the team and communicate the insights trends or issues with senior stakeholders  Ability to work well within global crossfunctional teams and partner effectively within all levels of the organization  An out of the box thinker who has ability to work objectively is soon solutions oriented proactive has good judgment and “growth mindset”  Ability to work with agility in a fastpaced multitask environment that demands initiative and communication alertness  Excellent communication skills written and oral presentation and organization skills  Working Knowledge of procurement systems – Ariba Fieldglass  Familiarity with data visualization tools like Tableau Power BI and Data Manipulation with R Python or similar is a plus  Prior consulting experience is a plus    NOTICE FOR INTERNAL APPLICANTS  In accordance with Managers Policy  Job Posting and Employee Placement all employees subject to this policy are required to have a minimum of twelve 12 months of service in current position prior to applying for open positions   If you have been offered a separation benefits package but have not yet reached your separation date and are offered a position within the salary and geographical parameters as set forth in the Summary Plan Description SPD of your separation package then you are no longer eligible for your separation benefits package To discuss in more detail please contact your HRBP or Talent Acquisition Advisor  Employees working in roles that the Company determines require routine collaboration with external stakeholders such as customerfacing commercial or researchbased roles will be expected to comply not only with Company policy but also with policies established by such external stakeholders for example a requirement to be vaccinated against COVID19 in order to access a facility or meet with stakeholders Please understand that as permitted by applicable law if you have not been vaccinated against COVID19 and an essential function of your job is to call on external stakeholders who require vaccination to enter their premises or engage in facetoface meetings then your employment may pose an undue burden to business operations in which case you may not be offered employment or your employment could be terminated Please also note that where permitted by applicable law the Company reserves the right to require COVID19 vaccinations for positions such as in Global Employee Health where the Company determines in its discretion that the nature of the role presents an increased risk of disease transmission   Current Employees apply HERE   Current Contingent Workers apply HERE   US and Puerto Rico Residents Only   Our company is committed to inclusion ensuring that candidates can engage in a hiring process that exhibits their true capabilities Please click here if you need an accommodation during the application or hiring process  We are an Equal Opportunity Employer committed to fostering an inclusive and diverse workplace All qualified applicants will receive consideration for employment without regard to race color age religion sex sexual orientation gender identity national origin protected veteran status or disability status or other applicable legally protected characteristics For more information about personal rights under the US Equal Opportunity Employment laws visit  EEOC Know Your Rights  EEOC GINA Supplement  Pay Transparency Nondiscrimination   We are proud to be a company that embraces the value of bringing diverse talented and committed people together The fastest way to breakthrough innovation is when diverse ideas come together in an inclusive environment We encourage our colleagues to respectfully challenge one another’s thinking and approach problems collectively   Learn more about your rights including under California Colorado and other US State Acts   US Hybrid Work Model   Effective September 5 2023 employees in officebased positions in the US will be working a Hybrid work consisting of three total days onsite per week generally Tuesday Wednesday and either Monday or Thursday although the specific days may vary by site or organization with Friday designated as a remoteworking day unless business critical tasks require an onsite presence This Hybrid work model does not apply to and daily inperson attendance is required for fieldbased positions facilitybased manufacturingbased or researchbased positions where the work to be performed is located at a Company site positions covered by a collectivebargaining agreement unless the agreement provides for hybrid work or any other position for which the Company has determined the job requirements cannot be reasonably met working remotely Please note this Hybrid work model guidance also does not apply to roles that have been designated as “remote”   Under New York State Colorado State Washington State and California State law the Company is required to provide a reasonable estimate of the salary range for this job Final determinations with respect to salary will take into account a number of factors which may include but not be limited to the primary work location and the chosen candidate’s relevant skills experience and education  Expected salary range 11140000  17530000    Available benefits include bonus eligibility health care and other insurance benefits for employee and family retirement benefits paid holidays vacation and sick days For Washington State Jobs a summary of benefits is listed here   Search Firm Representatives Please Read Carefully Merck  Co Inc Rahway NJ USA also known as Merck Sharp  Dohme LLC Rahway NJ USA does not accept unsolicited assistance from search firms for employment opportunities All CVs  resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no preexisting agreement is in place Where agency agreements are in place introductions are position specific Please no phone calls or emails   Employee Status Regular    Relocation No relocation    VISA Sponsorship No    Travel Requirements 10    Flexible Work Arrangements Hybrid    Shift Not Indicated    Valid Driving License No    Hazardous Materials NA    Required Skills Business Intelligence BI Data Management Data Modeling Data Visualization Metrology Project Management Stakeholder Relationship Management    Preferred Skills  Requisition IDR287825  </data></node>
<node id="n1724" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=48f4f57b96d3cf42&amp;bb=elsuOR9bLelBTDHLxukK-AqFf0qm-ercL5hFcPWKbhlWf8vyX_UY6Tq8WMjYqkayskxhElaYT7Xfr9MT-jh68s-O8rDVAh-2qs4vNXJ0T1E%3D&amp;xkcb=SoBE67M3CNkbJNQ8MZ0CbzkdCdPP&amp;fccid=fe2d21eef233e94a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePayFrom 134500 a yearJob typeFulltime LocationSeattle WA BenefitsPulled from the full job descriptionHealth insuranceWork from home Full job description Experience building complex software systems that have been successfully delivered to customers  AWS Neuron is the complete software stack for the AWS Inferentia and Trainium cloudscale machine  learning accelerators and the Trn1 and Inf1 servers that use them This role is for a senior software engineer in the Machine Learning Applications ML Apps team for AWS Neuron    This role is responsible for development enablement and performance tuning of a wide variety of ML model families including massive scale large language models like Llama2 GPT2 GPT3 and beyond as well as stable diffusion Vision Transformers and many more    The ML Apps team works side by side with chip architects compiler engineers and runtime engineers to create build and tune distributed inference solutions with Trn1 Experience optimizing inference performance for both latency and throughput on these large models using Python JAX is a must Deepspeed and other distributed inference libraries are central to this and extending all of this for the Neuron based system is key    Key job responsibilities  This role will help lead the efforts building distributed training and inference support into Pytorch Tensorflow using XLA and the Neuron compiler and runtime stacks This role will help tune these models to ensure highest performance and maximize the efficiency of them running on the customer AWS Trainium and Inferentia silicon and the TRn1  Inf1 servers Strong software development using CPython and ML knowledge are both critical to this role    A day in the life  As you design and code solutions to help our team drive efficiencies in software architecture you’ll create metrics implement automation and other improvements and resolve the root cause of software defects You’ll also    Build highimpact solutions to deliver to our large customer base  Participate in design discussions code review and communicate with internal and external stakeholders  Work crossfunctionally to help drive business decisions with your technical input  Work in a startuplike development environment where you’re always working on the most important stuff    About the team  Our team is dedicated to supporting new members We have a broad mix of experience levels and tenures and we’re building an environment that celebrates knowledgesharing and mentorship Our senior members enjoy oneonone mentoring and thorough but kind code reviews We care about your career growth and strive to assign projects that help our team members develop your engineering expertise so you feel empowered to take on more complex tasks in the future    Diverse Experiences  AWS values diverse experiences Even if you do not meet all of the qualifications and skills listed in the job description we encourage candidates to apply If your career is just starting hasn’t followed a traditional path or includes alternative experiences don’t let it stop you from applying    About AWS  Amazon Web Services AWS is the world’s most comprehensive and broadly adopted cloud platform We pioneered cloud computing and never stopped innovating — that’s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses    Inclusive Team Culture  Here at AWS it’s in our nature to learn and be curious Our employeeled affinity groups foster a culture of inclusion that empower us to be proud of our differences Ongoing events and learning experiences including our Conversations on Race and Ethnicity CORE and AmazeCon gender diversity conferences inspire us to never stop embracing our uniqueness    WorkLife Balance  We value worklife harmony Achieving success at work should never come at the expense of sacrifices at home which is why we strive for flexibility as part of our working culture When we feel supported in the workplace and at home there’s nothing we can’t achieve in the cloud    Mentorship  Career Growth  We’re continuously raising our performance bar as we strive to become Earth’s Best Employer That’s why you’ll find endless knowledgesharing mentorship and other careeradvancing resources here to help you develop into a betterrounded professional    Hybrid Work  We value innovation and recognize this sometimes requires uninterrupted time to focus on a build We also value inperson collaboration and time spent facetoface Our team affords employees options to work in the office every day or in a flexible hybrid work model near one of our US Amazon offices Our hybrid models allow you the freedom to work from home whenever inoffice collaboration isn’t necessary    We are open to hiring candidates to work out of one of the following locations    Seattle WA USA     Masters degree in computer science or equivalent  Amazon is committed to a diverse and inclusive workplace Amazon is an equal opportunity employer and does not discriminate on the basis of race national origin gender gender identity sexual orientation protected veteran status disability age or other legally protected status For individuals with disabilities who would like to request an accommodation please visit httpswwwamazonjobsendisabilityus    Our compensation reflects the cost of labor across several US geographic markets The base pay for this position ranges from 134500year in our lowest geographic market up to 261500year in our highest geographic market Pay is based on a number of factors including market location and may vary depending on jobrelated knowledge skills and experience Amazon is a total compensation company Dependent on the position offered equity signon payments and other forms of compensation may be provided as part of a total compensation package in addition to a full range of medical financial andor other benefits For more information please visit httpswwwaboutamazoncomworkplaceemployeebenefits This position will remain posted until filled Applicants should apply via our internal or external career site </data></node>
<node id="n1725" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=96a3bdf9de2d6a9e&amp;bb=elsuOR9bLelBTDHLxukK-CT56-Y4dsxXY5DJ93XyleMlQ7PJiWVxTfFwzB4jwEqM2MFSjIAVffPsWCpNLidYBq0ILR_U1dPhzRu_ul9gvBM%3D&amp;xkcb=SoDZ67M3CNkbJNQ8MZ0BbzkdCdPP&amp;fccid=1356e3241d5dfd13&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Visual BasicYesNoEducationDo you have a Doctoral degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationLincoln NE Full job description         Details     Posted 28Mar24 Location Lincoln Nebraska Type Full Time  Years of Experience      Less than 2      Preferred Education      Doctorate        The Department of Biological Systems Engineering at the University of NebraskaLincoln UNL Institute of Agriculture and Natural Resources IANR is seeking applications for an Agricultural Data Scientist at the rank of research assistant professor This 12month calendar year nontenure track appointment will collaborate with our existing interdisciplinary team in further development of data collection and analytical tools for integrating advanced research tools into typical crop production data systems The apportionment is 100 research This position will be located in Lincoln Nebraska though some instate travel is expected The Agricultural Data Scientist will contribute to the integrated research landgrant mission of the home unit and IANR as an effective scholar and citizen including supporting student recruitment and IANR science literacy Position Duties   Collaborate with an existing interdisciplinary team consisting of agricultural engineers agronomists economists statisticians and computer scientists to further the development of data collection and analytical tools for integrating advanced research tools into typical crop production data systems and providing novel solutions to agricultural data management issues   Develop hardware eg IoT and embedded systems and software platforms for collecting postprocessing managing and analyzing data layers of varying spatial resolutions and temporal frequencies to extract crop performance information   Develop software platforms for analyzing various datasets generated from agricultural field equipment combined with data streaming from research systems eg satellite aerial or UAV images   Develop analytical software tools capable of generating graphs charts reports andor other actionable items for future use by agricultural professionals throughout the crop production cycle   The incumbent is expected to secure extramural funding from federal and state agencies industry and private foundations as well as publish in peerreviewed journals and at national and international meetings The incumbent may be required to provide supervision to undergraduate and graduate students and serve on graduate committees In addition to the abovedescribed duties the individual will be expected to accept committee assignments reporting responsibilities and other special ad hoc assignments as requested at the administrative unit collegedivision institute andor university level  Minimum Required Qualifications  PhD in Biological Systems Engineering Agricultural Engineering Food Engineering Computational Sciences Arts and Sciences or a closely related field Demonstrated research experience in experimental data collection postprocessing and statistical analysis resulting in peerreviewed publications  Preferred Qualifications  Prior experience using database networking GIS andor analytical software such as SQL MATLAB Python ArcGIS or other similar software Programming experience with C VB R or related languages and working knowledge of database creation and statistical analysis methods Demonstrated experience with advanced sensing systems eg optical magnetic etc data collection Demonstrated experience in programming advanced analysis tools that could be deployed remotely by others to analyze datasets Working knowledge or understanding of agricultural crop production systems and data sets related to precision agriculture    </data></node>
<node id="n1726" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=3e862abbd7a94cbe&amp;bb=elsuOR9bLelBTDHLxukK-P_i-MRfwgAlTEGVrdWJS4oRD_KtulDPBXetGXSprCuSNxHxEMRw9y6GslhHIjio4QYQx77H3BUwvIasG37j6jw%3D&amp;xkcb=SoBt67M3CNkbJNQ8MZ0AbzkdCdPP&amp;fccid=aa766df99c78f464&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SASYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay113000  161000 a yearJob typeFulltime Location1 Edwards Way Irvine CA 92614 Full job descriptionPatients with mitral and tricuspid heart valve disease often have complex conditions with limited treatment options Our Transcatheter Mitral and Tricuspid Therapies TMTT business unit is boldly pursuing an innovative portfolio of technologies to address a patient’s unmet clinical needs It’s our driving force to help patients live longer and healthier lives Join us and be part of our inspiring journey    Imagine how your ideas and expertise can change a patient’s life We generate extensive clinical evidence to demonstrate the effectiveness and safety of our innovations and how our products transform patients’ lives As part of our Clinical Affairs team you’ll hone your scientific curiosity and passion for evaluating data to increase access to pioneering technologies for patients in need In close partnership with principal investigators dedicated medical professionals patient advocacy groups and regulatory authorities you will drive the evidence needed to optimize patient outcomes    How you’ll make an impact   Plan the execution of programs in statistical programming languages within project Develop complex analysis datasets specifications and summary output tables listings or graphs for inclusion in clinical reports or presentations Create complex programs that meet regulatory and company standards to permit efficient programming reporting and review utilizing statistical programming languages eg SAS Review and validate statistical programs and ensure that all appropriate program validation documentation to meet regulatory and company standards are consistently structured to permit efficient programming reporting and review Perform complex analysis in response to data requests in collaboration with designated statistician Collaborate with CDM Clinical Data Management and designated statistician to review draft CRFs Case Report Forms database specifications and perform edit checks  Other duties assigned by Leadership    What youll need Required  Bachelors Degree in Statistics Mathematics Computer Science or related field with 4 years of previous statistical programming experience eg SAS Required or  Masters Degree or equivalent in Statistics Mathematics Computer Science or related field with 2 years experience previous statistical programming experience eg SAS Required    What else we look for Preferred   Proven expertise in interfacing MS Office Suite with SAS Good written and verbal communication skills and interpersonal relationship skills including consultative and relationship management skills Full understanding and knowledge relevant to statistical programming Full understanding and knowledge of regulatory guidelines eg GCP ICH FDA ISO relevant to in PharmaceuticalMedical Device setting Strong problemsolving organizational analytical and critical thinking skills Experience working in clinical trials within a pharmaceuticalmedical device industry Good leadership skills and ability to influence change Strict attention to detail Ability to interact professionally with all organizational levels Ability to manage competing priorities in a fast paced environment Must be able to work in a team environment including interdepartmental teams and key contact representing the organization on projects Ability to interact with senior internal and external personnel on significant matters often requiring coordination between organizations Adhere to all company rules and requirements eg pandemic protocols Environmental Health  Safety rules and take adequate control measures in preventing injuries to themselves and others as well as to the protection of environment and prevention of pollution under their span of influencecontrol  Aligning our overall business objectives with performance we offer competitive salaries performancebased incentives and a wide variety of benefits programs to address the diverse individual needs of our employees and their families  For California the base pay range for this position is 113000 to 161000 highly experienced  The pay for the successful candidate will depend on various factors eg qualifications education prior experience Applications will be accepted while this position is posted on our Careers website </data></node>
<node id="n1727" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=7e94a94bd1447925&amp;bb=elsuOR9bLelBTDHLxukK-FqFnIEieTX9U-Vfbvbg22jigyjn23mwac52-fG-o5ufwMKT4j8_gnhDv08UXvjrUNy7_535LCYuXa643j1iLQQ%3D&amp;xkcb=SoDj67M3CNkbJNQ8MZ0HbzkdCdPP&amp;fccid=d13187a50c252e33&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo LocationArden Hills MN 55112 Full job description      Work mode Hybrid Preferred Remote Considered              Onsite Locations Arden Hills MN US 55112               Additional Locations NA        Diversity  Innovation  Caring  Global Collaboration  Winning Spirit  High Performance         At Boston Scientific we’ll give you the opportunity to harness all that’s within you by working in teams of diverse and highperforming employees tackling some of the most important health industry challenges With access to the latest tools information and training we’ll help you in advancing your skills and career Here you’ll be supported in progressing – whatever your ambitions          About the role  The Cardiology Customer Experience team is actively seeking a highly skilled and experienced Senior Data Analyst on the Strategic Transformation team Data strategy and analytics needs are expanding rapidly across our organization and we are seeking the right individual to help us on this journey        The Senior Data Analyst will play a pivotal role in shaping the future of data strategy and analytics across Cardiology through fostering crossteam collaboration and key partnerships to build a strong data culture and drive datadriven decisionmaking at all levels This role coalesces data from multiple commercial sources seeking to unearth and proactively deliver meaningful analytical insights while leading crossfunctional coordination creating reporting tools influencing culture and anticipating emerging business needs                This role will be based our of our Arden Hills MN location with a hybrid work mode Remote applicants will be considered          Your responsibilities will include   Develops data analytics strategy and communicates strategy through development and maintenance of phased capability roadmap Identifies and interprets data formulates insights and successfully communicates actionable recommendations and implications to Cardiology stakeholders with varying levels of technical knowledge Analyzes data to derive segmentation insights and identifying audience targets for commercial programs Develops selfservice tools for team members to increase data availability and accessibility with the goal of promoting broader data literacy and gaining operational efficiencies Engaging and developing relationships with key crossfunctional partners to foster crossteam alignment and collaboration      Required qualifications   Bachelor’s degree in Business Economics Mathematics Statistics or other related discipline 5 years experience in an analytical role within Marketing Finance or similar functional area Experience in extracting mining and synthesizing data into narratives for business partners with varying levels of technical knowledge Experience with SQL and querying databases to extract data for analysis Experience with Tableau Data Studio or other data visualization tool Experience with Salesforce or other CRM platform Travel up to 10   Preferred qualifications   Medical Device or Pharmaceutical industry experience Experience with syndicated datasets from IQVIA Definitive Healthcare etc          Requisition ID 580482                As a leader in medical science for more than 40 years we are committed to solving the challenges that matter most – united by a deep caring for human life Our mission to advance science for life is about transforming lives through innovative medical solutions that improve patient lives create value for our customers and support our employees and the communities in which we operate Now more than ever we have a responsibility to apply those values to everything we do – as a global business and as a global corporate citizen                So choosing a career with Boston Scientific NYSE BSX isn’t just business it’s personal And if you’re a natural problemsolver with the imagination determination and spirit to make a meaningful difference to people worldwide we encourage you to apply and look forward to connecting with you             Nearest Major Market Minneapolis  Job Segment Pharmaceutical Data Analyst Database CRM SQL Science Data Technology  </data></node>
<node id="n1728" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=ef72533596d9ae85&amp;bb=elsuOR9bLelBTDHLxukK-E9gHpqigwc8ZzZYM84mw1Yawk42uUTiuww3uP5QjmA7DPTaSKUMfaTNA7nPQLsLfvmITuwDt7PJyZF9wMrkzyLTBRvY4Fe4Zw%3D%3D&amp;xkcb=SoBX67M3CNkbJNQ8MZ0GbzkdCdPP&amp;fccid=86a0b79b9b9291a8&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay85000 a yearJob typeFulltime Location1500 Sullivan Road Aurora IL 60506 BenefitsPulled from the full job descriptionHealth insuranceRetirement plan Full job description  Overview    Position Summary  This position is responsible for leading the work of the Center for AI including handson learning experiences for students faculty and staff focused on AI capabilities and applications nurturing research and curricular connections to AI and prototyping future technologies The position reports to the Director of Experiential Learning     The internationally recognized Illinois Mathematics and Science Academy® IMSA develops creative ethical leaders in science technology engineering and mathematics As a teaching and learning laboratory created by the State of Illinois IMSA enrolls academically talented Illinois students grades 10–12 in its advanced residential college preparatory program It also serves thousands of educators and students in Illinois and beyond through innovative instructional programs that foster imagination and inquiry      The Illinois Mathematics and Science Academy IMSA is committed to an equitable diverse and inclusive teaching and learning environment IMSA has committed to advancing equity in STEM education and representation and creating a diverse inclusive community of global citizens who can realize their full potential and execute our mission to advance the human condition      This Equity and Excellence work is the intentional integration of Cultural Competence Diversity Equity EquityMinded Frame Excellence and Inclusion into every facet of the Academy with the understanding that it is an active and ongoing process involving structures processes and people and not an isolated initiative      This position will support IMSAs Diversity work by ensuring the delivery of quality equitable and inclusive constituentcentered support while promoting and maintaining an inclusive work environment and culture that embraces the diversity of people and perspectives collaborating at IMSA to ignite and nurture creative ethical scientific minds that advance the human condition   Responsibilities     Essential Duties  Responsibilities     To perform the job successfully an individual must be able to perform each essential duty satisfactorily Reasonable accommodation may be made to enable qualified individuals with disabilities to perform essential functions      Lead the development and delivery of the Center for AI’s programming for students staff and faculty including student research mentoring and providing AI learning opportunities focused on AI capabilities and applications  Serve as an AI research mentor in residence guiding faculty staff and students on AI tools principles techniques and cuttingedge digital technologies in response to institutional and programdiscipline needs  Maintain and promote the equitable and inclusive use of all AI resources in and related to the Center for AI  Develop maintain and promote AI Help Center resources  both inperson at the AI Center and digitally for asynchronous learning  Serve as the administrator for institutional AI development and educational platforms subscriptions and software to facilitate and monitor student access and provide technical guidance for AIenabled solutions  Provide consultation and assistance to faculty and students on the development and use of AI technologies for research and integration into classrooms and student projects  Develop facilitate and support AI tinkering and student research in AI  Support faculty by developing learners’ essential AI and digital literacy skills and digital resources to build knowledge and gain experience  Keep current with emerging technology trends and related resources  Utilize and mentor IMSA student interns and research teams to support the work of the AI Center  Develop and teach credentialed AI learning opportunities for students faculty and staff  Ensure responsible AI including a focus on soundness and fairness  Identify and remove barriers to support access to learners from all backgrounds  Other duties as assigned  Qualifications     Qualifications     The required qualifications for this position include       Education research or industry experience in Computer Science Artificial Intelligence Machine Learning Data Science or a related field  Bachelor’s Degree in a Related Field  Experience with Python and popular ML libraries such as TensorFlow PyTorch Keras and OpenAI  Effective communication with students staff parents and external partners  Active participation in a collaborative team environment  Excellent interpersonal skills for working in a team environment with the ability to function diplomatically and communicate effectively with colleagues administrators students parents and the public  An appreciation and understanding of working with diverse populations     Preferred Qualifications     Master’s degree in Computer Science Artificial Intelligence Machine Learning Data Science or a related field  A strong background in both artificial intelligence and machine learning  Experience in curriculum design  High school equivalent teaching experience  Experience with APIs Cloud environments and Frontend frameworks  Experience with data science  usage of large language models  Indepth knowledge of ObjectOriented Programming and Software Development Lifecycle     Salary and Benefits      Starting salary approximately 85000 and regionally commensurate with the training and experience The Illinois Mathematics and Science Academy offers an excellent comprehensive benefits package including health and retirement benefits The State Universities Retirement System SURS is reciprocal with other Illinois public retirement systems such as the Teachers’ Retirement Systems of Illinois TRS and the Illinois Municipal Retirement Fund IMRF      Application Process      Applicants must also submit a letter of interest a complete and current resumeCV and three current professional references      We are seeking a diverse applicant pool      The Illinois Mathematics and Science Academy is an Equal Employment Opportunity Employer providing equal employment opportunities without regard to race color sex age religion or national origin This policy also includes the handicapped and all disabled Vietnamera veterans IMSA utilizes only jobrelated criteria in making decisions concerning applicants and employees   </data></node>
<node id="n1729" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=a4179b8367589b5f&amp;bb=elsuOR9bLelBTDHLxukK-Np1lR-PFerQaVzixlMvpDDQLrSjvGSMzn8qmP2ZYvU4NBYiTCbL9pD4iBYeDeIcBBFK8I7Dw6j46pOVvo4tYwqy26tDgAoqiw%3D%3D&amp;xkcb=SoDK67M3CNkbJNQ8MZ0FbzkdCdPP&amp;fccid=52a28443b9a60e8d&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TensorFlowYesNo LocationCerritos CA 90703 BenefitsPulled from the full job description401kDisability insuranceFlexible spending accountHealth insuranceLife insurancePaid sick timeParental leaveShow morechevron down Full job description   What You Need To Know    Open the door to a groundbreaking tech career with an industry leader Southern Glazer’s Wine  Spirits is North America’s preeminent wine and spirits distributor as well as a familyowned privately held company with a 50 year legacy of success To create a new era in alcohol beverage sales and service we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity  As a fulltime employee you can choose from a full menu of our Top Shelf Benefits including comprehensive medical and prescription drug coverage dental and vision plans taxsaving Flexible Spending Accounts disability coverage life insurance plans and a 401k plan We also offer tuition reimbursement a wellness program parental leave vacation accrual paid sick leave and more  We offer continuous learning and career growth in a fastpaced environment where you are respected your voice is heard and technology is part of our strategy for success If you’re looking to fill your glass with opportunity come join our FAMILY      Overview    The Director Data Science will lead development of innovative data science and machine learning solutions to grow our customers and drive engagement and retention The Director will work closely with customer portfolio product engineering and marketing stakeholders to identify data science opportunities build strong partnership and deliver impactful solutions for the business      Primary Responsibilities     Develop customer data science roadmap and educate both internal and external stakeholders at all levels to drive implementation and measurement  Develop innovative data science solutions for segmentation CLTV next best action forecasting and experimentation that utilize the stateofart machine learning algorithms statistical and quantitative modeling approaches  Build a worldclass team of data scientists Be a handson leader to mentor the team in latest machine learning and AI approaches and to introduce new technologies and processes    Manage projects priorities and ensure timely delivery Develop and evangelize best practices for scoping building validating deploying and monitoring data science models    Prepare and present data science results and analytical insights to senior leadership       Additional Primary Responsibilities       Minimum Qualifications     Graduate degree in Computer Science Mathematics Physics Engineering Statistics Operations Research or other equivalent quantitative fields  5 years of work experience in Machine Learning AI and Data Science with a proven track record to drive innovation and business impacts  Strong machine learning deep learning and statistical modeling expertise such as neural networks reinforcement learning causal inference modeling NLP and computer vision  Strong knowledge and experience with big data platforms AWS Azure Spark and MLAI languages and tools Python TensorFlow PyTorch  Experience in deploying machine learning algorithms and advanced modeling solutions  Strong communication skills and the ability to explain complex analysis and algorithms to untechnical audience  Works effectively cross functional teams to build trusted partnership       Physical Demands     Physical demands with activity or condition for a considerable amount of time include sitting and typingkeyboarding using a computer eg keyboard mouse and monitor or mobile device  Physical demands with activity or condition may occasionally include walking bending reaching standing and stooping  May require occasional liftinglowering pushing carrying or pulling up to 25lbs       EEO Statement    Southern Glazers Wine and Spirits an Affirmative ActionEEO employer prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race color religion age sex national origin disability status genetics protected veteran status sexual orientation gender identity or expression or any other characteristic protected by federal state or local laws This policy applies to all terms and conditions of employment including recruiting hiring placement promotion termination layoff recall transfer leaves of absence compensation and training Southern Glazers Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience knowledge skills abilities and education of employees Unless otherwise expressly stated any pay ranges posted here are estimates from outside of Southern Glazers Wine and Spirits and do not reflect Southern Glazers pay bands or ranges    </data></node>
<node id="n1730" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=a091e1caff0e9d2e&amp;bb=N4MG9YEcnJXKQ9YrA7Zn6IinZ5Ry9srCJsSoTgCzJe8goHk8lx0tRxRq-n8BgPF41OCTrEdCKIRY70HJwAoEPO068OE2QscS_wxRmwWT2s-X5bB-QQJhMA%3D%3D&amp;xkcb=SoDu67M3CNkXVqWbHp0LbzkdCdPP&amp;fccid=a3b51ece17c02aae&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SmartsheetYesNoEducationDo you have a Bachelors degreeYesNo LocationNorth Chicago IL BenefitsPulled from the full job description401kDental insuranceHealth insurancePaid time offVision insurance Full job description Company Description  AbbVies mission is to discover and deliver innovative medicines and solutions that solve serious health issues today and address the medical challenges of tomorrow We strive to have a remarkable impact on peoples lives across several key therapeutic areas – immunology oncology neuroscience and eye care – and products and services in our Allergan Aesthetics portfolio For more information about AbbVie please visit us at wwwabbviecom Follow abbvie on Twitter Facebook Instagram YouTube and LinkedIn Job Description  Key Responsibilities   Align Data  Statistical Science DSS study teams with program  studylevel strategies  Lead the DSS Study Team  represent DSS as a member of crossfunctional study team  Act as single point of contact  accountable operational lead from DSS Coordinates associated DSS study teams to meet operational objectives  Utilize clinical trial systems including Electronic Data Capture EDC Interactive Response Technology IRT  Clinical Trial Management Systems CTMS  Utilize understanding of Laboratory data Clinical Outcomes Assessments COAeCOA ECG IRT Pharmocokinetic  other external data types  Engage  connect global functional  crossfunctional teams at study level Interact with  influence crossfunctional team members to achieve program objectives  Plan coordinate  deliver data management tasks within timeline  Utilize operational analytics  project management tools to optimize execution of programs  studies manage internal  external resources track study progress  prepare study status reports  Author revise  review data management related plans  documentation including Data Management Plan Data Review Plan Electronic Case Report Form eCRF  completion Guidelines  Anticipate  identify issues that could affect timelines or quality  develop options  solutions identify  mitigate risk and contribute to Risk Assessment  Management Plan  Ensure adherence to federal regulations  applicable local regulations Good Clinical Practices GCPs ICH Guidelines Standard Operating Procedures SOPs  functional quality standards  Perform project management skills including metrics analysis  reporting methodologies  Keep abreast of new or evolving local regulations guidelines  policies related to clinical development  Participate as DSS study owner in regulatory inspections  internal quality audits  Participate in oversight of vendors  provide feedback related to clinical trial operations issues  trends in performance  Utilize reporting  data visualization tools such as Spotfire JReview Business Objects or SAS to generate data review listings also project management tools such as Microsoft Project Smartsheet or equivalent  Responsible for coaching  mentoring team members  Lead DSS innovation  process improvement initiatives  participates in crossfunctional initiatives  Conduct study execution across functions  Conduct indirect supervision of employees as well as supervision of work of contract resources    Qualifications  Must possess a Bachelor’s degree or foreign academic equivalent in Biology Biotechnology Information Technology Business or a related field of study with at least 2 years of experience in the following   Electronic Data Capture EDC Interactive Response Technology IRT  Clinical Trial Management System CTMS  planning coordinating  delivering data management tasks within timeline  authoring revising  reviewing data management related plans  documentation including Data Management Plan Data Review Plan Electronic Case Report Form eCRF and Completion Guidelines  project management skills including metrics analysis  reporting methodologies   Spotfire JReview Business Objects or SAS to generate data review listings    Position requires work at various  unanticipated work locations throughout the US 100 telecommuting permissible  Apply online at httpscareersabbviecomen Refer to Req ID REF23193M  Additional Information  We offer a comprehensive package of benefits including paid time off vacation holidays sick medicaldentalvision insurance and 401k to eligible employees This job is eligible to participate in our shortterm and longterm incentive programs   AbbVie is committed to operating with integrity driving innovation transforming lives serving our community and embracing diversity and inclusion It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race color religion national origin age sex including pregnancy physical or mental disability medical condition genetic information gender identity or expression sexual orientation marital status status as a protected veteran or any other legally protected group status  </data></node>
<node id="n1731" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=af560ca1140c0b9a&amp;bb=N4MG9YEcnJXKQ9YrA7Zn6MFuSOjdOaVagw9WYY_iWt-tjfy7iiHxUvYDw2rAe3KBoUe1a2-wU3hypNpC3jQ44WgTG6lFH034SXBW3nd6hmo%3D&amp;xkcb=SoBa67M3CNkXVqWbHp0KbzkdCdPP&amp;fccid=c46d0116f6e69eae&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in System designYesNo Job detailsHere’s how the job details align with your profilePay128250  180000 a yearJob typeFulltime LocationTampa FL 33610 BenefitsPulled from the full job descriptionHealth insuranceRetirement planTuition reimbursement Full job description JOB DESCRIPTION  We have an exciting and rewarding opportunity for you to take your software engineering career to the next level  As a Software Engineer III at JPMorgan Chase within the Corporate  Investment Bank Payments Technology Team you serve as a seasoned member of an agile team to design and deliver trusted marketleading technology products in a secure stable and scalable way You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm’s business objectives  Job responsibilities  Executes software solutions design development and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems Creates secure and highquality production code and maintains algorithms that run synchronously with appropriate systems Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development Gathers analyzes synthesizes and develops visualizations and reporting from large diverse data sets in service of continuous improvement of software applications and systems Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture Contributes to software engineering communities of practice and events that explore new and emerging technologies Adds to team culture of diversity equity inclusion and respect   Required qualifications capabilities and skills  Formal training or certification on software engineering concepts and 3 years applied experience Handson practical experience in system design application development testing and operational stability Proficient in coding in two or more languages like Python Pyspark Scala Java  Hands on experience in building data pipelines data lakes entities and profiles Hands on experience with AWS cloud technologies like AWS Glue lambda flink spark dynamo Experience in developing debugging and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages Overall knowledge of the Software Development Life Cycle Solid understanding of agile methodologies such as CICD Application Resiliency and Security Demonstrated knowledge of software applications and technical processes within a technical discipline eg cloud artificial intelligence machine learning mobile etc   Preferred qualifications capabilities and skills  Familiarity with modern data engineering technologies Exposure to cloud technologies AWS Glue EMR lambda flink spark  ABOUT US      JPMorgan Chase  Co one of the oldest financial institutions offers innovative financial solutions to millions of consumers small businesses and many of the world’s most prominent corporate institutional and government clients under the JP Morgan and Chase brands Our history spans over 200 years and today we are a leader in investment banking consumer and small business banking commercial banking financial transaction processing and asset management     We offer a competitive total rewards package including base salary determined based on the role experience skill set and location For those in eligible roles we offer discretionary incentive compensation which may be awarded in recognition of firm performance and individual achievements and contributions We also offer a range of benefits and programs to meet employee needs based on eligibility These benefits include comprehensive health care coverage onsite health and wellness centers a retirement savings plan backup childcare tuition reimbursement mental health support financial coaching and more Additional details about total compensation and benefits will be provided during the hiring process  We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success We are an equal opportunity employer and place a high value on diversity and inclusion at our company We do not discriminate on the basis of any protected attribute including race religion color national origin gender sexual orientation gender identity gender expression age marital or veteran status pregnancy or disability or any other basis protected under applicable law We also make reasonable accommodations for applicants’ and employees’ religious practices and beliefs as well as mental health or physical disability needs Visit our FAQs for more information about requesting an accommodation JPMorgan Chase is an Equal Opportunity Employer including DisabilityVeterans      ABOUT THE TEAM   The Corporate  Investment Bank is a global leader across investment banking wholesale payments markets and securities services The world’s most important corporations governments and institutions entrust us with their business in more than 100 countries We provide strategic advice raise capital manage risk and extend liquidity in markets around the world  </data></node>
<node id="n1732" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=c90e885baa24aaba&amp;bb=N4MG9YEcnJXKQ9YrA7Zn6ACMSQoegef4b1zYk1cI3PFCyfbb_0_EgewPyPRzTq60olU_30i-UXbH5I66mqA4YYukR_iXJ1INxYq7Os48_82ID2oycnjqMw%3D%3D&amp;xkcb=SoDH67M3CNkXVqWbHp0JbzkdCdPP&amp;fccid=400544ebdc9bf2b7&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid Secret Clearance licenseYesNoSkillsDo you have experience in Software troubleshootingYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location1775 Tysons Boulevard Tysons VA 22102 BenefitsPulled from the full job descriptionOnthejob training Full job description  Overview    Paradyme Management is a rapidly growing government technology leader that puts service first for its customers its team and the communities it supports Paradyme harnesses DevSecOps and Agile development processes to deliver exceptional results for digital transformations With headquarters office in Tysons Corner VA Paradyme’s awardwinning culture sets it apart through its team’s deep commitment to service and collaboration with its customers each other and the community Learn more at wwwparadymemanagementcom Responsibilities    Paradyme has partnered with an industry leader in enterprise Artificial Intelligence software and is seeking a talented Junior AIML Software Developer to help deliver solutions to our customers Together we’re accelerating our client’s digital transformation through the building and deployment of datadriven scalable AI solutions   The AIML Software Developer will work alongside a team of highly skilled engineers in the development of AI applications A motivated and qualified candidate will not only have hands on development experience in JavaScript Python or Java but also a willingness to collaborate with teams to solve problems   Paradyme will provide the selected candidate with two weeks of intensive product training along with continuous on the job training and development People are our greatest asset and the staff on our AI team will benefit from mentorship and professional development opportunities throughout their time with Paradyme We’re seeking curious and creative technical staff who are excited about delivering quality solutions     Collaborate with other developers staff and leadership to design solutions that solve customer problems  Design develop and maintain technical solutions  Build and improve tools for users to understand and analyze largescale data  Perform debugging troubleshooting modifications and testing of software and solutions  Develop documentation and collaborate in the development of technical procedures and user support guides   Requirements   Bachelor’s degree in computer science or related field  Experience with JavaScript Java or other objectoriented programming language  Handson experience and understanding of objectoriented programming data structures algorithms profiling  optimization  Passion for developing teamoriented solutions to complex engineering problems  Must have Secret level US security clearance     Physical Requirements These are the essential physical requirements needed to successfully perform the job     Sedentary work      Requires sitting up to 8 hours per day  May require lifting up to 5 pounds unassisted  Fine repetitive motor skills with hands wrists and fingers in coordination with eyes   Hearing speaking and vision Adequate to perform job duties and communicate in person via video and telephone Includes reading information from printed sources and computer screens  Other Work may be performed in an office environment which may involve frequent contact with staff and the public Work may be stressful at times      Paradyme Management Inc is committed to the full inclusion of all qualified individuals In keeping with our commitment Paradyme will take the steps to ensure that people with disabilities are provided reasonable accommodations Accordingly if a reasonable accommodation is required to fully participate in the job application or interview process to perform the essential functions of the position andor to receive all other benefits and privileges of employment please contact Rose Luczak Director of People Operations at roseluczakparadymeus or at 571 2890548   EEO Statement       Paradyme is a federal contractor and an EEO and an Affirmative Action Employer All employment decisions shall be made without regard to age race creed color religion sex national origin pregnancyrelated disability physical or mental disability genetic information sexual orientation marital status familial status personal appearance occupation citizenship veteran or military status gender identity or expression or any other characteristic protected by federal state or local law    </data></node>
<node id="n1733" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=244d9ad6704a9ef1&amp;bb=N4MG9YEcnJXKQ9YrA7Zn6G4j12ihI9XK-4GRrsz-vd8z2FPQrk_qhfZdBk3KdwskLWGYqPrlQsRS6QSxGlSSgpcdlPteTwDlTUoM4qKS8jgO0Z6z-NkD6A%3D%3D&amp;xkcb=SoBz67M3CNkXVqWbHp0IbzkdCdPP&amp;fccid=9e399301940ed66e&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in UNIXYesNo Job detailsHere’s how the job details align with your profileJob typeContract LocationSanta Ana CA Full job description  Sr Data Scientist  Santa Ana CA  6 months   Job Description  What is the specific title of the position Senior Data Scientist What ProjectProjects will the candidate be working on while on assignment 1 Field Team ModernizationIntelligent Decision Management  build and support of new predictive data models 2 Coding Transformation  build and support of newexisting predictive data models 3 Ad hoc data analysis Is this person a sole contributor or part of a team If so please describe the team Name of team size of team etc Individual contributor within Decision Intelligence team  What are the top 510 responsibilities for this position Please be detailed as to what the candidate is expected to do or complete on a daily basis Analytic data model development Quality check of output from other developers Data explorationresearch Documentation of logic and results Presentation of logic and results to team and stakeholders  What software toolsskills are needed to perform these daily responsibilities See below  What skillsattributes are a must have Programming SQL Python Spark Hive Libraries ScikitLearn Numpy Analytics Regression Classification Clustering Decision Trees SVM Linear Regression Logistic Regression KNN KMeans Visualization Matplot Systems UNIX Windows Soft Integrity strong verbal and written communication teamwork creativity Experience 34 years related field  What skillsattributes are nice to have Analytics Neural Networks Graph Computation Approximate Nearest Neighbors Time Series NLP Visualization Tableau Plotly Excel Infrastructure Big Data Cloud Computing High Performance Computing GPUs Experience 5 years related field   </data></node>
<node id="n1734" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=92eaf9ca3de6fa6d&amp;bb=N4MG9YEcnJXKQ9YrA7Zn6JlK01BJ3moSdYFCy-8AGjaZs7cdxOPGatB124gcyNSvQ-L9AjoE2aZqvrqsLrPMLpEfC-mChITKd_kRYILZ_BxB6z1UZVlNoQ%3D%3D&amp;xkcb=SoD967M3CNkXVqWbHp0PbzkdCdPP&amp;fccid=b8df243f15e7ff2e&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay90000  150000 a yearJob typeFulltime LocationRemote Full job description  Description    SFI is searching for a Data ScientistDeveloper to join our team to support our Federal government customers The Data ScientistDeveloper will provide data science support The project scope is to research IT tools and techniques that potentially have a high payoff in improving IT systems functionality implementation maintenance and operations The project scope also includes all technical domains Innovation DataAI Development and SecurityOperations Target areas for improvement include but are not limited to software development productivity increased automation of DevOps and expanded selfservice customer support  Primary Responsibilities The Data ScienceDeveloper responsibilities may include but are not limited to   Identify valuable data sources and automate collection processes  Undertake preprocessing of structured and unstructured data  Analyze large amounts of information to discover trends and patterns  Build predictive models and machinelearning algorithms  Combine models through ensemble modeling  Present information using data visualization techniques  Propose solutions and strategies to business challenges  Collaborate with engineering and product development teams  Requirements    BS or BA in CS CIS EE Math or equivalent technical studies  10 years overall IT experience  6 years of experience in area of specialization  Experience in data mining  Understanding of machinelearning and operations research  Knowledge of R SQL and Python familiarity with Scala Java or C is an asset  Experience using business intelligence tools eg Tableau and data frameworks eg Hadoop  Analytical mind and business acumen  Strong math skills eg statistics algebra  Problemsolving aptitude  Excellent communication and presentation skills   Additional Information   In order to meet the clearance requirements for this opportunity candidates must be authorized to work in the US  All candidates will be subject to a complete background check to include but not limited to Criminal History Education Verification Professional Certification Verification Verification of Previous Employment and Credit History  Public Trust background investigations can take approximately four to eight weeks and requires fingerprinting   Other Information   The salary for this position is 90000  150000 annually  For information on SFIs benefits please visit httpwwwspatialfrontcompagescareerhtml  This is a fulltime W2 position  Please no agencies third parties or corptocorp  Spatial Front Inc is an Equalopportunity Employer all qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability or status as a protected veteran  Spatial Front Inc participates in EVerify   </data></node>
<node id="n1735" labels=":Skill"><data key="labels">:Skill</data><data key="name">data metrics</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1736" labels=":Skill"><data key="labels">:Skill</data><data key="name">data extracts</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1737" labels=":Skill"><data key="labels">:Skill</data><data key="name">cleaning scripts</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1738" labels=":Skill"><data key="labels">:Skill</data><data key="name">ms teams</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1739" labels=":Skill"><data key="labels">:Skill</data><data key="name">agi</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1740" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">artificial general intelligence</data></node>
<node id="n1741" labels=":Skill"><data key="labels">:Skill</data><data key="name">detail oriented</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1742" labels=":Skill"><data key="labels">:Skill</data><data key="name">modeling skills</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1743" labels=":Skill"><data key="labels">:Skill</data><data key="name">build prototype</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1744" labels=":Skill"><data key="labels">:Skill</data><data key="name">cleaning</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1745" labels=":Skill"><data key="labels">:Skill</data><data key="name">aggregating</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1746" labels=":Skill"><data key="labels">:Skill</data><data key="name">diagnosing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1747" labels=":Skill"><data key="labels">:Skill</data><data key="name">pricing analytics</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1748" labels=":Skill"><data key="labels">:Skill</data><data key="name">dash</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1749" labels=":Skill"><data key="labels">:Skill</data><data key="name">power point</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1750" labels=":Skill"><data key="labels">:Skill</data><data key="name">timeseries</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1751" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">communication skills</data></node>
<node id="n1752" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">multinomial logistic regression</data></node>
<node id="n1753" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">scientific techniques</data></node>
<node id="n1754" labels=":Skill"><data key="labels">:Skill</data><data key="name">reusability</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1755" labels=":Skill"><data key="labels">:Skill</data><data key="name">market trends</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1756" labels=":Skill"><data key="labels">:Skill</data><data key="name">foundry</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1757" labels=":Skill"><data key="labels">:Skill</data><data key="name">scaling</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1758" labels=":Skill"><data key="labels">:Skill</data><data key="name">troubleshoot</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1759" labels=":Skill"><data key="labels">:Skill</data><data key="name">coding standards</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1760" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">source control management</data></node>
<node id="n1761" labels=":Skill"><data key="labels">:Skill</data><data key="name">power pivot</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1762" labels=":Skill"><data key="labels">:Skill</data><data key="name">drive insights</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1763" labels=":Skill"><data key="labels">:Skill</data><data key="name">identify trends</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1764" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">facilitate analysis</data></node>
<node id="n1765" labels=":Skill"><data key="labels">:Skill</data><data key="name">identify issues</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1766" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">marketing insights</data></node>
<node id="n1767" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">quantitative analysis</data></node>
<node id="n1768" labels=":Skill"><data key="labels">:Skill</data><data key="name">ingestion</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1769" labels=":Skill"><data key="labels">:Skill</data><data key="name">provisioning</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1770" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">logical data model</data></node>
<node id="n1771" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data defect remediation</data></node>
<node id="n1772" labels=":Skill"><data key="labels">:Skill</data><data key="name">uat</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1773" labels=":Skill"><data key="labels">:Skill</data><data key="name">data management</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1774" labels=":Skill"><data key="labels">:Skill</data><data key="name">aws neuron</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1775" labels=":Skill"><data key="labels">:Skill</data><data key="name">aws inferentia</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1776" labels=":Skill"><data key="labels">:Skill</data><data key="name">llama2</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1777" labels=":Skill"><data key="labels">:Skill</data><data key="name">gpt2</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1778" labels=":Skill"><data key="labels">:Skill</data><data key="name">gpt3</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1779" labels=":Skill"><data key="labels">:Skill</data><data key="name">jax</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1780" labels=":Skill"><data key="labels">:Skill</data><data key="name">deepspeed</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1781" labels=":Skill"><data key="labels">:Skill</data><data key="name">aws trainium</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1782" labels=":Skill"><data key="labels">:Skill</data><data key="name">code review</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1783" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical programming</data></node>
<node id="n1784" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytical insights</data></node>
<node id="n1785" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">segmentation insights</data></node>
<node id="n1786" labels=":Skill"><data key="labels">:Skill</data><data key="name">technology trends</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1787" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">frontend frameworks</data></node>
<node id="n1788" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">quantitative modeling</data></node>
<node id="n1789" labels=":Skill"><data key="labels">:Skill</data><data key="name">metrics analysis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1790" labels=":Skill"><data key="labels">:Skill</data><data key="name">jreview</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1791" labels=":Skill"><data key="labels">:Skill</data><data key="name">microsoft project</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1792" labels=":Skill"><data key="labels">:Skill</data><data key="name">debugging</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1793" labels=":Skill"><data key="labels">:Skill</data><data key="name">troubleshooting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1794" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">develop documentation</data></node>
<node id="n1795" labels=":Skill"><data key="labels">:Skill</data><data key="name">linear regression</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1796" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">logistic regression</data></node>
<node id="n1797" labels=":Skill"><data key="labels">:Skill</data><data key="name">knn</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1798" labels=":Skill"><data key="labels">:Skill</data><data key="name">matplot</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1799" labels=":Skill"><data key="labels">:Skill</data><data key="name">graph</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1800" labels=":Skill"><data key="labels">:Skill</data><data key="name">computation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1801" labels=":Skill"><data key="labels">:Skill</data><data key="name">approximate</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1802" labels=":Skill"><data key="labels">:Skill</data><data key="name">nearest neighbors</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1803" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=25f50db4b1ded9d8&amp;bb=rUMdW0CXv9MkYKEBBqlMZtKFVK8TZdGhoUU6fQaWtvt_SGf_Hg1gxLKyoFmt5OlnNng7LCyOIFelIIvq3eOB6FWCIFQkwmX1Kb2jXYrybj8QZz7ndiLPgA%3D%3D&amp;xkcb=SoDC67M3CNum7-RSRx0AbzkdCdPP&amp;fccid=1a70f25b2e303d26&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Microsoft AccessYesNoEducationDo you have a Masters degreeYesNoLanguagesDo you know SpanishYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeContract LocationPuerto Rico Full job description  Description    V2A is looking for strong Business Analyst Consultant candidates for our growing practice Business analysts are an integral part of a two or three person engagement team They are responsible for contributing to the overall problem solving and engagement success by applying quantitative methods and analytical tools Our Business Analysts usually have a Bachelors degree plus 03 years of work experience  Key areas of responsibility   Build effective business cases with analytical software and tools including the development of financial and quantitative models  Conduct research on specific topics and apply it in support of engagement requirements  Explain and review analysisfindings with team members and client personnel  Help to develop communications both in written and in presentation formats that convey ideas and recommendations in a clear and convincing way  Identify implications of proposed solutions on people processes technology strategy and structure   What We Offer  We offer a unique experience for motivated individuals You will take on challenging work having a direct impact and high visibility with our leading industry clients You will also join a closeknit team of passionate individuals who share a unique culture where we work hard but also play hard As our firm continues to grow you will also be given the opportunity to grow with us to expand your capabilities and to play your part in the development of our organization  What you will find at V2A   Challenging work in a variety of industries and services  Direct client impact and high visibility  Early responsibility  Continuous learning  A collaborative nonhierarchical work environment  Top notch workmates  Impressive skill set and methodologies  Unique workhard playhard culture  Competitive benefits  Requirements    What we are looking for  Our Teams background and studies are as diverse as the work we tackle We are looking for smart curious and driven individuals who want to learn and contribute in a fast paced and fun environment  Education  Bachelors or masters degree in Engineering  Bachelors or masters in degree in Computer Science  Bachelors or masters degree in Business Administration  Bachelors or masters degrees in Other areas are also welcome to apply Some majors may include  Statistics Economics Mathematics and Social Sciences  Qualifications   03 years of work experience  Quantitative Conceptual and Analytical Thinking  Problem Solver  Capacity to apply knowledge and skills to solve complex problems  Team Player  Ability to build and manage relationships effectively with team and clients  High selfmotivation for learning and setting and achieving challenging goals  Compelled to excel and succeed in every task at hand  Thrives in an entrepreneurial resultsoriented environment  Very proficient use of Excel Access Powerpoint and Word Additional statistical simulation and Optimization software knowledge is a plus  Fully Bilingual Spanish and English   </data></node>
<node id="n1804" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=8f9182eaef87bd31&amp;bb=rUMdW0CXv9MkYKEBBqlMZvTkoAvtWDp6uNwSUWYwC6W702o3rSKylM0VjED2Xc8EfnW_dgi32ZSMDuM70zp-itjryk87jYDM2bDQO6g-t4P_oQufAi5nFQ%3D%3D&amp;xkcb=SoBM67M3CNum7-RSRx0HbzkdCdPP&amp;fccid=0950181599f58650&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid COTA certificationYesNoSkillsDo you have experience in StatisticsYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay75000  85000 a yearJob typeFulltime LocationNew York NY BenefitsPulled from the full job description401k matchingCommuter assistanceDental insuranceHealth insuranceLunch  learnsPaid parental leavePaid sick timeShow morechevron down Full job description ABOUT US  At COTA our vision is for datadriven cancer care to become the standard across healthcare We believe that everyone touched by cancer deserves a clear path to care Together we can make that vision a reality  Were searching for smart motivated people who share our passion for bringing clarity to cancer Connect with us introduce yourself and apply to one of our current openings  PERKS  Working at COTA comes with many perks At COTA we are committed to workplace wellness and employee happiness Some of the benefits for working fulltime at COTA include  Medical  dental  vision benefits 401k Match  retirement Monthly commuter benefits Annual bonus Stock option grants Flexible Fridays Quarterly COTA Wellness days Unlimited paid time off Paid sick time  40 hrsyear 11 paid holidays per year Paid Parental leave Company team building events Educational lunch  learns Causedriven employees Fun and productive culture Employeeled DEIB committee Healthy snacks Gourmet coffee and cold brew  LOCATION New York NY  OVERVIEW  As a Research Data Analyst you will collaborate with statisticians clinicians and life science partners to execute research analyses In addition you will help to enhance our research product offerings This position requires a person who is naturally curious has familiarity with various analytic tools and enjoys solving new problems In addition you should be able to continually unearth and present interesting trends that have clinical research and business importance for our customers and oncology patients at large You will be joining a friendly high performing and diverse research team with backgrounds in public health health economics biology and statistics  WHAT WILL YOU GAIN  Experience analyzing oncology realworld data Learning and implementing advanced statistical methodologies Confidence by owning endtoend analyses including drafting protocols and statistical analyses plans executing analyses and disseminating results to various audiences  WHAT TO EXPECT IN YOUR FIRST YEAR AT COTA  In thirty days you will  Familiarize yourself with COTAs research product offerings Begin exploring COTAs data model and standard analysis template  In three months you will  Rampup on oncology domain knowledge and healthcare analytic techniques and statistical methodologies  In six months you will  Become a subject matter expert on COTAs data model Partner with the operations engineering and clinical teams to define analytic requirements enhance data quality and deliver publicationcaliber analysis  In one year you will  Be executing analyses for life science and healthcare provider partners with minimal oversight  HOW YOU WILL IMPACT COTA  Collaborate with physicians statisticians data scientists business leaders and others within and outside COTA to conduct research projects including observational studies pharmacoeconomic analyses and other relevant study types Develop realworlddata analytic products for the oncology space Partner with the operations engineering and clinical teams to define analytic requirements enhance data quality and deliver publication caliber analysis Become an expert on our data model confidently representing our data capabilities to other teams as well as external partners In time suggest amp implement improvements to our data model based on business needs and client feedback  WHAT YOU BRING TO THE TABLE  Masters degree in computer science statistics applied math econometrics or similar quantitative field with prior researchinternship experience Curiosity collaborative spirit intellectual humility and time management ability Solid foundational knowledge of descriptive and predictive statistics Working knowledge of statistical programming primarily R SQL version control data maintenance and other relevant computational techniques and processes Passion for data quality and ability to clearly communicate and evangelize its importance to internal and external partners Collaborative selfstarter attitude COTA has a startup environment and you will be working crossfunctionally daily with data scientists engineers medical professionals and business professionals  NICE TO HAVE  Background in biology oncology or public health Understanding of realworld data sourced from EHR 2 years of analytic work experience in the healthcare space Research experience in oncology genomics or bioinformatics  Salary 7500085000  At COTA we are passionate about creating an inclusive workplace that celebrates and values diversity with the belief that it drives our innovation Our commitment to diversity and inclusion is a guiding principle on how we build teams and develop leaders As part of our commitment to building a respectful culture that encourages develops and celebrates different backgrounds experiences abilities and perspectives all qualified applicants will receive consideration for employment without regard to race color religion culture gender gender identity or expression sexual orientation national origin genetics disability age veteran status or other applicable legally protected characteristics All employment decisions including decisions to hire and promote will be based on merit competence business need and performance  We are a proud equal opportunity employer  All employees who work from or enter COTAs office location or attend company events or meetings inperson must be fully vaccinated unless an exemption applies  NOTICE OF COLLECTION OF APPLICANT PERSONAL INFORMATION UNDER THE CALIFORNIA CONSUMER PROTECTION ACT CCPA  This Notice applies only to the collection of personal information from California residents on and from January 1 2020 Cota we is committed to maintaining the privacy and security of our job applicants personal information In connection with your application for employment we will collect and process personal information that you provide to us or that we obtain through employment agencies background check agencies your professional or educational references or other third parties or service providers This information includes contact information such as name email address telephone number and other identifiers professional or employment related information and education information We may also collect information concerning your protected characteristics if voluntarily provided by you We will use your personal information and share it with third parties solely for purposes of considering your application for employment and should you be hired in connection with your employment  COTAs Privacy Policy      </data></node>
<node id="n1805" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d9f3993382952377&amp;bb=rUMdW0CXv9MkYKEBBqlMZhHr0bj_oJPlrB7ro0wMKXNP0udwA1wbpoK3SERu71qr9zQ8XIN8DnMPl2Ei1Xpw2pnbIV_KpOUVQti57JcfYhptOTA_HwGPPQ%3D%3D&amp;xkcb=SoD467M3CNum7-RSRx0GbzkdCdPP&amp;fccid=2cf39a1eec869aa7&amp;cmp=AKZAC-Global&amp;ti=IT+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay30  32 an hourJob typeTemporaryShift and scheduleDay shiftMonday to Friday Location1000 Chastain Rd NW Kennesaw GA 30144 Full job descriptionONLY LOCAL CANDIDATES WILL BE CONSIDERED KINDLY READ JD CAREFULLY BEFORE APPLYING LOCATION Hybrid 1000 Chastain Rd NW Kennesaw GA 30144 INTERVIEW TYPE Either Web Cam or In PersonThe IT Business Analyst will support the IT project request process by performing stakeholder interviews and facilitating workshops to define and understand business functions and processes identify painpoints inefficiencies and related functions and processes elicit and negotiate business requirements for new implementations and modifications to existing IT systems and services create process diagrams document use cases and partner with crossfunctional teams and stakeholders to identify and validate solutions to business unit requests and requirementsEducation RequiredBachelors Degree from an accredited institution of higher education in Information Systems or related field or equivalent combination of education andor experiencePreferred qualifications  IT Project Management experience Agile Facilitation experience International Institute of Business Analysis IIBACCBA certification PMI Professional in Business Analysis PMIPBA  Key Responsibilities  Schedule prepare and facilitate fact findingdiscovery analysis meetings with stakeholders and subject matter experts to understand and document current business processes and functions pain points assumptions timeframes and success measures Present business process documentation to business and IT decision makers for approval Define business process inputs outputs and data elements required for optimal execution Collaborate with business partners to confirm scope and objectives of the project and formulate a shared understanding of project priorities Assist with identifying scope creep and collaborate with technical and functional teams to evaluate feasibility effort and project impact of scope changes Facilitate identification of businesscritical process steps and the requirementspolicies driving those steps Identify and understand the governing rules and regulations that affect project teams business processes Designs user interface mockups to effectively meet business needs within native system functionality Document system processes functionality and recommend improvements as applicable Understand analyze and communicate benefits of product featuresfunctionalities to assist customer in evaluating go forward strategies and solutions Collaborate with IT team members and business area representatives to help identify and validate technical solutions to business requestsissues Create standardized templates for internal IT and Business Analyst processes Assists with devising test plans and coordinating User Acceptance Testing documents Plans and executes unit integration and acceptance testing to validate individual customer requirements against delivered product Contributes to and maintains an issues log and collaborates with appropriate teams to identify and mitigate risks  SkillsBusiness Analyst Experience in an IT Environment Required 2 YearsFacilitating inperson and virtual meetingsworking sessions Required 2 YearsDiscussing business requirements and elaborating on project requests with crossfucntional teams at all levels of the organization Required 2 YearsCreating business process documents including process diagrams swim lane diagrams and written documentation supporting the diagrams Required 2 YearsDocumenting Business Processes Using Visio or LucidChart Required 1 YearsCreating and Delivering Professional Presentations and Memos to Stakeholders and Leadership Required 2 YearsConverting Business Requirements to Technical Specifications Required 1 YearsMS Toolset  specifically experience with Microsoft Excel data entry filtering basic formulas Required 2 YearsExperience working multiple assignments at the same time Required 2 YearsAgile Methodology Highly desired Job Type Temporary Pay 3000  3200 per hour Schedule  Day shift Monday to Friday  Ability to Relocate  Kennesaw GA 30144 Relocate before starting work Required  Work Location Hybrid remote in Kennesaw GA 30144 </data></node>
<node id="n1806" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0ed0fd07963ef5db&amp;bb=rUMdW0CXv9MkYKEBBqlMZpyfY5tgddmEae1L1dRGmdy477RHCv841P2SuZG-fJDpjN9yQ5aaO5lhmX5TkpSyT2brwbCv_G-pQUyr597NynSzDe9eIuXYWg%3D%3D&amp;xkcb=SoBl67M3CNum7-RSRx0FbzkdCdPP&amp;fccid=827b81c4d03deda0&amp;cmp=Mendota-Insurance-Company&amp;ti=IT+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeShift and schedule8 hour shiftMonday to Friday LocationRemote BenefitsPulled from the full job description401k matchingADD insuranceDental insuranceDependent health insurance coverageDisability insuranceFlexible spending accountHealth insuranceShow morechevron down Full job descriptionMendota Insurance is a dynamic and rapidly growing insurance company committed to leveraging technology to streamline administrative processes and enhance operational efficiency We are looking for a Business Systems Analyst to join our IT department and help us deliver solutions that meet the needs of our customers and agents At Mendota innovation has been a goal since our founding We currently offer our partner agents many products that help us meet our goal of providing outstanding efficiency and customer service led by our proprietary internet based platform Mendota Max For the last 35 years Mendota has been supporting a cooperative work environment that is lead by a great team of 200 employees nationwide We take pride in offering a challenging and rewarding work place which has been proven by the number of employees who have found longevity in their careers at Mendota Experience Preferred  Bachelors degree in Computer Science Information Systems Business Administration or related field  At least 3 years of experience as a Business Systems Analyst or similar role Strong knowledge of software development life cycle methodologies and tools Proficient in SQL Excel Visio and other data analysis and modeling tools Excellent communication collaboration and problemsolving skills Experience in insurance industry or financial services is a plus  Position Requirements Job Summary  Analyze business requirements and translate them into functional and technical specifications for IT projects Work closely with stakeholders developers testers and project managers to ensure alignment and quality of deliverables Triage issue tickets submitted by business Manage issue and enhancement tickets assigned to software vendor Conduct system testing user acceptance testing and postimplementation support Document and communicate system changes issues and best practices Identify and recommend opportunities for process improvement and system enhancement  Supervisory Responsibility Minimal Note The above description is intended to describe the basic duties and requirements of this position It is not intended to be an exhaustive list of all duties responsibilities and skills You may be asked to perform tasks outside of your usual duties in support of the overall objectives of the organization Job Type Fulltime Benefits  401k matching ADD insurance Dental insurance Dependent health insurance coverage Disability insurance Flexible spending account Health insurance Health savings account Life insurance Paid holidays Paid time off Prescription drug insurance Tuition reimbursement Vision insurance Work from home  Compensation package  Yearly pay  Experience level  3 years  Schedule  8 hour shift Monday to Friday  Work Location Remote </data></node>
<node id="n1807" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=22c83602620237ec&amp;bb=dFv8xFxZXhuj0Phpcl29k0IgW3YWO3f3ZNt7on28z4Y9o0YpWjJdlhXE-DFFTx168VoKQ6cNLOMLAD2zj948rNDehWYiksQ0l0m84WKDUFg_iV9itS-SEw%3D%3D&amp;xkcb=SoCz67M3CNujB6wYxJ0SbzkdCdPP&amp;fccid=450aff5f7cce5591&amp;cmp=Embedded-Systems-Inc&amp;ti=Junior+Data+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in XMLYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePayFrom 25 an hourJob typeFulltimeShift and schedule8 hour shift LocationEmbedded Systems Inc in Saint Cloud MN 56301 BenefitsPulled from the full job description401k401k matchingDental insuranceHealth insuranceLife insuranceProfessional development assistanceRelocation assistanceShow morechevron down Full job descriptionData Analyst Embedded Techniques Inc is looking for a Data Systems Analyst who will be responsible for working with Product Technical and Business stakeholders to gather requirements do the impact assessment by doing the data analysis and work with Technical and Business teams to understand and access the impact design the process flow and create technical specification documents Requirement  Translate business requirements into business process and business requirement documents Experience with data models database design development data mining and segmentation techniques Knowledge of and experience with reporting packages databases SQL Server programming R XML JSON or ETL frameworks Must have technofunctional experience  Collaborating with the business interviewing stakeholders gathering and translating requirements Create Tableau dashboards to visualize Filter and “clean” data and review reports printouts and performance indicators to locate and correct code problems Work closely with management to prioritize business and information needs Providing technical expertise in data storage structures data mining and data cleansing  Qualification  Bachelor’s degree from an accredited university or college in computer science Work experience as a data analyst or in a related field Ability to work with stakeholders to assess potential risks Ability to analyze existing tools and databases and provide software solution recommendations Ability to translate business requirements into nontechnical lay terms Highlevel experience in methodologies and processes for managing largescale databases Demonstrated experience in handling large data sets and relational databases Understanding of addressing and metadata standards Highlevel written and verbal communication skills  Note Only Serious Candidates will be Selected Other Candidates Like C2C and W2 will not be Selected Please Note  Candidates with High Experience will not be Selected Job Type Fulltime Pay From 2500 per hour Benefits  401k 401k matching Dental insurance Health insurance Life insurance Professional development assistance Relocation assistance Visa sponsorship Vision insurance  Experience level  1 year Under 1 year  Schedule  8 hour shift  Experience  Data analytics 1 year Preferred Data mining 1 year Preferred SQL 1 year Preferred Analysis skills 1 year Preferred  Work Location Hybrid remote in Saint Cloud MN 56301 </data></node>
<node id="n1808" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=9b03d4ec65c4724a&amp;bb=dFv8xFxZXhuj0Phpcl29k84Ml4f8JHYi6Yoi1Gbs_PRi4200HUf1qmAd4iBdanTcHUGEgyEaR5UJbh4MfLMmWU3hhdN_IajORdJnF1X_apbvF0TZ4AEZSA%3D%3D&amp;xkcb=SoAU67M3CNujB6wYxJ0XbzkdCdPP&amp;fccid=ab283d08aaa89866&amp;cmp=Upen-Group-Inc&amp;ti=Business+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNo Job detailsHere’s how the job details align with your profilePay4000000  6372550 a yearJob typeFulltimeContractShift and scheduleMonday to Friday LocationUpen Group Inc in Irving TX 75038 BenefitsPulled from the full job descriptionFlexible schedule Full job descriptionWe are a leading job agency seeking an Analyst to join our team and work with some of our top vendors The ideal candidate will have strong analytical skills excellent attention to detail and a desire to learn and grow in the field of data analysis Responsibilities  Collect manipulate and analyze data from various sources Prepare reports and visualizations to communicate findings to stakeholders Identify trends and patterns in data to help inform business decisions Collaborate with crossfunctional teams to develop and implement datadriven solutions  Requirements  Expertise a relevant field such as Mathematics Statistics or Computer Science Proficient in programming languages such as SQL or Python Strong analytical skills and attention to detail Excellent communication skills to effectively present findings to stakeholders Ability to work collaboratively in a team environment  Must be a US Citizen This is an excellent opportunity for a recent graduate or entrylevel candidate to gain handson experience in data analysis and work with a dynamic team of professionals If you are passionate about data and want to kickstart your career in this field we would love to hear from you Apply today to join our team as a Junior Data Analyst WE are a Job Agency Must be a US Citizen Job Types Fulltime Contract Pay 4000000  6372550 per year Benefits  Flexible schedule  Schedule  Monday to Friday  Experience  SQL 1 year Preferred  Work Location Hybrid remote in Irving TX 75038 </data></node>
<node id="n1809" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=9cca22211f4cfb0a&amp;bb=dFv8xFxZXhuj0Phpcl29k2t2cGFt2C8rTXujrdXn7Wmrm6d_4wR7fKzbdfScGO71Ca9AR1FY5U8gCOmWTqHDlL50_dbwThGFtXS_PoxHr-VEbrtmmKXoTw%3D%3D&amp;xkcb=SoCJ67M3CNujB6wYxJ0UbzkdCdPP&amp;fccid=ab283d08aaa89866&amp;cmp=Upen-Group-Inc&amp;ti=Junior+Data+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeContractShift and scheduleMonday to Friday LocationUpen Group Inc in Irving TX 75038 BenefitsPulled from the full job descriptionFlexible schedule Full job descriptionWe are a leading job agency seeking an Analyst to join our team and work with some of our top vendors The ideal candidate will have strong analytical skills excellent attention to detail and a desire to learn and grow in the field of data analysis Responsibilities  Collect manipulate and analyze data from various sources Prepare reports and visualizations to communicate findings to stakeholders Identify trends and patterns in data to help inform business decisions Collaborate with crossfunctional teams to develop and implement datadriven solutions  Requirements  Expertise a relevant field such as Mathematics Statistics or Computer Science Proficient in programming languages such as SQL or Python Strong analytical skills and attention to detail Excellent communication skills to effectively present findings to stakeholders Ability to work collaboratively in a team environment  Must be a US Citizen This is an excellent opportunity for a recent graduate or entrylevel candidate to gain handson experience in data analysis and work with a dynamic team of professionals If you are passionate about data and want to kickstart your career in this field we would love to hear from you Apply today to join our team as a Junior Data Analyst WE are a Job Agency Must be a US Citizen Job Types Fulltime Contract Benefits  Flexible schedule  Schedule  Monday to Friday  Experience  SQL 1 year Preferred  Work Location Hybrid remote in Irving TX 75038 </data></node>
<node id="n1810" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=afd799d6cb29ec3d&amp;bb=dFv8xFxZXhuj0Phpcl29k5gnakrk7Tdnp-uIJfMX-h_crDu6SCmPCboCAurDivJB3i_mpTCajfEsbqLRjgogrHGtjNGLJgNbseobb6dNhXp1PYwf2HEGtQ%3D%3D&amp;xkcb=SoAy67M3CNujB6wYxJ0rbzkdCdPP&amp;fccid=450aff5f7cce5591&amp;cmp=Embedded-Systems-Inc&amp;ti=Business+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePayFrom 25 an hourJob typeFulltimeShift and schedule8 hour shift Location26300 112th Ave Saint Cloud MN 56301 BenefitsPulled from the full job description401k401k matchingDental insuranceHealth insuranceLife insuranceProfessional development assistanceRelocation assistanceShow morechevron down Full job descriptionBusiness Analysts Business analysts are sometimes referred to as management analysts business continuity planners or business operations specialists A business analyst identifies and analyzes an organizations business processes for the purposes of building a software application or platform improving productivity minimizing risk or waste or developing a new product or service line Business analysts must interface with several departments and job titles to glean information about how a process operates and analysts are often involved in highlevel discussions with company owners clients upper management and other important stakeholders Daily tasks of a business analyst might include Attending client and stakeholder meetings Presenting process documentation Creating case and process diagrams Coordinating Projects Interviewing business associates Writing documents Responsibilities  Follow the appropriate testing strategy for each case Create test plans and test conditions Explore features as they are being developed to identify design testability and usability issues Work with Developers to determine an appropriate test environment and tools for testing Execute manual tests interpret the results and diagnose basic test failures Set up test environments reading log files inspecting databasesdata files Create issue reports and work with Developers to resolve appropriately Represent product quality as part of regular bug triage and release process Perform integrity and regression testing using a combination of manual and automated techniques  Qualifications  College degree Domain experience in Finance Banking Insurance Retail or Healthcare andor benefit administration is a plus Ability to manage multiple priorities effectively and work in a fastpaced environment Ability to work in a highly collaborative team environment Selfstarter highly motivated and creative Excellent relationshipbuilding skills Strong verbal and written communication skills Strong analytical and organizational skills Excellent writingdocumentation communication analysis and organizational skills required Strong aptitude in both business and technology Selfstarter with excellent organizational and time management skills  Note Only Serious Candidates will be Selected Other Candidates Like C2C and W2 will not be Selected Please Note  Candidates with High Experience will not be Selected Job Type Fulltime Pay From 2500 per hour Benefits  401k 401k matching Dental insurance Health insurance Life insurance Professional development assistance Relocation assistance Visa sponsorship Vision insurance  Experience level  1 year Under 1 year  Schedule  8 hour shift  Experience  Business analysis 1 year Preferred Data analysis skills 1 year Preferred SQL 1 year Preferred Analysis skills 1 year Preferred  Work Location Hybrid remote in Saint Cloud MN 56301 </data></node>
<node id="n1811" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=ea16aff6f0541840&amp;bb=dFv8xFxZXhuj0Phpcl29k53p8EJWyOaJOP7H32D1DCgUSwI-zQF5W9DXPk9QSj7rAIs8avZOCuVrKB0E-6wwTC5DBar5Q_pNe4Y9yxnEuU-Rdm6geWPtKA%3D%3D&amp;xkcb=SoCv67M3CNujB6wYxJ0obzkdCdPP&amp;fccid=1c4fad651fc40173&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Transportation management systemsYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay70000  90000 a yearJob typeFulltime LocationFrisco TX Full job description  Ruiz Foods has been family owned since 1964 is one of the largest Hispanicowned companies in the country and is the proud maker of two 1 brands El Monterey frozen Mexican food and Tornados roller grill taquitos in grocery stores and convenience stores across the US and Canada  Are you looking for career development opportunities in an exciting company that cares about its team members and consumers alike Come join our team and help us continue our industryleading track record of growth quality and delicious food  COMPENSATION 70K  90kyear DOE  ESSENTIAL DUTIES AND RESPONSIBILITIES  Strong analytical skills capable of breaking down complex issues detail oriented Collect interpret and analyze data from the TMSe2open related to logistics including transport and delivery Identify areas for improvement and implement strategies to enhance the efficiency and effectiveness of Supply Chain Work closely with Operations to deliver cost savings andor service improvements Assist with the monthlyannual reporting deliverables as well as the development of new managementfinancial reports Support multiple Supply Chain functions including Transportation and Purchasing in accurate and timely reporting of KPIs Proactively identify trends gaps and industry shifts Use analytic methods and tools within the TMSe2open to improve internal processes Evaluate inventory transactions within receiving shipping and warehouse     REQUIREMENTS  Bachelors degree in Supply Chain preferred Five year minimum combined experience in business analytics Extensive analytical and problem solving skills Extensive knowledge of logistics and distribution including the TMSe2open platform Willing to take a handson approach to data analysis Strong organizational and leadership skills Excellent interpersonal and communication skills verbal and written Record of developing and implementing new processes in a cross functional environment Successful cross functional leadership and ability to effectively interact with senior leadership Proficient MS Excel MS PowerPoint and other MS Office Suite products    Experience Required    5 years Combined experience in business analytics   Education Preferred    Bachelors or better   Equal Opportunity EmployerProtected VeteransIndividuals with Disabilities  The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about discussed or disclosed their own pay or the pay of another employee or applicant However employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information unless the disclosure is a in response to a formal complaint or charge b in furtherance of an investigation proceeding hearing or action including an investigation conducted by the employer or c consistent with the contractor’s legal duty to furnish information 41 CFR 60135c  </data></node>
<node id="n1812" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=44fa94e8730e05dc&amp;bb=dFv8xFxZXhuj0Phpcl29k6Nep_Adn5LduuyoPlUjX27rO_nsFgIZ-up7Qy7zdSnE4Al-n0bd5AgGz7QkGCB_BCNO4nYwxMNrutGWZa3OUimSpIIEwsTNOA%3D%3D&amp;xkcb=SoAh67M3CNujB6wYxJ0vbzkdCdPP&amp;fccid=2a55a3f1fe7a8da2&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in VisioYesNoEducationDo you have a Bachelors degreeYesNo LocationBrussels IL Full job description  The world is more connected than ever before and the pace of change is bewildering We deliver the best combination of technology and services to enable our clients to fulfil the diverse needs of their customers By fostering a deeply and personal relationship with our clients we want to improve the lives of people and organizations   Axians is the VINCI Energies brand dedicated to ICT and Digital Transformation We’re present across 37 countries with more than 16 000 employees and an annual revenue of more than 36 billion euros in 2023   The best of ICT with a human touch   Function Business Analyst   Workplace Brussels    Main Functions   As a Business Analyst for our project you will gather data and analyse review and evaluate business systems processes and user needs internal as well as external users Based on the analysis you will identify improvement areas for which you will define requirements scope and objectives measurement methods and formulate all the documentation needed to improve the business processes   For the initiatives approved you will build the documentation to ease and speed up development stay in touch with the development and testing teams and define measurement systems and followup methods to monitor the improved processes    Description of tasks    Test business processes analyse process issues delays bottlenecks  Construct workflow charts and diagrams study process and system capabilities write specifications Gather review and analyse business data including KPIs reports and other key metrics using data Analytics Tools Prepare reports dashboards visualisations by collecting analysing and summarizing information and trends Assess options for process improvement automation opportunities Propose and design systems improvements by studying current practices and using operational metrics and reports Maintain documentation regarding processes and operations Define measurements for processes and improvement initiatives Liaise between various departments and groups Identify with key stakeholders and communicate effectively Collaborate with and drive crossfunctional teams   Knowledge and skills  Bachelor’s degree in appropriate field business analysis operational research mathematical modelling statistics data analytics or related fields or 2 years handson experience in similar role Detail oriented analytical and inquisitive great analytical critical thinking and problemsolving abilities Experience with data analysis and modelling Excellent MS Office skills Excel Visio PowerPoint Ability to effectively communicate and influence key stakeholders to support proposed strategies process improvements and operational decisions Familiarity with brainstorming and collaborative methods and tools Represent an advantage  Basic experience withknowledge of machine learning artificial intelligence Knowledge of R Python Certifications like Business Analyst Agilist Scrum LeanSig Sigma aCAP or other relevant   If you’re interested please send your application to recruitmentaxiansluxaxianscom with the subject BABS  </data></node>
<node id="n1813" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=b9b1c3eb2f8de26a&amp;bb=dFv8xFxZXhuj0Phpcl29k6XxSBIRndvV76I9eKj6LquEQMhYgIMxc-MuPumRuTtbdBlq_f7yz7Zbl2LsRhI2mLALe152fMQGjoZZbsaHWEd0oSXqLscoCA%3D%3D&amp;xkcb=SoCV67M3CNujB6wYxJ0ubzkdCdPP&amp;fccid=15d38301c801b563&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Project managementYesNoEducationDo you have a Bachelors degreeYesNo LocationNew York NY 10017 BenefitsPulled from the full job description401k matchingDental insuranceHealth insurancePaid time offVision insurance Full job description Job Description  You will support the Commercial team in multiple aspects of commercial operations and strategic decisionmaking and will support in driving profitable growth mitigating risk and maximizing growth across dentsu client business You will coordinate communication data management and ensure efficient project management to facilitate success Additionally you will collaborate with Commercial Managers and Commercial Directors on global new business and renewal pitches providing support in fee proposals briefing local markets analyzing fee proposals and preparing summaries of commercial offers and profitability reports You will report to the manager on the team   Responsibilities  Communication Coordination   Be a central contact for internal and external communications related to commercial matters  Coordinate responses to client inquiries requests for proposals and contract negotiations ensuring accurate information exchange  Draft and disseminate standardized templates for receiving and aggregating responses to client queries and proposals  Facilitate communication between our teams ensuring agreement on project requirements and deliverables    Data Management and Analysis   Take ownership of internal databases such as ensuring the commercial KPI database is kept up to date  Develop and maintain project timelines and trackers to ensure the delivery of commercial initiatives  Collect organize and analyze data related to contract compliance and scope of work management  Generate reports and provide insights based on data analysis to support client reporting processes  Collaborate with market and practice commercial and finance teams to reconcile billing discrepancies and ensure accurate financial reporting    Project Management Support   Help prepare commercial proposals including conducting research gathering relevant data and drafting proposals  Track project progress and milestones identifying potential risks and assisting in the implementation of mitigation strategies  Support the Commercial Manager in managing adhoc client requests and facilitating audits by coordinating information gathering and documentation    Qualifications    Bachelors degree in Business Administration Finance or related field  Proficiency in data reporting with experience using spreadsheet software and data visualization tools  You will handle confidential information requiring an ability to respect to such information  Familiarity with project management principles and software tools    Additional Information  The anticipated salary range for this position is 4500072450 Actual salary will be based on a variety of factors including relevant experience knowledge skills and other factors permitted by law A range of medical dental vision 401k matching paid time off andor other benefits also are available For more information regarding dentsu benefits please visit dentsubenefitspluscom  Dentsu the Company is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company on the basis of age sex sexual orientation race color creed religion ethnicity national origin alienage or citizenship disability marital status veteran or military status genetic information or any other legallyrecognized protected basis under federal state or local laws regulations or ordinances Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act andor certain state or local laws A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company Please contact recruitingdentsuaegiscom if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying  LIRemote  This is a remote role   About dentsu Dentsu is the network designed for what’s next helping clients predict and plan for disruptive future opportunities in the sustainable economy Taking a peoplecentered approach to business transformation dentsu combines Japanese innovation with a diverse global perspective to drive client growth and to shape society wwwdentsucom  We are champions for meaningful progress and we strive to be a force for good—for our people for our clients for the industry and for our society We keep our people at the center creating space for growth understanding and learning so they can thrive We embed diversity in our mindset in our solutions and in our teams to empower an inclusive equitable and culturally fluent environment Building this culture within our teams makes us better collaborators with each other and with our clients driving better outcomes for all  Dentsu the Company is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company on the basis of age sex sexual orientation race color creed religion ethnicity national origin alienage or citizenship disability marital status veteran or military status genetic information or any other legallyrecognized protected basis under federal state or local laws regulations or ordinances Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act andor certain state or local laws A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying  </data></node>
<node id="n1814" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d624b57eeeea5c1a&amp;bb=dFv8xFxZXhuj0Phpcl29k0UgOdlmdPu4WKdFRXAMFOJnNOc3kLwz5ype4Q-5FOzCQpRha1hPI5Guu5xYk7A8ZZChWU8DY1fP3YssxcIYnARNE4GvYOvuCw%3D%3D&amp;xkcb=SoAI67M3CNujB6wYxJ0tbzkdCdPP&amp;fccid=0ca3e25437f5db50&amp;cmp=BranCore-Technologies&amp;ti=Business+Intelligence+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SQLYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePayUp to 31 an hourJob typeTemporaryFulltimeContractShift and schedule8 hour shiftNo nightsDay shiftMonday to Friday LocationBranCore Technologies in Baltimore MD 21201 Full job descriptionJob Title Power BI Analyst Job Duration 0612 Months with the possibility of an extension Job Location Baltimore MD Pay Rate 31HR on W2 Without Benefits Job Summary The purpose of this position is to assist with the creation and development of complex education data reports dashboards and data visualizations for both internal and external stakeholders Scope of Work The selected offeror shall provide a qualified and responsible candidate for this desired work The candidate shall be experienced in creating Power BI reports dashboards and data visualizations Experience with K12 education data is preferred Job Specifications 1 Receive data collections and data elements and load the data into the Power BI data inventory 2 Develop guidelines for dashboards reports and data visualization 3 Perform data analysis based on MSDE data reporting guidelines 4 Create complex reports dashboards and data visualizations for internal and external stakeholders public 5 Develop report archives for historical reference and longterm secure storage and retention  7 years 6 Develop report and dashboard templates in accordance with MSDE branding initiatives 7 Analyze data received from various agencies and transform the data to meet the business and technical needs of the enduser 8 Upload data into the Power BI application and perform data validation 9 Ensure data accuracy and completeness 10 Adhere to data security and privacy best practices 11 Perform data quality checks to validate data accuracy 12 Provide weekly status reports which include but are not limited to a Tasks Completed b Tasks Planned c RisksIssues 13 Perform other duties related to MSDE Microsoft Power BI projects Required Experience 1 Education and Experience 1 Fouryear degree from an accredited college or university 2 Two years of experience with Microsoft Power BI 3 Must be available to work onsite 4 K12 education experience is preferred 5 State andor federal reporting experience based on data from an educational setting is preferred Job Types Fulltime Contract Temporary Salary Up to 3100 per hour Expected hours 40 per week Schedule  8 hour shift Day shift Monday to Friday No nights  Education  Bachelors Required  Experience  Power BI 3 years Required  Ability to Commute  Baltimore MD 21201 Required  Work Location In person </data></node>
<node id="n1815" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0bc52e1ee010941b&amp;bb=dFv8xFxZXhuj0Phpcl29k6Nep_Adn5LdkmmEv_FRt6U6V1szT4sjuh-VzHpsGuRrd9LyHQX108tlqsWtr3JzKjOBIToZc_1QQgeXWZqoAF8%3D&amp;xkcb=SoC867M3CNujB6wYxJ0sbzkdCdPP&amp;fccid=ffa0939cae523fb4&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid BLS Certification certificationYesNoSkillsDo you have experience in XSLTYesNo LocationBroomfield CO BenefitsPulled from the full job description401k401k matchingDental insuranceHealth insurancePaid parental leaveParental leaveVision insurance Full job description  Hunter Douglas is the worlds leading manufacturer of window coverings and a major manufacturer of architectural products We are a brand that you know and trust With more than 100 years of innovation weve defined our industry with proprietary products that deliver revolutionary style and functionality and can be found in millions of homes and commercial buildings globally  We are searching for candidates that are driven intelligent creative and entrepreneurial By offering challenging and accelerated opportunities for growth powered by a shared hunger for success we create a space for your career to thrive In return for your expertise we are committed to providing competitive and robust total compensation and benefit packages to ensure you feel valued Our dream is to become the fastest growing most loved window covering company in the world Whats yours   Position Overview  Lead the design development configuration testing implementation and support of manufacturing software solutions for Hunter Douglas business operations The analyst will be responsible for overseeing overall configuration and code modification efforts and managing data systems which interface between plant control systems and enterprise applications The developer will also have active support of process adherence with users  What youll do  An employee in this position may be called upon to do any or all of the following essential functions These examples do not include all of the functions which the employee may be expected to perform  Assist in software development and enhancement for enabling improved business performance in SAP MII SAP ME UI5 and SQL Support and implementation of operations projects in shade and component manufacturing Assist in planning and developing new application functionality with current application platform Define system requirements to match line design and machine integration development Help gather and translate business requirements into applied solutions across multiple manufacturing platforms and technologies Collaborate and direct development of IT strategy to support global business objectives Work with process teams to implement integrated business solutions Develop ME configuration custom applications on MII and technical support of MEMII Gather requirements write functional specifications provide business justifications and execute testing in relation to developing new manufacturing applications May be required to perform other related duties as assigned Ability to work in a constant state of alertness and safe manner Position requires regular and predictable attendance Employees are held accountable for all duties of this job  Who you are  An employee in this position upon appointment should have the equivalent of the following  Expert knowledge to lead full lifecycle deployment on at least one large scale application implementation in SAP MEMII Required Expert knowledge to develop applications and configure integrations in SAP MEMII with BLS SQL UI5 React AWS tools Python XML XSLT Required Solid knowledge to develop machine integrations with Kepware and SAP PCo Required Solid knowledge to lead design and delivery of MEMII capability within best industry practices Required Expert knowledge JIRA incident tracking software andor process experience Expert knowledge and strong troubleshooting skills for program issues within many SAP MEMII application platforms Expert knowledge and excellent interpersonal verbal and written communication skills Expert knowledge demonstrate strong planning organizing and decision making skills Solid knowledge in demonstrated leadership skills in application support and internalexternal resource supervision Solid knowledge and proven ability to design application modifications in the form of functional and technical specifications Basic knowledge of functional management skills Solid knowledge in technical documentation and application training skills Solid knowledge and ability to interface at all levels of the organization both internally and externally Solid knowledge of manufacturing application environment  Whats in it for you  Annual base salary range 110000  145000 Bonus target range 2035 Generous benefits package including medical dental vision life disability A company culture that prioritizes internal development and professional growth Time off with pay 401k plan with a degree of employer matching Paid parental leave Wellness programs and product discounts     Please note all offers presented to candidates are carefully crafted to ensure market competitiveness equity and reflect the individual candidates education experience skills and potential  LIKL1   Hunter Douglas is an Equal Opportunity Employer and complies with applicable employment laws EOEMFVetDisabled are encouraged to apply   </data></node>
<node id="n1816" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=c053f420ee676435&amp;bb=HTB3LzNbtvMzUp0-1YIE_zeOhFLCm84sjLTeTq8s_gWgqAiRspNfIGRF7F3YBiKY7bA2_HUhHRpO6K-M_tuLjDjXbi4BlF4k59v8zhD8F3zaOlGnP8kAcw%3D%3D&amp;xkcb=SoA_67M3CNu_pjxylh0LbzkdCdPP&amp;fccid=2d0126ed45419e18&amp;cmp=Global-IT-Resources&amp;ti=Senior+Application+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in System designYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePayUp to 62 an hourJob typeFulltimeShift and schedule8 hour shiftMonday to Friday LocationRemote BenefitsPulled from the full job description401kDental insuranceHealth insurance Full job descriptionSchedule Days Remote – must work PDT hours only  Fully Covid Vaccinated  Booster  Responsibilities The Sr Departmental Applications and Analytics Analyst will work with external vendors system owners community partners contractors and Health Science Campus leadership to design develop implement and make operational information systems This incumbent will identify and document information technology design specifications based on analysisassessment of user needs and generate needgap analyses This person will use expert knowledge to generate and develop system scope and objective analyze and evaluate existing or proposed systems and devise or modify procedures to solve problems using data processing Will work independently on complex system activities and serve as a subject matter expert as it relates to quality and regulatory reporting and how data applies to various reports Will provide quality assurance on various extracted data as it relates to hospital and outpatient reporting requirements Typical responsibilities may additionally include the following  Analyze and investigates data collet compile organize and analyze data Support quality reporting and provide data  reporting to support operations Prepare data analysis in the required format including tables charts and graphs with written summaries conclusions and recommendations Uses pertinent data and facts to identify and solve a range of problems Collaborate with business partners to review analyze and overseecoordinate applicable reporting and data request from an operations and regulatory perspective Utilize appropriate Cerner and Data Warehousing tools and processes to track regulatory reporting needs Investigates nonstandard requests and problems Work with EMR partner Cerner in concert to prepare forecast and deploy new regulatory reporting packages and builds  Minimum education Bachelor’s Degree in related field or equivalent experience or certifications Minimum ExperienceKnowledge 35 or more years of related experience including system design documentation maintenance upgrades and troubleshooting Healthcare experience required Cerner experience required Quality Reporting experience AMS support Lighthouse experience REQUIRED Knowledge and experience with MIPPS eCQM PIP DA2 LightsOn HealtheAnalytics and Population Health Job Type Fulltime Pay Up to 6200 per hour Expected hours 40 per week Benefits  401k Dental insurance Health insurance  Schedule  8 hour shift Monday to Friday  Travel requirement  No travel  Work Location Remote </data></node>
<node id="n1817" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d9c3e5bfda56d5a8&amp;bb=HTB3LzNbtvMzUp0-1YIE_zOS6XXzieaXhQt52EoIoDH_m8HzioSN6igLmItnvg4jP_nNM0-yIukvhk4IRGmkiFMpgaRCc0U7GL4hSgp5UWXcQ2q6_pQMTg%3D%3D&amp;xkcb=SoCL67M3CNu_pjxylh0KbzkdCdPP&amp;fccid=bf3ea8aeed7f2bc9&amp;cmp=Skill-Bird-LLC&amp;ti=EDI+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNo Job detailsHere’s how the job details align with your profilePay55  58 an hourJob typeFulltimeShift and schedule8 hour shift LocationRemote Full job descriptionJob Title EDI Analyst Location Remote Enterprise Req Skills EdivltraderlexicomautoreleaseINFORACM Job Title EDI analyst Electronic Data Interchange Top Skills Details 1LexiCom Administration Experience 2VLTrader Administration Experience 3INFOR Tools Experience AutoRelease ACM Module Workplace Type 100 Remote Experience Level Intermediate Level External Communities Job Description Were looking for someone to administer communicationsEDI products LexiCom VL Trader INFOR Work Environment Fully Remote Additional Skills  Qualifications This person will be working with stakeholders and customers globally so good communication skills will be critical Job Type Fulltime Pay 5500  5800 per hour Expected hours 40 per week Compensation package  1099 contract  Experience level  5 years 6 years 7 years 8 years  Schedule  8 hour shift  Experience  EDI 5 years Required LexiCom VLTrader 2 years Required INFOR 1 year Required  Work Location Remote </data></node>
<node id="n1818" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=6a35b24f71e6a5ea&amp;bb=HTB3LzNbtvMzUp0-1YIE_znkuF547CQIL9B-vZWbV3XUFo6lKVgi-6wlB5aHrmQHwazAEKw4tCCKBfcb-WST34bWJmRcBmxsbikqZq476je8Du41CYfMqhjoRAK0cDqr&amp;xkcb=SoAW67M3CNu_pjxylh0JbzkdCdPP&amp;fccid=1998356520fe6782&amp;cmp=Clare-Rose%2C-Inc.&amp;ti=Social+Media+Coordinator&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Social media managementYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay45000  47000 a yearJob typeFulltimeShift and schedule8 hour shiftMonday to Friday Location100 Rose Executive Boulevard Shirley NY 11967 BenefitsPulled from the full job description401k401k matchingDental insuranceHealth insuranceLife insurancePaid time offVision insurance Full job descriptionWe are currently seeking a dynamic and proactive Social Media and Data Analyst Coordinator to join our Sales team This pivotal role will be instrumental in enhancing our ecommerce presence refining branding strategies and fostering community engagement across social media platforms Additionally this position will play a crucial part in supporting our Business Development team through meticulous data analysis tracking and reporting We are in search of an individual who possesses a unique blend of creativity and analytical prowess The ideal candidate should be adept at navigating data sets while also having a keen eye for creative content development They must thrive in a fastpaced startup environment possess excellent multitasking abilities and exhibit a high level of attention to detail without losing sight of overarching goals If you are a selfmotivated individual who enjoys wearing multiple hats and is passionate about driving results we encourage you to apply Responsibilities Data Intelligence  Collaborate with the Business Development Team to track targets performance metrics and incentives on a regular basis Assist in monitoring and evaluating sales representatives performance against monthly Pay for Performance targets Track and provide updates on various key sales initiatives utilizing tools for data mining and topline analysis Develop and deploy targeted surveys to gather valuable insights from the sales team Compile and analyze performance data to generate comprehensive reports for payroll and strategic planning purposes  Social Media Management  Lead the planning and execution of social media content ensuring alignment with sales promotions launches and events  Maintain a strategic balance of campaign moments merchandising priorities and inapp experiences to drive engagement and brand awareness Optimize posting schedules across various social media channels to maximize reach and engagement Drive brand exposure by leveraging social media platforms effectively and proactively engaging with the community Conduct regular social listening exercises to identify trends monitor competitor performance and inform content strategy Source compelling social content for marketplace posts and craft engaging copy to accompany visuals Provide regular performance reports to the Vice President of Sales and other stakeholders highlighting key insights and recommendations for improvement  The Social Media and Data Analyst Coordinator reports directly to the Vice President of Sales and Marketing and plays a crucial role in representing our brands in the digital space Requirements  Bachelors degree in Marketing Business Administration or related field 12 years of relevant experience preferred Proficiency in Microsoft Excel and other data analysis tools Strong written communication skills with the ability to tailor content for online audiences Indepth knowledge of various social media platforms and their advertising capabilities Ability to collaborate effectively across departments and communicate insights clearly Familiarity with digital advertising platforms tactics and emerging trends Experience with graphic design software such as Photoshop and Illustrator is a plus Detailoriented with strong editing skills and the ability to organize information effectively Enthusiastic team player committed to contributing to the companys growth  Physical Demands These physical demands are representative of the physical requirements necessary for an employee to successfully perform the essential functions of the Social Media and Data Analyst Coordinator’s job Reasonable accommodation can be made to enable people with disabilities to perform the described essential functions of the job While performing the responsibilities of the Social Media and Data Analyst Coordinator’s job the employee is required to talk and hear The employee is often required to sit and use their hands and fingers to handle or feel The employee is frequently required to stand walk reach with arms and hands climb or balance and to stoop kneel crouch or crawl Vision abilities required by this job include close vision Work Environment While performing the duties of this job in the office environment the employee is occasionally exposed to moving mechanical parts and vehicles The noise level in the work environment is usually quiet to moderate While performing duties of this job in the field environment the employee will be exposed on a regular basis to moving mechanical parts motor vehicles and traffic In addition the exposure while in customer locations cannot be projected These conditions may vary and the employee will receive adequate training to recognize and avoid hazards that cannot be described as expected Noise levels will range from quiet to loud Job Type Fulltime Pay 4500000  4700000 per year Benefits  401k 401k matching Dental insurance Health insurance Life insurance Paid time off Vision insurance  Schedule  8 hour shift Monday to Friday  Ability to Commute  Shirley NY 11967 Required  Work Location In person </data></node>
<node id="n1819" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e96a818d81f60c02&amp;bb=HTB3LzNbtvMzUp0-1YIE_6rdXR0ZOaLZaT5d33AHA_nEgAO6ymfWLeyotnxRxoschS6cSoJKnnRLD7aLxehlfI_XpAfR-dRVFcBV899PUvo%3D&amp;xkcb=SoCi67M3CNu_pjxylh0IbzkdCdPP&amp;fccid=91444a28cb181211&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SAPYesNo LocationIrving TX BenefitsPulled from the full job description401k matchingADD insuranceDental insuranceDisability insuranceHealth insuranceHealth savings accountPaid time offShow morechevron down Full job description  INDIST    LIIST    Company Overview   Req ID 274321   NTT DATA Services strives to hire exceptional innovative and passionate individuals who want to grow with us If you want to be part of an inclusive adaptable and forwardthinking organization apply now   We are currently seeking a Lead Vistex Business Analyst to join our team in Dallas Texas USTX United States US    Job Description   Functional experience with Vistex Rebates and Chargebacks    At least 10 years of experience in Business Process Consulting problem definition Architecture Detailing of Processes Experience in supporting Vistex functions preferably Rebates and Chargebacks Knowledge on SAP OTC and P2P and RICEF Should have good understanding of business requirements analyzing the Root cause of the issues and communicating back to business Experience working in Production Support model Able to Coordinate with offshore team   About NTT DATA Services    NTT DATA Services is a recognized leader in IT and business services including cloud data and applications headquartered in Texas As part of NTT DATA a 30 billion trusted global innovator with a combined global reach of over 80 countries we help clients transform through business and technology consulting industry and digital solutions applications development and management managed edgetocloud infrastructure services BPO systems integration and global data centers We are committed to our clients longterm success Visit nttdatacom or LinkedIn to learn more    NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race color religion citizenship national origin ancestry age sex sexual orientation gender identity genetic information physical or mental disability veteran or marital status or any other characteristic protected by law We are committed to creating a diverse and inclusive environment for all employees If you need assistance or an accommodation due to a disability please inform your recruiter so that we may connect you with the appropriate team     Where required by law NTT DATA provides a reasonable range of compensation for specific roles The starting hourly range for this remote role is 65 TO 70 This range reflects the minimum and maximum target compensation for the position across all US locations Actual compensation will depend on several factors including the candidates actual work location relevant experience technical skills and other qualifications This position may also be eligible for incentive compensation based on individual andor company performance     This position is eligible for company benefits that will depend on the nature of the role offered Company benefits may include medical dental and vision insurance flexible spending or health savings account life and ADD insurance shortand longterm disability coverage paid time off employee assistance participation in a 401k program with company match and additional voluntary or legally required benefits   </data></node>
<node id="n1820" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=8fda5fb7cd4542bb&amp;bb=HTB3LzNbtvMzUp0-1YIE_2BmVL1MQ-8W8g5pqPvy-dEyFAa-nPa85CL_d82fZ6kNplNToj9BrfdjJXYOqeGPSGqpmXU9F6kMqlYoc52nMwzngOhcvIj8yg%3D%3D&amp;xkcb=SoAs67M3CNu_pjxylh0PbzkdCdPP&amp;fccid=03c234e7707704f5&amp;cmp=Kaizer-Software-Solutions&amp;ti=E-commerce+Specialist&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SalesforceYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePayFrom 45 an hourJob typeContractShift and schedule8 hour shiftMonday to Friday LocationHouston TX Full job descriptionPosition CloudCraze BA With Ecommerce Domain Location Houston TX Dayone Onsite Position Type 12 Months Contract Job Description Needs a Business Analyst to work with business leaders and developers to understand the requirements of business and get them implemented Provide datadriven insights and results that address these requirements Ensure accuracy completeness and timeliness of reporting through proactive awareness alerts testing and processes As a Cloud craze business analyst the candidate should have strong analytical skills strong Salesforce Platform knowledge effective project management skills and should be an effective problem solver Qualifications  Attributes · At least 6 years of IT progressive experience with a minimum of 2 years of experience working in Salesforce Service and Sales Cloud Lightning and B2B Commerce · Proficient in Microsoft Excel and SQL Language to query through different databases · Exposure to B2B Commerce previously Cloud Craze system administration · Must be willing to work in person 23 days a week onsite in Client’s Houston office if not located in Houston then relocation is required · Familiar with Project Management Change Control Incident Management IT ControlsAudits and System Development Lifecycles Job Type Contract Salary From 4500 per hour Experience level  6 years  Schedule  8 hour shift Monday to Friday  Ability to Commute  Houston TX Required  Ability to Relocate  Houston TX Relocate before starting work Required  Work Location In person </data></node>
<node id="n1821" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d255ab26d5334fc4&amp;bb=HTB3LzNbtvMzUp0-1YIE_zx7eH2NBut1IQkdED0e6iTr50O7jjwOiay6491uQpauFT9emsaGBlT-cF51B3Ny4Ntn2RsJIzrag6gMWy8JHaw-KXcZr7-hFg%3D%3D&amp;xkcb=SoCY67M3CNu_pjxylh0ObzkdCdPP&amp;fccid=339feaa66eeccb5d&amp;cmp=Forge-Forward%2C-Inc.&amp;ti=Management+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid Secret Clearance licenseYesNoSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay75000  90000 a yearJob typeFulltimeShift and schedule8 hour shiftMonday to Friday LocationLexington Park MD BenefitsPulled from the full job description401k401k matchingDental insuranceEmployee assistance programFlexible spending accountHealth insuranceLife insuranceShow morechevron down Full job descriptionForge Forward is searching for a Management Analyst to work in Lexington Park MD supporting a digital transformation effort Position will be required to be onsite 4 days a week Must be a US citizen to be considered as this position will require a security clearance  Applies knowledge of management functions processes and analytical methods or techniques to gather analyze and evaluate information required by program managers and customers Draws conclusions and devises solutions to problems relating to improvement of management effectiveness organizational structures work methods and procedures efficiency as well as resource requirements utilization or control Develops reports for management and executive leadership Supports organizational assessment and transformation process analysis and improvement facilitation of small group sessions and development of key communications Develops and drafts program and project milestones progress monitoring and supporting documentation Requirements47 years of relevant experienceBachelor’s degree in Business Management or any analytical disciplineMust be US Citizen and have ability to obtain a Secret clearance Existing clearance preferred Functional Skills and ResponsibilitiesDevelops and facilitates client sessions and data gathering exercisesAnalyzes information and requirements to identify challengesRecommends opportunities for efficiencies and improvementIntegrates concepts and capabilities within clientcorporate tools to improve processesCreates reports and deliverables for internal and external customersCollaborates with team members to collect analyze and process dataEngages with clients to guide project execution and implement identified efficienciesCreates presentations based on recommendations and findings Desired SkillsPrior experience supporting the Navy PMA support is preferredExperience working on or in collaboration with software development teamsExperience presenting information to midlevel andor senior leadershipFamiliarity with data analysis tools Excel Tableau PowerBI etc Forge Forward Inc is an equal opportunityaffirmative action employer All qualified applicants will receive consideration for employment without regard to sex gender identity sexual orientation race color religion national origin disability protected Veteran status age or any other characteristic protected by law Job Type Fulltime Pay 7500000  9000000 per year Benefits  401k 401k matching Dental insurance Employee assistance program Flexible spending account Health insurance Life insurance Paid time off Professional development assistance Referral program Tuition reimbursement Vision insurance  Schedule  8 hour shift Monday to Friday  Application Questions  This position will require a security clearance To obtain a clearance you must be a US citizen Are you a US citizen  Work Location Hybrid remote in Lexington Park MD </data></node>
<node id="n1822" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=7af3274976347aac&amp;bb=HTB3LzNbtvMzUp0-1YIE_3HYnhAuybZoIHwLjugn2mTuP9-hS7zJmwIexZI0P1eqdbOLgByQGK651ObdqFvlt0dgB-Q75FAkzKt-fVRiRNMZ4wwAb9BzBA%3D%3D&amp;xkcb=SoAF67M3CNu_pjxylh0NbzkdCdPP&amp;fccid=daa0f6f74bcce2cf&amp;cmp=Markati-Group-LLC&amp;ti=Market+Researcher&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Market researchYesNo Job detailsHere’s how the job details align with your profilePayFrom 38000 a yearJob typeFulltimeShift and schedule8 hour shift LocationHallandale Beach FL Full job descriptionJob OverviewWe are seeking a highly motivated and detailoriented Market Researcher to join our team As a Market Researcher you will be responsible for collecting and analyzing data to help us make informed business decisions You will work closely with our marketing and sales teams to identify market trends customer preferences and competitive landscapes This is an excellent opportunity for someone who is passionate about data analysis and has a strong interest in market research Duties Conduct market research to gather data on consumer behavior market trends and competitor analysis Collect and analyze data using various research methods Collaborate with crossfunctional teams to develop actionable insights and recommendations Assist in the development of marketing strategies based on research findings Monitor industry news and trends to stay uptodate with the latest market developments Support business development efforts by providing market insights and identifying potential opportunities Skills Strong analytical skills with the ability to collect organize analyze and disseminate significant amounts of information with attention to detail Knowledge of market research methodologies and techniques Excellent written and verbal communication skills to effectively present findings to stakeholders Ability to work independently as well as collaboratively in a team environment Nice to Have  Amazon FBA Experience  Hellium 10 Seller Central Sellerboard ECommerce Excel VLookups and Pivot Tables Jungle Scout Keepa If you are a proactive problem solver with a passion for market research and data analysis we would love to hear from you This is a fulltime position offering competitive compensation based on experience Join our team and contribute to our success in understanding consumer behavior and driving business growth Job Type Fulltime Pay From 3800000 per year Schedule  8 hour shift  Work Location In person </data></node>
<node id="n1823" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=769fe73d36ebd40b&amp;bb=HTB3LzNbtvMzUp0-1YIE_zeOhFLCm84s_EckKax2hHYRZAzt9vgyZfIcPR96QxjaMfwCX_ymSPFjBnNozjuB715bk1Of_G9_BvDdRvQA0tl3PDZ7hb26dQ%3D%3D&amp;xkcb=SoCx67M3CNu_pjxylh0MbzkdCdPP&amp;fccid=8ef25d91e8b3bfff&amp;cmp=International-Bank-of-Chicago&amp;ti=Operation+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Data analysis skillsYesNoEducationDo you have a Masters degreeYesNoLanguagesDo you know MandarinYesNo Job detailsHere’s how the job details align with your profileJob typeFulltimeShift and schedule8 hour shiftWeekends as neededDay shiftMonday to Friday Location405 Mannheim Road Bellwood IL 60104 BenefitsPulled from the full job description401k401k matchingDental insuranceEmployee assistance programFlexible spending accountHealth insuranceHealth savings accountShow morechevron down Full job descriptionOverview International Bank of Chicago’s Electronic Payment Services EPS department handles crossborder eCommerceexporter payment collection business and alternative payment distribution service for institutional clients including toptier financial institutions and wellestablished Money Service Businesses with annual volume over 6B in 2022 The EPS Operation Analyst I analyzes data and trends from different transaction streams facilitates daily report generation process by reviewing and running the automatic report writing tools and identifies improvements for the EPS business operation’s performance Language Fluent English and Mandarin speak read and write preferred Responsibilities  Analyze data and trends from different transaction streams Facilitate daily report generation process by reviewing and running the automatic report writing tools Identify issues relating to the EPS business operation’s performance Assist with EPS monthly billings and invoices Flexibility to onboard new corporate clients create client database profiles and collaborate with other departments to ensure products and services are successfully implemented in a timely accurate and efficient manner Review and follow the bank’s EPS policies and procedures Communicate with the bank’s customers regarding transaction exceptions Communicate with clients and other banking professionals to maintain Service Level  Agreements and drive the client experience  Escalate unresolved issues and deliver highquality customer service and effective operational support for the bank’s customers Stay abreast of industry and market trends Perform other EPS operation duties as assigned or required to support team  Qualifications and Skills  Bachelor’s degree in finance accounting mathematics data analysis computer science or related field required master degree preferred Exceptional verbal and written communication skills required Excellent analytical skills Ability to effectively research and resolve issues independently and in a team environment   Has the ability to learn and adapt to new policies and procedures and technology platforms Good interpersonal skills to effectively work with different management levels within the organization Sharp attention to detail strong organizational and time management skills Proficient in Microsoft Office applications  International Bank of Chicago is an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race color religion national origin sexual orientation gender identity disability protected veteran status or any other characteristic protected by state federal or local law Job Type Fulltime Benefits  401k 401k matching Dental insurance Employee assistance program Flexible spending account Health insurance Health savings account Life insurance Paid time off Referral program Vision insurance  Experience level  2 years  Schedule  8 hour shift Day shift Monday to Friday Weekends as needed  Work setting  Inperson Office  Application Questions  Do you have experience with data analysis  Education  Bachelors Preferred  Experience  onboarding new corporate clients 2 years Required  Language  Fluent English and Mandarin speak read and write Required  Work Location In person </data></node>
<node id="n1824" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=679f7fea57ca435e&amp;bb=HTB3LzNbtvMzUp0-1YIE_8Uf-SnmOdEKfx9xGXrcjc-YmWtCTUfFDa4XRcd43HeYJZHAIxXAPPTcYkafBxr0tPCBnly9XC0nG_StSZB3FVNWYqaqauOf0Q%3D%3D&amp;xkcb=SoBY67M3CNu_pjxylh0DbzkdCdPP&amp;fccid=d74b2038b113816f&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SalesforceYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location50 Applied Card Way Suite 300 Applied Corporate Center Glen Mills PA 19342 Full job description Job Description   The Salesforce Business Analyst role creates reviews analyzes and defines requirements for building Salesforce technology solutions for business opportunities The right candidate will have experience in creating analyzing and validating detailed functional specifications You will work within a team structure of project managers Business SMEs and technical resources   As a Salesforce Business Analyst you will    Work with business subject matter experts on building requirements documents  Map functional requirements to Salesforcecom features and functionality  Understand and document requirements for configuring testing and launching our CRM solution  Serve as the functional expert for Salesforce projects  Work with technical resources to interpret requirements into technical documents  Understand CRM requirements and Identify product requirements for future enhancements  Act as liaison between business partners and the technical team  Critically evaluate information gathered from multiple sources reconcile inconsistency and break down highlevel information into details  Report project statuses to project managers  Build testing strategies for project teams  Build test scripts for technical and business users to execute  Manage the execution of testing new or enhanced system  Work with Key users on documentation and training material  Work within a project team structure  Work closely with the Solution Architect and client resources to ensure the system meets the needs of the client    Qualifications   Required   Bachelor’s degree required Information Technology or similar preferred      If bachelor’s degree is nontech focused equivalent work experience may be acceptable in lieu of that focus   5 years’ experience as a BA or similar position  5 years of experience on SFDC Salesforcecom platform  Certified Administrator  Certified Platform App Builder  Certified Business Analyst  Experience with creating and completing deliverables including requirements design user stories and testing documentation  Proven ability to document and communicate business processes and requirements  Ability to work successfully in a multifunctional and multicultural global team  Experience with Salesforce architecture and automation using workflow and process builder    Preferred   SFDC Trailhead super badges strongly preferred      Business Admin Specialist  Lightning Experience Reports  Dashboard Specialist  Security Specialist  Flow Fundamentals Super Badge Unit  Flow Administration Super Badge Unit   Experience in training and engaging with users to drive adoption  Experience with Service and Experience Cloud modules  Strong problemsolving capability  Superior communication skills    Our Company   Axalta has remained at the forefront of the coatings industry by continually investing in innovative solutions We engineer technologies that protect customers’ products – whether they are battling heat light corrosion abrasion moisture or chemicals – and add dimension and beauty with colorful finishes We have a vast and everevolving portfolio of brands primed to play an important part in everything from modernizing infrastructure around the world to enabling the next generation of electric and autonomous vehicles   Axalta operates its business in two segments Performance Coatings and Mobility Coatings which serve four end markets including Refinish Industrial Light Vehicle and Commercial Vehicle across North America EMEA Latin America and AsiaPacific Our diverse global footprint allows us to deliver solutions in over 140 countries and coat 30 million vehicles per year We’ve recently set an exciting 2040 carbon neutrality goal in addition to 10 other sustainability initiatives and we take pride in working with our customers to optimize their businesses and achieve their goals   </data></node>
<node id="n1825" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=f1b7102f8c764369&amp;bb=HTB3LzNbtvMzUp0-1YIE_ysMjA5E2r5BVKTGOrYXFQY5FT9GVqHbNrHVe5qJUXuDsuSq6XdGlJC3idrriXXaJhlcQKiAB51yAJypPGTYdhMpdCKKjZrhXQ%3D%3D&amp;xkcb=SoDs67M3CNu_pjxylh0CbzkdCdPP&amp;fccid=f0104f3ccbfb9a31&amp;cmp=Burke-County-Government&amp;ti=Financial+Systems+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in QuickBooksYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay60000  76000 a yearJob typeFulltimeShift and schedule8 hour shift LocationMorganton NC BenefitsPulled from the full job description401kDental insuranceHealth insurance Full job descriptionFinancial Systems Analyst Department Finance Hiring Range 60000 – 76000 dependent on qualifications Closing Date Open Until Filled with review of applications beginning on April 15 2024 General Statement of Duties The purpose of this position is to assist in directing the department’s fiscal accounting budgeting and purchasing functions Successful performance ensures the efficiency and effectiveness of those functions the accuracy of financial records and compliance with relevant laws guidelines policies and procedures The employee must exercise considerable independent and sound judgment The work consists of varied financial duties Strict and frequently changing regulations contribute to the complexity of the position The employee also handles projects and special assignments Work is performed in accordance with established County finance procedures local ordinances and North Carolina General Statutes governing the responsibilities of local government fiscal operations Must understand and follow guidelines including GAAP GASB rulings Local Government Commission standards Yellow Book standards North Carolina statutes federal and state grant regulations and county and department policies and procedures These guidelines require judgment selection and interpretation in application Work is evaluated through conferences reports and an independent audit of financial records and procedures Must be able and willing to work extended schedules as needed to carry out the financial management functions for a variety of county programs Essential Duties and Tasks  Maintains accounting systems and procedures through completion of financial reviews and adjustments approves schedules and entries completed by coworkers for verification of compliance with county policies generally accepted accounting principles GAAP and Governmental Accounting Standards Board GASB rules Responsible for setup and training of all MUNIS users in conjunction with IT staff Works with all modules to insure best use of program options as feasible Serves as County resource for questions and updates to MUNIS Assists with the overall management of the Finance Department including the development and implementation of policies and procedures and assists finance personnel with complex questions or problems Assists with the annual audit process including coordinating preparing and verifying schedules adjustments and reconciliations provided to external auditors Serves on management team for the annual operating budget and capital improvement plan including longrange planning and financial forecasts coordinates departmental and external requests Assists with preparation of the Annual Comprehensive Financial Report ACFR the Annual Financial Information Report and other financial reports Assists with producing the annual budget document with fiveyear capital improvement plan and fee schedule Meets with department managers and staff regarding financial and budgetary issues as needed Reconciles or is responsible for insuring reconciliation of all bank accounts Administers purchasing card program for the County Handles general liability and automobile liability insurance claims for the County Maintain books for TDA using QuickBooks software Performs related duties as required  Desirable Education and Experience Position requires 35 years of progressive finance experience public finance preferred and a bachelor’s degree from an accredited college or university in a related field or an equivalent combination of education and work experience Knowledge Skills and Abilities  Knowledge of North Carolina General Statutes and of local ordinances governing county financial practices and procedures Knowledge of the principles and practices of public finance administration Knowledge of budget development and management principles and practices Knowledge of generally accepted accounting principles GAAP Knowledge of GASB guidelines Knowledge of computers and jobrelated software programs including Microsoft Office and Adobe products Knowledge of QuickBooks or other accounting software Ability to problem solve prioritize and plan Ability to establish and maintain effective working relationships with the public department heads governmental officials employee groups and others Ability to present information clearly and concisely in both oral and written form  Special Requirements  Prior experience using MUNIS software is preferred  Working Conditions Physical requirements include  This is sedentary work requiring the frequent exertion of up to 10 pounds of force and occasional exertion of up to 25 pounds of force Work regularly requires speaking or hearing and using hands frequently requires sitting and repetitive motions occasionally requires standing walking climbing or balancing kneeling or crawling reaching with hands and arms and pushing or pulling  Sensory requirements include  Work has standard vision requirements vocal communication is required for expressing ideas by means of the spoken word and hearing is required to receive detailed information through oral communication andor to make fine distinctions in sound Must possess the visual acuity to prepare and analyze data and figures accounting to operate a computer terminal and to read extensively  Environmental Exposures include  Work has no exposure to adverse environmental conditions Work is generally in a moderately noisy location eg business office light traffic  Application Process Burke County applications can be obtained at the Human Resources office during regular office hours at the address listed above or at the following address wwwburkencorg Completed applications should provide a complete work history including a detailed and thorough list of job duties Incomplete applications will not be processed To receive consideration submit a completed application to Burke County Human Resources Burke County is an Equal Opportunity Employer and does not discriminate on the basis of race color national origin sex religion age or disability in employment or the provision of services Job Type Fulltime Pay 6000000  7600000 per year Benefits  401k Dental insurance Health insurance  Schedule  8 hour shift  Ability to Relocate  Morganton NC Relocate before starting work Required  Work Location In person </data></node>
<node id="n1826" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=088369deea1bc11e&amp;bb=HTB3LzNbtvMzUp0-1YIE_6Bhk_y9Qc3CD0z2nAWnAEavbGSXTgkG0zs3m7NqasZ3bcb4k9uMNSY8N_SbYeyw2P5LdsFnrEcJQZZ8tAOLhMcONgbmY-TrDA%3D%3D&amp;xkcb=SoBx67M3CNu_pjxylh0BbzkdCdPP&amp;fccid=32d641d3102d46d2&amp;cmp=Genisis-Technology-solutions&amp;ti=Senior+Business+Analyst&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid Salesforce Certification certificationYesNoSkillsDo you have experience in ScrumYesNo Job detailsHere’s how the job details align with your profilePay4741  5709 an hourJob typeFulltimeShift and schedule8 hour shift Location205 South Rhode Island Avenue Newport RI 02840 BenefitsPulled from the full job description401kDental insuranceHealth insurance Full job descriptionJob TitleRole Salesforce Business Analyst Mandatory Skills SFDC BA Client Interview Needed for Selection Yes  No NO Detailed JD Pl share the Detailed Description 1 liner JD will not work Required SkillsExcellent knowledge of and demonstrated expertise with Sales Cloud Service Cloud Experience Cloud Marketing cloud and Knowledge of Salesforce Vlocity is a plusKnowledge of development tools and processes that will aid in formulating use cases and designing project requirementsGood understanding of AppExchange productssolutions and appropriately suggest for desired use casesSalesforce Marketing CloudPardot and Integration skills are added advantageExcellent Knowledge of Salesforcecom application Both Sales and Service Cloud preferred Formal Salesforcecom certifications strongly preferredUnderstanding of all Salesforce Lightning  lightning components and basic understanding of Salesforce Object and relationships Apex Visualforce Pages Triggers Workflows rules Page layouts Record Types process builder etcDetail oriented ability to translate business and user requirements into detailed system requirements that are actionable by development teamShould be experienced in executing the platform of Salesforce CRM using Software Development Lifecycle SDLCExcellent knowledge of Scrum Agile SAFe and tools such as Jira and ConfluenceSalesforce Certified Administrator or Salesforce Certified Platform App Builder certification is a plus ResponsibilitiesDocument and design current and future Salesforcecom enabled solutions and drive all relevant business analysis to ensure the most effective recommendations are made for successful solution and project plan completionExceptional interpersonal skills and written communication skills to frequently interact with all levels of the organization as well as global cultures Ability to interpret technical documentation to meet business needs Correctly identifies system  functional interdependenciesDemonstrates ability to effectively work both independently and within cross functional project teams Seen as a cross functional process and subject matter expertAccurately translating Business Requirements into detailed Salesforcecom functionality requirements High aptitude for interpreting technical documentation as well as authoring or updating documents as needed Functional Designs Business Process DesignsAbility to selfmanage multiple deliverables within tight timeframes and dynamic priorities Based on experience can accurately estimate the cost and time to implement complex enterprise level solutionsExtensive experience interpreting user needs and writing or editing functional specifications for new systems systems changes and  or system enhancements Has the ability to present ideas in a focused and concise manner Ability to create compelling business justification for recommended direction and designDemonstrates leadership skills by mentoring more junior level staff and leading meetingsCreates and executes complex test scenarios and scripts provides training to end users and responds to critical production support requestsAssist project managers in developing project plans executing project tasks and communicating issues and status in a timely manner Regards  Job Type Fulltime Salary 4741  5709 per hour Benefits  401k Dental insurance Health insurance  Experience level  7 years  Schedule  8 hour shift  Ability to Relocate  Newport RI 02840 Relocate before starting work Required  Work Location In person </data></node>
<node id="n1827" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">quantitative methods</data></node>
<node id="n1828" labels=":Skill"><data key="labels">:Skill</data><data key="name">analytical tools</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1829" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">quantitative models</data></node>
<node id="n1830" labels=":Skill"><data key="labels">:Skill</data><data key="name">social sciences</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1831" labels=":Skill"><data key="labels">:Skill</data><data key="name">problem solver</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1832" labels=":Skill"><data key="labels">:Skill</data><data key="name">health economics</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1833" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">statistical methodologies</data></node>
<node id="n1834" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">drafting protocols</data></node>
<node id="n1835" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">disseminating results</data></node>
<node id="n1836" labels=":Skill"><data key="labels">:Skill</data><data key="name">version control</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1837" labels=":Skill"><data key="labels">:Skill</data><data key="name">data maintenance</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1838" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">computational techniques</data></node>
<node id="n1839" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">facilitating workshops</data></node>
<node id="n1840" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">identify painpoints</data></node>
<node id="n1841" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">validate solutions</data></node>
<node id="n1842" labels=":Skill"><data key="labels">:Skill</data><data key="name">confirm scope</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1843" labels=":Skill"><data key="labels">:Skill</data><data key="name">mockups</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1844" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">validate technical solutions</data></node>
<node id="n1845" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">swim lane diagrams</data></node>
<node id="n1846" labels=":Skill"><data key="labels">:Skill</data><data key="name">lucidchart</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1847" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">recommend opportunities</data></node>
<node id="n1848" labels=":Skill"><data key="labels">:Skill</data><data key="name">impact assessment</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1849" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">technical specification</data></node>
<node id="n1850" labels=":Skill"><data key="labels">:Skill</data><data key="name">business process</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1851" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">interviewing stakeholders</data></node>
<node id="n1852" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">translating requirements</data></node>
<node id="n1853" labels=":Skill"><data key="labels">:Skill</data><data key="name">review reports</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1854" labels=":Skill"><data key="labels">:Skill</data><data key="name">data cleansing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1855" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">stakeholder meetings</data></node>
<node id="n1856" labels=":Skill"><data key="labels">:Skill</data><data key="name">test plans</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1857" labels=":Skill"><data key="labels">:Skill</data><data key="name">reading log files</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1858" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">regression testing</data></node>
<node id="n1859" labels=":Skill"><data key="labels">:Skill</data><data key="name">gaps</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1860" labels=":Skill"><data key="labels">:Skill</data><data key="name">gather data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1861" labels=":Skill"><data key="labels">:Skill</data><data key="name">user needs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1862" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">define requirements</data></node>
<node id="n1863" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analyse process issues</data></node>
<node id="n1864" labels=":Skill"><data key="labels">:Skill</data><data key="name">workflow charts</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1865" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">influence key stakeholders</data></node>
<node id="n1866" labels=":Skill"><data key="labels">:Skill</data><data key="name">brainstorming</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1867" labels=":Skill"><data key="labels">:Skill</data><data key="name">leansig</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1868" labels=":Skill"><data key="labels">:Skill</data><data key="name">sigma</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1869" labels=":Skill"><data key="labels">:Skill</data><data key="name">acap</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1870" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">drafting proposals</data></node>
<node id="n1871" labels=":Skill"><data key="labels">:Skill</data><data key="name">data reporting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1872" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">spreadsheet software</data></node>
<node id="n1873" labels=":Skill"><data key="labels">:Skill</data><data key="name">develop report</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1874" labels=":Skill"><data key="labels">:Skill</data><data key="name">upload data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1875" labels=":Skill"><data key="labels">:Skill</data><data key="name">mii</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1876" labels=":Skill"><data key="labels">:Skill</data><data key="name">me</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1877" labels=":Skill"><data key="labels">:Skill</data><data key="name">ui5</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1878" labels=":Skill"><data key="labels">:Skill</data><data key="name">sap memii</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1879" labels=":Skill"><data key="labels">:Skill</data><data key="name">bls</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1880" labels=":Skill"><data key="labels">:Skill</data><data key="name">xslt</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1881" labels=":Skill"><data key="labels">:Skill</data><data key="name">sap pco</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1882" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">functional management</data></node>
<node id="n1883" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">application training</data></node>
<node id="n1884" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">design specifications</data></node>
<node id="n1885" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">subject matter expert</data></node>
<node id="n1886" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">regulatory reporting</data></node>
<node id="n1887" labels=":Skill"><data key="labels">:Skill</data><data key="name">quality reporting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1888" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytical prowess</data></node>
<node id="n1889" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">creative content development</data></node>
<node id="n1890" labels=":Skill"><data key="labels">:Skill</data><data key="name">multitasking</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1891" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">attention to detail</data></node>
<node id="n1892" labels=":Skill"><data key="labels">:Skill</data><data key="name">targeted surveys</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1893" labels=":Skill"><data key="labels">:Skill</data><data key="name">photoshop</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1894" labels=":Skill"><data key="labels">:Skill</data><data key="name">illustrator</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1895" labels=":Skill"><data key="labels">:Skill</data><data key="name">sap otc</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1896" labels=":Skill"><data key="labels">:Skill</data><data key="name">p2p</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1897" labels=":Skill"><data key="labels">:Skill</data><data key="name">ricef</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1898" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">understand the requirements</data></node>
<node id="n1899" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">datadriven insights</data></node>
<node id="n1900" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">sales cloud lightning</data></node>
<node id="n1901" labels=":Skill"><data key="labels">:Skill</data><data key="name">b2b commerce</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1902" labels=":Skill"><data key="labels">:Skill</data><data key="name">change control</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1903" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">incident management</data></node>
<node id="n1904" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">management functions</data></node>
<node id="n1905" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytical methods</data></node>
<node id="n1906" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">evaluate information</data></node>
<node id="n1907" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">identify market trends</data></node>
<node id="n1908" labels=":Skill"><data key="labels">:Skill</data><data key="name">market research</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1909" labels=":Skill"><data key="labels">:Skill</data><data key="name">present findings</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1910" labels=":Skill"><data key="labels">:Skill</data><data key="name">vlookups</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1911" labels=":Skill"><data key="labels">:Skill</data><data key="name">jungle</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1912" labels=":Skill"><data key="labels">:Skill</data><data key="name">scout</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1913" labels=":Skill"><data key="labels">:Skill</data><data key="name">keepa</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1914" labels=":Skill"><data key="labels">:Skill</data><data key="name">resolve issues</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1915" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">interpersonal skills</data></node>
<node id="n1916" labels=":Skill"><data key="labels">:Skill</data><data key="name">time management</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1917" labels=":Skill"><data key="labels">:Skill</data><data key="name">follow guidelines</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1918" labels=":Skill"><data key="labels">:Skill</data><data key="name">munis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1919" labels=":Skill"><data key="labels">:Skill</data><data key="name">quickbooks</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1920" labels=":Skill"><data key="labels">:Skill</data><data key="name">safe</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1921" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">updating documents</data></node>
<node id="n1922" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=5bddec5615f1184d&amp;bb=HKH-ysDsRD4PV8MpMJn7w-ElJ7whwJS_70qbDnHEjwF7sJkOc6QuhKJUwvIaHrGRjAPY4XJ-RylRmKLqo-SABaW1Hc0GaVzeppB9JW4GZc-XTqPC1WJd2w%3D%3D&amp;xkcb=SoBV67M3CNjvTlWbHp0FbzkdCdPP&amp;fccid=67827b2d5b3e6ebe&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid Secret Clearance licenseYesNoSkillsDo you have experience in ISO 27001YesNoEducationDo you have a Bachelors degreeYesNo LocationLexington Park MD BenefitsPulled from the full job description401kDental insuranceHealth insuranceOpportunities for advancementProfit sharingTuition reimbursementVision insurance Full job description Job Location Lexington Park Maryland   Job Code 16424193   Imagine One Technology  Management is currently seeking a Data Engineer “contingent” on award of the associated work to the Imagine One Team This position supports the US Navy in Lexington Park Maryland  The Data Engineer discovers opportunities for data acquisition and sensor integration performs systems engineering for databases and data processing systems develops data systems architectures and models leverages highperformance computing infrastructures into systems architecture and recommends best architectures tools and technologies to address organizational needs  Security Requirements   Candidates must have US Citizenship  Candidates must have an ACTIVE DoD Secret Clearance or higher    Experience Requirements   Minimum of ten 10 years of experience carrying out duties similar to the functions above     Educational Requirements   Bachelor’s degree from an accredited fouryear college or university  Candidates who do not possess a bachelors degree can still qualify with one of the following   Six 6 years of additional experience for a total of 16 years of experience OR Associate’s degree plus four 4 years of additional experience for a total of 14 years of experience      Imagine One offers a full package of benefits and competitive salary excellent group medical vision and dental programs 401K savings plan 4K annual tuition reimbursement 5K if pursuing Master’s degree employee training development and education programs profit sharing advancement opportunities and much more   Imagine One is an EmployeeOwned Business  ISO 90012015 ISO 2000012018 ISO 270012013  CMMI Development Level 3   Imagine One Technology  Management Ltd is an Equal OpportunityAffirmative Action Employer Protected veterans and individuals with disabilities encouraged to apply    Imagine One “Contingent” offers for employment may stipulate that one or more requirements be satisfied before final commitment between candidate and Imagine One is established namely award of contract to the Imagine One Team Contingent requirements vary and may also include but not be limited to additional factors ie the position still being available after negotiations with the Government final approval of your qualifications by the Government or ability to successfully acquire andor transfer a DoD security clearance        Get job alerts by email Sign up now Join Our Talent Network      Job Snapshot  Employee Type Contractor      Location Lexington Park MD Onsite      Job Type Engineering      Experience Not Specified      Date Posted 03192024      Job ID 16424193      </data></node>
<node id="n1923" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=a1b3553c1d019b53&amp;bb=HKH-ysDsRD4PV8MpMJn7w5lgy9lyYzgLpPSEyoxK96qk-UY3bk7EcEqRU9jftHW-eH25MSLgEZvRbWj29SVyAxfatds1jYi08N4zcp2XfI9U6zyL3HeL0Q%3D%3D&amp;xkcb=SoDh67M3CNjvTlWbHp0EbzkdCdPP&amp;fccid=aba787904aa14d6f&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid TSSCI licenseYesNoCertificationsDo you have a valid ITIL Certification certificationYesNoSkillsDo you have experience in VirtualizationYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationAnnapolis Junction MD Full job description  Overview       Abile Group has an exciting and challenging opportunity for a Data Center Engineer InfrastructureCloud supporting a DoD Customers Classified Network Services The mission will include Operations Compliance Cyber Security Customer Service and Engineering The right Desktop Systems Administrator candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally   Responsibilities    Designs delivers and optimizes Virtual Infrastructure Services to improve security availability performance and resource utilization  Reviews current virtual infrastructures architectures designs and operations to identify potential improvements  Analyzes refines and documents the requirements for transition from legacy architectures into modern virtualized infrastructure environment  Designs virtual Infrastructure services to maximize available capacity of physical compute storage and network hardware resources  Documents security configuration and performance benchmarks of Virtual Infrastructure Services  Qualifications     Clearance Required TSSCI      Degree and Years of Experience 5 to 8 years with BSBA or 3 to 5 years with MSMA or 0 to 2 years with PhD      Required Certifications    DoD 85701M IAT Level II certification ie Security    Desired Certifications    ITIL Foundations certification  Professional level VMware certification    Required Skills    Proven expertise in the VMware suite of cloud computing and virtualization technology  About Abile Group Inc       Abile Group Inc was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics  Performance Management IT  Systems Engineering and Program  Project Management We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise   EEO Statement       Abile Group Inc is an Equal Opportunity Employer All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin or protected veteran status and will not be discriminated against on the basis of disability Anyone requiring reasonable accommodations should email careersabilegroupcom with requested details A member of the HR team will respond to your request within 2 business days      Please review our current job openings and apply for the positions you believe may be a fit If you are not an immediate fit we will also keep your resume in our database for future opportunities    </data></node>
<node id="n1924" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0e4a3df056005be3&amp;bb=HKH-ysDsRD4PV8MpMJn7w20bFxJz9c-vTbw1i8c4rvHd-95zSL3ThX9PaFpkID_f-DT7ZP6R92yc5Zd3f6MFDRYLBBOhlJ0PKGPD_8JcNKYvMXQEy7Mi9Q%3D%3D&amp;xkcb=SoDG67M3CNjvTlWbHp0bbzkdCdPP&amp;fccid=578fa8376f4eec04&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid Secret Clearance licenseYesNoSkillsDo you have experience in ServiceNowYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationSan Antonio TX 78201 BenefitsPulled from the full job descriptionOpportunities for advancement Full job description Secure our Nation Ignite your Future   Become an integral part of a diverse team while working at an Industry Leading Organization where our employees come first At ManTech International Corporation you’ll help protect our national security while working on innovative projects that offer opportunities for advancement   Currently ManTech is seeking a motivated career and customeroriented SalesforceServiceNow Data Engineer to join our team This is hybrid role splitting time working remote and on client site   Responsibilities include but are not limited to   Researches and integrates design strategies product specifications  development schedules and user expectations into product capabilities  Develops technical designs and specifications for complex document file data  pipelinesdata flows and data migrations with ServiceNow or Salesforce  Applications  Uses ETL tools or languages to build test and maintain product modules  components and subsystems for data  Performs data integrations between systems using industry standard tools and  connectivity protocols  Identifies data gaps and potential remediation or integration activity for  consideration by PM andor customer  Leads and influences team on project deliverables  Drives quality assurance program for project deliverables  Creates quality deliverables for customers  Drives full life cycle of servicessolution delivery for projects  Provides technical leadership to lowerlevel engineers    Basic Qualifications   Bachelor’s degree in computer science Business Engineering Math or related field OR10 years of comparable work experience  The successful candidate must be able to work remotely and be able to travel  occasionally as needed  5 years of experience as a Data Engineer or similar role with a strong track  record of architecting and implementing complex solutions using data migration  and data integration tools  Must have experience with migrating managing connecting and sustaining  document management databases  Experienced in processing and building automated ETL pipelines for  documentation files such as PDFs Excels between source and target systems    Security Clearance Requirements   US citizenship requiring a background check  Ability to obtain a SECRET Clearance    For all positions requiring access to technologysoftware source code that is subject to export control laws employment with the company is contingent on either verifying USperson status or obtaining any necessary license The applicant will be required to answer certain questions for export control purposes and that information will be reviewed by compliance personnel to ensure compliance with federal law ManTech may choose not to apply for a license for such individuals whose access to exportcontrolled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone   ManTech International Corporation as well as its subsidiaries proactively fulfills its role as an equal opportunity employer We do not discriminate against any employee or applicant for employment because of race color sex religion age sexual orientation gender identity and expression national origin marital status physical or mental disability status as a Disabled Veteran Recently Separated Veteran Active Duty Wartime or Campaign Badge Veteran Armed Forces Services Medal or any other characteristic protected by law   If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system please contact ManTechs Corporate EEO Department at 703 2186000 ManTech is an affirmative actionequal opportunity employer  minorities females disabled and protected veterans are urged to apply ManTechs utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunityaffirmative action policies ManTech does not accept resumes from unsolicited recruiting firms We pay no fees for unsolicited services   If you are a qualified individual with a disability or a disabled veteran you have the right to request an accommodation if you are unable or limited in your ability to use or access httpwwwmantechcomcareersPagescareersaspx as a result of your disability To request an accommodation please click careersmantechcom and provide your name and contact information  </data></node>
<node id="n1925" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=77139425a9ee8408&amp;bb=HKH-ysDsRD4PV8MpMJn7w1vYq7-sZMKKj4nmQVzFTlPRYhbvNYi9R_JzlRF7HjryZad9gaXJMF77mIsq7mrzmCQT0yrQnjZcHT5E3bMr6C2b3DBshAiHxg%3D%3D&amp;xkcb=SoBy67M3CNjvTlWbHp0abzkdCdPP&amp;fccid=e2c1d9254247b2b7&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid TSSCI licenseYesNoCertificationsDo you have a valid CompTIA Security certificationYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationSan Antonio TX Full job descriptionTekPro Support Services LLC TSS is seeking a Data and Cloud Engineer    Datacloud engineer supportsintegrates enterprise database systems using sound database management practices to organize and store data Interacts with development and enduser personnel to determine application data access requirements transaction rates volume analysis and other pertinent data required to develop and maintain integrated databases Provides support and reviews webbased applications including online and internal applications to support client operations Support the implementation of interfaces to applications review both client side and serverside code to allow webbased applications to deliver the correct content Be able to translate applications requirements into the design of complex web sites including integrating web pages and applications Must be able to apply new and emerging technologies to the site development process  Qualifications   CompTIA Security CE required Active TSSCI required  </data></node>
<node id="n1926" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=598d9eae2a89ab8b&amp;bb=UkWqgXROFXK6Hr5wr58UhXxZ7PuS9roaHE59gxhnPO32ilsSBGIjtsz1_RFMyx4W3IAAAGyz9udzbnFVRGGvYtvEMct7px1qqNV07YS3e2I%3D&amp;xkcb=SoC467M3CNjr8KwiIp0LbzkdCdPP&amp;fccid=66403b30a2c0d89c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in System designYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRemote BenefitsPulled from the full job description401k401k matchingFlexible spending accountHealth insurancePaid time offStock options Full job description             Remote United States                      Data Science and Engineering                     Netflix is enjoyed by more than 230 million members globally entertaining new audiences every day We are in the midst of major transformative developments for the Netflix product with the launch of an Advertisingsupported plan Games and Live content These new products alongside our streaming service have resulted in significant increases in the complexity and breadth of our internal data ecosystem We manage one of the largest paid subscription businesses and are committed to handling our members’ data with a high degree of care towards appropriate use as well as compliance with local privacy laws and regulations in the 190 countries where we operate        The Privacy and Legal Data Engineering pod builds scalable data management and extraction frameworks which are at the core of our ability to hold ourselves to the highest data privacy and hygiene standards in the industry        We are looking for a Data Engineer to help augment our ability to build these robust scalable privacycentric data frameworks As part of this team you will work on diverse data technologies such as Spark Presto Flink Kafka and others to build insightful scalable and robust data pipelines write ETL jobs to collect and aggregate data and build highquality data frameworks that enable appropriate handling of customer personal data        The ideal candidate will bring a strong track record of having built data systems and frameworks to strengthen the privacy posture at large consumer businesses They will have a deep background in distributed data processing and share our passion for continuously improving the ways we handle data to make Netflixs data privacy posture better        Who you are   Passionate about consumercentric data privacy and risk mitigation for the business  Highly proficient in at least one of Java Python or Scala with at least 10 years of softwaredata engineering experience  Proficient in advanced SQL and effective in a complex data environment  Highly experienced in engineering data pipelines using big data technologies Hive Presto Spark Flink on medium to large scale data sets  Comfortable working crossfunctionally with multiple types of stakeholder groups  Able to successfully lead large complex systems design and implementation challenges independently     What you will do   Design and implement elegant frameworks to scalably meet various internal and consumerfacing data privacy and legal needs  Engineer efficient adaptable and scalable data pipelines to process structured and unstructured data  Develop a deep understanding of the data ecosystem at Netflix from a privacy lens  Partner with the privacy engineering teams to understand product goals and provide data that enables us to respond to customer and regulatory data requests  Mentor and inspire teammates while elevating the impact of the team  Enable Netflix to effectively manage legal and regulatory risk and compliance while maintaining a high standard of consumer data privacy expectations         A few more things to know           Our culture is unique and we live by our values so its worth learning more about Netflix at jobsnetflixcomculture We regularly share examples of our work on our tech blog You will need to be comfortable working in the most agile of environments Requirements will be vague Iterations will be rapid You will need to be nimble and take smart risks        Our compensation structure consists solely of an annual salary we do not have bonuses You choose each year how much of your compensation you want in salary versus stock options To determine your personal topofmarket compensation we rely on market indicators and consider your specific job family background skills and experience to determine your compensation in the market range The range for this role is 170000  720000        Netflix provides comprehensive benefits including Health Plans Mental Health support a 401k Retirement Plan with employer match Stock Option Program Disability Programs Health Savings and Flexible Spending Accounts Familyforming benefits and Life and Serious Injury Benefits We also offer paid leave of absence programs Fulltime hourly employees accrue 35 days annually for paid time off to be used for vacation holidays and sick paid time off Fulltime salaried employees are immediately entitled to flexible time off See more details about our Benefits here        Netflix is a unique culture and environment Learn more here        We are an equal opportunity employer and celebrate diversity recognizing that diversity of thought and background builds stronger teams We approach diversity and inclusion seriously and thoughtfully We do not discriminate on the basis of race religion color ancestry national origin caste sex sexual orientation gender gender identity or expression age disability medical condition pregnancy genetic makeup marital status or military service         </data></node>
<node id="n1927" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=8113001dad7c7efc&amp;bb=UkWqgXROFXK6Hr5wr58UhZQXTF4woyLPcmnoj5YTf3mmYLYjULKWK6RSAl9e24raReVRr3Qp0F7nn7E5Ol-BL7pEYnQtybTDBpyK3l7QCaE%3D&amp;xkcb=SoAM67M3CNjr8KwiIp0KbzkdCdPP&amp;fccid=1ea0475711674f3b&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location100 Connell Drive Berkeley Heights NJ 07922 Full job description Calling all innovators – find your future at Fiserv   We’re Fiserv a global leader in Fintech and payments and we move money and information in a way that moves the world We connect financial institutions corporations merchants and consumers to one another millions of times a day – quickly reliably and securely Any time you swipe your credit card pay through a mobile app or withdraw money from the bank we’re involved If you want to make an impact on a global scale come make a difference at Fiserv   Job Title Senior Data Engineer    What does a successful Senior Data Engineer do at Fiserv   As a member of our Data Commerce Solutions group you will build and take ownership of the design and development of data engineering projects within Fiserv’s Enterprise Data Commerce Solutions division You will be responsible for building and taking ownership over largescale data engineering integration and warehousing projects build custom integrations between cloudbased systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by using the Data integration tool with coding across several languages such as Java Python and SQL Additional responsibilities include but are not limited to architect build and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create   What you will do   Collaborate with crossfunctional teams to design scalable data architecture and create robust data processing pipelines  Design and implement data models that align with business requirements enabling seamless data access and analytics  Identify opportunities to enhance data processing efficiency and implement performance optimizations for our data pipelines  Implement data quality checks and validation processes to ensure the accuracy and integrity of our data  Work closely with data scientists analysts and software engineers to understand data needs provide technical support and troubleshoot datarelated issues  Stay up to date with the latest data engineering technologies and tools and recommend improvements to our existing systems  Maintain comprehensive documentation of data engineering processes data flows and system configurations   What you will need to have   610 years of overall industry experience with at least 6 years experience in building largescale big data applications development and a bachelor’s degree in computer science or a related field  Possess strong technical leadership skills demonstrating expertise in developing data solutions building frameworks and designing solutions for processing large volumes of data using data processing tools and Big Data platforms    Handson experience building Data Lake EDW and data applications on Azure cloud Proficiency in major programmingscripting languages like Java andor Python  Strong understanding of cluster and parallel architecture experience with highscale databases and SQL and exposure to NoSQL databases like Cassandra HBase DynamoDB and Elastic Search  Conduct code reviews and strive for improvement in software engineering quality  Experience in realtime data processing and streaming technologies like Kafka and Apache Beam as well as a successful track record in delivering big data projects using Kafka and Spark    What would be great to have   Experience in Banking Financial domain  Advanced certifications in data engineering or related fields  Familiarity with machine learning frameworks and data science workflows  Knowledge of containerization technologies like Docker and orchestration tools like Kubernetes  Experience working with PCI Data and collaborating with data scientists Knowledge of data governance security and privacy principle    This role is not eligible to be performed in Colorado California Hawaii New York or Washington   Please note that salary ranges provided for this role on external job boards are salary estimates made by outside parties and may not be accurate   Thank you for considering employment with Fiserv Please    Apply using your legal name  Complete the stepbystep profile and attach your resume either is acceptable both are preferable    What you should know about us   Fiserv is a global fintech leader with 40000plus and growing associates proudly serving clients in more than 100 countries As a FORTUNE™ 500 company one of Fast Company’s Most Innovative Companies and a top scorer on Bloomberg’s GenderEquality Index we are committed to excellence and purposeful innovation   Our commitment to Diversity and Inclusion   Fiserv is an Equal Opportunity Employer and we welcome and encourage diversity in our workforce that reflects our world All qualified applicants will receive consideration for employment without regard to race color religion sexual orientation gender identity national origin disability protected veteran status or any other category protected by law   We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process to perform essential job functions and to receive other benefits and privileges of employment Please contact us to request accommodation   Warning about fake job posts   Please be aware of fraudulent job postings that are not affiliated with Fiserv Fraudulent job postings may be used by cyber criminals to target your personally identifiable information andor to steal money or financial information   Any communications from a Fiserv representative will come from a legitimate business email address We will not hire through text message social media or email alone and any interviews will be conducted in person or through a secure video call We won’t ask you for sensitive information nor will we ask you to pay anything during the hiring process We also won’t send you a check to cash on Fiserv’s behalf   If you see suspicious activity or believe that you have been the victim of a job posting scam you should report it to your local FBI field office or to the FBI’s Internet Crime Complaint Center  </data></node>
<node id="n1928" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=d94a7763ac4ec289&amp;bb=UkWqgXROFXK6Hr5wr58UhdPjWsPtgFSnh83meCL14_iW_4J3JqvxJNXf61IBcx4JZTjGpqlkfUw-tBs73qy4ErWRtNF6p9ds3_sS3TvbNwg%3D&amp;xkcb=SoCR67M3CNjr8KwiIp0JbzkdCdPP&amp;fccid=3d30677097704d8f&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionCertificationsDo you have a valid AWS Certification certificationYesNoSkillsDo you have experience in TerraformYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationTexas BenefitsPulled from the full job description401k401k matchingAdoption assistanceDental insuranceDisability insuranceEmployee assistance programFlexible scheduleShow morechevron down Full job description Date Posted 20231120   Country United States of America   Location HTX99 Field Office  TX Remote Location Remote City TX 73301 USA   Position Role Type Remote     Do you want to be part of the team that builds the Data Platform at the center of Transforming the Aviation Industry   As a Principal Data Engineer you will be on a mission to ensure that our Data Platform can leverage data from various sources to help transform the passenger journey as well as help our customers leverage data to improve their operations This role is responsible for the design development and maintenance of data processes and pipelines supporting critical Strategic Business Unit SBU Data initiatives in support of the Digital Transformation The scope of the role encompasses many facets of the aviation industry You will work with Data Scientists and Application Developers to build databased products that benefit commercial airlines airports and passengers Your work will influence the next generation of connected aviation products   You will work with a team where you will be able to share your ideas and vulnerabilities and will be treated with care and empathy You will work with a team that shows courage in doing the right thing not because it is easy This is a great opportunity with room to grow and learn about new and interesting technology solutions   Our business unit Connected Aviation Solutions CAS is leading the Connected Ecosystem strategic pillar for Collins Aerospace The Data Management  Data Science DMDS team has endtoend responsibility to ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications  whether via API’s analytics andor data visualizations   What YOU will do   YOU will contribute to establishing Data Engineering best practices building the DataOps model and enabling the ML and AI roadmap of the future  YOU will develop automation and monitoring processes that support the data pipelines  YOU will work closely with the architecture team to implement modern data repositories that support the CAS use cases Pipelines API’s Data Science Applications and Visualizations  YOU will work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption  YOU will work with the CAS and DT Enterprise Data Architects to automate cloud deployments as well as build CICD pipeline to support cloudbased workloads This includes developing views materialized views and SQL scripts  YOU may travel domestically and internationally up to 15  YOU will work on a distributed and diverse team that collaborates and communicates well    What YOU will learn   YOU will learn all about the datasets that are produced in the aerospace ecosystem such as how the various components in an aircraft interact with each other  YOU will learn how to enable Data Scientists to perform ML and AI experiments  YOU will gain exposure to large scale data processing leveraging modern technology stacks including Databricks and AWS native services  YOU can take flight to becoming a subject matter expert and leader in Data Engineering with exposure to the variety of business and products in an everevolving aerospace industry CAS is growing and so can you    Education  Experience   Typically requires a degree in Science Technology Engineering or Mathematics STEM unless prohibited by local lawsregulations and minimum 8 years prior relevant experience or an Advanced Degree in a related field and minimum 5 years of experience or in absence of a degree 12 years of relevant experience    Qualifications You Must Have   Must be authorized to work in the US without sponsorship now or in the future RTX will not offer sponsorship for this position  Demonstrated engineering experience in system integration and design data pipeline development or softwareservice development and deployment  Experience building data pipelines leveraging tools like Spark Python PySpark  SQL as well as working with AWSAzure Cloud platforms and related services and Terraform Gitlab and similar CICD tools  Experience with Databricks Platform and leveraging that platform to build out a Data Lake    Skills We Value   Professional background developing complex SQL queries and programming in Python with ability to transform raw data into valuable insights  Experience designing cloudbased data platforms that ensure cost optimization scalability performance and ease of use for end users of the platform  Experience with Micro Services Architectures  AWSAzure certifications    Collins Aerospace an RTX company is a leader in technologically advanced and intelligent solutions for the global aerospace and defense industry Collins Aerospace has the capabilities comprehensive portfolio and expertise to solve customers’ toughest challenges and to meet the demands of a rapidly evolving global market   reempowerprogram  This role is also eligible for the ReEmpower Program The ReEmpower Program helps support talented and committed professionals as they rebuild their capabilities enhance leadership skills and continue their professional journey Over the course of the 14week program experienced professionals will gain paid onthejob experience have an opportunity to participate in sessions with leadership develop personalized plans for success and receive coaching to guide their returntowork experience Upon completion of the program based on performance and contributions participants will be eligible for a career at RTX   Minimum Program Qualifications   Be on a career break of one or more year at time of application  Have prior experience in functional area of interest  Have interest in returning in either a fulltime or parttime position    Connected Aviation Solutions  Our Connected Aviation Solutions team provides advanced information management systems products and services that enable the connected ecosystem by bringing together Collins’ unique breadth of aviation products with our smart digital solutions to help us enhance every aspect of the endtoend travel experience We help airlines airports and business aircraft turn data into value to streamline operations increase efficiency and reduce cost enhance the passenger experience and contribute to sustainable flight By combining the best networks connectivity and dataanalytics solutions we’re solving big problems for our customers and the world while enhancing the security and connectivity of systems both on and off the aircraft to help operators and passengers stay more connected and informed and create a more sustainable efficient reliable and enjoyable travel experience Aviation connects the world Our Connected Aviation Solutions team connects aviation Sustainably Seamlessly Securely   Diversity drives innovation inclusion drives success We believe a multitude of approaches and ideas enable us to deliver the best results for our workforce workplace and customers We are committed to fostering a culture where all employees can share their passions and ideas so we can tackle the toughest challenges in our industry and pave new paths to limitless possibility  WE ARE REDEFINING AEROSPACE    Please ensure the role type defined below is appropriate for your needs before applying to this role    Remote Employees who are working in Remote roles will work primarily offsite from home An employee may be expected to travel to the site location as needed   Position is remote however if you live within a reasonable commute of a Collins site with other colleagues you interact with your manager will discuss whether there is a degree of onsite presence associated with this role     Some of our competitive benefits package includes   Medical dental and vision insurance  Three weeks of vacation for newly hired employees  Generous 401k plan that includes employer matching funds and separate employer retirement contribution including a Lifetime Income Strategy option  Tuition reimbursement program  Student Loan Repayment Program  Life insurance and disability coverage  Optional coverages you can buy pet insurance home and auto insurance additional life and accident insurance critical illness insurance group legal ID theft protection  Birth adoption parental leave benefits  Ovia Health fertility and family planning  Adoption Assistance  Autism Benefit  Employee Assistance Plan including up to 10 free counseling sessions  Healthy You Incentives wellness rewards program  Doctor on Demand virtual doctor visits  Bright Horizons child and elder care services  Teladoc Medical Experts second opinion program  And more    At Collins the paths we pave together lead to limitless possibility And the bonds we form – with our customers and with each other  propel us all higher again and again   Apply now and be part of the team that’s redefining aerospace every day   The salary range for this role is 96000 USD  200000 USD The salary range provided is a good faith estimate representative of all experience levels RTX considers several factors when extending an offer including but not limited to the role function and associated responsibilities a candidate’s work experience location educationtraining and key skills   Hired applicants may be eligible for benefits including but not limited to medical dental vision life insurance shortterm disability longterm disability 401k match flexible spending accounts flexible work schedules employee assistance program Employee Scholar Program parental leave paid time off and holidays Specific benefits are dependent upon the specific business unit as well as whether or not the position is covered by a collectivebargaining agreement   Hired applicants may be eligible for annual shortterm andor longterm incentive compensation programs depending on the level of the position and whether or not it is covered by a collectivebargaining agreement Payments under these annual programs are not guaranteed and are dependent upon a variety of factors including but not limited to individual performance business unit performance andor the company’s performance   This role is a USbased role If the successful candidate resides in a US territory the appropriate pay structure and benefits will apply   RTX anticipates the application window closing approximately 40 days from the date the notice was posted However factors such as candidate flow and business necessity may require RTX to shorten or extend the application window    RTX is An Equal OpportunityAffirmative Action Employer All qualified applicants will receive consideration for employment without regard to race color religion sex sexual orientation gender identity national origin disability or veteran status age or any other federally protected class   Privacy Policy and Terms  Click on this link to read the Policy and Terms  </data></node>
<node id="n1929" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e425cb2bbab68927&amp;bb=UkWqgXROFXK6Hr5wr58UhfUgKJJAX3HpR3aJ0UIATp1q341ctfv2Mz1xHm4EWr3xAf6cMZYYY7b89MPE6kwK2305EjJRK94u2Nv9ojqs5qj-tBpw4cOCTg%3D%3D&amp;xkcb=SoAl67M3CNjr8KwiIp0IbzkdCdPP&amp;fccid=f766f8bfbc3effb7&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in System designYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay158000 a yearJob typeFulltime LocationSunnyvale CA Full job description About the Team   The Global Intelligence Team focuses on making Uber take meaningful marketplace decisions with better data and algorithms The ambitious problems include modeling sophisticated marketlevel dynamics rider and driver choices crossservice decisions across rideseats and finetuning Ubers pricing with dataalgorithms from this team The software engineers on the team use meaningful amounts of data to address these challenges building scalable engineering solutions    We are looking for people who are passionate about solving ambitious businessproduct issues with welltrained data engineering expertise with prior experience or interest in science models and methodologies and who are also passionate about seeking the truth via deepdiving into the complicated structured and unstructured data    About the Role    What the Candidate Will Do    Work on creating a platform that powers data driven decision making for Uber Rides and Eats line of business  Design and develop new systems to empower fast datadriven decisions  Build distributed backend systems serving realtime analytics and machine learning features at Uber scale  Work with the product and science team to build and drive technical roadmap and vision for the team   Basic Qualifications    3 years of fulltime engineering experience  Experience working with multiple multifunctional teams product scienceproduct ops etc  Understanding of Big data architecture ETL frameworks and platforms  Expertise in one or more objectoriented programming languages eg Python Go Java C and the eagerness to learn more  Experience with datadriven architecture and systems design  Proven experience in largescale distributed storage and database systems SQL or NoSQL eg MySQL Cassandra and data warehousing architecture and data modeling   Preferred Qualifications    Experience or interest in learning science models and methodologies  Experience building complex systems and knowledge of Hadoop related technologies such as HDFS Kafka Hive and Presto  BSMSPhd in Computer Science or related field required   For San Francisco CAbased roles The base salary range for this role is USD158000 per year  USD175500 per year For Sunnyvale CAbased roles The base salary range for this role is USD158000 per year  USD175500 per year For all US locations you will be eligible to participate in Ubers bonus program and may be offered an equity award  other types of comp You will also be eligible for various benefits More details can be found at the following link httpswwwubercomcareersbenefits  </data></node>
<node id="n1930" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=7359cb27d82b1399&amp;bb=UkWqgXROFXK6Hr5wr58UhVyN2uYldS5Wfk99rxDhk2cv9rTyt5aCteSTzdZwzyvUoTALQHQfb5iulkv4Eojt94VNVBrbrmQnEIeYFQ0KquRbdP3AWx33dw%3D%3D&amp;xkcb=SoCr67M3CNjr8KwiIp0PbzkdCdPP&amp;fccid=ccbb60cd599a87fc&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationSugar Land TX Full job descriptionPosition Title Senior Data Engineer    Position Summary  The Senior Data Engineer is responsible for designing building and maintaining the enterprise data warehouse ensuring data quality and supporting data lake and surrounding technology ecosystem including Master Data Management Extract Load Transform Data Security and Data Catalog Domain Financial Services – Banking    Essential Duties and Responsibilities  Designs builds and maintains data architectures pipelines and Extract Load Transform processes to ensure efficient and reliable data ingestion processing and storage Develops and maintains data models schema and metadata to enable effective data analysis Collaborate with data analysts data scientists and other stakeholders to understand data requirements and ensure data quality Implements data validation and cleansing processes to ensure data accuracy and consistency as required Monitors and troubleshoots data pipelines to ensure that they are running smoothly and efficiently Stays current with the latest data engineering trends and technologies and applies them as appropriate and in alignment with data strategy    Minimum Qualifications  Education  Bachelors degree in Computer Science Information Systems or a related field or an equivalent combination of education and work experience is required    Certification  Azure DP203 Data Engineering required  Snowflake DEAC01 SnowPro Advanced Data Engineer preferred    Experience  Seven plus years relevant experience in data modeling data integration or data management  Experience in dimensional modeling at large enterprise  Experience on Git Azure DevOps  Experience with distributed computing frameworks such as Hadoop and Spark  Strong experience with SQL and database management systems  Experience with cloud data technologies eg GCP Dataproc Big query Airflow Snowflake Data Factory preferred    Knowledge Skills and Abilities  Strong programming skills in languages such as Python Java or Scala  Proven ability to build data pipelines and framework for a scalable solution to support the dynamic cloud environment  Ability to write complex queries and stored procedures  Ability to manage multiple priorities simultaneously  Excellent written and oral communication skills  Ability to identify research and resolve technical problems  Ability to work both independently and as part of a team    Physical Demands and Work Environment  The physical demands and work environment characteristics described herein are representative of those that must be met by an employee to successfully perform essential functions of this position andor may be encountered while performing essential functions Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions    While performing the essential duties of this position an employee would frequently be required to stand walk and sit  Specific vision abilities required by this position include close vision distance vision and the ability to adjust focus  The noise level in the work environment is usually moderate  Our company offers a dynamic hybrid work arrangement which requires three days onsite in the Sugar Land TX office Our retail roles are required to be onsite at the branch locations    Disclaimer  The above statements are intended to describe the general nature and level of work being performed by people assigned to this job They are not intended to be an exhaustive list of all responsibilities duties and skills required of personnel so classified    Texas Dow Employees Credit Union is an equal opportunity employer dedicated to a policy of nondiscrimination in employment on any basis including race color age protected veteran status sex religion disability genetic information national origin or other status protected by federal state or local law Consistent with the American Disabilities Act applicants may request accommodations needed to participate in the application process </data></node>
<node id="n1931" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e8a9e3e75cef1843&amp;bb=UkWqgXROFXK6Hr5wr58UhVyN2uYldS5WUFAXeGchZvbbnhIe3kQaq6SJ-4aCUvKNQ2nKqrdTB4DLpwiqhMVXBMiABssqyZe_42NTzh0pnw_vwHkqgFIEaQ%3D%3D&amp;xkcb=SoAf67M3CNjr8KwiIp0ObzkdCdPP&amp;fccid=808a3c26240b15ba&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Electrical experienceYesNoEducationDo you have a Trade schoolYesNo Job detailsHere’s how the job details align with your profilePay31 an hourJob typeContractShift and schedule8 hour shiftDay shiftMonday to Friday LocationDublin OH BenefitsPulled from the full job descriptionDental insuranceVision insurance Full job description        Dublin                      Ohio           Data Center             Remote Work Option             Yes                 Job ID           348196             Employment Type           Contract             Pay Rate           Base Salary                      3100           hr           The Data Center Technical Operations Engineer will be responsible for risk management and mitigation corrective and preventative maintenance of critical infrastructure vendor management and metric reporting    MF 8Hr days Day shift    THE OPPORTUNITY FOR YOU    Ensure that all work performed is in accordance with established practices and procedures Establish performance benchmarks conduct analyses and prepare reports on all aspects of the critical facility operations and maintenance Work with IT managers and other business leaders to coordinate projects manage capacity and optimize plant safety performance reliability and efficiency Operate and manage both routine and emergency services on a variety of critical systems such as switchgear generators UPS systems power distribution equipment chillers cooling towers computer room air handlers building monitoring systems etc May assist in the design and build out of new facilities May assist in projects to increase current facility efficiency Responsible for asset and inventory management Assist in recruiting efforts Deliver quality service and ensure all customer demands are met   KEY SUCCESS FACTORS    Degree in Electrical Engineering Mechanical Engineering or relevant discipline or trade schoolmilitary experience 1 years of Mechanical andor Electrical Operations experience 1 years of experience with Mechanical andor electrical troubleshooting Experience with utilizing building management systems Strong verbal and written communication skills Strong leadership and organizational skills Strong attention to detail Ability to prioritize in complex fastpaced environment Experience with emergency response to facility related issues Experience with critical electrical and cooling systems Experience with computer systems excel word etc   PREFERRED QUALIFICATIONS    12 years of Data Center Engineering Experience 12 years of Data Center Management Experience Related technical certifications   BENEFITS    Companysponsored Health Dental and Vision insurance plans    EQUAL OPPORTUNITY STATEMENT  Advantis Global is an equal opportunity employer and makes employment decisions on the basis of merit qualifications and abilities Company policy prohibits unlawful discrimination based on race color religion sex including gender gender identity gender expression pregnancy childbirth or medical condition related to pregnancy or childbirth sexual orientation national origin ancestry age physical or mental disability genetic information political affiliation union membership marital or registered domestic partnership status military or veteran status or any other characteristic protected by law “Protected Characteristic” Additionally Advantis Global is committed to promoting pay equity and prohibits harassment of any employee on the basis of any Protected Characteristic  Advantis Global is a progressive and openminded collective If you’re smart optimistic and care about being awesome at what you do come as you are We welcome you with open arms  This policy applies to all terms and conditions of employment including recruiting hiring placement promotion termination layoff recall transfer leaves of absence compensation and training   LIMW1  </data></node>
<node id="n1932" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=46b1845d965e80c7&amp;bb=UkWqgXROFXK6Hr5wr58UhSAcmIPBOZdMxvXY2qTsM4nkbUGL_Up7uqFKakEbIGXYBkQjsXaIGULy5AEvyAB8JqzuOAkjAKZ6RNYMdQGzL_4DzmnnFlIq3g%3D%3D&amp;xkcb=SoA267M3CNjr8KwiIp0MbzkdCdPP&amp;fccid=e2c1d9254247b2b7&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionLicensesDo you have a valid Secret Clearance licenseYesNoSkillsDo you have experience in SQLYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRemote Full job descriptionVista Innovative Services LLC is seeking a Data Warehouse Manager to add to our Enterprise Analytics and Systems Support Services EASSS team in support of Human Resources Command HRC Enterprise Modernization Directorate EMoD    Responsibilities Include   This role will partner crossfunctionally with all areas of the business to create a single source from where teams can pull data and insights Defines an overarching data strategy to ensure the right people have the right data at the right time while overseeing data design and the creation of database architecture and data repositories Responsible for the design implementation maintenance and support of the data warehouse systems and projects Ensures that reporting modeling and ETL projects and initiatives are supported fully and run smoothly Installs processes for auditing data warehouses and ensuring data quality Establishes connections between data warehousedata models and BI visualization tools Ensures stability of data warehouses including security uptime and troubleshooting of database errors Maintains data warehouses including rollout of upgrades and constant evaluation of emergent technologies Work in a multiprotocol multiplatform environment Providing a managerial role while providing daytoday support of the data warehouse and troubleshooting existing procedures and processes Guides the project in identifying any new data needs and deliver mechanisms for acquiring and reporting such information as well as addressing the actual needs Designs and develops systems for maintenance of the data warehouse ETL processes and business intelligence Highly collaborative with other team members and data consumers within the business to gather and populate data warehouse table structure Establishes the documentation of reports develops and maintains technical specification documentation for all reports and processes  Qualifications   Bachelor’s degree in management information systems Computer Science or a related field Education may be substituted for years of experience 59 years of experience in database administration database architecture or a related field Mastery of SQL and experience standing up cloud data warehouses Extensive knowledge of Data Warehouse design concepts and tools Secret Clearance  </data></node>
<node id="n1933" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=5aa36a1e7a8daca1&amp;bb=UkWqgXROFXK6Hr5wr58UhccMGObYIx1dRax496VeGfn4ldvBG9xVDSiAl5BmFb81WhiVstMC8jH898jVeI3k5_Cr81XyddHixMYVBRHTqNY%3D&amp;xkcb=SoBC67M3CNjr8KwiIp0AbzkdCdPP&amp;fccid=994a93a3bf96eb2c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in XMLYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location1155 Mill Street Reno NV 89502 Full job description        Position Purpose               Renown Health is looking for a Senior Data Engineer familiar with the Microsoft Azure Cloudbased platform This position will develop design implement and maintain a Microsoft Azure Data Warehousing environment and help architect the Enterprise Data Warehouse for all corporate entities of Renown Health The role will include setting up and automating data pipelines via ETL  ELT processes with internal departments and external third parties verifying data accuracy and optimizing the data environments to enable the work of data scientists and analysts The engineer will be expected to know SQL and be comfortable discussing complex computer science or statistical concepts with data scientists and analysts Innovation is critical to this role as an ideal candidate will possess the ability to lead the development of a cloudbased enterprise data warehouse along with engineering solutions to support the development of new reporting systems analytic engines and machine learning algorithms                      Nature and Scope               This role can be either remote or hybrid   Primary Responsibilities  In collaboration with data scientists build full technology stack of services for commercialization purposes including PaaS Platform asaservice IaaS Infrastructure asaservice SaaS software asaservice operations and management   Manage and optimize the movement and validation of data from an Epic EMR system to Renown Health’s Enterprise Data Warehouse EDW   Accountable for data engineering lifecycle including research proof of concepts architecture design development test deployment and maintenance   Oversee the development of novel data pipelines that integrate and normalize large data from a variety of sources eg electronic health record claims wearable device publicly available data etc to enable learning health machine learning model development and deployment   Design direct and implement ETL processes including data capture data quality testing and validation methods   Knowledge of interface engines and protocols Experience with HL7 X12 andor XML and OPENLink   Provide guidance on synchronizing the Epic EMR data architecture with customized data models that facilitate reporting and analytics   Layer in instrumentation in the development process so that data pipelines can be monitored Measurements are used to detect internal problems before they result into user visible outages or data quality issues   Build processes and diagnostic tools to troubleshoot maintain and optimize engineering environments and respond to production issues   Provide subject matter expertise and hands on delivery of data capture curation and consumption pipelines for Microsoft Azure   Ability to build Azure data solutions and provide technical perspective on storage big data platform services serverless architectures Hadoop ecosystem vendor products RDBMS DWDM NoSQL databases and security   Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform   Develop documentation such as data dictionaries guides or data flow diagrams that assists staff in identifying locating and using the organization’s data    Incumbent Must Possess   Minimum of 3 years of SQL programming experience and associated SQL tools SSIS SSMS SSRS etc   Experience with Visual Studio is preferred   At least 3 years of experience in developing data ingestion data processing and analytical pipelines for big data relational databases NoSQL and data warehouse solutions   Minimum of 3 years of RDBMS experience   Extensive handson experience implementing data migration and data processing using Azure services ADLS Azure Data Factory Azure Functions SynapseDW Azure SQL DB Event Hub IOT Hub Azure Stream Analytics Azure Analysis Service HDInsight Databricks Azure Data Catalog Cosmo Db ML Studio AIML etc   Familiarity of the environments needed to facilitate the work of data scientists and analysts in healthcare   Knowledge of medical terminology especially ICD10 codes CPT codes DRG codes and an understanding of adjudicated claims data   Excellent verbal and written communication An applicant may be asked to provide examples of written work to demonstrate technical writing proficiency    This position does not provide patient care                      Disclaimer           The foregoing description is not intended and should not be construed to be an exhaustive list of all responsibilities skills and efforts or work conditions associated with the job It is intended to be an accurate reflection of the general nature and level of the job                      Minimum Qualifications Requirements  Required andor Preferred               Name    Description       Education    Must have workinglevel knowledge of the English language including reading writing and speaking English Master’s degree with 10 years’ experience preferred bachelor’s degree with 13 years of equivalent experience will be considered in place of master’s degree requirement       Experience    Requires a minimum of 10 years’ experience with at least five 5 years working in data management data engineering or data architecture Enterprise Data Warehouse development preferred Requires at least five 5 years working with healthcare data more is preferred SQL proficiency is required       Licenses    None       Certifications    Ability to obtain Epic System’s Caboodle and Clarity Development Certificates and relevant data model badges required within 12 months of hire Must stay current on new version certification as applicable       Computer  Typing    Must be proficient with Microsoft Office Suite including Outlook PowerPoint Excel and Word and can use the computer to complete online learning requirements for jobspecific competencies access online forms and policies complete online benefits enrollment etc                     Location Renown Health · 100612 Enterprise Data Analytics   Schedule Full Time  Eligible for Benefits Day 40  </data></node>
<node id="n1934" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=9da902ba5f721bfa&amp;bb=9kdXdebxtoL5LCu5AWPBVR6ENYgklyLasKKaLrlqG3fU-lR3rwrBG6xFmKHCtMJxvJxM-jnKboDUX7ifjxPKEXHDDPqxAbiw6sQ_x2Ry6DeDTAVdYpz1qg%3D%3D&amp;xkcb=SoDQ67M3CNjobgRSRx0LbzkdCdPP&amp;fccid=8dc4399ddb463d4a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime Location6700 Tower Circle Franklin TN 37067 BenefitsPulled from the full job descriptionOpportunities for advancement Full job description  Job Description    Schneider Electric USA Inc seeks a Senior Data Engineer in Franklin TN   Job Description Ensure data warehouse and other data sources are structured documented and updated Use API integration skills SQL coding and leadingedge data housing skills Optimize data systems Make raw data easier to access and more useful for a global team of Data Scientists Machine Learning Engineers and Data Analysts to consume Eligible for up to 3 days remote work per week Qualifications    Requirements Position requires a Master’s or Bachelor’s degree or foreign equivalent in Computer Science or related field and experience 1 year with Master’s or 3 years with Bachelor’s in data engineering or related occupation which must include at least some education in or experience with the following skills Programming using SQL Python Spark Scala and Java Advanced data mining methods and automation using ETL and scripting Creating and supporting optimal data pipeline architectures for scalability Cloud technologies with APIs or AWS cloud services Redshift Athena RDS DynamoDB Batch Glue Deploying applications using Docker Rancher or Kubernetes Ingesting analyzing and summarizing large datasets Performing complex data modeling and ETL design and using large databases in a business environment and Indexes partitions and distributed storage and access   EOE    To Apply Visit httpcareerssecom and search Req68692   About Our Company     Why us      Schneider Electric is leading the digital transformation of energy management and automation Our technologies enable the world to use energy in a safe efficient and sustainable manner We strive to promote a global economy that is both ecologically viable and highly productive      €257bn global revenue       137 000 employees in 100 countries       45 of revenue from IoT       5 of revenue devoted for RD      You must submit an online application to be considered for any position with us This position will be posted until filled      It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting hiring training transferring and promoting all qualified individuals regardless of race religion color gender disability national origin ancestry age military status sexual orientation marital status or any other legally protected characteristic or conduct Concerning agencies Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such    </data></node>
<node id="n1935" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=7013afacedbce8e8&amp;bb=9kdXdebxtoL5LCu5AWPBVaopXf_Z3XgR0-kk35vuXvopJlMO13s51c5NwWx76qgRqZnfKiwlfwVbkqiTi0XlwmHYkVaEg40c6DPsVro7VoA%3D&amp;xkcb=SoBk67M3CNjobgRSRx0KbzkdCdPP&amp;fccid=13f5bb3a3abd1e37&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Bachelors degreeYesNo LocationKansas City MO BenefitsPulled from the full job description401k matchingAdoption assistanceDental insuranceEmployee assistance programFlexible spending accountGym membershipHealth insuranceShow morechevron down Full job description    UMB Bank NA seeks two Sr Data Engineers for Kansas City MO with option to telecommute work from anywhere within US        The Enterprise Data Warehouse  Analytics team provides support of our data warehouse environment and analytics tools This allows our business leaders access to business intelligence data in order to develop strategies for our organization to grow        As a Sr Data Engineer you will be a part of UMB’s data team during an exciting time of growth and modernization As part of this job you will be working to implement cloud strategies modernizing legacy systems and coding for the future of data processing at UMB As you grow in technical expertise in one or more areas of specialization as well as leadership capabilities your position and role within the team would advance and reflect this growth accordingly If you have a passion for new technology love to solve complex problems aren’t afraid to learn technology and love change we want to talk to you        The Enterprise Data Warehouse  Analytics team is a closeknit group of data engineers coming from diverse backgrounds and experiences We share a strong commitment of providing high quality enterprise data solutions and we believe in fully supporting each other to achieve this goal Trust and open communication are the cornerstones of how we roll and we have plenty of fun while doing it Selfeducation peer consultation mentor guidance and formal trainingseminars are some of the methods that we share information and knowledge      How you’ll spend your time      The Sr Data Engineer will be responsible for building and maintaining optimal data pipelines architectures and data sets required for extraction transformation and loading of data from a wide variety of data sources using SQL and integration technologies Leverage existing data infrastructure to fulfill all datarelated requests perform necessary data housekeeping data cleansing normalization hashing and implementation of required data model changes Build and utilize appropriate data platform structures to organize and store data in a particular manner Work with big data technologies onprem cloud and hybrid Implement data lifecycle management strategies around the flow of data within the organization implementing policy and automated approaches Implement BI business intelligence platforms such as Power BI both onpremises and cloud Establish governance and strategies around visualization of creating reports and dashboards to help improve upon operational and analytical reporting Examine assess translate and classify data recognize collect and analyze data to encourage the advancement execution and application of data platform systems Analyze data to spot anomalies trends and correlate similar data sets Design develop and implement natural language processing software modules Create models and standards to govern which data is collected and how it is stored arranged integrated and use in data systems and in organizations Perform major tasks deliverables and formal application delivery methodologies deliver new or enhanced applications Perform additional database administrator duties as assigned      We’re excited to talk if you have      Bachelor’s degree in Computer Science or a closely related field       Five years’ progressive experience in job offered or related which must include experience in the following concurrently     5 years in a technical role supporting or designing application technologies  5 years in data development or engineering  Experience serving as technical lead or project management formal or informal  5 years with Data Pipeline Development or Business Intelligence Implementation and subject matter expertise in Modem Data Platform Implementation  Experience in the following  Analyzing rearchitecting and replatforming onpremise data warehouses to data platforms on AWS cloud or similar using AWS or similar or 3rd party services  Designing and building production data pipelines from ingestion to consumption within a big data architecture using Java Python and Scala or similar technologies  Designing and implementing data engineering ingestion and curation functions on AWS cloud or similar using AWS native or similar or custom programming  3 years of experience designing and developing solutions using AWS or similar services such as Lamdba Glue SQS SNS Redshift or similar  Architecting and implementing very largescale data intelligence solutions around Snowflake Data Warehouse or similar technologies  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes Snow SQL  Writing SQL queries against Snowflake or similar technologies       Demonstrated ability to design build and operationalize large scale enterprise data solutions and applications using one or more of AWS or similar data and analytics services in combination with 3rd parties  Spark EMR DynamoDB RedShift Kinesis Lambda Glue Snowflake or similar Up to 10 local travel required        Applicants must have legal authority to work in the United States Work Visa sponsorship not available for this position   xaxa       UMB offers competitive and varied benefits to eligible associates such as Paid Time Off a 401k matching program annual incentive pay paid holidays a comprehensive company sponsored benefit plan including medical dental vision and other insurance coverage health savings flexible spending and dependent care accounts adoption assistance an employee assistance program fitness reimbursement tuition reimbursement an associate wellbeing program an associate emergency fund and various associate banking benefits Benefit offerings and eligibility requirements vary        Are you ready to be part of something more    Youre more than a means to an end—a way to help us meet the bottom line UMB isnt comprised of workers but of people who care about their work one another and their community Expect more than the status quo At UMB you can expect more heart Youll be valued for exactly who you are and encouraged to support causes you care about Expect more trust We want you to do the right thing no matter what And expect more opportunities UMBers are known for having multiple careers here and having their voices heard      UMB and its affiliates are committed to inclusion and diversity and provide employment opportunities to all employees and applicants for employment without regard to race color religion sex including gender pregnancy sexual orientation and gender identity national origin age disability military service veteran status genetic information or any other status protected by applicable federal state or local law If you need accommodation for any part of the employment process because of a disability please send an email to   talentacquisitionumbcom  to let us know the nature of your request     If you are a California resident please visit our      Privacy Notice for California Job Candidates    to understand how we collect and use your personal information when you apply for employment with UMB   </data></node>
<node id="n1936" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=070847f493a6c894&amp;bb=9kdXdebxtoL5LCu5AWPBVRgEh-JXr5tXANM7EtsfX2ipacFIehBuN3dYVDpJ1HApmFxqMcWunHCLcMlpWapf74DfFuBHuwgvluOlNIkgPc7UWB9ttnAXmg%3D%3D&amp;xkcb=SoD567M3CNjobgRSRx0JbzkdCdPP&amp;fccid=64b757511be5394c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Power BIYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationDes Plaines IL 60018 Full job description   General Summary   GTI Energy the nation’s leading research development deployment and training organization serving energy markets has an opportunity for a Data EngineerAnalyst in our Research  Engineering group The Data Engineer will collaborate with a team of engineers and scientists to deliver technological solutions for energy projects This role involves applying engineering practices to develop and deploy instrumentation controls and connected systems The engineer will contribute to development documentation and commissioning of data visualization of instrumentation systems     Why GTI Energy   GTI Energy offers generous benefits competitive salaries career advancement and the opportunity to work in a professional RD environment Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions  We are proud of what we do because our work matters GTI Energy is working toward solving global energy challenges in transitioning to a lowcarbon economy We have a proven track record in producing innovative ideas and commercializing solutions to deliver clean and reliable energy At GTI Energy we deliver innovative technology solutions for safe efficient and responsible energy  So what does that mean for you You will work in a positive and respectful work culture that fosters growth collaboration and opportunity You will be supported by competitive compensation incentives and benefits while enjoying purposeful work that drives improvement of delivering clean energy to the world     Work Location   The position will be based in the Chicagoland area at the GTI Energy Headquarters We offer a hybridcore work week where employees may be remote Mondays and Fridays and required to be onsite Tuesday through Thursday     Primary Responsibilities   This position is within a group of engineers and scientists that bring technological solutions to a broad range of energy project developers utility companies and endusers The candidate must be able to apply standard practices to the development and deployment of data visualizationsdashboards for instrumentation systems The candidate must be able to work within the structure of an overall project team while being able to work independently on subtasks Position responsibilities include but are not limited to  Participate in developing data requirements from work proposals Research and identify components and software that match requirements Document work in written graphical and presentation form Assist in the handson commissioning data acquisition systems Perform other duties as assigned      Required Knowledge Skills Abilities and Other Characteristics    Specific knowledge of the MS Azure and PowerBI stack General knowledge of data visualization methodologies Data storage and management tools such as SQL Server OPC and Cosmos DB Data historian tools such as PARCview or PI Industrial protocols such as MODBUS MQTT or Ethernet Wireless IoT protocols such as LoRaWAN LTE or Monnit Preferred candidate will have prior experience with live sensor data      Education and Experience   BSc or MSc degree in an engineering discipline 5 years of relevant professionallevel experience after a BS 3 years of relevant professionallevel experience after an MS      EEO Statement   GTI Energy is committed to developing a barrierfree recruitment process and work environment If you require any accommodation please email us at HumanResourcesgtienergy and we’ll work with you to meet your accessibility needs You must have legal authorization to work for GTI Energy on your date of hire with no further action required by GTI Energy We are an Equal Employment Opportunity employer and consider qualified applicants without regard to race color age religion sexual orientation gender identity national origin disability veteran status pregnancy or genetic information   Equal Opportunity EmployerProtected VeteransIndividuals with Disabilities  The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about discussed or disclosed their own pay or the pay of another employee or applicant However employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information unless the disclosure is a in response to a formal complaint or charge b in furtherance of an investigation proceeding hearing or action including an investigation conducted by the employer or c consistent with the contractor’s legal duty to furnish information 41 CFR 60135c  </data></node>
<node id="n1937" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e4551550092579f0&amp;bb=9kdXdebxtoL5LCu5AWPBVTPqW0IaWKoD75j2k-6qS9J9lCQQ-zVoiOhG4Q1KU5qggLpL9OmzxZwUIbfKL49tec9tha5HnrXHtHCppDV9XSY%3D&amp;xkcb=SoBN67M3CNjobgRSRx0IbzkdCdPP&amp;fccid=b85c5070c3d3d8c8&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationMcLean VA BenefitsPulled from the full job descriptionHealth insurance Full job description  Locations VA  McLean United States of America McLean Virginia   Data Engineer    Do you love building and pioneering in the technology space Do you enjoy solving complex business problems in a fastpaced collaborative inclusive and iterative delivery environment At Capital One youll be part of a big group of makers breakers doers and disruptors who solve real problems and meet real customer needs We are seeking Data Engineers who are passionate about marrying data with emerging technologies As a Capital One Data Engineer you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One  What You’ll Do   Support the design and development of scalable data architectures and systems that extract store and process large amounts of data  Build and optimize data pipelines for efficient data ingestion transformation and loading from various sources while ensuring data quality and integrity  Collaborate with Data Scientists Machine Learning Engineers Business Analysts andor Product Owners to understand their requirements and provide efficient solutions for data exploration analysis and modeling  Implement testing validation and pipeline observability to ensure data pipelines are meeting customer SLAs  Use cutting edge technologies to develop modern data pipelines supporting Machine Learning and Artificial Intelligence    Basic Qualifications   Bachelor’s Degree  At least 2 years of experience in application development Internship experience does not apply  At least 1 year of experience in big data technologies    Preferred Qualifications   3 years of experience in application development including Python Scala or Java  1 years of experience using Spark  1 years of experience working on data stream systems Kafka or Kinesis  1 years of data warehousing experience Redshift or Snowflake  1 years of experience with Agile engineering practices  1 years of experience working with a public cloud AWS Microsoft Azure Google Cloud    At this time Capital One will not sponsor a new applicant for employment authorization for this position   Capital One offers a comprehensive competitive and inclusive set of health financial and other benefits that support your total wellbeing Learn more at the Capital One Careers website Eligibility varies based on full or parttime status exempt or nonexempt status and management level  This role is expected to accept applications for a minimum of 5 business days   No agencies please Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace All qualified applicants will receive consideration for employment without regard to sex including pregnancy childbirth or related medical conditions race color age national origin religion disability genetic information marital status sexual orientation gender identity gender reassignment citizenship immigration status protected veteran status or any other basis prohibited under applicable federal state or local law Capital One promotes a drugfree workplace Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries including to the extent applicable Article 23A of the New York Correction Law San Francisco California Police Code Article 49 Sections 49014920 New York City’s Fair Chance Act Philadelphia’s Fair Criminal Records Screening Act and other applicable federal state and local laws and regulations regarding criminal background inquiries    If you have visited our website in search of information on employment opportunities or to apply for a position and you require an accommodation please contact Capital One Recruiting at 18003049102 or via email at RecruitingAccommodationcapitalonecom All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations   For technical support or questions about Capital Ones recruiting process please send an email to Careerscapitalonecom   Capital One does not provide endorse nor guarantee and is not liable for thirdparty products services educational tools or other information available through this site   Capital One Financial is made up of several different entities Please note that any position posted in Canada is for Capital One Canada any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp COPSSC  </data></node>
<node id="n1938" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=e60c74293b10ec2b&amp;bb=9kdXdebxtoL5LCu5AWPBVaUq_bQ1tC8NBjCndQDTVXXvEe4GTgUSqyCsZC-5X28oNwDK-fxLvUXJGxmpecwK3fkb7Yd_-D5MOeucaAkcfPfCcFOlLcsPSQ%3D%3D&amp;xkcb=SoDD67M3CNjobgRSRx0PbzkdCdPP&amp;fccid=0c746cab37cd5dd5&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Unit testingYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay105000 a yearJob typeFulltime Location4350 La Jolla Village Drive Suite 140 San Diego CA 92122 BenefitsPulled from the full job description401k401k matchingDental insuranceFlexible spending accountHealth insuranceHealth savings accountLife insuranceShow morechevron down Full job description  Axos Bank       Target Range   8500000 Yr  10500000 Yr      Actual starting pay will vary based on factors including but not limited to geographic location experience skills specialty and education    Eligible for an Annual Discretionary Cash Bonus Target 10   Eligible for an Annual Discretionary Restricted Stock Units Bonus Target 10     These discretionary target bonuses may be awarded semiannually based upon your achievement of performance goals and targets       About This Job   As our Data Engineer you will work within the Enterprise Data Management group using SQL SSIS MS SQL Server python etc to create revenue generating enterprise data solutions       Responsibilities     Work with Technical and business team to understand the business requirements functional and technical specifications  Design code and maintain new and existing complex SQL stored procedures and functions  Performance tune existing stored procedures tables and indexes  Work with other engineers to troubleshoot repair and performance tune databases  Review SQL code written by other developers to ensure compliance to coding standards and best practices as well as maximum performance  Create SSIS packages for data transformation cleansing caching aggregation staging and transfer  Troubleshoot problems that may come up with database environments performance issues replication issues or operational issues  Perform data analysis and data profiling tasks to provide support and recommendations for development and design decisions  Analyze and define data flow requirements and prepares applicable system documentation and operation manuals as needed  Support production data loads and ongoing refreshes of the database systems  Define prepare execute and implement data validation and unit testing methods to ensure data quality  Maintain reusable development standards that help implement each solution andor enhancements to existing systems to meet current and future needs  Perform enhancements and bug fixes as required  Perform any additional duties as assigned       Requirements     Bachelors degree in a related field or relevant experience may be used in lieu of a degree  3 years with relational DBs in a production environment  3 years with Microsoft SQL Server versions  3 years of SSIS packages  2 years in an AgileSCRUM environment required  Experience in Performance Tuning  Deliver high quality high traffic scalable database objects  Deep understanding of data warehousing concepts including complex data structures data transformations and data analytics  Experience with multiplatform distributed application implementations across SQL Server Cloud based data warehouses such as AWS       Axos Employee Benefits May Include             Medical Dental Vision and Life Insurance              Paid Sick Leave 3 weeks’ Vacation and Holidays about 11 a year              HSA or FSA account and other voluntary benefits              401k Retirement Saving Plan with Employer Match Program and 529 Savings Plan              Employee Mortgage Loan Program and free access to an Axos Bank Account with SelfDirected Trading             About Axos        Born digitalfirst Axos delivers financial tools and services that allow individuals small businesses and companies to access and manage their money how when and where they want We’re a diverse team of dynamic insightful and independent innovators who are excited to provide technologydriven solutions that offer unbeatable value to our customers        Axos Financial is our holding company and is publicly traded on the New York Stock Exchange under the symbol AX NYSE AX          Learn more about working at Axos          PreEmployment Background Check and Drug Test        All offers are contingent upon the candidate successfully passing a credit check criminal background check and preemployment drug screening which includes screening for marijuana Axos Bank is a federally regulated banking institution At the federal level marijuana is an illegal schedule 1 drug therefore we will not employ any person who tests positive for marijuana regardless of state legalization        Equal Employment Opportunity        Axos is an Equal Opportunity employer We are committed to providing equal employment opportunities to all employees and applicants without regard to race religious creed color sex including pregnancy breast feeding and related medical conditions gender gender identity gender expression sexual orientation national origin ancestry citizenship status military and veteran status marital status age protected medical condition genetic information physical disability mental disability or any other protected status in accordance with all applicable federal state and local laws        Job Functions and Work Environment        While performing the duties of this position the employee is required to sit for extended periods of time Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse calculator telephone copiers etc        The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position    </data></node>
<node id="n1939" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=1e58a20a21a2ea06&amp;bb=9kdXdebxtoL5LCu5AWPBVaUq_bQ1tC8NaWT88S2RI2zqisSjfkCe4_e4GhoUb1Zb9yRLnuX2Ctfs67e_PoH1oHyyZdzvGrjZauLRO4-xJJM%3D&amp;xkcb=SoB367M3CNjobgRSRx0ObzkdCdPP&amp;fccid=a3f737e511d9fc8c&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in Testdriven developmentYesNoEducationDo you have a Masters degreeYesNo Job detailsHere’s how the job details align with your profilePay130100  188750 a year LocationAustin TX BenefitsPulled from the full job description401kDental insuranceFlexible spending accountHealth insuranceHealth savings accountLife insurancePaid time offShow morechevron down Full job description Company Description  Visa is a world leader in payments and technology with over 259 billion payments transactions flowing safely between consumers merchants financial institutions and government entities in more than 200 countries and territories each year Our mission is to connect the world through the most innovative convenient reliable and secure payments network enabling individuals businesses and economies to thrive while driven by a common purpose – to uplift everyone everywhere by being the best way to pay and be paid  Make an impact with a purposedriven industry leader Join us today and experience Life at Visa Job Description  Payments are a very exciting and fastdeveloping area with a lot of new and innovative ideas coming to market With strong demand for new solutions in this space it promises to be an exciting area of innovation VISA is a strong leader in the payment industry and is rapidly transitioning into a technology company with significant investments in this area  If you want to be in the exciting payment space learn fast and make big impacts Ecosystem  Operational Risk technology which is part of Visa’s ValueAdded Services business unit is an ideal place for you  In Ecosystem  Operational Risk group Payment Fraud Disruption team is responsible for building critical risk and fraud detection and prevention applications and services at Visa This includes idea generation architecture design development and testing of products applications and services that provide Visa clients with solutions to detect prevent and mitigate fraud for Visa and Visa client payment systems  This position is ideal for an experienced data engineer who is passionate about collaborating with business and technology partners in solving challenging fraud prevention problems You will be a key driver in the effort to define the shared strategic vision for the Payment Fraud Disruption platform and defining tools and services that safeguard Visa’s payment systems  The right candidate will possess strong ML and Data Science background with demonstrated experience in building training implementing and optimized advanced AI models for payments risk or fraud prevention products that created business value and delivered impact within the payments or payments risk domain or have experience building AIML solutions for similar industries  As a Data Engineer you will be responsible to establish processes automations structures and big data systems based on business and technical requirements to channel multiple requirements route appropriately and plan proper big data technology using combination of open source and vendor supported big data technologies databases and other applicable big data technologies as required  A successful candidate should have the ability to engage in high bandwidth conversations with business and technology partners and be able to think broadly about Visa’s business and drive solutions that will enhance the safety and integrity of Visa’s payment ecosystem The candidate will help deliver innovative insights to Visas strategic products and business This role represents an exciting opportunity to make key contributions to strategic offering for Visa This candidate needs to have strong academic track record and be able to demonstrate excellent software engineering skills The candidate will be a selfstarter comfortable with ambiguity with strong attention to detail and excellent collaboration skills  The ideal candidate will bring the excitement and passion to leverage Generative AI to advance existing fraud detection mechanisms and to innovate and solve new fraud use cases This engineer will use code generation capabilities like GitHub copilot to drive efficiencies in software development  Essential Functions  As a Data Engineer you will help design enhance and build next generation fraud detection solutions in an agile development environment Formulate business problems as technical data problems while ensuring key business drivers are captured in collaboration with product stakeholders Drive development effort endtoend for ontime delivery of highquality solutions that conform to requirements conform to the architectural vision and comply with all applicable standards Responsibilities span all phases of solution development Collaborate with project team members Product Managers Architects Analysts Developers Project Managers etc to ensure development and implementation of new data driven business solutions Work with architects where applicable or work as architect with senior technologists to develop architecture and design Deliver on all code commitments and ensure a complete endtoend solution that meets and exceeds business expectations Identify proper service metrics and measurements for ensuring performance against Service Level Agreement Assist in scoping and designing analytic data assets implementing modelled attributes and contributing to brainstorming sessions Build and maintain a robust data engineering process to develop and implement selfserve data and tools for Visa’s data scientists Perform other tasks on RD data governance system infrastructure analytics tool evaluation and other cross team functions on an asneeded basis Find opportunities to create automate and scale repeatable analyses or build selfservice tools for business users Execute data engineering projects ranging from small to large either individually or as part of a project team Ensure project delivery within timelines and budget requirements Mentor and train other juniors on the team on key solutions Able to work on multiple projects and initiatives with differentcompeting timelines and demands Present technical solutions capabilities considerations and features in business terms Effectively communicate status issues and risks in a precise and timely manner   This is a hybrid position Hybrid employees can alternate time between both remote and office Employees in hybrid roles are expected to work from the office 23 set days a week determined by leadershipsite with a general guidepost of being in the office 50 or more of the time based on business needs Qualifications   Basic Qualifications     5 years of relevant work experience with a Bachelor’s Degree or at least 2  years of work experience with an Advanced degree eg Masters MBA JD    MD or 0 years of work experience with a PhD OR 8 years of relevant work    experience     Preferred Qualifications     6 or more years of work experience with a Bachelors Degree or 4 or more  years of relevant experience with an Advanced Degree eg Masters MBA JD    MD or up to 3 years of relevant experience with a PhD     Experience with creating data driven business solutions and solving data  problems using a wide variety of technologies such as Hadoop MapReduce    Hive Spark Scala MongoDB NoSQL as well as traditional data technologies    like MySQL RDBMS     Experience building ETLELT data pipelines data quality checks and data  anomaly detection and notification systems     Experience with successful design architecture and development using  Hadoop Spark and Scala for large data processing and transaction systems     Experience in applications and services development using Java Experience in developing and architecting large scale enterprise class  distributed systems of high availability low latency  strong data consistency     Experience with agile development incorporating Continuous Integration and  Continuous Delivery utilizing technologies such as GITStash Maven Jenkins    Selenium and Chef     Experience in full stack technology in one or more of the following  technologies Java Spring MVC Spring Boot Angular JavaScript MySQL    Maven Design Patterns Test Automation framework     Expertise in web applications and webservices technology standards and  frameworks     Experienced in testdriven development and test automation Experience in card industry or fintech delivering solutions in fraud risk or  payments space     Strong facilitation and analytical skills with excellent problemsolving ability Strong interpersonal and leadership skills Ability to present complex ideas in a clear concise way Passionate about delivering zero defect code that meet or exceed the  proposed defect SLA and have high sense of accountability for quality and    timeliness of one’s own deliverables and team deliverables     Be systematic and be able to do deep research wanting to uncover the details Have good work ethics and be a team player to bring the best results as a  team     You have the passion to understand people and always strive to improve our  products     Highly driven resourceful and results oriented Demonstrated ability to lead and navigate through ambiguity While youll have the skill to see and understand the big picture youre able to  stay focused on the task at hand to achieve immediate goals   Additional Information  Work Hours Varies upon the needs of the department  Travel Requirements This position requires travel 510 of the time  MentalPhysical Requirements This position will be performed in an office setting The position will require the incumbent to sit and stand at a desk communicate in person and by telephone frequently operate standard office equipment such as telephones and computers  Visa is an EEO Employer Qualified applicants will receive consideration for employment without regard to race color religion sex national origin sexual orientation gender identity disability or protected veteran status Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law  Visa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law including the requirements of Article 49 of the San Francisco Police Code  US APPLICANTS ONLY The estimated salary range for a new hire into this position is 13010000 to 18875000 USD per year which may include potential sales incentive payments if applicable Salary may vary depending on jobrelated factors which may include knowledge skills experience and location In addition this position may be eligible for bonus and equity Visa has a comprehensive benefits package for which this position may be eligible that includes Medical Dental Vision 401 k FSAHSA Life Insurance Paid Time Off and Wellness Program  </data></node>
<node id="n1940" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=44f9fad47fc734ae&amp;bb=9kdXdebxtoL5LCu5AWPBVQcX_XoxABY2xn-rc75kJBuivaKOBfLlBqPeKzUNQMkke72xodzt_nnx-AqT0HBG_roHKQCAT0iXfxWWcNm11Ug%3D&amp;xkcb=SoDq67M3CNjobgRSRx0NbzkdCdPP&amp;fccid=da3c7fed78dd1607&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNoEducationDo you have a Bachelors degreeYesNo Location55 Challenger Road Ridgefield Park NJ 07660 Full job description    Position Summary    Implement complex big data projects with a focus on collecting parsing managing analyzing and visualizing large sets of data to turn information into actionable deliverables across customerfacing platforms       Role and Responsibilities        Implement complex big data projects with a focus on collecting parsing managing analyzing and visualizing large sets of data to turn information into actionable deliverables across customerfacing platforms Translate complex functional and technical requirements into detailed design Perform Hadoop technical development and implementation Load from disparate data sets by leveraging various big data technology including AWS EMR S3 Hadoop hive and pyspark Preprocess and support queries using Hive Impala Spark Presto and Pig Design and implement data modeling using hivepresto Hue and Erwin Data Modeler Maintain security and data privacy in an environment secured using Kerberos and LDAP Perform highspeed queries using inmemory technologies including Spark Follow and contribute best engineering practice for source control release management and deployment Provide production support job scheduling and monitoring ETL data quality and generate data freshness reporting        Requirements Bachelor’s degree in Computer Science Electronic Engineering Electrical Engineering Information Systems Technology or a related field and 5 years of progressive post baccalaureate experience as Engineer III Data Engineering or related occupation in the Big Data software development engineering Of the 5 years of experience must have included       Experience in writing complex SQL queries and highperformance reliable and maintainable code Experience with Python development for data analysis and data modeling Experience with Hadoop including Hive Spark Impala Pig and Oozie and Experience with AWSCloud components including EC2 S3 and EMR        LIDNI        Skills and Qualifications      At Samsung we believe that innovation and growth are driven by an inclusive culture and a diverse workforce We aim to create a global team where everyone belongs and has equal opportunities inspiring our talent to be their true selves Together we are building a better tomorrow for our customers partners and communities     Samsung Electronics America Inc and its subsidiaries are committed to employing a diverse workforce and provide Equal Employment Opportunity for all individuals regardless of race color religion gender age national origin marital status sexual orientation gender identity status as a protected veteran genetic information status as a qualified individual with a disability or any other characteristic protected by law       Reasonable Accommodations for Qualified Individuals with Disabilities During the Application Process       Samsung Electronics America is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process If you have a disability and require a reasonable accommodation in order to participate in the application process please contact our Reasonable Accommodation Team 8555573247 or SEAAccommodationsExtseasamsungcom for assistance This number is for accommodation requests only and is not intended for general employment inquiries    </data></node>
<node id="n1941" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=8be09d8eec212ba3&amp;bb=9kdXdebxtoL5LCu5AWPBVTgBsVhrRxqQ4LpRud1ubHaC-k5q7O8ziNR8LJNqGnnq-abN1nEbG4bIVwxBmZEHzR2iN05a-FGZnfd3im94BpRDCyvMj04EmQ%3D%3D&amp;xkcb=SoBe67M3CNjobgRSRx0MbzkdCdPP&amp;fccid=51dc2e5aecda9e29&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in SparkYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationRemote Full job description    Job Description       The Opportunity        Our Data Engineering team within our Data Services Organization builds and maintains the infrastructure essential to delivering highvolume businesscritical data to the organization to enable datadriven decisions       We are focused on expanding our curated and modeled data that unify sources of truth across our multiple products and domains You’ll have the opportunity and empowerment to guide the team on best practices using modern distributed data tools like Snowflake Spark Kafka and dbt       This is an ideal opportunity for someone that has strong opinions on how things should be done and loves figuring out what the right solution is for the scenario at hand Your voice will be heard and will be given the opportunity to make an impact with the direction and delivery of our data platform to internal stakeholders        Who you are           5 years of experience designing and delivering data warehouses and marts to support business analytics            Hands on experience with dbt certification preferred            Solid foundation in SQL development on RDBMS Snowflake and Postgres preferred            Experience with dimensional data modelingdata workflow diagrams using Kimball methodology conceptual logical and physical            Experience with source control and deployment workflows for ETL dbt airflow preferred            Hands on experience with scripting languages Python BASH etc            Experience with metadata management and data quality            Knowledge of software engineering standard processes with experience with implementing CICD Gitlab Github Actions Teamcity etc monitoring  alerting for production systems           What you’ll own           Data Warehousing and modeling delivery            Support and evolution of data environment to deliver highquality data speed and availability            Curation of sourcesystem data to deliver trusted data sets            Involvement on data cataloging and data management efforts            Production ETL performance tuning and environmentlevel resource consumption and management            Migration of POC pipelines to production data processes           Experience you’ll need           Strong capability to manipulate and analyze complex highvolume data from a variety of sources            Good experience crafting and building endtoend data models and pipelines as well as alerting            Knowledge of data management fundamentals and data storage principles            Experience in data modeling for batch processing and streaming data feeds structured and unstructured data           Experience that’s a bonus           Expertise in streaming  realtime data processing using a technology like Spark Kafka ksqlDB or Databricks etc and best practices on production deployment of these platforms            Experience working with AWS services such as DynamoDB Glue Lambda Step Functions S3 CloudFormation           Bring yourself Pluralsight is an equal opportunity employer All qualified applicants will receive consideration for employment without regard to race color religion sex national origin sexual orientation gender identity disability age or protected veteran status Pluralsight will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law       We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process to perform essential job functions and to receive other benefits and privileges of employment Please visit the        bottom of our website    to learn how to request an accommodation       For more information on Pluralsight’s commitment to building a more diverse and inclusive workforce please review our most recent Diversity Equity Inclusion and Belonging report        here           LISW1    </data></node>
<node id="n1942" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=0b75f6bbaaafb80b&amp;bb=9kdXdebxtoL5LCu5AWPBVTJk9L_scjWSq6oe0XrEzHVptQDsf4gcABPIfIrEpc0AKfpxUcqXr_FXJ-8ll6J7JZjmf3D51WWr3GDKRAVZzgbliMziD7Zv5g%3D%3D&amp;xkcb=SoC367M3CNjobgRSRx0DbzkdCdPP&amp;fccid=2ecf4575019bf07a&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in WindowsYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profileJob typeFulltime LocationAlpharetta GA BenefitsPulled from the full job descriptionOnthejob training Full job description Title Big Data Engineer  Alpharetta GA   Terms of Hire Full Time  Salary  Open  yr  Benefits   Job description   Develops moderately complex code using both front andor back end programming languages within multiple platforms as needed in collaboration with business and technology teams for internal and external client software solutions Designs creates and delivers moderately complex program specifications for code development and support on multiple projectsissues with a wide understanding of the application  database to better align interactions and technologies   Provides broad and indepth knowledge of analysis modification and development of complex codeunit testing in order to develop concise application documentation Performs and advises on testing validation requirements and corrective measures for complex code deficiencies and provides systemic proposals   Participates in client facing meetings joint venture discussions vendor partnership teams to determine solution approaches   Provides advise to leadership on the design development and enforcement of business  infrastructure application standards to include associated controls procedures and monitoring to ensure compliance and accuracy of data Applies a full understanding and indepth knowledge of procedures methodology and application standards to include Payment Card Industry PCI security compliance   Develops administers and recommends billable hours and resource estimates on complex initiatives projects and issues   Assists with onthejob training and provides indepth expertise and advice to software engineers  What Are We Looking For in This Role  Minimum Qualifications     BS in Computer Science Information Technology Business  Management Information Systems or related field   Typically minimum of 6 years  Professional Experience In Coding Designing Developing And Analyzing Data Typically has an advanced knowledge and use of two or more opposing front  back end languages  technologies from the following but not limited to two or more modern programming languages used in the enterprise experience working with various APIs external Services experience with both relational and NoSQL Databases Preferred Qualifications     BS in Computer Science Information Technology Business  Management Information Systems or related field   8 years professional Experience In Coding Designing Developing And Analyzing Data and experience with IBM Rational Tools  What Are Our Desired Skills and Capabilities   Skills  Knowledge  Having wideranging experience uses professional concepts and company objectives to resolve complex issues in creative and effective ways Some barriers to entry exist at this level eg deptpeer review  Job Complexity  Works on complex issues where analysis of situations or data requires an indepth evaluation of variable factors Exercises judgment in selecting methods techniques and evaluation criteria for obtaining results Networks with key contacts outside own area of expertise  Supervision  Determines methods and procedures on new assignments and may coordinate activities of other personnel Team Lead Operating Systems     Linux distributions including one or more for the following Ubuntu CentOSRHEL Amazon Linux   Microsoft Windows   zOS   TandemHPNonstop  What are the 34 nonnegotiable requirements of this position   What are the 34 nonnegotiable requirements of this position   Oracle  PLSQL    must be strong in this area Senior level or Lead level ONLY   You Will Enjoy     An opportunity to be a part of a great culture an awesome team a challenging work environment and some fun along the way   Apply today to learn more and be part of our Growth story  All applications will be kept strictly confidential and once shortlisted our team will be in touch with you for further discussions  </data></node>
<node id="n1943" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=309eef7c680fc212&amp;bb=9kdXdebxtoL5LCu5AWPBVfWK_VNM_rH8XfyRisvaKkvrU2TVzQSQ-O5H24xP0YuRlxEMnvThze3JQqrderLaHfvPzwqaXB4haHJNl7OKAtUli44fxNtjRA%3D%3D&amp;xkcb=SoAD67M3CNjobgRSRx0CbzkdCdPP&amp;fccid=28ee277926be0542&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in S3YesNo LocationTrevose PA 19053 Full job description Veolia Water Technologies  Solutions VWTS is a worldwide leader in water recovery treatment and reuse We design and supply a range of water systems from food and beverage applications to municipal water to microelectronics ultrapure water and heavy industrial wastewater treatment We serve more than 50000 customers worldwide and treat more than 11 million cubic meters of water every day We simply aim to be the benchmark company for ecological transformation across the world   At WTS we realize diverse teams make smarter decisions deliver better results and build stronger communities We are an organization that champions diversity and inclusion at every rung of the ladder and are proud to be an equal opportunity workplace We offer challenging and meaningful careers with competitive benefits and flexible work arrangements   Summary of the Job The data engineer snaplogic architect will play a crucial role in designing implementing and optimizing integration solutions using the Snaplogic platform The role would require collaboration with cross functional teams to understand business requirements and translate them into efficient and scalable snaplogic solutions   Key Characteristics  Create endtoend integration solutions using Snaplogic that align with business goals requirements and technical requirements  Implement and configure Snaplogic pipelines to integrate diverse systems and data sources  Continuously improve and optimize existing Snaplogic workflows to enhance performance scalability and maintainability    Duties  Responsibilities  Diagnose and resolve issues related to Snaplogic integrations ensuring smooth and reliable operation  Implement data quality checks and monitoring processes to ensure data accuracy and consistency  Define maintain integration workbooks with supporting documentation around design end point connectivity and security  Define and implement improvements to increase system reliability security and performance  Monitor the health and performance of the HR Integrations    Hard Skills  Indepth knowledge and handson experience with Snaplogic platform features components and best practices  Strong working knowledge of modern programming languages ETLData Integration tools preferably SnapLogic and understanding of Cloud Concepts  Experience in leading integration solutions between Successfactors and other downstream platforms using snaplogic as the middleware    Soft Skills  Communication Strong communication and interpersonal skills to collaborate effectively with crossfunctional teams and stakeholders  Problem Solving Ability to identify opportunities and designing solutions which are sustainable  Achievement Ability to come up with new ideas based on experience and competencies to develop or implement new procedures or systems to improve the business and the client experience  Inclusive Passionate for working in different cultures and environments in a matrixed organization with a more universal and a diversity team  Team Player Ability to work with others toward a shared goal participating actively accountable and committed to the entire team respecting peers leadership stakeholders and clients    Education  Experience Required  8 years of SnapLogic ETL Services experience  5 years of related experience including MySQL NoSQL ETL and Data Integration  AWS Technologies RDS SQS S3 EC2 etc  RedShift data warehouse    Preferred  Preferred global experience    Working Conditions  Annual Travel at 5 of time    </data></node>
<node id="n1944" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=a7a5fa4d1987ae78&amp;bb=9kdXdebxtoL5LCu5AWPBVVx1GUCu7P8XhoH_HZgn6tZ4czQTWWujFLLddBpaALTWumBkOKBFUk0-47PV6OIoSS1cj2MzLh8tdYkTRyh-bu0%3D&amp;xkcb=SoCe67M3CNjobgRSRx0BbzkdCdPP&amp;fccid=8e5dab8caa064d1b&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo LocationRemote BenefitsPulled from the full job description401kDental insuranceFlexible spending accountHealth insurancePaid time offParental leavePrescription drug insuranceShow morechevron down Full job description Salary Range 55000  140000   We’ve Got You Under Our Wing  We are the duck We develop and empower our people cultivate relationships give back to our community and celebrate every success along the way We do it all…The Aflac Way   Aflac a Fortune 500 company is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of Americas bestknown brands Aflac has been recognized as Fortune’s 50 Best Workplaces for Diversity and as one of World’s Most Ethical Companies by Ethispherecom   Our business is about being there for people in need So ask yourself are you the duck If so there’s a home and a flourishing career for you at Aflac   Work Designation Depending on your location within the continental US this role may be hybrid or remote   If you live within 50 miles of the Aflac offices located in Columbus GA this role will be hybrid This means you will be expected to work in the office for at least 60 of the work week You will work from your home within the continental US for the remaining portion of the work week Details of this schedule will be discussed with your leadership  If you live more than 50 miles from the Aflac offices located in Columbus GA this role will be remote This means you will be expected to work from your home within the continental US If the role is remote there may be occasions that you are requested to come to the office based on business need Any requests to come to the office would be communicated with you in advance    What does it take to be successful at Aflac   Acting with Integrity  Communicating Effectively  Pursuing SelfDevelopment  Serving Customers  Supporting Change  Supporting Organizational Goals  Working with Diverse Populations    What does it take to be successful in this role   Advanced working SQL knowledge and experience working with relational databases query authoring SQL as well as working familiarity with a variety of databases  Advanced knowledge of SSIS SSRS and Business Objects  Experience building and optimizing ‘big data’ data pipelines architectures and data sets  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement  Strong analytic skills related to working with unstructured datasets  Build processes supporting data transformation data structures metadata dependency and workload management  A successful history of manipulating processing and extracting value from large disconnected datasets  Working knowledge of message queuing stream processing and highly scalable ‘big data’ data stores  Strong project management and organizational skills  Experience supporting and working with crossfunctional teams in a dynamic environment    Education  Experience Required   Bachelor’s Degree in Computer Science Information Systems Analytics or related field  Four or more years of experience in data analytics or other related experience   Or an equivalent combination of education and experience   Experience Preferred   Experience in Tableau Infoworks  Devx and Github is preferred    Principal Duties  Responsibilities   Create and maintain optimal data pipeline architecture  Assemble large complex data sets that meet functional  nonfunctional business requirements  Identify design and implement internal process improvements automating manual processes optimizing data delivery redesigning infrastructure for greater scalability etc  Build the infrastructure required for optimal extraction transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies  Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition operational efficiency and other key business performance metrics  Work with stakeholders to assist with datarelated technical issues and support their data infrastructure needs  Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader  Work with data and analytics experts to strive for greater functionality in our data systems  Performs other duties as required    Total Rewards  This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including but not limited to education experience licensure certifications geographic location and internal equity The range has been created in good faith based on information known to Aflac at the time of the posting Compensation decisions are dependent on the circumstances of each case This salary range does not include any potential incentive pay or benefits however such information will be provided separately when appropriate The salary range for this position is 55000  140000   In addition to the base salary we offer an array of benefits to meet your needs including medical dental and vision coverage prescription drug coverage health care flexible spending dependent care flexible spending Aflac supplemental policies Accident Cancer Critical Illness and Hospital Indemnity offered at no costs to employee 401k plans annual bonuses and an opportunity to purchase company stock On an annual basis you’ll also be offered 11 paid holidays up to 20 days PTO to be used for any reason and if eligible state mandated sick leave Washington employees accrue 1 hour sick leave for every 40 hours worked and other leaves of absence if eligible when needed to support your physical financial and emotional wellbeing Aflac complies with all applicable leave laws including but not limited to sick and safe leave and adoption and parental leave in all states and localities  </data></node>
<node id="n1945" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=64653758a2053a9b&amp;bb=9kdXdebxtoL5LCu5AWPBVWH76GIAstAtue0XatjB_83dImP8HdciNepDdkf1R4aF1K2aYaR2RWNnsNElUmv8Y3wvm5fwJVNl37zfFuKcHcg%3D&amp;xkcb=SoAq67M3CNjobgRSRx0AbzkdCdPP&amp;fccid=9f38ffb6fcd14039&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay173500  234700 a yearJob typeFulltime Location25 Lafayette St Newark NJ 07102 BenefitsPulled from the full job description401k matchingDental insuranceDisability insuranceEmployee stock purchase planHealth insuranceLife insuranceMilitary leaveShow morechevron down Full job description    Job Classification   Technology  Engineering  Cloud       Are you interested in building capabilities that enable the organization with innovation speed agility scalability and efficiency The Global Technology team is a purpose driven organization that takes great pride in our culture where digital transformation is built into our DNA When you join our organization at Prudential you’ll unlock an exciting and impactful career – all while growing your skills and advancing your profession at one of the world’s leading financial services institutions     Your Team  Role     As a Director Data Engineer you will partner with talented architects infrastructure engineers machine learning engineers data scientists and data analysts to improve many different products and services You will analyze design develop test and perform ongoing maintenance to build high quality data pipelines that drive platform delivery You will implement capabilities to solve sophisticated business problems deploy innovative products services and experiences to delight our customers In addition to deep technical expertise and experience you will bring excellent problem solving communication and teamwork skills along with agile ways of working strong business insight an inclusive leadership attitude and a continuous learning focus to all that you do     Here is What You Can Expect on a Typical Day     Build and optimize data pipelines logic and storage systems with latest coding practices and industry standards and modern design patterns and architectural principles remove complex technical impediments Develop high quality well documented and efficient code adhering to all applicable Prudential standards Conduct complex data analysis and report on results prepare data for prescriptive and predictive modeling combine raw information from different sources Collaborate with data analysts scientists and architects on data projects to enhance data acquisition transformation organization processes data reliability efficiency and quality Write unit integration tests and functional automation researching problems discovered by quality assurance or product support developing solutions to address the problems Bring a deep understanding of relevant and emerging technologies give technical direction to team members and embed learning and innovation in the daytoday Work on significant and unique issues where analysis of situations or data requires an evaluation of intangible variables and may impact future concepts products or technologies If people leader please add Use programming languages including but not limited to Python R SQL Java Scala PysparkApache Spark Shell scripting   The Skills  Expertise You Bring      Bachelor of Computer Science or Engineering or experience in related fields Experience in working with DevOps automation tools  practices Knowledge of full software development life cycle SDLC Ability to lead independently with minimal guidance and effectively leverage diverse ideas experiences thoughts and perspectives to the benefit of the organization Knowledge of business concepts tools and processes that are needed for making sound decisions in the context of the companys business Ability to learn new skills and knowledge on an ongoing basis through selfinitiative and tackling challenges Excellent problem solving communication and collaboration skills enjoy learning new skills Significant experience andor deep expertise with several of the following  o Programming Language Python R SQL Java Scala PysparkApache Spark Shell scripting    o Data Ingestion Integration  Transformation Moving data from multiple sources formats and volumes to analytics platforms through various tools Preparing data for further analysis transforming and mapping raw data to generate insights and wrangling data through tools    o Database Management System Storing organizing managing and delivering data using relational DBs NoSQL DBs Graph DBs and data warehouse technologies including AWS Redshift and Snowflake    o Database tools Data architecture to store organize and manage data Experience with SQL and NoSQL based databases for storage and processing of structured semistructured  unstructured data    RealTime Analytics Spark Kinesis Data Streams    Data Buffering Kinesis Kafka    Workflow Orchesration Airflow AppFlow Austosys Cloudwatch Splunk    Data Visualization Tableau Power BI MS Excel    o Data Lakes  Warehousing Building Data Models Data Lakes and Data Warehousing    o Data Protection and Security Knowledge of data protection security principles and services data loss prevention role based access controls data encryption data access capture and core security services    o Common Infrastructure as Code IaC Frameworks Ansible CloudFormation    o Cloud Computing Knowledge of fundamentals of AWS architectural principles and services Strong ability on cloud formation and to write code Knowledge of AWS core services    o TestingQuality Unit interface and end user testing concepts and tooling inclusive of nonfunctional requirements performance usability reliability securityvulnerability scanning etc including how testing integrated into Dev Ops accessibility awareness     Preferred qualifications  o Serverless data pipeline development using AWS Glue Lambda and Step functions    o Other Certifications        What we offer you     Market competitive base salaries with a yearly bonus potential at every level  Medical dental vision life insurance disability insurance Paid Time Off PTO and leave of absences such as parental and military leave  Retirement plans  401k plan with company match up to 4  Companyfunded pension plan  Wellness Programs to help you achieve your wellbeing goals including up to 1600 a year for reimbursement of items purchased to support personal wellbeing needs  WorkLife Resources to help support topics such as parenting housing senior care finances pets legal matters education emotional and mental health and career development  Tuition Assistance to help finance traditional college enrollment toward obtaining an approved degree many accredited certificate programs and industry designations  Employee Stock Purchase Plan Shares can be purchased at 85 of the lower of two prices Beginning or End of the purchase period after one year of service       To find out more about our Total Reward package visit        Work Life Balance  Prudential Careers    Some of the above benefits may not apply to parttime employees scheduled to work less than 20 hours per week     You’ll Love Working Here Because You Can    Join a team and culture where your voice matters where every day your work transforms our experiences to make lives better As you put your skills to use we’ll help you make an even bigger impact with learning experiences that can grow your technical AND leadership capabilities You’ll be surprised by what this rocksolid organization has in store for you    Note Prudential is required by state specific laws to include the salary range for this role when hiring a resident in applicable locations The salary range for this role is from 17350000 to 23470000 Specific pricing for the role may vary within the above range based on many factors including geographic location candidate experience and skills Roles may also be eligible for additional compensation andor benefits Eligibility to participate in a discretionary annual incentive program is subject to the rules governing the program whereby an award if any depends on various factors including without limitation individual and organizational performance In addition employees are eligible for standard benefits package including paid time off medical dental and retirement                                                                      Prudential Financial Inc of the United States is not affiliated with Prudential plc which is headquartered in the United Kingdom                                                                 Prudential is a multinational financial services leader with operations in the United States Asia Europe and Latin America Leveraging its heritage of life insurance and asset management expertise Prudential is focused on helping individual and institutional customers grow and protect their wealth The companys wellknown Rock symbol is an icon of strength stability expertise and innovation that has stood the test of time Prudentials businesses offer a variety of products and services including life insurance annuities retirementrelated services mutual funds asset management and real estate services                                                                 We recognize that our strength and success are directly linked to the quality and skills of our diverse associates We are proud to be a place where talented people who want to make a difference can grow as professionals leaders and as individuals Visit                                                               wwwprudentialcom                                to learn more about our values our history and our brand                                                                 Prudential is an equal opportunity employer All qualified applicants will receive consideration for employment without regard to race color religion national origin ancestry sex sexual orientation gender identity national origin genetics disability marital status age veteran status domestic partner status  medical condition or any other characteristic protected by law                                                                 The Prudential Insurance Company of America Newark NJ and its affiliates                                                                 Note that this posting is intended for individual applicants Search firms or agencies should email Staffing at                                                               staffingagenciesprudentialcom                                for more information about doing business with Prudential                                                                 PEOPLE WITH DISABILITIES                                If you need an accommodation to complete the application process which may include an assessment please email                                                               accommodationshwprudentialcom                                                                                                Please note that the above email is solely for individuals with disabilities requesting an accommodation If you are experiencing a technical issue with your application or an assessment please email                                                               careerstechnicalsupportprudentialcom                                to request assistance                                                                    </data></node>
<node id="n1946" labels=":Job"><data key="labels">:Job</data><data key="url">http://www.indeed.com/rc/clk?jk=cfe5b2f2b0a996ea&amp;bb=9kdXdebxtoL5LCu5AWPBVeLJaaN-StCUkdnYlAhsfOVrBFRmzZFJxN4PeeQWSJUQJHIRAXrGsc9YX-5f3VEKw-e3G0pwmWCYXeau6FPsU7GQQxj3selDAg%3D%3D&amp;xkcb=SoCk67M3CNjobgRSRx0HbzkdCdPP&amp;fccid=0ae5fffdc0fd2f94&amp;vjs=3</data><data key="content">Profile insightsFind out how your skills align with the job descriptionSkillsDo you have experience in TableauYesNoEducationDo you have a Bachelors degreeYesNo Job detailsHere’s how the job details align with your profilePay165000  185000 a yearJob typeFulltime LocationSan Francisco CA BenefitsPulled from the full job description401kDental insuranceHealth insurancePaid time offVision insurance Full job description   Company     Gantri is the world’s first digital manufacturer for creative lighting We help independent designers studios and influencers to develop original sustainably made lighting designs and sell directly to consumers We manufacture and fulfill all orders ondemand using 3D printing from innovative plantbased materials       Since launching in 2017 we’ve collaborated with the world’s best designers and studios including Ammunition Beats by Dre Smart Design OXO and Karim Rashid We’re the most awarded lighting brand in the US winning Time’s Best Inventions Fast Company’s Most Innovative in Design and many other awards       Role       Join Gantri as our first ever Data Engineer       Gantri is a highly collaborative vertically integrated organization that operates across manufacturing hardware software product design and marketing We believe that data is the key to helping us prioritize the right problems to solve and deliver even better products for our customers       As our founding Data Engineer you’ll have the opportunity to shape our data strategy and architecture You’ll get to design and implement scalable data pipelines build robust data warehousing solutions and enable advanced analytics capabilities This role offers immense growth potential as you establish data engineering best practices across the company and mentor other team members You’ll sit within our software organization and collaborate closely with all teams across the company       Responsibilities    Design build and maintain scalable and efficient data pipelines to collect process and store data from various sources including our inhouse manufacturing software and ecommerce platforms  Develop and maintain data warehouse architecture to ensure reliability accuracy performance and accessibility of data for analytics and reporting purposes  Collaborate with software engineers to integrate data collection and instrumentation into existing and new systems ensuring data quality and consistency  Create data models and schema designs to support reporting analytics and visualization needs of different teams within the organization  Implement data governance and security best practices to protect sensitive information and ensure compliance with regulations  Choose design and develop data visualization dashboards and tools to enable stakeholders to monitor key performance metrics and make datadriven decisions  Conduct exploratory data analysis and provide insights to support strategic decisionmaking and identify opportunities for business improvement  Stay current with industry trends tools and technologies related to data engineering analytics and visualization and recommend and implement improvements to our data infrastructure and processes       Requirements    Bachelors degree or higher in Computer Science Engineering or a related field  Proven experience 5 years as a data engineer or similar role with a strong track record of building and maintaining data infrastructure and pipelines  Proficiency in SQL and experience working with relational databases eg PostgreSQL MySQL and data warehousing solutions eg Redshift BigQuery  Experience with cloud platforms such as AWS Azure or Google Cloud and familiarity with services like S3 EC2 EMR or equivalent  Strong programming skills in languages such as Python Java or Scala and experience with data processing frameworks like Apache Spark or Apache Flink  Experience with data visualization tools such as Tableau Looker or Power BI and proficiency in data manipulation and visualization techniques  Excellent communication and collaboration skills with the ability to work effectively in crossfunctional teams and translate business requirements into technical solutions  Strong problemsolving skills and attention to detail with a passion for continuous learning and improvement  Bonus Experience working in a directtoconsumer or manufacturing industry with knowledge of manufacturing processes and systems  Bonus Experience with machine learning predictive analytics or data science techniques       Benefits    Competitive salary and equity  Medical dental and vision insurance  401k  Paid vacation days and paid holidays  Work from X benefits  Access to 3D printers for your personal projects  Monthly team lunches  And much more          165000  185000 a year      </data></node>
<node id="n1947" labels=":Skill"><data key="labels">:Skill</data><data key="name">data acquisition</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1948" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">sensor integration</data></node>
<node id="n1949" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">systems engineering</data></node>
<node id="n1950" labels=":Skill"><data key="labels">:Skill</data><data key="name">customer service</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1951" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">virtual infrastructure services</data></node>
<node id="n1952" labels=":Skill"><data key="labels">:Skill</data><data key="name">user expectations</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1953" labels=":Skill"><data key="labels">:Skill</data><data key="name">technical designs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1954" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">connectivity protocols</data></node>
<node id="n1955" labels=":Skill"><data key="labels">:Skill</data><data key="name">data gaps</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1956" labels=":Skill"><data key="labels">:Skill</data><data key="name">store data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1957" labels=":Skill"><data key="labels">:Skill</data><data key="name">data access</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1958" labels=":Skill"><data key="labels">:Skill</data><data key="name">transaction rates</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1959" labels=":Skill"><data key="labels">:Skill</data><data key="name">volume analysis</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1960" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">translate applications requirements</data></node>
<node id="n1961" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">integrating web pages</data></node>
<node id="n1962" labels=":Skill"><data key="labels">:Skill</data><data key="name">legal data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1963" labels=":Skill"><data key="labels">:Skill</data><data key="name">aggregate data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1964" labels=":Skill"><data key="labels">:Skill</data><data key="name">data frameworks</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1965" labels=":Skill"><data key="labels">:Skill</data><data key="name">azure cloud</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1966" labels=":Skill"><data key="labels">:Skill</data><data key="name">data processes</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1967" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">transform raw data</data></node>
<node id="n1968" labels=":Skill"><data key="labels">:Skill</data><data key="name">data strategy</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1969" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">dimensional modeling</data></node>
<node id="n1970" labels=":Skill"><data key="labels">:Skill</data><data key="name">cloud data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1971" labels=":Skill"><data key="labels">:Skill</data><data key="name">risk management</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1972" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">preventative maintenance</data></node>
<node id="n1973" labels=":Skill"><data key="labels">:Skill</data><data key="name">metric reporting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1974" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">organizational skills</data></node>
<node id="n1975" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">emergency response</data></node>
<node id="n1976" labels=":Skill"><data key="labels">:Skill</data><data key="name">data design</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1977" labels=":Skill"><data key="labels">:Skill</data><data key="name">ssms</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1978" labels=":Skill"><data key="labels">:Skill</data><data key="name">visual studio</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1979" labels=":Skill"><data key="labels">:Skill</data><data key="name">synapsedw</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1980" labels=":Skill"><data key="labels">:Skill</data><data key="name">stream analytics</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1981" labels=":Skill"><data key="labels">:Skill</data><data key="name">raw data</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1982" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">pipeline architectures</data></node>
<node id="n1983" labels=":Skill"><data key="labels">:Skill</data><data key="name">batch</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1984" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">distributed storage</data></node>
<node id="n1985" labels=":Skill"><data key="labels">:Skill</data><data key="name">cloud strategies</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1986" labels=":Skill"><data key="labels">:Skill</data><data key="name">data housekeeping</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1987" labels=":Skill"><data key="labels">:Skill</data><data key="name">hashing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1988" labels=":Skill"><data key="labels">:Skill</data><data key="name">anomalies</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1989" labels=":Skill"><data key="labels">:Skill</data><data key="name">create models</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1990" labels=":Skill"><data key="labels">:Skill</data><data key="name">lamdba</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1991" labels=":Skill"><data key="labels">:Skill</data><data key="name">sqs</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1992" labels=":Skill"><data key="labels">:Skill</data><data key="name">sns</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1993" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">match requirements</data></node>
<node id="n1994" labels=":Skill"><data key="labels">:Skill</data><data key="name">data storage</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1995" labels=":Skill"><data key="labels">:Skill</data><data key="name">data ingestion</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1996" labels=":Skill"><data key="labels">:Skill</data><data key="name">validation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1997" labels=":Skill"><data key="labels">:Skill</data><data key="name">design code</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1998" labels=":Skill"><data key="labels">:Skill</data><data key="name">cleansing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n1999" labels=":Skill"><data key="labels">:Skill</data><data key="name">caching</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2000" labels=":Skill"><data key="labels">:Skill</data><data key="name">aggregation</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2001" labels=":Skill"><data key="labels">:Skill</data><data key="name">staging</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2002" labels=":Skill"><data key="labels">:Skill</data><data key="name">design decisions</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2003" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data flow requirements</data></node>
<node id="n2004" labels=":Skill"><data key="labels">:Skill</data><data key="name">unit testing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2005" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">establish processes</data></node>
<node id="n2006" labels=":Skill"><data key="labels">:Skill</data><data key="name">drive solutions</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2007" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">designing analytic data</data></node>
<node id="n2008" labels=":Skill"><data key="labels">:Skill</data><data key="name">mapreduce</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2009" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">design architecture</data></node>
<node id="n2010" labels=":Skill"><data key="labels">:Skill</data><data key="name">data consistency</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2011" labels=":Skill"><data key="labels">:Skill</data><data key="name">spring boot</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2012" labels=":Skill"><data key="labels">:Skill</data><data key="name">collecting</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2013" labels=":Skill"><data key="labels">:Skill</data><data key="name">parsing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2014" labels=":Skill"><data key="labels">:Skill</data><data key="name">data modeler</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2015" labels=":Skill"><data key="labels">:Skill</data><data key="name">lidni</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2016" labels=":Skill"><data key="labels">:Skill</data><data key="name">kimball</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2017" labels=":Skill"><data key="labels">:Skill</data><data key="name">coding</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2018" labels=":Skill"><data key="labels">:Skill</data><data key="name">plsql</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2019" labels=":Skill"><data key="labels">:Skill</data><data key="name">cloud concepts</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2020" labels=":Skill"><data key="labels">:Skill</data><data key="name">manipulating</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2021" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">large disconnected datasets</data></node>
<node id="n2022" labels=":Skill"><data key="labels">:Skill</data><data key="name">message queuing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2023" labels=":Skill"><data key="labels">:Skill</data><data key="name">stream processing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2024" labels=":Skill"><data key="labels">:Skill</data><data key="name">infoworks</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2025" labels=":Skill"><data key="labels">:Skill</data><data key="name">devx</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2026" labels=":Skill"><data key="labels">:Skill</data><data key="name">extraction</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2027" labels=":Skill"><data key="labels">:Skill</data><data key="name">generate insights</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2028" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">security principles</data></node>
<node id="n2029" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">data loss prevention</data></node>
<node id="n2030" labels=":Skill"><data key="labels">:Skill</data><data key="name">role based access</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2031" labels=":Skill"><data key="labels">:Skill</data><data key="name">data encryption</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2032" labels=":Skill"><data key="labels">:Skill</data><data key="name">end user testing</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2033" labels=":Skill"><data key="labels">:Skill</data><data key="name">product design</data><data key="category">BD-ML-AI-Job</data></node>
<node id="n2034" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">analytics capabilities</data></node>
<node id="n2035" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Job</data><data key="name">predictive analytics</data></node>
<node id="n2036" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/computer-scientist-8459b18deaaa4a7998287c064f49a41b</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary     Experienced Software Engineer offers ten years of complete software development lifecycle expertise in the software industry       Highlights         C C C Java JavaScript and Python proficiency with multiple development tools Eclipse Visual Studio PyScripter and software development experience on Linux Windows and Mac OS platforms Currently employed as a Computer Scientist for US Army Corps of Engineers C C CNET ASPNET Java and JavaScript Integrated Development Environments used Visual Studio 2013 Eclipse PyScripter Brackets Document Preparation Software used Visual StudioNET MPI Microsoft VISIO UML LaTex Telecommunications Protocol Experiences LAPD SAAL V51V52                       Accomplishments              Experience        062008   to   Current   Computer Scientist    Gap Solutions Inc                          Primary Responsibilities Primary Responsibilities Design develop test maintain and provide customer support for a range of USACE software systems including Development of a pattern detection system to detect facility resource usage trends in Smart Buildings both residential and commercial  Maintained and improved the Corps of Engineers Bridge Information System CEBIS  I am also involved in benchmarking activities for new equipment acquisitions for Department of Defense Shared Resource Center DSRC            011998   to   072002   Senior Software Engineer    Pega Pegasystems Inc                          Designed developed tested released maintained and provided customer support and customer training for realtime telecommunications protocol software products conforming to telecommunications standards published by ITUT and ETSI standardization organizations  The published standards define protocols used at different layers of the seven ISOOSI layers            082003   to   052008   Graduate Research Assistant    Purdue University                          Assisted several faculty members with research in the High Performance Computing area  Software Development Experiences  Professional As the Principal Software Engineer I have developed custom AutoCAD palettes using AutoCAD NET API for Virtual Forward Operating Base VFOB  The palette holds the different blocks that may be dragged and dropped onto the AutoCAD drawing canvas  DCAM As the Principal Software Engineer I have redesigned restructured and developed the Detailed Component Analysis Module DCAM of the VFOB software  The original DCAM was developed using monolithic ad hoc structure  I have redesigned the module by adopting objectoriented paradigm  LCM As the Primary Responsible Engineer I have developed a software module for sensor data analysis and prediction to support the unified building information modeling BIM framework for sustainable missionready facilities  The BIM framework provides a facility control lifecycle model that allows comparison between expected and actual facility usage and detect any usage anomalies thereof  The developed software implements a Cluster Analysis algorithm for comparing and identifying patterns in historical sensor data  It also implements the Fast Fourier Transformation FFT for noise reduction in the data  As the Primary Responsible Engineer I have designed developed and successfully released the V51V52 V Interfaces at the digital Local Exchange to support Access Networks protocol software product  I have used C and C for development  I have tested and validated the V51V52 product for quality acceptance and conformance to the ITUT standards  I used proprietary scripting language for testing conformance  I have provided postrelease maintenance bug discovery patch generation etc and customer support  The product is used to establish maintain and control communication between telephone Access Networks and Local Exchanges  The corresponding specifications describe both OSI layer 3 network layer and 2 data link layer protocols  Both protocol software products were developed during the same product lifecycle  As the Primary Responsible Engineer I have reengineered and modified the LAPD Link Access Procedure for ISDN Dchannels and SAAL Signaling ATM Adaptation Layer protocol software source codes to upgrade the products from dysfunctional code bases and released them as fullfeatured and stable products resulting in an overall increase of more than 50 in sales for both products  I used C and C languages  I have modified LAPD and SAAL source code to support their use in FaultTolerantHigh Availability architecture include provision of runtime debug control and make extension of the products management interface controls  I have tested and validated the LAPD and SAAL products for quality acceptance and conformance to the ITUT standards  I have provided postrelease maintenance bug discovery patch generation etc and customer support to LAPD and SAAL customers  Software Development Experiences  Research As a research assistant at the Mississippi State University I have developed a resource management system for adaptive parallel applications for LINUX cluster environments using C TCPIP Sockets the POSIX thread library and the C Standard Template Library  Designed and developed a LINUX based NIC module for the Myrinet programmable M3MPCI64B NIC and the embedded Myrinet Control Program MCP for the LANai 9 RISC processor of the NIC  The MCP is developed using the LANai Tools version 295216  Software Development Experiences  Academic Developed a LINUX kernel module for driving the Standard Microsystems Corporation LAN83C171 Ethernet card using C  Developed a parallel Quicksort program using MPI C and STL          Education        Expected in      MS       Computer Science    Mississippi State University          MS      GPA       Computer Science          Expected in      MS       Computer Science    University of Southern California     Los Angeles     CA      GPA       Computer Science          Expected in      MSc       Computer Science    University of Dhaka                GPA       Computer Science          Expected in      BSc Honors       Applied Physics and Electronics    University of Dhaka                GPA       Applied Physics and Electronics        Skills     NET CNET ASPNET Academic acquisitions ad API Army ATM AutoCAD benchmarking C C canvas customer support data analysis driving Eclipse Engineer I Ethernet Fast drawing ISDN ISO Java JavaScript LAN LINUX Mac OS Access C MCP Exchange Microsoft VISIO Windows modeling network Networks NIC objectoriented OSI PCI POSIX Programming protocols Python quality realtime Research sales scripting Sockets Software Development Software Engineer I TCPIP telecommunications telephone troubleshooting UML upgrade validation Visual Studio</data><data key="id">185536162681982826117724132482947365103</data></node>
<node id="n2037" labels=":CV"><data key="labels">:CV</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary      Ready to assume the leadership responsibilities of a fastpaced high pressured environment to serve and improve product quality and turnaround time To direct the process improvement initiatives of a global leader design implement and support the adherence to corporate goals collaborating with employees customers and suppliers insuring continuous improvement of process products and services        Highlights           Data analysis  Perform timely reviews  Prioritize tasks  Conduct lab investigations      Participate in audits  Prepare SOPs  Knowledge of cGMPs  Troubleshooting issues                       Professional Experience        052013   to   2016   Analytical Research  Scientist    Emd Millipore         Santa Fe De Bogota     CO            Develop and initiate analytical assay plans for novel and routine customer products  Demonstrate the ability to be flexible to adapt to shifting project time lines  Establish benchmarks for onquality technology transfer to GMP QCManufacturing  Demonstrate the ability to maintain customer focus of multiple projects  Demonstrate the ability to effectively execute and communicate assigned tasks and conduct complex assignments  Assist in the development of new products by providing analytical support as required for all assigned projects  Generate written and oral reports eg  progress reports experimental results marketing data etc and effectively present the information  Support and coordinate analytical testing needed for successful transfer of products and manufacturing processes to the Transfer group and Operations  Develop appropriate analytical specifications experimental design method development and documentation required for successful transfer of products  Assist in the development of new assays for Quality ControlAnalytical Services            062007   to   112012   Senior Associate Scientist    Emd Millipore         Schnelldorf     DE            Spearhead sample testing primarily by HPLC SECMALLS and capillary electrophoresis in support of Biologics Process   Development and DAI studies Instrumental in performing routine testing and participate in activities such as assay troubleshooting and assay optimization   Analyze level of impurities in newly patented drugs prior to their release into the market   Research and develop new analytical models in chemical composition organization properties and the processes and transformations which transpire   Demonstrate the ability to effectively execute assigned tasks and conduct complex assignments ie determination of metal residue substance levels via ICPMS in a timely manner            022004   to   062007   Senior Associate Scientist    Pfizer Inc         City     STATE            Perform all work in compliance with GXP company SOPs and regulatory standards  Conduct internal audits of laboratories and laboratory data  Perform timely data review to ensure accuracy completeness cGMP compliance and applicable regulatory requirements  Document and report results in accordance with GMP  Reconcile Quality Control laboratory workbooks and logbooks  Assist in the review and approval of GMP documents such as SOPs and associated forms and initiate revisions  Assist in preparing internal scientific reports and testing of GMP manufacturing and release of clinical API DP inprocess controls and intermediates            032000   to   012004   Science Associate II    Pharmacia Inc         City     STATE            Work alongside scientists in discovery biomarkers and clinical to design experiments analyze and interpret data and effectively communicate results   Perform and develop cGMP SOPs for cleaning process  Revise and review various cGMP documents that focus on the determination of the efficacy and safety of compounds the impact of dose decisions selection of novel indication for targets and compounds the biological interpretation of results and the identification of prediction biomarker  Develop and maintain a strong professional network with the Internal and External scientific communities  Conduct internal audits of laboratories laboratory data and computer systems          Education        Expected in   2012   Masters of Science       Clinical Research Management    Washington University     St Louis     Missouri      GPA       Clinical Research Management          Expected in   1995   Bachelors of Science       Chemistry    Chicago State University     Chicago     Illinois      GPA       Chemistry          Expected in      Bachelor of Arts       Curriculum  Instruction    Chicago State University     Chicago     Illinois      GPA        Curriculum  Instruction Grade K12 Chemistry Teacher for 4 years         Certifications      Certified Lead Auditor of Quality Systems BSI QS 9000  Certified Auditor BSI Certified ISO 134852003 Lead Auditor BSI Certified ISO 180012007 Auditor        Skills      Communicate effectively with colleagues across functional areas as well as with team members managers suppliersvendors  Develop document and enforce Operating Procedures as they relate to departmental laboratory processes knowledge of cGMP guidelines  Strong skills in conducting a variety of USP assays HPLC Atomic  Absorption Capillary Electrophoresis UV Atomic Absorption AA  Strong computer proficiencies Windows Microsoft Office Suite Chromeleon and Empower</data><data key="id">53988271923513810304169348759186310616</data><data key="url">https://www.livecareer.com/resume-search/r/analytical-research-scientist-84fa5c1334414ed1a6d938631f16d6a4</data></node>
<node id="n2038" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary     To join a highperformance team of professionals in design and development and to make an impactful contribution to quality of services and products that help improve people’s lives Experience and skills that span a number of disciplines required for research and development quality control novel assay design and optimization troubleshooting complex issues enabling system integration crossfunctional collaboration and expertise in utilizing sophisticated platforms for a vast array of applications Over 5 years of GMPGLP experience specializing in system verification and validation and quality control with leading and bringing projects to fruition       Skills           Experienced working independently and in a teambased environment  A crossfunctional collaborator with a diverse work experience and academic background  Critical thinker with excellent analytical organizational and time management skills  Demonstrated ability to take initiative and ownership of projects acquire and apply scientific knowledge and techniques rapidly and efficiently      Adept in handling various analytical instrumentation and SW  Good design and test skills focused on quality and attention to details  EXTENSIVE EXPERIENCE IN DATA  ANALYSIS AND APPLICATIONS JMP MICROSOFT Office Knowledge of GMPGLP and FDA regulations quality and process management techniques QMS IQ OQ PQ and PPQ Visio and Smartsheet                       Experience       Scientist IIEngineer       112017   to   102022     American Electric Power         Avinger     TX             Completed assessment of automated equipment system to ensure system’s functionality for production  Authored TMV for vision systems and IOQ protocols in collaboration with engineering  Managed outcome results images analyzed and validated the system to verify proper functionality of the Vision system software  Assisted with assessment and characterization of automatic BGMS Blood Glucose Monitoring System to support product launch and help ensure system functionality  Managed multiple postlaunch crossfunctional process optimization projects resulting in increased production efficiency improved process flow and clear easily followed SOPs  Lead product manufacturing optimization for desiccating process resulting in dramatical reduction of production time and cost and workforce cost savings  Developed and validated data analysis and tracking spreadsheets essential for production process quality monitoring  Handled chemical shelflife assessment studies and subsequent required implementation tasks including updating part specification and incoming QC documents in a timely manner  Participated in clinical studies and provided feedback to SW engineer and Mobile Application developers on application design and functionality at each development stage  Lead and performed product testing analyzed data using JMP and Microsoft Excel reviewed and presented results to colleagues and management to provide feedback on product performance  Supervised and supported team members in performing their jobs by providing training and suggestions for experiment execution and data analysis  Initiated and implemented changes for product test process optimization and lab workflow efficiency  Developed data analysis and tracking spreadsheets for lab use ensuring quality records and data traceability  Guided personnel through regular spreadsheets‘ optimization and their validation process  Provided a feedback to Clinical Affairs and Marketing departments on product inserts and user manuals ensuring clear userfriendly instructions  Participated in DOEs tracking the quality of SOPs ensuring the content is reflective of current processes monitored the performance and calibration of the equipment used to support testing  Coordinated training sessions and trained personnel across different departments  Conducted ongoing shelflife program to monitor product performance and stability           Engineer       012016   to   012017     BD Biosciences         City     STATE             Responsible for assessment and characterization of medical diagnostic platforms to support product development and ensure systems functionality  Involved in complex computer modeling augmenting BD’s existing integration systems to normalize CST performance during the launch of BD newest LyricMelody analysissorting platforms  Analyzed and fixed macro code to improve the performance  Contributed to ground zero efforts to establish next generation setup for BD’s premium platform 1250 detector system  Helped to establish an efficient method for channel specific instrument characterization via LEDs which dramatically reduced assessment time by 94 improved accuracy of test steps and finalized workflow for further study           SYSTEMS       012014   to   012016     VERIFICATION SCIENTIST BD         City     STATE             Participated in system verification validation and characterization testing of CEIVD medical diagnostic products and systems including a combination of hardware and software components  Led and executed systems’ characterization and verificationvalidation tests analyzed and interpreted the results of the system studies to confirm functionality of the system  Directed and coordinated training and testing activities of personnel to ensure project progress  Developed test plans presented in study design reviews and assisted in approval processes for the VV protocols to assure a smooth workflow  Designed test steps for PIR investigation to confirm the system performance  Identified and entered defects using the TFS to improve the quality of the BD products  Developed the test protocolsprocedures for system level testing for RoHS program ensuring optimization of the process resulting in 5fold improvement in efficiency and workforce cost savings           GRADUATE RESEARCH ASSISTANTTEACHING ASSOCIATE       012008   to   012013     NORTHEASTERN UNIVERSITY         City     STATE             Managed crossfunctional projects focusing on nanotechnologybased platforms as optimal systems for gene delivery  Assessed characterized and analyzed biochemical changes associated with the growth of an early embryo as well as 3D tumor microenvironment and its subsequent organization pattern  Trained and supervised students in variety of analytical techniques ensuring the students put their theoretical knowledge into the practice gaining proper handson experience          Education and Training       Certified ScrumMaster CSM certification Issued by          Expected in   112022     Scrum Alliance      Denver     CO     GPA         Bcertmesvcksncmx  Data Science R Basics           Course offered by HarvardX          Expected in   122020                     GPA                  Management Skills Training offered by ADP September 2020  Introduction to Systems Engineering Course offered through Coursera            Expected in   042015     University of New South Wales                GPA                PhD     Physical Chemistry     Expected in   012013     Northeastern University      Boston     MA     GPA       Major Research interest was in imaging modalities and microspectroscopic methods of analysis         Diploma     Optics and Photonics     Expected in   012012     University of Illinois      UrbanaChampaign          GPA                Master’s Degree     Chemistry     Expected in        Voronezh State University      Voronezh          GPA</data><data key="id">97269310519276101270265679569681554673</data><data key="url">https://www.livecareer.com/resume-search/r/scientist-ii-engineer-80079ae74c34455f98b6583af7c4d280</data></node>
<node id="n2039" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/computer-scientist-7500518fca1f4ab4b77ea6dd8280d916</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Profile       Computer Scientist at Adobe Systems Inc bringing 3 years of experience in superior design and debugging capabilities innovative problem solving skills and dedication to quality   Manages large projects while maintaining high team morale and energy Skilled mentor and mediator who excels at bringing out the best in team members  Experience working directly with high profile customers and drive new features through every stage of product life cycle  Software Engineer at Siemens Product Lifecycle Management PLM Inc with primary focus on Software Development Life Cycle Process  Rapid Application Prototyping  Project Engineer at Wipro Technologies whose role stretches beyond that of a developer to that of a user advocate designer developer and tester  Supportive and enthusiastic team player dedicated to streamlining processes and efficiently resolving project issues  Excellent communication  Organization skills with experience in risk assessment and mitigation             Skills                   Programming C Java C HTML          Testing Tools Jmeter XML Matlab ObjectiveC          Agile Project Management Version Platforms iOS Android Windows          One Unix Linux          Bug Tracking Tools JIRA Watson Scripting Languages Perl          Express JavaScript Basic PHP Basic TCL          IDE MS Visual Studio Eclipse Xcode Basic Python          Flash Builder Net Beans Databases Oracle SQL PLSQL          Third Party Tools Splunk Charles MSACCESS          Proxy iExplorer PhoneDisk Ozzy Simulators NS2 Wireshark TINYOS          Browser GLOMOSIM          Version Control Perforce GIT Application Server Apache Tomcat          Design Tools InDesign CS55 6 7 JBoss   Project management                Programming C Java C HTML          Testing Tools Jmeter XML Matlab ObjectiveC            Agile Project Management Version   Platforms iOS Android Windows          One Unix Linux            Bug Tracking Tools JIRA Watson   Scripting Languages Perl          Express JavaScript Basic PHP Basic TCL           IDE MS Visual Studio Eclipse Xcode Basic Python          Flash Builder Net Beans Databases Oracle SQL PLSQL          Third Party Tools Splunk Charles MSACCESS          Proxy iExplorer PhoneDisk Ozzy Simulators NS2 Wireshark TINYOS          Browser GLOMOSIM          Version Control Perforce GIT Application Server Apache Tomcat          Design Tools InDesign CS55 6 7 JBoss                     Education and Training       Univesity of Cincinnati    Cincinnati     Ohio      Expected in   2011                    Master of Science        Computer Science          GPA        GPA 3940   Computer Science GPA 3940         Anna University    Chennai     Tamil Nadu      Expected in   2007                    Bachelor of Science        Computer Science          GPA        GPA 3940   Computer Science GPA 3940          Accomplishments       Sun Certified Java Programmer SCJP for securing a top score of 95 in the SCJP exam  Fundamentals of DB2 administrator certified by IBM  White Belt  Green Belt Security Certification  Publications Technical Reports Network Selection Algorithm for Satisfying Multiple User Constraints Under Uncertainty in a Heterogeneous Wireless Scenario Masters Dissertation University of Cincinnati  Technical Papers Handling Network Uncertainty in Heterogeneous Wireless Networks at IEEE Infocom 2011 IEEE Computer Soceity  Using Minimization of Maximal Regret for Interface Selection in a Heterogeneous Wireless Network at 2nd IEEE International Workshop on Smart Communication Protocols and Algorithms  Activities Events Lead  an active volunteer of Vibha an organization to educate and empower underprivileged children  A volunteer at Eastside Baby Corner that provides the basic necessities for every childs needs  Research Member of Education Development  Services EDAS         Professional Experience       Gap Solutions Inc      Computer Scientist   Hamilton     MT                   092012      Current     Mastery in Digital Publishing suite and the various tools utilized to publish magazines for mobile devices on iOS and Android platforms  Collaborated with product and engineering team members to define and develop new product concepts  Created Splunk dashboards and monitoring tools for customers to track their overall performance  Developed software tools to improve efficiency in diagnosing and resolving customerreported software defects  Debug and test mobile applications developed for iOS and Android devices  Liasion with key customers and publishers to understand problematic workflows and isolate potential defects  Designed and implemented enhancements to client and serverside solutions  Well versed with scrum methodologies and can effectively work in an Agile environment  Interacted with the many DPS engineering teams as well as other teams within Adobe such as Creative Cloud and Adobe Experience Manager AEM and also interacted with crossdiscipline teams such as product marketing technical support sales and customer enablement  Delivered software solutions consistent with the product roadmap and released plan milestones  Provided crossservice and debugging of complex software code defects that evade reproduction  Log reporting tools by leveraging mastery of a wide range of HTTP traffic  Coordinated continued performance assurance of software applications and automated performance test scripts  Proficient in detailed log analysis and researching the problems to fine detail           AlarmCom      Software Engineer   Boston     MA                   082010      082012     Experienced Application Developer for Teamcenter Rich Client and Web Client  Designed and developed prototypes and proof of concept for Teamcenter Thin Client using Javascript and Yahoo Toolkit  Implemented specific client side modules for Service Scheduling application using Java SWT components  Performed a thorough risk assessment and mitigation stratergy for Change Management Enhancements by developing prototypes using Java and C  Worked on coding phase of Content Management for Teamcenter application  Code development for Graphic Attribute Mapping and XML Attribute Mapping using C  Worked on SAX parsing of XML content during document Compose  Created graphic user interfaces for ImportExport of documents and graphics using Java  Generated XML rendered stylsheets for client side components and implemented innovative UI customatizations  Spearheaded the MultiFieldKey project in Maintenance Repair  Overhaul MRO domain and executed the complete set of SDLC steps  Resolved critical problem reports and customer patches significant to Teamcenter 91 Release to Market  Analysed and documented Service Data and Operations for SOAs implemented in MRO  Actively involved in developing challenging modules to enhance Change Manager and Workflow application in Teamcenter  Conducted periodical meetings and demo sessions to brainstorm the technical aspects of the product and provide valuble input  Generated functional specifications design documents and project test plans for various projects  Executed a full suite of system intergration and regression testing for MRO applications  Instrumental in conducting the xUnit and Rich Client Automation Framework RCAF tests periodically           University Of Maine      Research Assistant   Orono     ME                   042009      122010     Thesis titled Network Selection Algorithm for Satisfying Multiple User Constraints Under Uncertainty in a Heterogeneous Wireless Scenario  Ad hoc  Sensor Networks  Simulated topology discovery localization and MAC Access protocol in a random deployment of sensors using Java and TinyOS  Wireless and Mobile Systems  Designed and developed an application in Java to simulate the Automatic Repeat request ARQ techniques for flow control in a communication network  Developed code in Matlab for Fuzzy Rule based classification and a quantitative comparison of various membership functions based on experimental data           Apex Systems      Project Engineer   Thousand Oaks          India              082007      072008     Worked on code configuration and error handling team covering various segments of vehicle insurance for an overseas client using ASPNet  Created project documents and test plans  conducted JUnit testing for Carfax vehicle history reports  Underwent training in Service Oriented Architecture and Web Services          Affiliations              Skills     ASPNet Adobe Ad Agile Apache Automation Basic C Change Management client and server concept content Content Management Client Version Control Databases debugging Eclipse XML Flash functional graphics Graphic HTML HTTP PHP IDE InDesign insurance Java Beans JavaScript JBoss Linux MAC Market Matlab meetings MSACCESS Access C Windows Network Networks ObjectiveC OS Oracle Developer PLSQL Perl product marketing coding Programming Project Management Proxy Python Express reporting researching risk assessment sales Scheduling scrum SDLC scripts Scripting SQL TCL technical support Tomcat Unix MS Visual Studio Workflow</data><data key="id">188404313780883442747736164134401233511</data></node>
<node id="n2040" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Professional Overview      Motivated Research Scientist with over 18 years experience in compound management Adept at laboratory management and extensively trained on multiple liquid handling platforms        Core Qualifications           Experience with the distribution of both wet and dry compounds  Advanced knowledge of liquid handling automation AgilentTecan  Labcyte   Advanced knowledge of storage and retrieval systems  Quality assurance and control  Process improvement      Detail Oriented  Superb time management skills  Experience with various LIMS systems  Experience with Microsoft Office WordExcelOutlookPowerPoint                        Experience       Research Scientist I Compound Management Lead Discovery  Optimization       062011   to   Current     BristolMyers Squibb         City     STATE             Managed DMSO stock compound collection and all incoming sample plating requests supporting all early discovery research  Managed resource allocation of staff and automation resources supporting all wet compound plating activities  Served as primary point of contact for HTS  Member of core leadership team responsible for the creation of functional specification documentation defining all compound handling requirements which facilitated the development and integration of a highly flexible automated wet compound sample plating and replication platform  Developed the training protocols and supporting documentation encompassing all wet compound sample handling processes utilized by Compound Management colleagues  Generated process mappings and gap analysis of current wet plating processes in preparation for migration to new LIMS system Mosaic  Writing and executing detailed test scripts for all wet plating processes related to changing of internal inventory software           Associate Research Scientist II       062007   to   062011     BristolMyers Squibb         City     STATE             Established methods on the Biocel and tested all necessary informatics to allow the transfer of over 12 MM compounds from the Haystack Tube Store to the new HomeBase tube Store   Developed and tested liquid handling methods for the implementation of direct 96 to 1536 plating which reduced our compound consumption for retests by 50   Developed and tested Agilent Biocel methods and assisted in testing informatics for the integration of a large acquisition of compoundsThis resulted in 1MM new samples being introduced into the screening deck   Assisted with the implementation of the new Inbound and Shipping application This streamlined the process of receiving as well as shipping out samples to our other BMS sites  Between 20082009 distributed over 30k plates and replicated over 30MM wells to Lead Discovery  Collaborated with other members of the team to establish a method for the creation of concentration response curve ready plates CRC directly from tubes to 1536 well plates This resulted in over an 80 reduction in compound consumption for this process  Created 1536 well mother plates over 500k compounds for initial load of HTS plate store  Migrated all wet plating processes from old Tecan Genesis platform to new Tecan Evo            Associate Research Scientist I       062003   to   062007     BristolMyers Squibb         City     STATE             Managed plating systems used for the preparation and distribution of all assay ready plates with an average yearly distribution exceeding 10 million wells per year   Inventory conversion project which included the retrieval reformatting and loading of over 1000000 wet compounds into our new TAP HomeBase tube store   Drove natural products project which required reformatting 13000 natural products extracts into plates for Oncology screening  Weekly QC of all liquid handling instruments   Reporting of monthly metrics  Distributed over 30k plates and replicated over 30MM wells to Lead Discovery Processed 50k compounds in 1 month for external collaboration          Assistant Research Scientist II       1997   to   062003     BristolMyers Squibb         City     STATE             Collaborated with BMS engineers to design develop and implement a custom fully automated microtube capping and decapping stationThis significantly increased throughput and allowed increased walk away time for the end user Prior to this the process was time consuming and labor intensive  Responsible for the preparation storage and distribution of wet plates   Verified that all inventory files and platemaps were accurate and available to the screening community   Improved the efficiency of plate replication through the evaluation and integration of new forms of liquid handling automation  Preparation of daily shipments to other sites   Became experienced user of Haystack Automated Tube store  Proficient in the use of Tecan Zymark Hamilton and Velocity 11 automation   Capable of developing liquid handler methods troubleshooting and basic maintenance          Awards       RD Star Award for Compound Management Cherry Pick Process Optimization 2014  RD Star Award for Regression testing effort during server upgrade project 2013  RD Star Award for Supplying over 20k compounds for two external technology evaluations 2011  RD Star Award for providing 28k compounds to Virology while concurrently plating an additional 47k compound for HTS in 1 month 2011  RD Star Award for Providing retests in a timely manner and helping achieve our screen in a week campaign 2009  Triumph Team Award for contributions to the HTS Miniaturization Initiative 2008  RD Star Award for Initial load of Homebase tube store 2007  RD Star Award for Management of Tube Store and Assay Ready Plating 2005  RD Star Award for Natural Products Reformat for Oncology Screening 2003         Publications      Technical Note  John E Leet James V Belcastro Jessica J Claire 2015 HPLC Biogram Analysis  A Powerful Tool Used for Hit Confirmation in Early Drug Discovery  Journal of Biomolecular Screening June 2015        Skills       Advanced knowledge of multiple LIMS systems specifically OSCAR The Automation Partnership and Mosaic Titian  Experience with liquid handling automation Tecan Agilent BioCel  Agilent BenchCel and Labcyte Echo  Knowledge of liquid handling software Tecan Genesis  Evoware Agilent Vworks and BenchWorks  Strong verbal and written communication skills  Process development and optimization  Metrics analysis and reporting         Education       Bachelor of Science          Expected in   1996     Teikyo Post University                GPA</data><data key="id">26177723055545340250068126093365306708</data><data key="url">https://www.livecareer.com/resume-search/r/research-scientist-i-compound-management-lead-discovery-optimization-6c46e71c35cc4915b8201a5b4a864bbc</data></node>
<node id="n2041" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/research-scientist-65004837630440beac498c4b59388a98</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary      Conservation biologist with more than 6 years of experience in research restoration conservation planning and outreach K nowledgeable about zoological gardens and animal sanctuaries Exceptional communicator who works well on diverse teams             Highlights         Expert in data analysis    Microsoft Office Suite expert    Specialized software Matlab Sigmaplot FishStatJ Aquatracker    Molecular genetics    Experimental design      Grant writing expertise    Fluent in Spanish and Catalan    Elementary proficiency in Japanese    Event planning    Detailoriented                     Professional Courses and Certifications       Stony Brook University    Stony Brook     NY      Expected in   2012                    Master of Science MS        Marine and Atmospheric Sciences          GPA            Concentration in Ecology and Molecular genetics  Graduate Student Club Officer  Activities Coordinator          Universitat de Barcelona  Facultat de Biologia    Barcelona           Expected in   2009                    Licentiate degree        Biology          GPA            Major in Zoology and Physiology  Active member of the Scuba Diving Club          Stony Brook University    Stony Brook     NY      Expected in   2015                    PhD Candidate        Marine and Atmospheric Science          GPA            Ecological interactions on estuarine communities  Otolith Microchemistry  Cofounder and organizer of the annual SoMAS Photocompetition 20122015                         Expected in                       Marine Conservation Activities CoFounder and Organizer of the Annual SoMAS Photo Competition 20122015 Certifications Marine Mammal Training and Enrichment Barcelona Zoo Spain Open Water Scuba Diver                   GPA                    Stony Brook University               Expected in                       Radiactive safety management        Basic Forensic Science          GPA           Basic Forensic Science         Universitat de Barcelona UB Barcelona University               Expected in                                         GPA                     Experience       Jacobs Engineering Group Inc      Research Scientist   Houston     TX                   072012      082015     As a member of the IOCS scientific staff I collaborated in several projects in conservation of natural resources and biodiversity such as Shinnecock Bay Restoration Program SHIRP  The goal of this project is to permanently restore the water quality and fisheries of the Shinnecock Bay  The duties included but were not limited to scientific surveys of the Bay clameelgrass restoration efforts scientific project and budget design educational outreach fundraising marketing and coordination of volunteer staff  United Nations Development Programme UNDP  Creation of review reports and executive summaries on the fisheries status of Tuna and Shark species in the Pacific Ocean  Prepared recommendations on regulations requirements and needed improvements of Shark Sanctuaries in the Pacific Ocean  Those tasks required intensive scientific literature reviews ability to summarize important information and excellent writing skills  Seafood Watch Monterey Bay Aquarium Exceptional Species  Member of the scientific team in charge of developing a new ecological concept for marine species conservation and regulation  Required extensive literature reviews as well as creative writing in the creation of professional scientific reports  United Nations  OneOcean Achieving Sustainability Through Sanctuaries  Event cosponsored by the IOCS  Required expertise in Ocean Sanctuaries and international etiquette  Served as a liaison responsible for scientific support to attending Ambassadors Consuls and political leaders of various countries           University Of Utah      Graduate Research Assistant   Provo     UT                   082010      072012     The duties performed as a member of the scientific team of the CARIACO Project Carbon Retention In A Colored Ocean were To inventory and set up equipment supplies and chemical reagents needed to participate in the biannual scientific cruises to the Cariaco Basin Venezuela  Spoken Spanish was required  Became proficient in the performance and troubleshooting of molecular analyses PCR qPCR FISH and cloning of Ammonium related microbial species Performed extensive data analyses of time series with specialized software Matlab Sigmaplot Trained supervised and evaluated the performance of undergraduate Laboratory Technicians and volunteers           Music And Arts      Instructor   Lafox     IL                   2010      052010     Undergraduate studies instructor BIO205Fundamentals of Scientific Inquiry in the Biological Sciences II  Oversaw a class of 24 students  The position required Excellent verbal and written skills Keeping students motivated and focused on their work Promote the development of laboratory skills and critical thinking Produced objective student evaluations and tools to improve student performance           Us Government Other Agencies And Independent Organizations      Research Assistant   Freeport     NY                   082007      072009     Volunteer research assistant for the Superior Council of Scientific Investigations  Institute of Marine Sciencewithin the Spanish government  Member of the team involved in the project AFORO which required Laboratory skills for fish dissection and otolith processing Digital image analysis Windows office expertise Data base update management and proofing           Seaworld Parks      Maitre Event Supervisor   Langhorne     PA                   082003      112008     High standing catering company  Promoted within two years from waitress to Maitre Event Supervisor  Requirements for the position were Leadership and ability to organize and supervise a team Time management and budget control Training of new employees and periodic performance reports on new and senior employees Excellent etiquette and decorum numerous events involved the Royal House and other special VIPs Costumer care and fidelity Capability of rapid thinking and troubleshooting           Barcelona Zoo      Directors Assistant  Department of Mammals   City     STATE                   082001      082003     Internship as the assistant to the Director of Mammals  The duties and skills required for the position were To perform behavioral studies on the Zoos mammals assessing integration with other members of the exhibit abnormal behavior and enrichment needsresults To participate in the animal exchangebreeding program with other Zoos and Aquariums which consisted of selecting candidates to be exchanged as well as assessing the needs of new specimens into the mammal collection To oversee the proper status of the enclosures and animal care based on the Zoos standards To directly interact with the zookeeper and assess their performance To assist in the handling of mammals for veterinary and tagging procedures To manage and update the mammal database and consolidate information with the veterinary services on site           Barcelona Zoo      Educator   City     STATE                   051999      082001     Internship in the Zoos Department of Education  The duties and skills required for this position were To perform guided tours in the zoo for groups of kids and teenagers PreK through Highschool To oversee specialized summer courses in diverse topics eg  marine mammals reptiles endangered species which included handling of animals and controlled interaction of the students with animals To improve and create didactic materials for the courses To oversee and participate on Birthday Events in the Zoo To oversee and participate on summer camps in the Zoo          Work History      Affiliations              Languages     Spanish  Native or bilingual proficiency  English  Full professional proficiency  Catalan  Native or bilingual proficiency  Japanese  Elementary proficiency  Swahili  Elementary proficiency         Skills     Photo budget bi concept Council creative writing critical thinking Data base database English fundraising Genetics government image instructor inventory Japanese Leadership Director marketing materials Matlab exchange Windows office natural resources Organizer PCR proofing Research Safety Scientific Spanish Supervisor surveys Time management troubleshooting water quality written skills writing skills</data><data key="id">121742314592502610666900984044045933035</data></node>
<node id="n2042" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/supervisor-and-research-scientist-612b3e04321c4b41921071e61edc9594</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional Overview      To secure a rewarding research position in the pharmaceutical industry Cell and molecular biologist with 15 years of experience in cardiovascular research Organized complex studies involving multiple institutes and facilitating external collaborations Built and l ed successful research teams with established solid procedure and protocols for entire study group Prepared funded federal grant applications published manuscripts in high profile journals  and presented vaClaireable results in prestigious conferences such as AHA  ADA and ACC  scientific session             Core Qualifications         Excellent communication skills and expertise in clinical data organization multiple institute study  management human sample processing lipoprotein chemical and structural analyses  Expertise in chromatography FPLC HPLC protein affinity MSC stem cell isolation flow cytometry cell culture and transfection exosome purification animal processes and histological analyses  Skilled in various molecular techniques incClaireding DNARNA isolation mutagenesis plasmid construction RTPCR DNA sequencing analyses SouthernNorthernWestern blot gene expression profiling                         Education       University of Houston at Clear Lake    Houston     TX      Expected in   1999                    MS        Biology          GPA           Biology         Hunan Medical University    Changsha     Hunan      Expected in   1994                    MD                  GPA                     Accomplishments       Coordinated collaboration and communications between 5 national and international laboratories and institutions set strategy and adjusted priorities to meet deadlines  Provided training to 2 postdoctoral fellows and 5 internssummer students  Taught undergraduate Biochemistry lab course University of Houston at Clear Lake Houston         Experience       International Association For Great Lakes Research      Supervisor and Research Scientist   Cleveland     OH                   012009      Present     Supervising a cardiovascular research laboratory with 8 members  Exploring lipoprotein metabolism in STEMI pulmonary hypertension and type II diabetes  Independently developing research projects and designing experimental protocols  Utilizing flow cytometry FPLCHPLC chromatography cell assays MSC stem cell isolation exosome purification animal processes and histological analyses           Baylor College Of Medicine      Research Associate and Laboratory Manager   City     STATE                   012005      012009     Investigated the roles of electronegative lowdensity lipoprotein LDL in atherosclerosis by analyzing its chemical components and signaling pathways in human endothelial and smooth muscle cells  Developed novel approaches in mass spectrometry and nextgeneration sequencing for chemical analyses  Established nanoparticle labeling for in vitro studies  Efficiently managed studies involving multiplegroup collaborations clinical data collection and analyses and trained research staff and students           Baylor College Of Medicine      Postdoctoral Associate   City     STATE                   012001      012005     Conducted research on electronegative LDL and its roles in atherosclerosis development  Established standard laboratory procedures for patient sample processing ultracentrifugation chromatography FPLC HPLC protein affinity proteinlipid analyses and cell culturesorting  Management and Mentoring Experience  Laboratory Manager at the Texas Heart Institute efficiently managed studies to timelines and deliverables served as team leader in study design protocol development execution and data analysis          Work History       Texas Heart Institute     Scientist   Houston     TX    062009      112015            Baylor College of Medicine     Research Associate   Houston     TX    062005      062009           Skills     cell culture data analysis data collection designing diabetes HPLC laboratory procedures team leader Mentoring MSC next novel processes protocols research sorting Supervising type II       Publications      1Ke LY Chan HC Chen CC Claire J Marathe GK Chu CS Chan HC Wang CY Tung YC McIntyre TM Yen JH Chen CH Enhanced sphingomyelinase activity in apolipoprotein B100 contributes to the atherogenicity of electronegative LDL J Med Chem 2015 in press   2Stancel N Chen CC Ke LY Chu CS Claire J Sawamura T and Chen CH Interplay Between CRP Atherogenic LDL and LOX1 and Its Potential Role in the Pathogenesis of Atherosclerosis Clinical Chemistry 2015 Nov 25 PMID 26607724  3Hsu JF Chou TC Claire J Chen SH Chen FY Chen CC Chen JL Elayda M Ballantyne CM Shayani S Chen CH Lowdensity lipoprotein electronegativity is a novel cardiometabolic risk factor PLoS One 2014 99e107340  4Claire YC Chen CN Chu CY Claire J Wang BJ Chen CH Huang MC Lin TH Pan CC Chen SS Hsu WM Liao YF Wu PY Hsia HY Chang CC Lee H Calreticulin activates 1 integrin via fucosylation by fucosyltransferase 1 in J82 human bladder cancer cells Biochem J 2014 46016978   5Chan HC Ke LY Chu CS Lee AS Shen MY Cruz MA Hsu JF Cheng KH Chan HB Claire J Lai WT Sawamura T Sheu SH Yen JH Chen CH Highly electronegative LDL from patients with STelevation myocardial infarction triggers platelet activation and aggregation Blood 2013 12222363241   6Chu CS Wang YC Walton B Yilmaz HR Huang RY Sawamura T Claire LS Chen CH Claire J Electronegative lowdensity lipoprotein increases endothelial expression of Creactive protein through LOX1 receptor PLoS One 2013 88e70533   7Chang PY Chen YJ Chang FH Claire J Huang WH Yang TC Lee YT Chang SF Claire SC Chen CH Aspirin protects human coronary artery endothelial cells against atherogenic electronegative LDL via an epigenetic mechanism A novel cytoprotective role of aspirin in acute myocardial infarction Cardiovasc Res 2013 99113745   8Tsai MH Chang CL Yu YS Lin TY Chong CP Yang JY Lin YS Su MY Shu TY Claire J Chen CH Liu MY Chemical analysis of Creactive protein synthesized by human aortic endothelial cells under oxidative stress Anal Chem 2012 8496469654   9Chen CH Claire J Chen SH Huang RY Yilmaz RH Dong JW Elayda MA Dixon RA Yang CY Effects of Electronegative VLDL on Endothelium Damage in Metabolic Syndrome Diabetes Care 2012 35364853  10Ke LY Engler DA Claire J Matsunami RK Chan HC Wang GJ Yang CY Chang JG Chen CH Chemical composition oriented receptor selectivity of L5 a naturally occurring atherogenic lowdensity lipoprotein Pure Appl Chem 2011 831731 1740 doi101351PACCON101207  11Claire J Yang JH Burns AR Chen HH Tang D Walterscheid JP Suzuki S Yang CY Sawamura T Chen CH Mediation of electronegative LDL signaling by LOX1 A possible mechanism of endothelial apoptosis Circ Res 2009 104619627  12Claire J Jiang W Yang JH Chang PY Walterscheid JP Chen HH Marcelli M Tang D Lee YT Liao WSL Yang CY Chen CH Electronegative LDL impairs vascular endothelial cell integrity in diabetes by disrupting FGF2 autoregulation Diabetes 2008 57158166          Presentations      1Ke LY† Chan HC Chen CC Claire J Marathe GK Chan HC Wang CY Tung YC McIntyre TM Yen JH Chen CH OGlycosylation of ApoB100 is associated with enhanced sphingomyelinase activity and atherogenicity of human electronegative LDL Presented at the American Heart Association Scientific Sessions in Orlando FL November 7 11 2015   2Hsu JF Chou TC Hsieh JY Claire J Chen SH Shayani S Chen CH A novel total lipoprotein electronegativity index for predicting cardiometabolic risk Submitted to the American Diabetes Association 73rd Scientific Sessions in Chicago Illinois June 2125 2013  3Wang YC Claire LS Dong JW Claire J Lee AS Chen SH Dou H Kuzan TY Chang KC Dixon RA Chen CH The DNA Damage Response Mediates Endothelial Cell Senescence Induced by Electronegative LDL Circulation 2012126A16796 Presented at the American Heart Association Scientific Sessions in Los Angeles CA November 3 7 2012  4Chang KC Wang YC Chang SS Lo PH Claire J Sawamura T Lee YT Burns AR Chen CH Expression of the Highly Electronegative LDL Receptor LOX1 is Increased in Thrombi of STEMI Patients Circulation 2012126A13127 Presented at the American Heart Association Scientific Sessions in Los Angeles CA November 3 7 2012  5Wang YC Claire J Chen SH Dong JW Huang RY Dixon RAF Elayda MA Chen CH Electronegative lowdensity lipoprotein is increased early in cardiometabolic derangement Published for the American Diabetes Association 72nd Scientific Sessions in Philadelphia Pennsylvania June 812 2012  6Dong JW Yilmaz HR Claire J Claire LS Chen SH Wang YC Dixon RAF Chen CH Mitochondrial damage by electronegative lowdensity lipoprotein in metabolic syndrome Published for the American Diabetes Association 72nd Scientific Sessions in Philadelphia Pennsylvania June 812 2012  7Claire J Chu CS Wang YC Dong JW Chen SH Huang RY Chen CH Electronegative LowDensity Lipoprotein Increases Endothelial Expression of CReactive Protein Through LOX1 Receptor Presented at the 61st American College of Cardiology’s ACCs Scientific Session in Chicago IL March 2012  8Chen CH Claire J Dean J Huang RY Wang YC Elayda M Dong JW Chen SH Dixon RA Mechanistic Implication of Electronegative LowDensity Lipoprotein Surge in Acute Myocardial Infarction Presented at the 61st American College of Cardiology’s ACCs Scientific Session in Chicago IL March 2012     9Ma Y Claire J Cheng N Wu G Abbasi S Cheng J Chen CH Xi Y L5 induces cardiomyocytes damage and reduction of cardiac ATPsensitive K Channels J Am Coll Cardiol 201259E1055 Presented at the 61st American College of Cardiology Scientific Sessions in Chicago IL March 24 27 2012  10Claire J Chu CS Yilmaz HR Dong JW Chen SH Huang RY Sawamura T Chen CH Atorvastatin Decreases Atherogenic LowDensity Lipoprotein Which Induces the Endothelial Expression of CReactive Protein Accepted by the American Heart Association Basic Cardiovascular Sciences 2011 Scientific Sessions July 2326 New Orleans LA   11Chen CH Claire J Engler DA Ke LY Yilmaz HR Dixon RA Chen SH Yang CY Biological Significance of ChargeBased Heterogeneity in Diabetic VLDL Subfractions Presented at the 71st American Diabetes Association Scientific Sessions in San Diego CA June 24  28 2011  12Cheng N Claire J Sun JP Wu GR Abbasi S Zhang J Cheng J Chen CH Xi YT Most Negatively Charged Subfraction L5 of Plasma LDL Prolongs Action Potential Duration of Rat Cardiomyocytes via Lox1 Receptors Presented at the 60th American College of Cardiology Scientific Sessions in New Orleans LA April 25 2011  13Claire J Wang GJ Tung CH Chan HC Han MS Ke LY Poucher SM Chen CH Electronegative LDL in cardiometabolic disorders unleashes converting enzymeregulated endothelin1 synthesis normally modulated by Akt in vascular endothelial cells Presented at the 4th International Congress on Prediabetes and the Metabolic Syndrome Madrid Spain April 69 2011  14Dixon RA Claire J Chen CH Negatively charged LDL subfraction contributes to both atherosclerosis and hypertension in type 2 diabetes Presented at The 3rd World Congress on Controversies to Consensus in Diabetes Obesity and Hypertension CODHy Prague Czech Republic May 1316 2010  15Tang D Burns AR Claire J Chen HH Wu H Morrisette JL Sawamura T Yang CY Chen CH Electronegative LDL disrupts mitochondrial homeostasis a novel mechanism for cigarette smokingassociated endothelial dysfunction Presented at the FASEB 2008 meeting San Diego CA April 5 9 2008  16Tang D Chen HH Claire J Engler DA Sawamura T Pownall HJ Yang CY Chen CH Chronic smokingyielded electronegative LDL induces MEKERK mediated metalloproteinase activation in human endothelial progenitor cells J Am Coll Cardiol 200851suppl AA278 Presented at the American College of Cardiology 2008 Scientific Sessions Chicago IL March 29 April 2008  17Tang D Claire J Walterscheid JP Chen HH Yang JH Engler DA Sawamura T Marcelli M Yang CY Chen CH Electronegative LDL in smoking subjects inhibits endothelial progenitor cell differentiation by impairing Akt phosphorylation via the LOX1 receptor Arterio Thromb Vasc Biol 2007 Presented at the ATVB Annual Conference April 1921 2007 Chicago IL   18Claire J Walterscheid JP Chen HH Yang CY Poucher SM Chen CH Diabetic LDL subfraction induces smooth muscle cell proliferation by enhancing endothelial release of endothelin1 Diabetes 200655 suppl 1A160 Presented at the American Diabetes Association 66th Scientific Sessions in Washington DC June 9 13 2006   19Claire J Chen HH Yang CY Waltersheid JP Pownall HJ Morrisett JD Chen CH C4 Plateletactivating factor mimics electronegative LDL circulating in patients with metabolic syndrome in inducing endothelial cell apoptosis Diabetes 200554suppl 1A193 Presented at the American Diabetes Association 65th Scientific Sessions in San Diego CA June 10 14 2005   20Claire J Yang JH Chen HH Yang CY Henry PD Walterscheid JP Morrisett JD Sawamura T Chen CH Effective attenuation of electronegative LDLinduced endothelial cell apoptosis by gene silencing of the lectinlike receptor for oxidized LDL J Am Coll Cardiol 2005 Presented at the 54th American College of Cardiology Scientific Sessions in Orlando FL March 6 9 2005        21Tai MH Kuo SM Huang MT Yang JH Chen HH Claire J Sawamura T Chen CH Yang CY Activation of caspase 13 by diabetic electronegative LDL through lectinlike oxidized LDL receptor1 LOX1 in vascular endothelial cells Circulation 2005112II1105 Presented at the American Heart Association 2005 Scientific Sessions Dallas TX November 1116 2005   22Claire J Marcelli M Liao WSL Yang JH Henry PD Yang CY Pownall HJ Chen CH Dominantnegative Akt impairs endothelial cell survival by mimicking the inhibitory effect of electronegative LDL on FGF2 transcription Circulation 2004110III87 Presented at the American Heart Association 2004 Scientific Sessions New Orleans LA November 710 2004  23Chen CH Pace PW Karakoc ND Claire J Chen HH Henry PD Pownall HJ Foreyt JP Ballantyne CM Yang CY Effective reduction of novel atherogenic LDL subfraction by atorvastatin in patients with hypercholesterolemia J Am Coll Cardiol 200443suppl A486A Presented at the 53rd American College of Cardiology Scientific Sessions in New Orleans LA March 7 10 2004  24Claire J Chen HH Suzuki S Yang CY Henry PD Sawamura T Chen CH Role of the LOX1 receptor in transducing the apoptotic signals of circulating atherogenic LDL J Am Coll Cardiol 200443suppl A26A  Presented at the 53rd American College of Cardiology Scientific Sessions in New Orleans LA March 7 10 2004  25Jiang W Claire J Yang JH Chang PY Lee YT Marcelli M Henry PD Liao WSL Chen CH Atherogenic LDL impairs vascular endothelial cell survival by disrupting the FGF2PI3KAkt autoregulatory loop J Am Coll Cardiol 200443suppl A499A  Presented at the 53rd American College of Cardiology Scientific Sessions in New Orleans LA March 7 10 2004  26Claire J Marathe GK Yang CY Jiang W Yang JH Pownall HJ Henry PD Chen CH Plateletactivating factorlike lipids mediate endothelial cell apoptosis induced by hypercholesterolemic LDL  J Am Coll Cardiol 200341Supple243A Presented at the 52nd American College of Cardiology Scientific Sessions in Chicago IL March 30 April 2 2003  27Chen CH Jiang T Yang JH Jiang W Pownall HJ Ballantyne CM Claire J Henry PD Yang CY Induction of vascular endothelial cell apoptosis through the plateletactivating factor receptor in hypercholesterolemia Atherosclerosis 2002388 Presented at the 73rd European Atherosclerosis Society Congress Salzburg Austria July 710 2002  28Claire J Suzuki S Safi HJ Jiang W Henry PD Yang JH Morrisett JD Chen CH  OxLDL LDLinduced vascular endothelial cell apoptosis involves ceramidemediated Akt dephosphorylation protective role of FGF2 overexpression Atherosclerosis 2002388 Presented at the 73rd European Atherosclerosis Society Congress Salzburg Austria July 710 2002  29Jiang W Claire J Chang PY Yang JH Henry PD Marcelli M Chen CH Oxidized LDL induces endothelial cell apoptosis by impairing the FGF2PI3KAkt pathway Circulation 2002105e86e119 Presented at the American Heart AssociationAsia Pacific Scientific Forum HonoClaireClaire Hawaii April 2326 2002  30Claire J Suzuki S Safi HJ Jiang W Henry PD Yang CY Chen CH FGF2 overexpression prevents ceramidemediated Akt deactivation in endothelial cells exposed to oxidized LDL Arterioscler Thromb Vasc Biol 200222878a Presented at the American Heart Association3rd Annual ATVB council meeting Salt Lake City Utah April 68 2002  31Chen CH Yang JH Morrisett JD Claire J Jiang T Henry PD Yang CY Proapoptotic activity of a novel LDL subfraction in hypercholesterolemic human plasma Circulation 2001104II297   Presented at AHA 2001 Scientific Sessions 2001 of the American Heart Association Anaheim CA November 1114 2001        MembershipsScholarly Societies      •American Diabetes Association  •American Heart Association Atherosclerosis Council  •International Atherosclerosis Society</data><data key="id">163640270838014906443783861563387895774</data></node>
<node id="n2043" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/principal-scientist-and-principal-scientist-ii-618c4127005349bc9f3e3f486fc320f1</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary      Experienced Industry professional with 15 years of expertise in Immunoassay IA Clinical Chemistry CC Molecular Core lab invitro diagnostics IVD development and product launches in US and Worldwide WW        Skills           Results driven servant leader with the below skill sets    Technical skills   Designdeveloplaunch Qualitative  Quantitative diagnostic assays  Application of advanced Scientific Principles and knowledge  Chemistry  Biochemistry Immunology molecular Immunology  Experienced handling broad spectrum of health conditions from infectious disease allergy autoimmunebioterrorism agents to cancers  Clinical laboratory training and working with patients in outpatient clinics IRB regulations and ICH guidelines  Plan Design execute IVD Verification and ValidationClinical studies studies using CLSI guidelines based on international regulations for CE mark IVDD IVDR FDA 510KPMA CFDAPMDA and other regulatory agencies  Data CollectionAnalysis trouble shooting and ProblemSolving   Provide strategic initiatives and alternate cost effective solutions for trouble shooting technical issues  Analytical Thinking and Technical Leadership  Plan Design and execute fundamental research  Establish design specifications Manufacturing Processes and documentation DMRS DHFS Risk management control plans and process and design FMEAS   Product development with Customer centric focus   Customer centric product development and translating customer medical and regulatory requirements into Product and process requirements  Technical expertise in bringing study results and claims to Package insert and trouble shooting guides Technical Bulletins and Product information letters to customers  Support global commercialTechnical Operations and Service with on market field complaints and escalations to address immediate actions to help resolve customer issues   Quality and Regulatory guidelines implementation   Quality by design and quality by validation for maintaining quality systems  Implementation of quality system  Polices operating procedures and forms  Design plan and Quality system implementation and documentation  Working Knowledge of US and International regulations and quality standards affecting IVDs  Drafting and executing Design Verification and Clinical studies based on CLSI and regulatory and compliance agencybased guidelines including FDA JPN PMDA China CFDA Indian RegulatoryExperienced in Designing and developing clinical studies in applying the applicable sections of regulationsStandards guidelines for medical devices to ensure protection of human subjects and specimen and data integrity  Knowledgeable in establishing design specifications Manufacturing Processes and documentation DMRS DHFS Risk management control plans and process and design FMEAS      International Product master files chapters and 510 K chapters for regulatory submissions Obtaining and collecting the necessary documentation designing the strategy for regulatory submission purposes and preparing Regulatory chapters for CE IVDD and IVDR letters to concerned FDA authorities  prepare and present quality milestone Deliverables   Written and communication skills   verificationvalidation protocols drafting  results and record documentation  Publication DevelopmentPaper Proof reading editorial review and Report Writing  Written and Verbal Communication  Adept at communication and presentation to all levels of management  Skilled in messaging elevations to the Elevation boards and DVPS while bringing forward alternative solutions for resources and budgets consolidate and rationalize budget needs  Identify and present needs for capital facilities and product and process improvements   Project and Product Management    SelfDriven and Motivated  Effective Multitasking  Time Management  Project Management and industry stage gating PDP process  Holding stage gate reviews verification  validation and Product Launch reviews  Product Life cycle Management from conceptualization through commercialization  Maintaining inventory and logistics  SOP SOWEntity and LRP budget development resource management  Budgeting and financial planning using business metrics and analytics for decisions   Mentoring and teaching skills    Staff Training and mentoring  Elementary  middle  highschool undergrad and medicine Student mentoring and training to build passion in STEM  Effective team player with multiple global crossfunctional teams  Interpersonal and cross functional leadership skills and ability to identify and achieve scientific collaborations with academic industrial research groups and Contract research organizations to complete time sensitive projects                       Experience       Principal Scientist and Principal Scientist II        062015   to   Current     Abbott Diagnostics Abbott Labs         City     STATE            Designed Developed and Launched  Qualitative Quantitative and semiquantitative IA and CC assays   Technical lead PARVO B19 Ag and Ab assay development for Japanese red cross JRC JPN PMDA PARVO B19 Antigen assay Launched March 2018 for JRC in JAPAN on Abbotts Architect platform  CA 724  Cancer marker 724 for quantitative detection and aid in monitoring of Gastrointestinal and Ovarian cancer patients IVDDIVDR compliant assay launched worldwide Jan 2021 on Abbotts Architect and Alinity platforms  TRAb Thyroid receptor Autoantibody to aid in differential diagnosis of Graves’ disease IVDDIVDR worldwide launch July 2021 launched on Abbotts Architect and Alinity platforms  HAVAb IgG US FDA 510K submission SeptOct 2022 on Abbotts Alinity platform  Worked efficiently with Abbott global crossfunctional RD and Ops teams in US WIE Japan Longford Sligo sites overcoming time and cultural barriers and with TPMS including Denka bringing assays from bench to on market Launched 5 IVDS worldwide in a span of 7 years at ADD and ongoing efforts to launch HAVAb IgG within US           Scientist and Senior Scientist and Safety Officer       092008   to   012014     Arryx Division Of HAEMONETICS         City     STATE              Assay development and optimization Instrument Prototype design skills  Developed and patented Immunodiagnostics for blood typing and screening A novel cuttingedge technology used for ABO blood typing and unexpected antibody screening and identification leading to WW patents and development of next generation diagnostic instrumentation  Independently conceived designed and developed the feasibility of assay development and manufacturing for multiple projects  Identified purified characterized and optimized proteins and diluents and assay conditions for developing the blood typing and screening diagnostics Optimized product design processes and procedures and setting specifications for scaling up and manufacturing   Mentoring and Communication skills  Managed and supervised technical team presented complex technical findings and conclusions   to internal and external audiences staying current with scientific and industry literature competitive technologies and products promoting internal and external collaborations   Led and mentored several technicians with SOP and protocols drafting and execution and helped build the design and verification and validation methods assay performance on prototype instrument  Developed inhouse SOP’s and trained junior scientists and technicians to store plasma red blood cells and screen and identify the unexpected antibodies in plasma Built a plasma library of rare plasmas procuring from various blood banks across the country by networking   Specimen procurement and collection and Inventory Management Proactively built a network with blood banks across the United States to procure rare plasmas and red cells for building an inhouse plasma and cells library used to develop diagnostic tests Built a strong network with hospitals in order to perform large scale double blinded preclinical trials  Procured human donor blood samples from blood banks to conduct preclinical trials manually in house to assess the test performance Performed statistical analysis and analyzed complex data critically by applying advanced computer skills and identifying the source of variables in the test   IP and Patent identification Skills  involved in identifying the patentable aspects of the technology and bringing scientific discovery to IP team Partnered with the legal department in drafting the patent Highlighted the uniqueness of the test developed compared to the commercially available tests to strengthen the relevant claims performed extensive literature and database searches to exclude any infringements  Quality and regulatory internal and external standards development and Lab safety implementation skills  Initiated and implemented QC by design and validation principles and also cGMP processes during bulk manufacturing of disposable immunodiagnostic Established Communication with FDA for requirements for submission  Led trouble shooting activities and identified the key components responsible for the quality control and repeatability and reliability of the test  Served as the Safety Officer required to maintain OSHA standards at workplace performed safety and security training for blood borne pathogens and biohazard materials specimen handling workplace injuries filing and biohazard and chemical hazardous materials proper handling and disposal   Clinical studies and External collaborations with Customers and Clinical laboratories Performed customer surveys with blood banks for obtaining customers’ requirements Northwestern and Uni of Chicago and Mt Sinai hospitals and physicians for value added understanding not only from scientific perspective but also from customer need for improvising the product Identified proper sample volumes for blood testing applying real world use cases in order to design relevant tests  Conducted external COE and Clinical studies in clinical laboratories to assess the existing limits of detection on commercially available diagnostic platforms Identified the existing limits of detection to then translate the product requirements and establish the inhouse diagnostic   Budget and Finance decision making Skills Managed an annual budget of 250000 for instrumentation involved in budget and resource planning and cost effectiveness and execution and people recruitment and LRP  Actively involved in recruitment of technicians and scientists           Associate and Assistant Scientist       062002   to   082008     Oklahoma Medical Research Foundation         City     STATE            Developed scientific questioning troubleshooting grant and scientific writing communication skills as an independent contributor while Managing Multiple projects at OMRF  OUHSC Bioterrorism consortium as a postdoctoral trainee   Project 1 Molecular mechanisms involved in CD4CD8 T cell differentiation   o Identified and published novel genes and potential Transcriptional regulators in CD4CD8 T cells lineage by differential gene expressions and studying their functional significance in the lineage decisions by dissecting the signaling pathways utilizing microarray Real time PCR PCR and flow cytometry techniques  ·  Project 2  Role of lef1lymphoid enhancer 1 in CD4CD8 T cell differentiation  Identified and Cloned genes in lck driven expression lenti viral vectors in DP cells to study role of growth factors in transcription of Hematopoietic stem cell to CD4CD8 T cell lineage choice in Egr KO mice  ·  Project 3  Mechanisms of immune tolerance Molecular basis of T cell Anergy   o Identified and published novel signal transduction pathways essential in induction and maintenance of anergy in T cells Studied Balbc and transgenic mice with Staphylococcal enterotoxins induced anergy model Functional significance of the differential gene display was studied using microarray real time PCR FISH flow cytometry of surface biomarkers This was the first study to establish the potential role of Notch and Wnt members in T cell tolerance  ·  Project 4 Identification of sequential B cell epitopes of recombinant Anthrax Lethal Factor LF and protective antigen PA   o Identified and published potential vaccine targets for Anthrax a bioterrorism agent Identified novel B cell epitopes and studied the efficacy of the recombinant vaccine with both the LF and PA antigens in protection against anthrax          PhD Immunology       011997   to   052004     All India Institute Of Medical SciencesAIIMS         City     STATE            Successfully managed multiple projects with clinical laboratories patients and AIIMS Hospitals and worked as teaching and lab assistant to MDMS and Medicine undergrad students and mentored Junior scientists  ·  Project 1  Expression of CoStimulatory Molecules in Peripheral Blood Derived Monocyte Macrophage and T Cells from Patients Infected with Mycobacterium Leprae  Identified and Published Tcell stimulatory and immunomodulatory molecules with cytokine profiles and surface biomarker fluorescent evaluation studies in Leprosy patients utilizing isolated PBMCs T cell and B cell and cytokine assays  ·  Project 2 34 Identified and published efficacious vaccine and Diagnostic candidate peptides and proteins for infectious diseases HIV Dengue Malaria and Plague by utilizing B and T cell solid phase epitope mapping techniques in collaboration         Education and Training       Product Management Certification      Product Management      Expected in   122022     Kelloggs School Northwestern University        Chicago IL          GPA                Certificate in Clinical And Regulatory Affairs     Clinical and Regulatory Affairs      Expected in   032014     NUCATS Northwestern University      Evanston IL          GPA                Grants Writing And Management Training     Medical Writing And Communication      Expected in   082006     Oklahoma State University      Oklahoma City OK          GPA                PhD     Immunology     Expected in   052004     ALL INDIA INSTITUTE of MEDICAL SCIENCES      NEW DELHI  INDIA           GPA                Diploma Systems Management      Computer And Information Sciences     Expected in   121996     APTECH      Visakhapatnam India           GPA                Master of Science     Biochemistry     Expected in   091996     Andhra University      Visakhapatnam INDIA           GPA               Languages       English            Full Professional      Negotiated                      Telugu            Native Bilingual      Negotiated                      Hindi            Full Professional      Negotiated                      French            Limited      Negotiated                      Tamil            Limited      Negotiated                     Accomplishments       Received 15 Excellence awards from Abbott Senior Management CFT and Abbott President for the technical leadership skills and driving projects successfully to Launching PARVO TRAB CA724 assays and reaching milestones and meeting business needs  2015current  Received Service recognition awards for developing and mentoring Women in STEM and Children and high School University and preMedicine and Medicine students of IPSD 204 203 in Stem Careers from Nonprofit organizations TTA 2015 current  Received Abbott Presidents Award for developing and launching PARVO B19 assay for Japanese Red cross JRC 2018  OCAST fellowship for postdoctoral training Oklahoma Center for advanced Science and Technology 20022006  Received regional science award in appreciation for the research on leprosy patients in the Asian Pacific Congress for Clinical Biochemistry 2002  Selected as best paper of the year contest for Prof GP Talwar’s Young Scientist Award at the Indian Immunology Society meeting 2002  Awarded CSIR’s Travel grant for young scientists to attend the 11th International Immunology Congress in Stockholm Sweden  Invited speaker and Best paper award in Asia Pacific Leprosy Congress 2000  Qualified National Eligibility test conducted by CSIR the Council of industrial and Scientific Research CSIR a government funded institution for the award of JRF and lectureship Scholarship for a period of five years 19972002  Scored in the 97th Percentile on the GATE examination conducted by Indian Institute of Technology to admit students to MSPhD Program 1997         Additional Information       Authored 5 US and WW patents and 14 international Publications of repute can be found online in Pubmed and Google scholar or orcid 0000000298348720   Note When searching PubMed the publications appear with   Jessica K    Claire S    or    Jessica Claire    as the author</data><data key="id">121412180689443066002785156788212266991</data></node>
<node id="n2044" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/sr-clinical-research-analyst-scientist-1-5810e713858e4ae489081f456cd53d17</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Experience       Sr Clinical Research Analyst  Scientist 1       022018   to   Present     Alorica Inc         Humble     TX             Developed and implemented strategies to improve access to quality care emphasizing individual and family engagement adolescent transition to adult life and medical home care  Secured 2M grant for project to reduce readmission rates for patients with sickle cell disease after initial pilot recorded savings of 300000 in one year designed and executed strategy exceeding Emergency Utilization and Length of Stay project goals by 25 and 35 respectively Managed clinical operations for all clinical trials within division Developed realtime tracking dashboard to improve patient tracking and behavior notifications driving increased resource allocation and improved patient care and dramatically simplifying patient identification and screening for clinical research projects Identified need for and proposed concept of virtual pagers for clinical and nonclinical staff use adopted divisionwide eliminating communication blocks from network coverage lapses Wrote and maintained necessary Claires study binders and personnel training records that are required to be in compliance with GCP Helped develop and implement specifications Claires and plans for electronic data capture data management and statistical analysis of clinical data that meet GCP and project requirements Introduced structure to stagnated project by diagramming and providing training on all team members responsibilities resulted in improved workflow and newfound clarity around roles expectations and communication Helped qualify initiate and audit external CROs conducting clinical studies to ensure they meet GCP and company requirements Worked with partners to resolve issues and escalate issues that could impact study timing and quality Mentored student research analysts on data analytics data quality research methods and good clinical practice to such success that position was reclassified to include personnel management coaching and training of 15 research assistant students 2 social work students and multiple volunteers Created website designed to advertise and promote all new and ongoing clinical studies maximizing reach to new patients and growing participant enrollment by 655 within one year built additional online data collection tools to collect patient surveys and other electronic data sharply reducing rate of duplication data loss and input and transcription errors and increasing data quality and ease of access           Data Manager  Research Analyst       022014   to   022018     Labcorp         Lakewood     CO             Reviewed and processed data analysis requests to identify opportunities for quality of care improvement utilization reduction and healthcare costs reduction  Analyzed and reported on provider client and managed care trends in healthcare market provided quantitative analysis for NIH grant funded studies Designed tested and documented algorithm database and software requirements per research investigators specifications structured and executed endtoend analysis for internal and client consumption Supported Human Participant Protections Program by maintaining current knowledge of federal and state regulations researchmedical ethics and IRB policies and procedures conducted prereview analysis of IRB submissions to ensure applications were adequately prepared Monitored ongoing clinical studies including monitoring data collection patient sample usage and testing supplies usage Set up the GCP infrastructure necessary to conduct simple inhouse clinical studies such as method comparison and precision Helped develop and execute procedures to close clinical studies and reconcile clinical samples supplies and data Worked with external vendor and project team to configure test and implement eDC system as per specifications           Remote Automation Specialist       062017   to   032018     DJO Global         City     STATE             Managed customerfacing help desk within MotionMD system for Office Care and Direct customers  Reviewed analyzed and validated datareports to ensure quality of delivery to internal and external clients located and corrected data problems identified and interpreted trends and patterns in complex data sets  Oversaw Help Desk site content and metrics while maintaining top levels of system security and data integrity HIPAA HiTrust  Supported management by prioritizing businessinformation needs defined and implemented new process improvement opportunities           Help Desk Analyst       082014   to   062017     The Yeomen         City     STATE             Applied innovative interface development projects to databases alongside biomedical and software engineers supported teams of client and personnel in project development and management  Documented and demonstrated solutions and best practices by developing original protocols flowcharts and FAQs performed software analysis and automation review for relevant modules Provided remote technical support and troubleshooting in English and French Investigated and responded to both internal and external inquiries regarding access Cloud products verifications and complaints Provided regular communications updates and presentations on impacts and outcomes to CEO Marketing partners and our Service Provider Sales leads           Reference Test Clerk       042013   to   102014     Laboratory Corporation Of America         City     STATE             Applied innovative interface development projects to databases alongside biomedical and software engineers supported teams of client and personnel in project development and management  Documented and demonstrated solutions and best practices by developing original protocols flowcharts and FAQs performed software analysis and automation review for relevant modules Provided remote technical support and troubleshooting in English and French Investigated and responded to both internal and external inquiries regarding access Cloud products verifications and complaints Provided regular communications updates and presentations on impacts and outcomes to CEO Marketing partners and our Service Provider Sales leads           Radiology Facilitator       072008   to   052013     Medical Center Radiologists         City     STATE             Provided sidebyside assistance to radiologists interpreting medical images and physicians working with complex imagine and therapeutic techniques  Served as liaison between patient and referring radiology physician  Updated maintained and verified medical records throughout practice  Coordination of medical services to include care intake screenings and implementations of care plans to promote effective utilization of healthcare services          Education       Master of Sciences     Biomedical Engineering     Expected in   2016     Virginia Commonwealth University      Richmond     VA     GPA       Biomedical Engineering         Bachelor of Sciences     Biology     Expected in   2011     Old Dominion University      Norfolk     VA     GPA       Biology         Master of Sciences     Biomedical Engineering     Expected in   2008     Tidewater Community College      Virginia Beach     VA     GPA       Biomedical Engineering        Summary     Experienced multilingual scientist with thorough expertise in medical devices clinical research data management and software as a service SaaS Diverse background in healthcare services and higher education with special focus on business improvement  particularly in efficiency designing analyzing and executing projects which results driven outcomes       Certifications     Analytics                 Data Management                 Agile Technologies  Technical Computing                 GCP                 Engineering  Project Management                 Visual Modeling                 Clinical Research       Languages     Proficient in English and French languages       Skills     Agile automation Clinical Research clinical trials coaching concept content client clients data analysis data collection Data Management databases database delivery driving English French good clinical practice GCP Help Desk home care interface development market Marketing access Office Modeling network patient care personnel personnel management personnel training policies presentations process improvement Project Management project development and management protocols quality quantitative analysis radiology realtime research Sales software analysis statistical analysis strategy structured surveys technical support therapeutic techniques transcription troubleshooting website workflow       Additional Information       Certificates Human Subjects Biomedical Research Biomedical Responsible Conduct of Research Information Privacy Security Clinical Research Coordinator Good Clinical Practice Clinical Research Coordinator Nuts  Bolts Quality in Clinical Research 2</data><data key="id">102823340460798011888612308302318044710</data></node>
<node id="n2045" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional objectives      Challenging position as postdoctoral scientist  incorporating skills in immunological research drug design and development and immunotherapy            Core Qualifications         Immunological research and Cancer Immunotherapy  cell and molecular biology  cancer research tumor models in mice  recombinant protein Design expression and purification  work with tarnsgenic mice models managing a mouse colony      Data analysis including statistical analysis  project design optimization documentation   scientific writing  critical thinking and problem solving                     Education       RWTH Aachen university Faculty of Natural Science    Aachen     NRW      Expected in   2015                    PhD        Biotechnology Immunology          GPA            Status Thesis submitted awaiting supervising committee approval  Practical work of PhD research was conducted in the  Pharmaceutical Product Design group Fraunhofer IME Institute  in the ForSaTum funded project for the rapid development and in vivo testing of cancer targeted therapeutics           Technion Institute of Technology    Haifa           Expected in   2010                    Master of science        Immunology          GPA            Final grade 93100          Technion Institute of Technology    Haifa           Expected in   2005                    Bachelor of science        BiologyChemistry          GPA            Molecular Biochemistry program  Graduated with honors    cum laude           Certifications       Laboratory Animals Research training  Certification to work with laboratory animals from the Technion Institute of Technology and Stanford University   Flow Cytometry training  training to work with BD FACSCalibur™ and BD LSRFORTESSA    Confocal microscopy training  intensive course in confocal microscopy working on Leica TCS SP5 microscope          Additional Research Experience       University Of Utah      Visiting Scientist   Payson     UT                   062015      102015    Researching targeting skin Dendritic cells with therapeutic antibodies to induce tolerance towards allergens           Marmic Fire Safety      Internship   Joplin     MO                   2014      032014    investigated the role of different Immunological cell types in the induction of oral tolerance          Leidos Holdings Inc      Research Assistant   Conroe     TX                   2006      072007    Designed and purified synthetic Sphingolipids and investigated their therapeutic potential in various disease models         Teaching Experience       Biotechnology Department RWTH Aachen University       Teaching assistant and Laboratory Instructor   Aachen     Germany                             20112013          Fraunhofer Institute IME      Practical Supervisor   Aachen     Germany                             20122013          Medical Science Faculty Technion Institute of Technology      Teaching Assistant and laboratory Instructor   Haifa     Israel                             20082010         ThesisDissertation       PhD Thesis    EpCAM and CSPG4 scFv based cytolytic fusion proteins for the treatment of triple negative breast cancer   work focused on generating and optimizing the design expression system and purification of cancer Immunotherapeutics and characterizing their anti tumor activity in cell lines and  in vivo  in mouse xenograft models      Masters Thesis    IgG1B cell  receptor promotes development of B cells in the Bone Marrow and confers extended survival in the periphery of chimeric mouse model  work focused on characterizing and studying B cells signaling survival and functionality  ex vivo  and  in vivo  in chimeric mouse models         Fellowships and Awards       Keynote speaker PhD students annual seminar Fraunhofer Institute 2013  ForSaTum grant for Doctoral studies 20102013   Deans Honor for accomplishments in Bachelors studies 2004         Skills       Cell and Molecular Biology Techniques  Bacterial cultures human and mouse cell lines and primary cells Immunoassays Cloning PCR RTPCR Drug Evaluation assays Flow Cytometry Microscopy   Work with Laboratory Mice  tumor xenografts models EAE colitis model Immunological studies with transgenic and chimeric mice models  in vivo  live imaging   Recombinant Proteins  Design and expressionMammalian and Bacterial systems Protein and antibody Purification   Data Analysis and Documentation  statistical data analysis sequence analysis DNA and Protein Work in BSL2 and S2GMP lab documentation under the German Genetic Engineering Safety Regulations GenTSV   Computer Skills  Microsoft Office CLC workbench Gimp GraphPad PRISM software AIDA image analyzer FlowJo BD cell quest BD FACS Diva work with NCBI Immgen and UniProt databases        Publications      1 Hristodorov D  Claire M  et al EpCAMselective elimination of carcinoma cells by a novel MAPbased cytolytic fusion protein Mol Cancer Ther 2014 Sep1392194202 PubMed PMID 24980949   shared first authors  2 Reis BS Lee K Fanok MH Mascaraque C  Claire M  et al Leptin receptor signaling in T cells is required for Th17 differentiation J Immunol 2015 Jun 119411525360 PubMed PMID 25917102  3  Claire M  Blume T et al SNAPtag based agents for preclinical in vitro imaging in malignant diseases Curr Pharm Des 20131930542936 PubMed PMID 23431985  4 Hussain AF  Claire M  Barth S SNAPtag technology a powerful tool for site specific conjugation of therapeutic and imaging agents Curr Pharm Des 20131930543742 Review PubMed PMID 23431986    Submitted manuscripts not published yet   1  Jessica Claire  Stefan Barth et al Granzyme Bbased cytolytic fusion protein targeting EpCAM specifically kills triple negative breast cancer cells in vitro and inhibits tumor growth in a subcutaneous mouse tumor model Submitted to Cancer letters   2  Jessica Claire  Dirk Bauerschlag Ahmad Fawzi Hussain et al On demand activatable photoimmunotheranostics for triple negative breast cancer cells diagnosis and treatment    submitted to theranostics  3  Jessica Claire  Radoslav Mladenov Stefan Barth et al A novel approach for targeted elimination of CSPG4positive triplenegative breast cancer cells using a MAP taubased fusion protein Submitted to Int J of Cancer currently working on revisions</data><data key="id">50431172171274667187411572247833896397</data><data key="url">https://www.livecareer.com/resume-search/r/visiting-scientist-4e633fb9040a4f0cbbceff2dd0fce524</data></node>
<node id="n2046" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/staff-materials-scientist-547fbeb2853f4c18b58efec53966817a</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Profile     A senior research or application scientistengineer position to drive new material technology innovation or a new product development and efficiency for medical or industrial application Accomplished and focused professional with demonstrated experience of materials science and engineering solution for worldclass medical device manufacturer 10 years career track to combine exceptional technical knowledgeproficiency with unique capability to lead and manage a cross function team to initialize and drive cuttingedge technology development for peripheral intravenous catheter PIVC medical product for global market leader Work with cross functions and regional team members to synthesize and implement new technology to expand current market or exploringdeveloping new markets in aligning with key business objectives Project and program management skills Strong team builder and coach leading and steering the team to commit to project success Ability to handle multiprojects highly selfmotivated productive breakthrough thinking and team working Authorized to work in the United States Core Competency Catheter Tubing Material Development    Design and Development of Catheter Product          Catheter Tubing Processing Extrusion  Injection Molding          Process Scale upDevelopmentValidation          Six Sigma Quality Control Polymer Synthesis  Formulation          Material  Design for IV Administration Set          Needless Connector Plastic Bonding  Assembly          Biopolymers          ISO  ASTM Standard Thermoplastic Elastomer          StructureProcessingProperty Relationship          Design of Experiment Material Characterization          FDA Regulation  Medical Device Design Control          Statistical Data Mining Project PlanningManagement          Technology Roadmap Development          Voice of Customer Technology Landscape Assessment         Crossfunction Team BuildingStaff Management          Global Strategic Insight           Skills         Guest services  Inventory control procedures  Merchandising expertise      Loss prevention  Cash register operations  Product promotions                     Education and Training       the University of Akron    Akron     OH      Expected in   2006                    PhD        Polymer Engineering          GPA           Polymer Engineering         Beijing University of Chemical Technology    Beijing           Expected in   2000                    BS        Polymer Chemical Engineering          GPA           Polymer Chemical Engineering          Accomplishments       3  Claire Jessica Han ChangDae Synthesis and Characterization of a Combined MainChainSideChain LiquidCrystalline Polymer Exhibiting Both Thermotropic and Lyotropic Characteristics and Its Lyotropic Phase Behavior Macromolecules 2005 3823 96029609  4  Claire Jessica Han ChangDae Rheology of a Combined MainChainSideChain LiquidCrystalline Polymer in the Thermotropic and Lyotropic States Macromolecules 2006 391 232242  AFFILIATION Society of Plastic Engineering         Professional Experience       Formlabs      Staff Materials Scientist                           042011      Present     Inventor and Technology Leader of Design and Development of BD Nextgeneration Vialon Polyurethane Plastic Catheter Tubing Material for BD Peripheral Intravenous Catheter Product for Global PIVC Market  Actively look for new technology and business opportunities and consistently generate innovative concepts and submit invention disclosures  Led a technology development team to define technology roadmap conduct technology landscapeIP assessment project resource planning built a crossfunction team and project planning and management  Collaborated with crossfunctional team members and explored for new market opportunities and catheter tubing technologies for BD Infusion Therapy business helped preparing business case conducted technology and IP assessment supplier identification and qualification  Worked with business team to conduct multiplephase voice of customer study to understand the need of customer in various regions  Proposed new concepts and drove crossfunction new technology development to demonstrate the technology feasibility Polymer synthesis and polymerization scale up compounding grinding multilayer catheter tubing extrusion RF tipping product prototyping and assembly biocompatibility test and associated downstream processes  Led the technology development team to collaborate with Product Development Team to transfer and integrate technology invented to new product development platform  Technology business impact 16 BD PIVC product platforms and 550 MM global sales US China Europe Brazil Coinventor and Project Leader of BD New Thermoplastic Elastomer TPE for BD Integrated Intravenous Catheter Development  Collaborated with external vendor to develop new TPE for BD integrated intravenous catheter product for global market  Extensive assessment of multiple types of TPE materials in the market SEBS SBC TPV TPU POE OBC PB  Led a crossfunction team RD Quality Marketing Medical Affairs Regulatory Affairs and Manufacturing from different regions US Mexico and China to drive and manage the program from early technology development phase to the end of Product Development and Launch Phase by following the best practice of FDA Design Control design control activity including FMEA Design InputProduct Requirement Development User Risk Assessment Design Jessica Claire Page 2 Verification Design Reviews Process Validation and 510k submission and BD Global Product Development System guideline  Managed and coached both onsite engineers and technicians and remote contract engineers in India consulting company for project planning Product Design Reviews test method development Design Verification protocolreport writing  lab testing  Coacheddirected manufacture engineers to develop multiple new processes to implement new TPE on current manufacture lines and later new process validationqualification  Collaborated with both internal and external extrusion converters to develop new tubing extrusion technology for TPE extension tubing  Collaborated with internal injection molding engineer and external vendor to develop a 16cavity production mold for new TPE material  Supported regional Regulatory Affairs Group for new product registration in local FDA and product commercialization  Business impact 50 MM global sales Primary Investigator to Explore New Medical Plastic and Elastomer Technology for BD New Electronic Diagnostic Products  Collaborated with external material vendors for material selection feasibility testing and screening of plastic and elastomer in the market  Supported process engineers for highcavity production mold development for injection molding  Plastic and elastomer chemical compatibility assessment  Plastic bonding technology development including UV  thermal adhesive bonding solvent bonding ultrasonic welding and laser welding  Planning of global product registration and planning strategy           Westat      Senior Materials Scientist                           042006      042011     Primary investigator on new RD coextrusion line development for BD Vialon PIVC catheter tubing  Vendor assessment and contract negotiation of 500000 RD coextrusion line  Tooling and process developmentoptimization Development and set up of RD compounding line Leistritiz Micro18 compounder auto feeder quench tank and pelletizer           University Of Akron      Research Assistant   City     STATE                   082001      032006     Synthesis characterization and rheology of combined mainchain and sidechain liquid crystalline copolyester polymer Professionally trained in organic synthesis and instrumental analysis DSC TGA DMA NMR FTIR GPC  Proficiency in Minitab Solidworks SigmaSoft softwares  AWARD 2012 BD Delta Transformational Hero for significant contribution to Technology Innovation Efficiency and Talent through the year 2014 Inventor of the Year for the breakthrough invention of BD nextgeneration Vialon catheter tubing material 2015 BD Corporate OSMT Technology Award for significant contribution to BD new TPE material development for PVC alternative          Work History       Dept of Polymer Engineering                                  Affiliations              Publications     1   Siddarth K Shevgoor Jonathan Karl Burkholz Huibin Liu Yiping Ma Jessica Claire Antimicrobial Inserters For Medical Devices US 20150231307 A1 August 20 2015    2   Bryan Fred Bihlmaier Weston F Harding Janice Lin Huibin Liu Shiddarth K Shevgoor Jonathan Karl Burkholz Yiping Ma Jessica Claire Antimicrobial Inserters For Medical Devices US 20150231309 A1 August 20 2015       Skills     business case consulting contract negotiation downstream DSC engineer FTIR functional Innovation IP lab testing laser Market Marketing material development materials material selection Minitab new product development Next NMR optimization process development processes Product Design Product Development Project Leader project planning and management project planning prototyping Quality Regulatory Affairs report writing Requirement Risk Assessment sales Solidworks strategy Therapy UV Validation welding</data><data key="id">203798239827680123718507570315647526520</data></node>
<node id="n2047" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/information-technology-scientist-58e5047fd8ef4f65b405a7b0c3455186</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Career Overview      Performance driven systems thinker adept at meeting customer expectations while achieving business goals I have seventeen years experience working with customers to analyze design customize build integrate implement and support information technology business system solutions specializing in strategic resource management Besides strong technical skills I have strong functional skills being a subject matter expert at helping customers align their money manpower personnel programs and capabilities to their business strategy While working on clientfocused business solutions I promote my organization through business development activities including proposal writing creating proof of concepts being conference vendor and supporting business systems user groups Internally I lead and manage my teams my projects through continual process improvement activities to improve quality business productivity and customer service            Qualifications         Project Management Professional PMP  IT Project  Agile Certified Scrum Master CSM  Certified Information Systems Security Professional CISSP  Certified in Risk and Information Systems Control CRISC      Oracle Certified Professional DBA OCP  Teradata Certified Professional  Microsoft Dynamics AX Certified Master Trade and Logistics Financials Project AX Development   Former Certified Novel Engineer CNE Microsoft Certified Systems Engineer MCSE                     Education and Training       Thunderbird School of Global Management    Norfolk     VA      Expected in   2008                    Select One                  GPA            Executive Certificate in International Management Global Management Global Leadership Global Strategy          Old Dominion University    Norfolk     VA      Expected in   1996                    Select One                  GPA            Courses in Management Information Systems          University of Virginia    Charlottesville     VA      Expected in   1996                    Bachelor of Arts        Psychology          GPA                     Skills       Project Management Agile Development  Process Improvement Operational Planning  Compliance and Accreditation  Risk Management  Helpdesk Management  Knowledge Management  Enterprise Architecture  Solutions Integration Systems Data Architecture  Information security Information Systems Control Business Continuity Planning Disaster Recovery Planning  SOA XML REST SOAP  Software Engineering SDLC Software Testing Optimization and Performance  Data Virtualization  Composite  Databases  Oracle SQL Server NoSQLTeradata  Business Intelligence  COGNOS Business Objects Actuate  Predictive Analytics  SPSS R  ERP  Oracle Financials Microsoft Dynamics AX Momentum  ETL and Data Quality  DataStage DataQuality Ab Initio  Applications Development ObjectOriented Programming  Coldfusion Java NET Javascript AJAX CASL Script         Accomplishments       Part of team that won EGov Pioneer Award as well as Team Eagle Award  Received Technical Eagle award as well as several accolades from customers I have supported and from other project teams that I have aided         Work Experience       CACI International      Information Technology Scientist   City     STATE                   021998      Current    Progressive Change in Responsibilities  August 2006  Current  Information Technology Scientist  April 2002  August 2006  Project Manager Deputy Technical  March 1998  April 2002  Systems Engineer Lead   Programs  2011  Current   Office of Under Secretary of Defense Comptroller  Program and Financial Control Directorate  OUSDC PFC   Served as Functional and Technical SME Subject Matter Expert    Mission and Application Support Team    1 Created prototypes to help establish features list for the creation of a Next Generation Resource Management System The prototypes included the following a increase business agility through a single integrated solution using SOA architecture b increase situational awareness for funds management through virtualized data integration and master data management of comptroller legacy systems and c use data science techniques and predictive analytics models to forecast obligations and outlays to predict the business impact of budget cuts to the economy and government spending for hundreds of accounts Note many of the prototypes were implemented as part of operations while waiting for the Next Generation Resource Management System contract proposal  2 Support OUSDC PFC operations by creating new business intelligence capabilities in OBIEE that increased situational awareness during times of declining budgets by smashing legacy system silos and improving the business processes between them and by allowing analysts to better plan using historical data Many of the new capabilities have been well received by the customer such that they want to expand on the capabilities  Besides supporting new capabilities I also provide support for existing systems  3 Support the Department of Defense planning for Sequestration as well as the budget cycle of Planning Programming Budgeting and Execution    Enterprise Funds Distribution  Team a Momentum ERP implementation   Tracks Presidents Budget through Congressional Scoring and Adjustments to AppropriateAuthorizeAllocate funding to the various Department of Defense components and agencies  1 Provide operational support for EFD Performance Budget and Budget Execution activities including integrating EFD with the rest of OUSDC business systems as well as components and defense agencies systems  2 Provide EFD Help Desk support  3 Lead EFD Business Intelligence Business Objects    Performance Budget Redesign PBR  increase business agility and usability by moving EFD PB from a Momentum ERP module to a custom NET application   1 Design data architecture and performed database applications development  2 Help migrate data from Momentum ERP PB to PBR and produce interfaces back to EFD Budget Execution    Next Generation Resource Management System NGRMS    1 Assisted in writing the technical portion of the winning proposal and built the demo for the proposal  2 Aid in the design of the new system   2009  2011    Special Operations Command SOCOM   Served as Functional and Technical SME Subject Matter Expert   Special Operations Resource Business Information System SORBIS    1 Assisted in writing the technical portion of the winning proposal and answering questions used by the customer to filter out weaker competitors  2 Wrote and collected Planning Programming Budgeting and Execution PPBE requirements  3 Served as the Data Architect for the SORBIS data warehouse  4 Served as the lead ETL architect using IBM Information Server tools such as DataStage QualityStage and Information Analyzer  5 Designed and built COGNOS dashboards using COGNOS Business Intelligence and Metrics Server  6 Designed and built SORBIS modules using COGNOS Planning Tools Contributor and TM1   2006  2008   Army Resource Management Tool RMT   Served as Functional SME as well as provided operational planning support and process improvement to the CACI RMT team   1 Assisted in writing the winning proposal for RMT  2 Partnered with incumbent system builder to provide help desk services for RMT where by a small business was the prime contractor 3 Aid in leading the CACI portion of the tier3 help desk team to write design documents for and researchresolve trouble tickets for RMT a classic ASPSQL Server budget application system  4 Besides providing help desk support produced a migration strategy to move Army commands using alternative budget applications to RMT including data migration and command standup  5 Train CACI RMT help desk personnel on help desk processes and Resource Management functional knowledge  6 Interviewed sized and forecasted the CACI RMT help desk team  7 Performed gap analysis and collectedwrote requirements for RMT enhancement tasks   2006  2007   Army Resource and Manpower Aligned with Personnel REMAP   Served as Functional SME and Deputy Project Manager   REMAP allows the Army to see the cost saving from doing what if scenarios of restructuringrealigning its Manpower and Personnel  The chosen scenario would then be applied to the resource management systems for programming and budgeting   1 Assisted in writing the winning proposal for RMT  2 Performed business analysis and wrote the requirements for REMAP  3 Lead Agile development team to produce a custom REMAP NET application  4 Trained team in using specific NET technologies and a mandated customer NET framework software architecture and software tools These mandated customer tools were designed to integrate each application hosted at the data center to the custom security system at the data center   2004  2006   CACI Enterprise Budget   a product built in partnership with Microsoft to enhance Microsofts ERP Public Sector offerings  Served as Functional SME User Experience Designer and Technical Evangelist   1 Created the product road map prioritized requirements and managed product releases  2 Translated functional knowledge of business processes cost modeling strategic planning and other financial management initiatives into core requirements for CACI Enterprise Budget which is built on top of Microsoft Dynamics AX ERP  3 Worked with teams in Europe and India to build the Enterprise Budget product  4 Aligned the product to work with accounting systems used by CACI government clients  5 Built tested marketed and demonstrated the product  6 Performed search engine optimization SEO to support CACI Enterprise Budget   2003  2004   AFSS   Prototyped an accountingasset management system focusing on Property Plant and Equipment to meet FASAB Federal Accounting Standards Advisory Board and JFMIP Joint Financial Management Improvement Program standards in order to support an auditable financial statement   2000  2004   PARIS  a resource management system  Served as Systems Engineer Lead and Performance lead    1 Assisted in writing the winning proposal for PARIS and building the demonstration portion of the competitive bid  2 Prototyped created and designed a budget system emphasizing formulas to forecast cost and human capital projections  3  Ensured that the application and its supporting infrastructure met performance requirements using SilkPerformer as the testing tool  4 Maintained the project infrastructure including the application servers the software development platform and the business intelligence platform   5 Wrote the technical operations document and interface documents supporting the project   1999  2009   Resource Management Online RM Online  a collection of projects falling under RM Online umbrella supporting many different customers including Army Commands Treasury agencies and Department of Homeland Security agencies  RM Online has also spawned other programs including Enterprise Budget PARIS and others not included in this resume RM Online has won the EGovernment Pioneer award and the CACI Team Eagle award   RM Online grew out of the failure of a large complex comprehensive Army program  SBIS Sustaining Base Information Services  The remnants of BMISMHeritage was converted to a web based application using Coldfusion a rapid applications development language and championed by a single major army command  As RM Online grew in capabilities and functionality more army commands began to utilize and champion RM Online adding additional capabilities to support the mission of each army command RM Online grew and attracted other government customers outside the army Instead of a single comprehensive one size fit all system RM Online grew out of a community of supporters each adding to and tailoring RM Online to fit their own needs   Progressively served as Systems Engineer Lead Database Administrator Web Applications developer Deputy Project Manager Information Technology ScientistArchitect for RM Online   Functional Skills  Subject Matter Expert on Resource Management including budgeting manpower personnel financial strategic planning balance scorecard capabilities management knowledge management accounting cost modeling accreditation training   Techical skills  Was the initial web application developer that wrote many of RM Online modules and prototyped many that did not make it into RM Online including writing the web application code as well as the database application code Migrated the data of many RM Online customers into RM Online and integrated RM Online to many of customers own   internal  systems Integrated RM Online to align with many of the popular interfacing systems such as Oracle Financials using Oracle SOA Suite Stood up many RM Online systems which requires that the systems be accredited and certified to go live at each customer site The accreditation and certification process include reviewing the system for security disaster recovery and continuity of operations    LeadershipManagement skills  Lead multiple project teams as well as the shared services team for the RM Online product The shared services team included the system engineers the DBAs the testers the technical writers the trainers configuration management quality assurance and help desk teams   Instituted CMMI processes CACI Defined Processes and Lean Processes to the product team Served as the CMMI Technical Solutions and Decision Analysis Resolution process owner  Instituted secured software development processes into the software development life cycle Lead the RM Online team through many DIACAP security certifications using such technologies as GoldDisk Retina Nessus and NTOSpider  Managed and established RM Online partnership with vendors including Microsoft Oracle and Adobe  Wrote and managed the RM Online portal which includes many of the RM Online processes which includes onboarding procedures hiring tests internal resource management training as well as a management portal to track RM Online projects personnel labor and monetary forecasts and actuals    1998  1999   Department of the Army     BMISMHeritage  a client server budgeting system that was one piece of a larger comprehensive Army financial management system  SBIS Sustaining Base Informations Services    1 Served as the systems engineer maintaining all the environments for BMISMHeritage including development test training and production sites  2 Wrote all the technical documents including the systems interface documents and the concepts of operations document  3 Programmed systems interfaces to disparate Army financial systems including screen scraping software to work with mainframe systems and COBOL  4 Supported the DBA function doing database administration tasks   5 Supported command standup and provided user training to the client server budgeting system</data><data key="id">248086481247810232808100650703820546036</data></node>
<node id="n2048"><data key="name">pyscripter</data></node>
<node id="n2049"><data key="name">autocad</data></node>
<node id="n2050"><data key="name">cluster analysis</data></node>
<node id="n2051"><data key="name">identifying patterns</data></node>
<node id="n2052"><data key="name">sensor data</data></node>
<node id="n2053"><data key="name">testing conformance</data></node>
<node id="n2054"><data key="name">product testing</data></node>
<node id="n2055"><data key="name">graphic attribute mapping</data></node>
<node id="n2056"><data key="name">xml attribute mapping</data></node>
<node id="n2057"><data key="name">fuzzy rule based classification</data></node>
<node id="n2058"><data key="name">quantitative comparison</data></node>
<node id="n2059"><data key="name">experimental data</data></node>
<node id="n2060"><data key="name">initial load</data></node>
<node id="n2061"><data key="name">sigmaplot</data></node>
<node id="n2062"><data key="name">fishstatj</data></node>
<node id="n2063"><data key="name">writing skills</data></node>
<node id="n2064"><data key="name">realtime research</data></node>
<node id="n2065"><data key="name">scientific writing</data></node>
<node id="n2066"><data key="name">feasibility testing</data></node>
<node id="n2067"><data key="name">instrumental analysis</data></node>
<node id="n2068"><data key="name">lab testing</data></node>
<node id="n2069"><data key="name">predictive analytics models</data></node>
<node id="n2070"><data key="name">forecast</data></node>
<node id="n2071"><data key="name">write design documents</data></node>
<node id="n2072"><data key="name">cost modeling</data></node>
<node id="n2073"><data key="name">search engine optimization</data></node>
<node id="n2074"><data key="name">silkperformer</data></node>
<node id="n2075" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/lead-bi-developer-principal-data-analyst-32054a80aded41e18624a6e2cbe7c111</data><data key="resume">Jessica    Claire                                 609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor   Home   555 4321000        Cell           resumesampleexamplecom                  Summary      Proactive BI DeveloperData Analyst with Master’s in Computer Science and 7 years of extensive IT experience in Design Development and Delivering of Business Intelligence solutions in Financial Utilities and Retail industries Extensive experience working with Tableau Desktop Tableau Server in various versions of Tableau in creating highly interactive Data Visualization Reports and Dashboards using complex functionalities Database Design PL Designing Development Integration Implementation and maintenance of Business Intelligence and the related Database Platforms Worked with ETL and ELT process to Extract Transform and Load  Extract Transform and Load data into stage area and data warehouse Deploy the reports and Scheduled Reports for the use of end users and customers on the server Good interaction with clients understanding Business Applications Business Data Flow and Data Relations Good understanding of technical trends architectures and highly motivated to know more about latest technology new software and products Team player with good communication and interpersonal skills Responsible for interacting with business partners to identify information needs and business requirements for reports Ability to handle multiple tasks concurrently and meet the deadlines Talented Analyst with background analyzing competitors synthesizing business intelligence and evaluating trends to enhance business results Forwardthinking and enterprising when meeting expected demands with realtime data and strategic recommendations Natural leader with resourceful and systematic approach        Skills           Tableau Desktop Server Public Online  Reader v 2020x 2019x2018x MS Excel SSRS  Snowflake DB Cassandra MEMsql Oracle 11g10g  9i MS SQL Server 2005  2000 MS Access Postgres Amazon S3      SQL Python  TSQL HTML CSS Java  Microsoft Dynamics Salesforce MS Word MS Excel Outlook FrontPage PowerPoint                       Experience      012019   to   Current     Lead BI Developer Principal Data Analyst      Danaher    –    Coralville     IA             Responsible for the data and Reporting ecosystem for all the Sales and Amazon Connect reports  Gather the report requirements from various stakeholders cross functional departments business users  Build publish customized interactive reports and dashboards report scheduling using Tableau desktopserver  Designed and built critical hierarchy rollup tables for RANKING the associatesterritories based on the performance quality economics customer relations SLA inventory and call metrics  Perform Data Quality checks analysis on the source tables and developed metric tables as per the business definitions  Designed and owned Python framework that automates the addition of new metrics into the reports without manual efforts  Worked with a diverse team on decommissioning projects from Cassandra to Snowflake DB  MEMSQL warehouses  Developed persistent metric layers for the Tableau by coding the metrics in a way to process only the incremental records in the base table to improve the SQL performance and execution time drastically  Worked on internal frameworks to export data to Amazon S3 and create Symphony pipelines to flow data from Amazon S3 buckets to Snowflake Extract Load and Transform  Create metadata and lineage about the table developed classify the sensitivity of data NPI API PCI etc as per the Data risk policies and document all the code process for easy transition to the business teamsData Risk Management and Data Governance  Prepare and update documentations Data Analysis Data Mapping File Business Requirement Description Data Validation etc to ensure all the procedures and knowledge are traceable in future cycles  Analyze complex business problems and issues using data from internal and external sources to provide insight to decisionmakers  Environment Tableau Desktop 2019220183 Tableau server Snowflake DB Cassandra MemSQL Oracle 10g AWS Postgres Excel files Salesforce MS Access          082017   to   012019     BI Consultant  Sr Data Analyst      Iqvia Holdings Inc    –    Fort Wayne     IN             Worked on gathering and converting data over to Tableau using SQLSASExcel data over to Tableau reports and Dashboards  Created metrics attributes filters reports and dashboards created advanced chart types visualizations and complex calculations to manipulate the data Creating New Schedules and checking the tasks daily on the server  Involved in creating adhoc analysis and reporting requests in a timely manner for Business Risk related to fraud Approvals Declines daily monthly and yearly  Created organized customized analysis and visualized projects and dashboards to present to executive leadership  Created Complicated Calculation Based LOD Level of detail feature  Designing and developing data warehouse and Amazon Redshift BI based solutions  Evaluated database performance issues and executed database optimization  Designing and developing prototyping the various dashboards using TableauDesktop  Extensively involved in building the dashboards such as creating Extracts refreshing extracts Layout designing worksheet Actions functions connectors Live and Extract Dashboard color coding formatting and report operations sorting filtering Quick Filters Cascading filters context Filters ranking TopN Analysis hierarchies  Worked with clients to better understand their reporting and dashboarding needs and present solutions using a structured Waterfall and Agile project methodology approach          012016   to   072017     Sr Data Analyst      Mantri Inc    –    City     STATE             Interpret data from primary and secondary sources using statistical techniques and provide ongoing reports  Compile and validate data reinforce and maintain compliance with corporate standards  Develop and initiate more efficient data collection procedures  Build publish customized interactive reports and dashboards report scheduling using Tableau desktopserver  Mastered the ability to design and deploy rich Graphic visualizations with Drill Down and Drop down menu option and Parameterized using Tableau          032013   to   072014     Data Analyst      Rise Technologies    –    City     STATE             Used SAS Data Integration Studio to develop various job processes for extracting cleansing transforming integrating and loading data into Data marts and Data warehouse database  CreationModification of Data Sets on the Remote Server using SASBASE and Macros  Prepared graphs using the modified data for business analysis  Coordinating the production of monthly quarterly and annual performance reports for senior management  Extensively used SAS Macro facility to provide reusable programs that can be conveniently used to update reports and to run weekly and monthly reportsShell scriptings  Prepared documentation for end users  Trained Power users and Business users on building their skills to assist in the development and testing of new reports          Education and Training      Expected in   122015     Master’s     Computer Science                     GPA       GPA 36        Expected in   052013     Bachelor of Technology     Electronics and Communication Engineering                     GPA       GPA 33        Certifications       Certified BASE SAS DEVELOPER ADOBE EXPERIENCE MANAGER DEVELOPER</data><data key="id">305688115928869989677251199110310744808</data></node>
<node id="n2076" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-dictionary-analyst-1beed068903f41a19100e8de6c5bb495</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional Summary     Experienced Information Technology professional Key strengths include but not limited to systems support in health data management business analysis quality assurance and functional testing ability to manage multiple projects simultaneously adapt to process procedures and implementation to client specification excellent communication critical thinking investigative  problem resolution strategies exceeding customer expectations           Education and Training       American Intercontinental University    Dunwoody     GA      Expected in        –      –       Master of Information Technology MBA                  GPA                    Long Island University  CW Post    Brookville     NY      Expected in        –      –       Bachelor of Science BS                  GPA                   Skill Highlights         Experience in all phases of the System Development Life Cycle for enterprise application implementations  Experience with application level support for multifunctional end user expertise in clinical data management  Experience in project management methodology effective independently or in a team environment  Knowledge of information security concepts cryptography incident response policyaudit and threat analysis  Knowledge of security Firewalls IDSIPS PKI TCPIP Operating Systems LANWAN and the OSI Model  Knowledge of HIPPASOXPCI compliance standards and Management of Information Systems Access CoordinatorEnterprise applications Safety  ComplianceRisk mitigation and Disaster Response  System Administrator United Health Care Optum Cloud Availity CIGNA Humana and BCBS Payer Websites  ICD9CPT4 coding systems Access Blue Care Medic Claim Logic EDI Comtec EPIC Health Quest  IDXGE Centricity Business I Suite Power Chart and Microsoft Office Suite software applications  Lawson Lotus Notes Micro Strategy data warehouseFPSC webbased reporting database applications  Cryptography Concepts Computer Forensics Computer NetworkingTCPIP Information Security Principles  JAVA Programming ORACLEdb10g OPNET IT Guru Windows OS UNIXLinux OS                         Accomplishments       Building Coordinator  Office of Quality  Safety  Emory Healthcare Reports Management Committee  Administrative Offices  Emory Healthcare Advisory Board  Administrative Offices  Emory Healthcare Epsilon Pi Tau   Delta Delta Chapter  American Intercontinental University Spotlight Award Winner  Specialty Brands  MCIVerizon Business         Professional Experience       Global Indemnity Limited      Data Dictionary Analyst   Scottsdale     AZ                   012004      Present     Perform system administration business analysis and maintenance of healthfinancial Database Dictionaries for internalexternal clinicalbusiness units Query webbased research toolssystem modules for data integrity and operations compliance maintain finance charge master manage security setupaccess control to billing interfaces for end users SME for Electronic Data Interchangesystem configurations for revenue cycle optimization  Accomplishments Executed billing application configurations for ICD10 implementation for CPT code compliance Instrumental in transitioning encounter based charge capture to electronic formats for provider services Maintained access managementsecurity to secure payer web portals for enterprise of 10000 end users Subject matter expert for GE Centricity Business implementation increased database integrity by 95 Designed testing scripts executed testing in Enterprise Dictionaries for system buildoperational validation Developed technical documentation of department process improvement initiatives for application support Audited EDI system code logic English translation improved data transparencyecommerce AR by 25           DERRICK FROST MCIVerizon      Operations Analyst III   City     STATE                   011999      012004     Senior member of the marketing operations team administered audits controls reporting and coordinated account setup Maintained EDIecommerce PINCard order process utilizing EPICLawson and WebO interface applications for logistics with West coast Hub maintained weekly quality assurance reporting providing a snapshot of metrics in operational efficiency Accomplishments Project managed web based product catalogue portal Tracked all custom and branded applications in one repository Achieved 98 service level compliance in salesmarketing of product imaging merchandising plans and account setup Executed audits of inventory practices that led to a twoday reduction in sales to fulfillment rates          Skills     billing business analysis Computer Networking CPT4 CPT Cryptography data management data warehouse database applications Database ecommerce Electronic Data Interchange EDI English finance financial Firewalls Hub ICD10 IDS imaging Information Security Information Systems inventory JAVA Programming LAN Lawson Linux OS Logic logistics Lotus Notes marketing merchandising Access Microsoft Office Suite Windows OS 98 Micro Strategy Enterprise Operating Systems optimization OSI PCI process improvement marketing of product coding project management quality assurance Quest reporting research Safety sales scripts System Administrator system administration TCPIP TECHNICAL TRAINING technical documentation translation UNIX validation Websites WAN</data><data key="id">164611178195860836193375889079593369238</data></node>
<node id="n2077" labels=":CV"><data key="labels">:CV</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Professional Summary      Perceptive and logical data analyst with proven ability to communicate with both technical professionals and endusers to identify and translate business requirements Offering 9 years of experience driving data accuracy and integrity Proven ability to build and lead teams of talented professionals to develop valuable process solutions to meet business objectives As a dedicated and flexible individual that will bring a charismatic energy years of developed communication skills and my ability to work sufficiently with others and individually with strategic project management and organization skills With time my experience in customer service patient care and health insurance has helped me to excel within the Health Care and Data Analytic Industry If given the opportunity I will continue to grow and learn within your company        Skills           Excel  SQL  Clients relations and Customer Service  Data management  Data Mining  MS Office  Business operations  Data interpretation  Analyzing trends  Operational improvement  Some VBA reading capability  Negotiations and Contracts      Medical Assistance  Data Analysis  Creating new Policies and Procedures for workflows  Workflow Optimization  Quality Assurance and Auditing  Project Management and Organization  Medical Billing and Coding  Process improvement  Accounts Payable and Purchasing Support  Software programming  Problem resolution                       Work History        042018   to   Current   Master Data Management Analyst I    Badger Daylighting         Forest Lake     MN            Assists master data management team to ensure system data integrity using SQL Excel and some VBA  Identify areas for data quality improvements to resolve quality issues through error detection correction process control improvement andor process redesign strategies  Training new and existing employees on building and maintaining items and contracts within Lawson as well as all other standard everyday workflow  Conduct data cleansing using Excel and SQL to rid the system of old unused inaccurate or duplicative data Upload sort and analyze item and contract data to ensure accurate data  Collaborates with subject matter experts and data stewards to define and implement data strategy policies controls and programs to ensure the Global data is accurate complete secure and reliable  Support item vendor master maintenance General Ledger GL category maintenance item agreements template creation and RequestorLocationPar Location setup  Demonstrate an understanding of the data maintenance impact upon clinical and business process operations and use that knowledge for easier daily workflow and department connection  Work with Accounts Payable and Purchasing team members to identify master data related improvement opportunities  Maintain a consistent taxonomy for new item and contracts master data  Developed data tables and databases to create relation between tables on a intermediate level to allow efficient data retrieval  Performed data cleaning and data preparation to support highquality data and catch errors before processing  Evaluated statistical techniques and extracted SQL and Lawson information from research data to provide different ways to assess robustness of research outputs  Created workflow steps for Unit of Measure changes within Excel connection to Lawson  Maintain detailed communication with affiliated Hospitals with over 200 service request monthly  Conducted quality checks on data sources to check for inconsistencies and perform data cleaning activities  Evaluated reliability of source information by weighing raw data to make data easier to interpret            082014   to   042018   Cost ManagementMedical Claims Analyst    Hope Credit Union  Hope Enterprises         New Orleans     LA            Provided direct contact to Claims with OON pricing for proper payment of claims  Handle disputes adjustments and negotiations for specific networks  Cost management for multiple plans working with Health Scope  Selfmanaging time to properly work end of week due report each week  Handling large quantities of data from multiple networks and handle pricing based on specific guidelines provided by each network  Manage and update excel report of over 11000 medical codes creating excel spreadsheets to accommodate average payouts and generating weekly reports via access into excel to distribute throughout the department  Maintained knowledge of benefits claim processing claims principles medical terminology and procedures and HIPAA regulations  Paid or denied medical claims based upon established claims processing criteria  Managed large volume of medical claims on daily basis  Reviewed provider coding information to report services and verify correctness  Processing and evaluating medical dental and vision claims manually according to precise and specific Summary Plan Description  Obtaining information necessary by telephone as well as by hard copy and entering into computer system  Prioritization of individual work flow associated with case assignments while meeting a set production daily  Reviewing and Auditing each individual claim Hospitals Outpatient and Inpatient Physician Offices Skilled Nursing Facilities Medical Equipment Rentals etc to ensure proper reimbursements to the assigned plan  Maintaining and assuring accuracy of documentation  Discuss with members clients and providers of issues relating to claim administration and establishing and maintaining professional rapport with clients and providers physicians and hospitals            052014   to   082014   Collection Specialist    Wood Personnel Services         City     STATE            Monitor insurance claims by running appropriate reports and contacting insurance companies to resolve claims that are not paid in a timely manner  Identify coding or billing problems from EOBs and work to correct the errors in a timely manner Identify problem accounts and escalate as appropriate  Work with patients and guarantors to secure payment on outstanding account balances  Achieved performance goals on consistent basis  Monitored accounts for compliance with established payment plans and flagged those in violation  Used scripted conversation prompts to convey current account information and obtain payments            102013   to   052014   Member Service Representative    Wood Personnel Services         City     STATE            Processing inbound calls from State of Tennessee members  Assisted them with their requirements to uphold insurance policy  Problem solving and special projects assigned by management  Completed inbound and outbound calls to member on specific requirements needed for insurance coverage  Completed training and worked effectively under highpressure client services environments  Evaluated customer information to explore issues develop potential solutions and maintain highquality service  Evaluated customer information to explore issues develop potential solutions and maintain highquality service  Learned all internal systems and related service role duties to provide skilled team backup in handling customer demands            092011   to   102013   Intake Coordinator Communication Specialist    MedSolutions Inc         City     STATE            Processed inbound Calls from physician’sproviders offices regarding prior authorization for radiologycardiac test  Resolve Customer Service Issues while keeping professional control on the call  Handle confidential information daily in a professional manner  Completing special assigned projects for CMI fax team Projects including but not limited to process outbound calls to providers’ offices for missing information verification of contact info and sending awareness of fax processing Work incoming Intake mail and fax or mail to appropriate recipient Attach incoming faxes to ISAAC Problem Solving for missing faxes in ISAAC and Incoming Folder  Maintained current and accurate medical records for over 50 patients daily  Established and developed highly efficient and dependable administrative team by delivering ongoing coaching and motivation and fostering career advancement  Trained staff established and monitored goals conducted flow performance for elite staff reviews          Education        Expected in   5 2011   Diploma       Medical Assisting    Fortis Institute MedVance     Nashville TN           GPA        Graduated with Medical Assisting Diploma with Medical Billing and Coding           Expected in                 Middle Tennessee State University     Murfreesboro     TN      GPA                 Expected in   2009   High School Diploma           Centennial High School     Franklin     TN      GPA               Skills       Excel  SQL  Clients relations and Customer Service  Data management  Data Mining  MS Office  Business operations  Data interpretation  Analyzing trends  Operational improvement  Some VBA reading capability  Negotiations and Contracts    Medical Assistance  Data Analysis  Creating new Policies and Procedures for workflows  Workflow Optimization  Quality Assurance and Auditing  Project Management and Organization  Medical Billing and Coding  Process improvement  Accounts Payable and Purchasing Support  Software programming  Problem resolution         Work History        042018   to   Current   Master Data Management Analyst I    Community Health Systems       Antioch     TN     Assists master data management team to ensure system data integrity using SQL Excel and some VBA  Identify areas for data quality improvements to resolve quality issues through error detection correction process control improvement andor process redesign strategies  Training new and existing employees on building and maintaining items and contracts within Lawson as well as all other standard everyday workflow  Conduct data cleansing using Excel and SQL to rid the system of old unused inaccurate or duplicative data Upload sort and analyze item and contract data to ensure accurate data  Collaborates with subject matter experts and data stewards to define and implement data strategy policies controls and programs to ensure the Global data is accurate complete secure and reliable  Support item vendor master maintenance General Ledger GL category maintenance item agreements template creation and RequestorLocationPar Location setup  Demonstrate an understanding of the data maintenance impact upon clinical and business process operations and use that knowledge for easier daily workflow and department connection  Work with Accounts Payable and Purchasing team members to identify master data related improvement opportunities  Maintain a consistent taxonomy for new item and contracts master data  Developed data tables and databases to create relation between tables on a intermediate level to allow efficient data retrieval  Performed data cleaning and data preparation to support highquality data and catch errors before processing  Evaluated statistical techniques and extracted SQL and Lawson information from research data to provide different ways to assess robustness of research outputs  Created workflow steps for Unit of Measure changes within Excel connection to Lawson  Maintain detailed communication with affiliated Hospitals with over 200 service request monthly  Conducted quality checks on data sources to check for inconsistencies and perform data cleaning activities  Evaluated reliability of source information by weighing raw data to make data easier to interpret            082014   to   042018   Cost ManagementMedical Claims Analyst    HealthScope Benefits       Nashville     TN     Provided direct contact to Claims with OON pricing for proper payment of claims  Handle disputes adjustments and negotiations for specific networks  Cost management for multiple plans working with Health Scope  Selfmanaging time to properly work end of week due report each week  Handling large quantities of data from multiple networks and handle pricing based on specific guidelines provided by each network  Manage and update excel report of over 11000 medical codes creating excel spreadsheets to accommodate average payouts and generating weekly reports via access into excel to distribute throughout the department  Maintained knowledge of benefits claim processing claims principles medical terminology and procedures and HIPAA regulations  Paid or denied medical claims based upon established claims processing criteria  Managed large volume of medical claims on daily basis  Reviewed provider coding information to report services and verify correctness  Processing and evaluating medical dental and vision claims manually according to precise and specific Summary Plan Description  Obtaining information necessary by telephone as well as by hard copy and entering into computer system  Prioritization of individual work flow associated with case assignments while meeting a set production daily  Reviewing and Auditing each individual claim Hospitals Outpatient and Inpatient Physician Offices Skilled Nursing Facilities Medical Equipment Rentals etc to ensure proper reimbursements to the assigned plan  Maintaining and assuring accuracy of documentation  Discuss with members clients and providers of issues relating to claim administration and establishing and maintaining professional rapport with clients and providers physicians and hospitals            052014   to   082014   Collection Specialist    Wood Personnel Services       Franklin     TN     Monitor insurance claims by running appropriate reports and contacting insurance companies to resolve claims that are not paid in a timely manner  Identify coding or billing problems from EOBs and work to correct the errors in a timely manner Identify problem accounts and escalate as appropriate  Work with patients and guarantors to secure payment on outstanding account balances  Achieved performance goals on consistent basis  Monitored accounts for compliance with established payment plans and flagged those in violation  Used scripted conversation prompts to convey current account information and obtain payments            102013   to   052014   Member Service Representative    Wood Personnel Services       Franklin     TN     Processing inbound calls from State of Tennessee members  Assisted them with their requirements to uphold insurance policy  Problem solving and special projects assigned by management  Completed inbound and outbound calls to member on specific requirements needed for insurance coverage  Completed training and worked effectively under highpressure client services environments  Evaluated customer information to explore issues develop potential solutions and maintain highquality service  Evaluated customer information to explore issues develop potential solutions and maintain highquality service  Learned all internal systems and related service role duties to provide skilled team backup in handling customer demands            092011   to   102013   Intake Coordinator Communication Specialist    MedSolutions Inc       Franklin     TN     Processed inbound Calls from physician’sproviders offices regarding prior authorization for radiologycardiac test  Resolve Customer Service Issues while keeping professional control on the call  Handle confidential information daily in a professional manner  Completing special assigned projects for CMI fax team Projects including but not limited to process outbound calls to providers’ offices for missing information verification of contact info and sending awareness of fax processing Work incoming Intake mail and fax or mail to appropriate recipient Attach incoming faxes to ISAAC Problem Solving for missing faxes in ISAAC and Incoming Folder  Maintained current and accurate medical records for over 50 patients daily  Established and developed highly efficient and dependable administrative team by delivering ongoing coaching and motivation and fostering career advancement  Trained staff established and monitored goals conducted flow performance for elite staff reviews</data><data key="id">29538441000279048987845801832797412421</data><data key="url">https://www.livecareer.com/resume-search/r/master-data-management-analyst-i-1afd6b210ad5400e86c8b9032740f65b</data></node>
<node id="n2078" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-ii-1a53949428be42b0bd5b594c00a5e189</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Professional Summary     4 years analytic experience in the cost accounting department managing data for profit and loss summary use Extracts manipulates validates submits and documents data from numerous sources Maintains business relationships as part of an overall business model for cost accounting purposes Innovates tools as needed to keep up with changing business environments 13 years of professional experience with Excel for varied purposes Adept with charts pivot tables formulas and formatting Familiar with SAS AddIn capability to expand Excels data storage 20 years of office experience at different levels Production Supervisor Administrative Team Lead Communications Team Lead Communications Team Administrative Team Produced the highest completion rate in the Colorado region Adult LEC Forums Introduction to the Leadership Program Braveheart Team Computer Information Systems Team Keywords data analysis data analyst report analyst business analyst adaptable programmer detail oriented selfmotivated streamline efficient innovative project oriented quick learner quantitative analyses qualitative analyses dashboard formatting SAS Add In customer refresh user friendly customer service courteous networking Less common systems knowledge available upon request Professional Skills Adaptability fluidly handles frequent change Innovation created or updated 13 tools for producing statistic files for DSS Communication in all positions have spoken with customers at all levels of business Creative Problem Solving create new tools improve QAs call to ask find another database to get info etc Planning and Organizing Rolling Wave MS Project GANTT chart OODA PDSA etc Calendar management for team documentation upkeep deliverable audit database upkeep Presentation Meeting facilitation educational presentations analysis explanation etc Pivot tables Power Pivot Waterfall chart line graph slicers pie chart dashboard etc Learning lifelong avid learner Team Player I truly believe that the best work comes from informed collaborating teams of proficient selfdriven workers        Licenses              Skill Highlights           Analysis  SAS EG  Metadata Server  Query Builder  Import Excelcsv  Export Excel  MS Access  Formatting  SAS EG  Metadata Server  Query Builder  Import Excelcsv  Export Excel  MS Access  Formatting  MS Access  ODBC  Data  Regularization  Build calculated  Fields  Relational DB  MS Access  ODBC  Data  Regularization  Build calculated  Fields  Relational DB  MS Excel  Pivot tables  SAS Add In  Formulas  Macros  Charts  Conditional Formatting  MS Excel  Pivot tables  SAS Add In  Formulas  Macros  Charts  Conditional Formatting  Base SAS  ODBC  Export Excel  Base SAS  ODBC  Export Excel  Business Objects  Created Queries  Canned Reports  Adapting to SQL  Business Objects  Created Queries  Canned Reports  Adapting to SQL  Advanced Query Tool  SQL Queries  Query Builder  Table Search Function  Advanced Query Tool  SQL Queries  Query Builder  Table Search Function  Other  MS Visio  MS SharePoint  MS OneNote  Apple Sheets  Apple Pages  Open Office  Photoshop  Other  MS Visio  MS SharePoint  MS OneNote  Apple Sheets  Apple Pages  Open Office  Photoshop  Presentations and Clerical  MS PowerPoint  Charts  Formatting  Images  Slicers  Objects  MS PowerPoint  Charts  Formatting  Images  Slicers  Objects  MS Word  Templates  Charts  Formatting  Images  Tables  Review  Comments  Objects  Fields  Mail Merge  MS Word  Templates  Charts  Formatting  Images  Tables  Review  Comments  Objects  Fields  Mail Merge  MS Excel  Charts  Formatting  Images  Slicers  Objects  MS Excel  Charts  Formatting  Images  Slicers  Objects  Programming  SQL  SAS  Java  VBNet  Programming  SQL  SAS  Java  VBNet  Other  Windows 10  OneLink  EPIC  Lotus Notes  Android  Clarity  Remedy  MS Project  Wrike  Asana                         Professional Experience       Data Analyst II       122011   to   Present         –                      Produce 24 deliverables on a monthly 20 work day production cycle Assumed responsibilities from 4 colleagues giving them the bandwidth to analyze potential savings for KP Collaborate dynamically with production partners to keep workflow synchronized Updates tools as systems change average of 1 tool per month Innovate tools as opportunity arises  total of 5 Innovate Quality Controls as opportunity arises  total of 10 Develop relationships with department managers to maintain understanding of business operations Attend 510 meetings per week with notes that later helped resolve issues Maintain documentation quality checks and process changes Facilitate staff meetings once a quarter Create analyses for impact of changes Analyze potential changes before implementing Analyze impact of business changes on workflow or results           PME Clerk       012007   to   012011     Cox Communications Inc    –                      Collaborated to resolve overdue invoices with internal customers external vendors AP Clerks Purchasing Agents and Facility Buyers Interface Macess MMS in TPX Lotus Notes and vendor websites for research to deduce the error keeping an invoice unpaid then creatively problem solved to resolve unusual invoice errors Consulted on workflow improvements Continuing Care Admin oncall   Kaiser Permanente   2006 Implemented transition to Sprint card access for physician laptops  Entered admission and discharge data into a spreadsheet and eventually HealthTrac Assisted in testing of HealthTrac  Provided clerical and administrative support as requested temp   spherion           Integrated Mail Systems Operator       012006   to   012011         –                      Nominated for Peak Award 2009  The Peak Award is for employee excellence  I was nominated for this honor because I assumed the team lead responsibilities in addition to my own when she was on medical leave for a year  Updated mailing system  Collaborated with vendor and Information Technology Field Services to implement an updated mailing system including integrating software and hardware with the Kaiser Permanente network  Streamlined automated mailing processes  Collaborated with various department managers to ascertain and meet requirements for automated mailings Assumed Lead role to accommodate vacation and sick time           Continuing Care       012003   to   012006         –                      Entered admission and discharge data into a spreadsheet Provided clerical and administrative support as requested  Coding Data entry Contracts Minor clerical support Recognition Programs Awarded Summit Seeker gift cheques Entered award information into the proprietary Access database Learned to edit minor portions of the Access database to fulfill reporting needs Provided clerical support          Education and Training       Bachelors of science     Information Technology     Expected in   March 2011     University of phoenix                GPA       Information Technology         Java VBnet Data Analyst Dedicated to business performance analysis and streamlining          Expected in                        GPA               Community Service              Skills     VBNet administrative support Photoshop AP Apple Business Objects business operations Charts Clerical hardware Contracts Data Analyst Data entry MMS edit Information Technology Java laptops team lead notes Lotus Notes Macros mailing meetings access Access database MS Access MS Excel Excel Mail Office MS PowerPoint MS Project SharePoint Windows MS Word network ODBC DB paint Peak performance analysis Pivot tables Presentations processes Coding Programming Purchasing Quality Maintain documentation reporting research SAS spreadsheet SQL Summit Tables Visio websites workflow</data><data key="id">231524530561093797984927294815123911371</data></node>
<node id="n2079" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/senior-analyst-data-quality-governance-1b79a14827b04f91ae39bd1469595df9</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Background      Analyst versed in data analysis and reporting user acceptance testing as well as solving complex problems in highpressure environments  Excels at cultivating managing and leveraging relationships        Skill Highlights           Microsoft Access Microsoft Excel SAS SQL Server Manager Cognos Crystal Reports      Business Objects SQL Tableau Server and Desktop Project Management Data Analysis                       Professional Experience        Senior Analyst Data Quality  Governance       042018      Current     Milliman    –    Pasadena     CA            Create policies and procedures for actuarial and analytics group  Monitor adherence to policies with scheduled reviews  Conduct training on quality and governance policies  Lead analyst for HIPAA compliance  Create and analyze validation reports for health plan data  Lead discussions regarding data issues identified           Healthcare Data Analyst       042012      Current     General Motors    –    Detroit     MI            Experience with process analysis and other analytic functions  Ability to interact effectively with different business units team members and external client at all levels  Creates ad hoc data analyses and reports as needed Prepares monthly quarterly and annual reports of established clinical utilization and financial metrics for all Health Ministries and SmartHealth  Proficient in computer and analytic tools such as SAS SQL and MS Office excel and access  Performs medical and prescription drug claims analysis necessary to understand relative unit pricing across provider networks and to inform decisions about provider fee schedules  Generates eligibility medical and pharmacy claims data needed for each Health Ministry to support budgeting rate development and manage the reinsurance program           Business Analyst       022008      042012     Honeywell    –    Memphis     TN            Responsible for identifying processes to streamline and assist with various projects  Created and maintained access databases for departmental use   Identified system issues and coordinated with vendor for resolution   Involved in project planning and testing system enhancements   Defined business requirements for various projects   Created adhoc reports as needed   Created audit process for all securities applications   Created reports for SEC and FINRA audits           Finance Analyst       052005      042008     Fidelity National Information Services    –    New Castle     DE            Backup to department manager   Created and maintained managerial reporting daily monthly quarterly and yearly   Created work schedule for department on weekly basis   Created and distributed department stats and analysis to upper management monthly   Primary contact for electronic invoicing candidates   Created multiple databases for interdepartmental need           Trust Analyst       012003      022005     Accenture    –    Frisco     TX            Responsible for daily operations of 401k accounts for retirement services clients  Balanced 401k accounts on a weekly monthly quarterly and yearly basis  Researched and resolved out of balance situations quickly             Conversion Analyst       012002      012003     COMERICA INC    –    City     STATE            Responsible for managing incoming and outgoing account conversions daily DTCC reconciliation troubleshooting and problemsolving  Revised an existing Microsoft Access program for incoming assets for accuracy  Created a new reporting process in Access to identify asset balance discrepancies  Resolved daily out of balance situations with various depository institutions           Registration Analyst       012000      012002     COMERICA INC    –    City     STATE            Responsible for reregistration of physical assets for trust customers   Streamlined processes by automating applications for quicker processing and problem resolution   Solved reregistration issues through brainstorming and troubleshooting   Created a Microsoft Access program to track pending registration issues and generate reports   Collaborated with company legal counsel for asset restriction removal   Authored and updated procedures for registration process          Education and Training       Masters of Business Information Technology       Project Management       Expected in   2007                Walsh College      Troy     MI     GPA        Status         Project Management         Bachelor       Business Administration       Expected in   1996                Rochester College      Rochester Hills     MI     GPA        Status         Business Administration        Certifications      Certificate in Health Care Informatics        Affiliations      Volunteer weekly in local elementary school in reading development   Lead business meetings on a weekly basis  Volunteer in childrens church on a weekly basis  Taught Sunday school classes        Skills     Streamline Processes Business Objects Cognos Crystal Reports Customer Service Data Analysis Databases Financial Invoicing Managing Microsoft Access Microsoft Excel Microsoft Office ProblemSolving Problem Resolution Process Analysis Project Management Project Planning User Acceptance Testing Reporting SAS Securities Reporting SQL Troubleshooting Annual reports Tableau Server Tableau Desktop</data><data key="id">202876060256537823025654321274434092626</data></node>
<node id="n2080" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Professional Summary     To obtain a full time opportunity in the area of Information Systems Database and Business  Administration Managing business stakeholders Configurations Designing Testing Troubleshooting Maintenance and monitoring for honing my skills To be recognized as an important asset to the institution and to keep myself updated regularly Designing and Implementing Tests of Processes   Authentication management technologies    GUI testing   Gap Analysis   Documentation   Reporting   Organize Joint Application Developments JAD sessions and JRP  Risk Assessment   Statistical Analysis   Validate Functionality   Problem Solving  BRD  Process Modeling   Project Management   Rapid Application Development RAD sessions   Adhoc testing   Backend testing Interpersonal Skills   Collaboration   Customer Service   Influencing Others   Negotiation   Teamwork   Facilitating Meetings   FollowUp   Navigating a Matrix Reporting Relationship   Critical Thinking   Scoping    Detail Oriented   Decision Making   Ability to Work with CrossFunctional Teams   Attention to Detail       Core Qualifications         Microsoft Access   Microsoft Excel   MS Office   Microsoft Project   PowerPoint   SharePoint SQL Queries   Tableau   Visual Studio   Spotfire   Visio   CRM   TFS   Workflow tools   SQL Server   AWS                       Experience      011   to   011     Business Data Analyst Intern      Internet Brands Inc    –              USA        Total Work experience of 46 years in Information System and Agile Environment Baby deedee USA          IT Admin Intern          Oct16  Dec16  Deployed and Delivered enhanced web plugins  security services website data analysis as per business requirements  Collaborated with design strategy and sales teams to find solutions for client website projects and developing Complex SQL Queries to illustrate possible solutions  Drafted and Managed business stakeholder expectations to achieve scope in complex and challenging environment  Designing website on Drupal Modifying Data Warehouses  creating detailed reports on research to provide ideas          011   to   011     Technical Analyst      General Dynamics    –              India        Trained a freshers batch of 25 engineers and led the cloud platform maintenance team of 30 technical engineers for MS project process which gathered transformed  performed Analytics data  Managed responsibilities of scoping and business meetings with business stakeholders and then provided support to the business analysis team in preparing business plans and strategies          011   to   011     Technical System Engineer      Cisco Systems Inc    –              India        Coordinated system development tasks to include design integration  and  formal testing  Created and maintained programmatic and technical documentation to insure efficient planning and execution Designated as an Active SME for Microsoft Project server Team of 30 technical engineers  Awarded Certificate of Excellence for Outstanding Performance as a Best Support Engineer 2nd  4th Quarter 2014  Appreciated for designing a Project Blog site for improvising internal process to post their research and publish ongoing service  Valued for developing a site on Cosmos Server which collaborates the data and generates performance reports  Designed and deployed a Data Breach Detection application for Big Data repositories to identify which user has accessed the component in HDFS for a given day match it against a rules of access table and created a line in a report for users that do not match the allowed access criteria in a rules access file  Project and Change Management Planned project decisionmaking methods on MS project through various project cycles and structured systems development techniques to apply them to the complex world of information development  Information Security Management Evaluated background in managing information security in organizations which included risk identification and assessment security policy and planning personnel and security privacy and security audit          011   to   011     India          Trainee System Engineer      Vishesh Pvt Ltd    –                      Assisted engineers and scientists as they create modify and test products and processes  Performed extensive research and development during creation phase of product          Education      Expected in   May 17     Master of Science     Information Systems     Pace University Seidenberg School of CSIS      New York     NY     GPA       Information Systems        Expected in   August 13     Bachelor of Engineering     Electronics and Communication     Mumbai University      Mumbai          Mumbai          GPA       Electronics and Communication        Professional Affiliations              Skills     Agile Big Data business analysis business plans Change Management CRM client data analysis Data Warehouses Dec decisionmaking Designing Drupal Engineer Information Security managing meetings access Microsoft Access Microsoft Excel MS Office PowerPoint MS project Microsoft Project SharePoint Oct personnel processes test products research sales SQL SQL Server strategy structured systems development Tableau technical documentation Visio Visual Studio website Workflow</data><data key="id">74884865642840805378139428129526365368</data><data key="url">https://www.livecareer.com/resume-search/r/business-data-analyst-intern-1c9c8b9cd081412a830e84b7ce894d2b</data></node>
<node id="n2081" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/tableau-developer-data-analyst-28fc2c68d29c415fb844059b31106408</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Experienced Tableau Engineer with critical analytical statistical knowledge in developing Tableau Dash Boards Performance Tune and prepare insightful Tableau reports Built and published customized interactive reports dashboards and report scheduling Indepth knowledge on Tableau Desktop Tableau Reader and Tableau Server Extensive Tableau Experience in Enterprise Environment and Tableau Administrator experience including technical support troubleshooting report design and monitoring of system usage Performed data audits data profiling activities providing strategic data to BI teams Familiar with Installation Configuration and administration of Tableau Server in a multiserver and multitier environment Working knowledge in Windows Servers API Network technologies with Active Directory SAML for authentication Strong knowledge in SQL PLSQL XML Python programming and understanding of Data modeling Dimensional  Relational concepts Experience in Tableau Administration Tool for Configuration adding users managing licenses and data connections scheduling tasks embedding views by integrating with other platforms like SharePoint Involved in Trouble Shooting Performance tuning of reports tuning long running SQL and PLSQL queries for better performance and resolving issues Involved in Trouble Shooting Performance tuning of reports and resolving issues within Tableau Server and Reports Involved in upgrading Tableau platforms in clustered environment and performing content upgrades Strong understanding of advanced Tableau features including calculated fields parameters table calculations rowlevel security R integration joins data blending and dashboard actions Expertise with data architecture data interoperability and data analytics using Tableau and SQLs Experience in different project methodologies  Waterfall and AgileSCRUM Selfstarter with ability to motivate teamwork and lead competently in diverse groups Decision making ability to make sound Engineering Management judgment Innate problem solver able to intellectually articulate business problems identify critical problems and achieve targeted results Worked closely with ETL developers and stakeholders to assist them with their agile development life cycle Excellent Software Development Life Cycle SDLC with good working knowledge of testing methodologies disciplines tasks resources and scheduling Excellent knowledge in Data Analysis Data Validation Data Cleansing Data Verification and identifying data mismatch Very proficient in working with large Databases in DB2 Oracle Teradata and SQL Server Experience in data modeling with high proficiency in using Erwin for Dimensional Modeling Star and Snowflake schemas Strong understanding of Data warehouse concepts ETL Star Schema Snowflake data modeling experience using Experience in data modeling with high proficiency in using Erwin for Dimensional Modeling Star and Snowflake schemas Normalization Business Process Analysis Reengineering Dimensional Data modeling FACT and dimensions tables physical and logical data modeling        Highlights           BI Tools				 Tableau  ETL Tools           Informatica Data Stage Microsoft SQL Server Integrated Services  Requirements Management      	 Requisite Pro Microsoft Office Word PowerPoint   Excel  Change Management           HP Open View CA Service Manager Lotus Notes  amp Service Management  Front End Tools           Microsoft Project Microsoft Office Visible Analyst  Databases           Oracle 9i SQL Server DB2 Sybase Teradata MS Accessetc  Document management           Visual Source Safe 60 Share point Ultra Edit Documentum  Defect management tool           HP Quality center 100  Programming Languages           C C HTML SQL PLSQL XML Python C  Data modeling           ERwin ER Studio MS Visio Reviewed basic SQL queries and edited inner left and right joins in Tableau Desktop by connecting livedynamic and static datasets  Prepared several analytical reports comprised of different data modeling techniques such as time series analysis financial modeling and trend mapping and supported in MATLAB programming  Collecting data text standardization and performing text mining using different python libraries to create clusters and classify to compare the results and improve search of the platform  Environments SQLPLSQL ETL Oracle Windows Server VMware Active Directory Virtualization Linux Systems Engineering Servers Cisco Systems Products Microsoft Exchange MS Office Suite MS Visio HTML MS Access MS SQL JavaScript HTMLHTML5 CSS3 DOM XHTML AJAX and jQuery Bootstrap AngularJS SQL Server SSIS 2008 SSRS 2008 DB2 TSQL Oracle and SSMS AspNet C JQuery MATLAB Cognos 84101 connection MS Office Suite MS Visio HTML UML C Java MS Visual Basic VB Script Test Director Client Server Windows XP MS Access MS SQL NET Agile SOA                         Accomplishments              Experience       Tableau Developer Data Analyst       011      Present     Alteryx Inc    –         OR            Develop various Tableau dashboards and analytical reports and solutions for marketing efforts on medical products and business groups using Calculations Filters Parameters and complex statistical analytics features of Tableau  Work on technical strategy provided hardware and software configuration understood clientwide BI implementation project design and develop visualizations as per requested by CEO and CFO Adjust viewsdashboards after initial POC presentation to CEO and CFO  Create views and dashboards on end clients sales data  Produce powerful dashboards using pie bar geo and line charts that are viewed daily by CEO and CFO  Developed and modified database procedures functions packages to enhance and improve functionality using PLSQL  Design and development of Tableau software applications to support reporting and Business Intelligence initiatives using data analytics visualization reporting using OLAP Online Analytical Processing data mining SQL UNIX Linux and Java portal technologies Perform Tableau Server admin duties add usersgroups from Active Directory and schedule instances in Tableau Server including weekly data dump  Design and develop workbooks global filter page and complex parameters based calculations  Create action filters parameters and calculate sets for preparing dashboards and worksheets in Tableau Access and transform massive campaign datasets through filtering grouping aggregation and statistical calculation  Develop Tableau workbooks from multiple data sources using Data Blending  Work with team of developers to design and develop BI solutions on various marketing and campaign KPIs  Responsible for the design and development of data warehouse analytics and visualization to meet the big data needs of various products and businesses  Design and deploy rich Graphic visualizations with drill down and drop down and parameters from ERP databases and complex Dashboard and Storyboard Attributes Hierarchies Transformations Filters Prompts Calculated fields Sets Groups  Created organized and formatted reports folders and other contents on Cognos Connection portal  Involved in migration of Cognos adhoc reports to Tableau workbooks  Converted business requirements into technical specification and design for Reporting Databasedata mart and Adhocstandard reporting using Cognos 8BI  Perform query search via SQL knowledge and understanding of Data modeling Dimensional  Relational concepts Design different frontend forms  dynamic pages using CSS HTML  given functionality with JavaScript HTML5 CSS JavaScript JQuery Angularjs Environments Tableau Server Tableau Desktop AgileLinux HTML5 CSS JavaScript JQuery Angularjs Java Portal PLSQL Windows Server Oracle 10g9i Sybase Crystal XI Info view CMS CMC Excel JAVA SCRIPT J2EEPLSQL Tomcat MS Visio MS SQL Teradata IBM Cognos 84101 connection           Sr BI Consultant Tableau Developer       011      011     Galaxy Solutions    –         CT            Created and assisted users in Tableau dashboard development  Created views in Tableau Desktop that were published to internal team for review and further data analysis and customization using filters Trend Lines Statistics and Log Axes  Groups hierarchies Sets to create detail level summary report and Dashboard using KPIs  Developed and designed User Interface using JavaScript HTMLHTML5 CSS3 HTML5 jQuery SASS and AngularJS Performed relational database design and normalization clientserver development OLAP analysis using pivot charts Migrated Cognos dashboards to Tableau showing Year over Year comparison of different key metrics Premiums Vs Losses IBNR etc  Developed Cognos Reports in compliance with user reporting standards Created Tableau extract files for improving the performance  Implemented various Validation Controls for form validation and implemented custom validation controls with JavaScript validation controls  Wrote DDL or DML SQL commands visualizations in Tableau to improve data quality analyzed complex data systems and document data elements data flow relationships and dependencies describing whathowwherewhy data is stored and how to access the data  Created SSIS packages using C scripting script components and script tasks for various Control Flow Tasks and Data flow  Worked with DBAs in order to schedule SSIS packages as jobs either daily weekly monthly  Wrote complex PLSQL queries and designed SSIS packages to load the data into warehouse  Use various SSIS transformations such as Conditional Split Derived Column etc which did Data Scrubbing including data validation checks during Staging before loading the data into the Data warehouse  Developed complex SQL scripts for Teradata database for creating BI layer on DW for tableau reporting  Created the test environment for Staging area loading the Staging area with data from multiple sources           Business Intelligence Analyst       011      011         –    City     STATE            Involved in upgrading Tableau platforms in clustered environment and performing content upgrades  Prepared Tableau dashboards using advanced features including calculated fields parameters table calculations rowlevel security R integration joins data blending and dashboard actions  Created detailed technical design documents  Created Data Model for interface  Performed Logical and Physical data modeling within Tableau Server Experienced in Tableau Server Infrastructure Installation  configuration of Tableau Server in clustered environment and REST API implementation  Involved in the production support and enhancement of the Sharepoint databases  Redesigned standards and procedures and bills of material tables  Created data maps using Informatica Power Exchange 512  Extensively used lookup Expression Aggregator Normalizer transformation Performed Informatica code reviews  Created ETL design for real time data processing  Designed and implemented Informatica maps workflows schedules and scripts  Implemented mapping level data auditing and error handling source to target mapping  Performed performance tuning of Informatica mappings and databases Involved in upgrading Tableau platforms in clustered environment and performing content upgrades  Wrote stored proceduresTriggersViews in SQL server and Oracle Developed implementation plans for the SAP modules PP and PPPI  Implemented Material Variant Object dependencies Variant Table Functions Variant Pricing Order BOM and also managed implementation of PP and PPPI design on SAP 47C release Experience in designing Business View Manager Business Objects universes complex Crystal and BO reports both Full and Thin Client Reports  Developed reports using Business Objects Web Intelligence to cater Adhoc reporting needs  Created reports using Business Objects functionalities like Queries Drill Down Cross Tab Master Detail and Formulaes  Used SQL Server Integrations Services SSIS to extract data ETL and quickly load data into SQL Server table and handle data quality issues  Used SQL Server Reporting Services SSRS  Reporting Services Components and Business Intelligence Development Studio BIDS in order to design and develop reports  Worked in Windows Servers API Network technologies with Active Directory SAML for authentication Environments SAP BWBO SAP Business Objects XI R2 SP2XIR 31 SP3 Designer Infoview Web Intelligence Data Stage 81Sql PLSQL SQL server 2008 Xcelsius 2008Live Office Oracle 10g Windows Server 2003Clear CaseClear QuestMS Excel MS SQL Server 200812 R2 Win Server 2008 R2 Active Directory Oracle Windows Server VMware Virtualization Linux Systems Engineering Servers Cisco Systems Products VMware ESX Microsoft Exchange MS Office Suite MS Visio HTML MS Access JavaScript HTMLHTML5 CSS3 DOM XHTML AJAX and jQuery Bootstrap SASS AngularJS           Business Analyst       082016      112016     NA    –    City     STATE            Worked as a Data Analyst Business System Analyst and assisted the project manager in defining the Scope Best Practices Schedule Business Change Management and Develop Feasibility Study Report for Preliminary Study of the Project and Budget of the Project  Assisted the team members with documentations like Business Requirement Document and Use Case Specification Document by using DOORs and Best Practice initiatives  Assisted in Data Analysis by Data Mapping then worked with the design team in the designing of the Databases with extensive use of ERDs for Data Validation Data Modeling Data Migration Reference Data Analysis and Data Conversion  Designed and developed Use Case Diagrams Activity Diagrams and Sequence Diagrams by using MS Visio  Created a Decision Support System via Dimensional Modeling by ensuring appropriate FACTS tables DIMENSIONS tables attributes and entities in order to generate MEASURES as required in Data Warehousing EDW and Data Migration by business users  Resource monitoring and repository Audit reporting  Worked on Backup and restoration          Education       Bachelor of Science       Chemical Engineering       Expected in                   Oklahoma State University      Stillwater     Oklahoma     GPA        Status         Chemical Engineering        Interests     Tableau Desktop 9 Qualified Associate Activities Youth Mentor Tutor at risk teens Dream Center       Additional Information       Tableau Desktop 9 Qualified Associate Activities Youth Mentor Tutor at risk teens Dream Center         Skills     NET AspNet Active Directory Ad Agile AJAX Analyst API Audit reporting auditing Backup basic big data Budget BI Business Intelligence Business Objects C C Change Management charts Cisco Client Server clientserver CMS Cognos 84 Cognos CA hardware content Crystal CSS CSS3 Client Data Analyst Data Analysis Data Conversion data processing Data Migration data mining Data Modeling Data Validation data warehouse Databases Database Data Stage 81 Data Warehousing Decision Support designing DIMENSIONS DML Document management Documentum DOM DOORs Edit ERP ERwin ETL XML Feasibility Study features financial modeling forms Graphic HP Open View HTML HTML5 IBM DB2 Informatica J2EE Java JavaScript JAVA SCRIPT JQuery JQuery Linux Lotus Notes marketing MATLAB Access MS Access C MS Excel Excel Exchange 512 Microsoft Exchange Microsoft Office MS Office Suite Office PowerPoint PP Microsoft Project Sharepoint Share point Win Windows 31 Windows XP Word migration Modeling Network OLAP Oracle 9 Oracle PLSQL PLSQL page Pricing Programming Python Quality Quest real time relational database design reporting Requirement sales SAP BW SAP SAP 47 Servers scripting scripts script Visual Source Safe 60 Specification Sql MS SQL Server MS SQL Microsoft SQL Server SQL Server Statistics strategy Sybase System Analyst Systems Engineering Tableau tables Teradata Test Director Tomcat TSQL Trend UML UNIX upgrades upgrading User Interface Validation VB Script View Visio MS Visual Basic Web Intelligence Windows Server XHTML</data><data key="id">115865741952916462448075087710526158444</data></node>
<node id="n2082" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Career Overview     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market       Qualifications           DatabasesFile Systems  Oracle  MS Access  Netezza  HadoopBig Data  Amazon Redshift  S3  Software Products  Oracle D2K Forms  Reports 10g  Oracle Apps 11g  Hadoop Impala HiveSpark  Amazon Web services Redshift Programming Languages  Oracle PLSQL  SQL Impala SQL  Python  JavaBasics C  Visual Basic Coding in MS Excel using VBA Power Query  UnixVMS  Software Tools  PLSQL Developer TOAD DB  Visualizer NPS Admin tool   Aginity for NetezzaRedshift  Form  Report Builder  Elastic Search Logstash and Kibana  Cloudwatch Exports to excel  Eclipse PyCharm  MS Office MS Project MS Visio Power Query with Excel  Putty Attachmate Reflection  HPSD Remedy  Service Point  NSI VSS IBM Rational Clear Case  Clear Quest Tortoise SVN GitHub  HP Quality Center  JIRA  Hadoop Hue Cloudera Manager    The details of the various assignments that I have handled are listed here in chronological order  01  TMSIS Transformed Medicaid Statistical Information System  Client Centers for Medicare and Medicaid Services CMS  Location Windsor Mill Maryland  Duration 092015Till date  Role Technical AnalystSenior Developer Architect Data Analyst  Solution Environment Oracle 11g Informatica UNIX Amazon Web Services AWS Redshift                         Technical Skills                Accomplishments              Work Experience       Team LeadSenior Developer Architect Data Analyst       062014      082015     Exela Technologies Inc    –    Stamford     CT            92 years of extensive experience in business analysis system analysis design development and Implementation of various projects with strong experience in Database system and Project management  Good knowledge of Hadoop ecosystems HDFS AWS Netezza Oracle Hive Sqoop Impala and Amazon Redshift  Implemented Proof of concepts on running Hadoop migration from multiple databases Netezza Oracle to Hadoop  Experience in working with different data sources like Flat files XML files and Databases Key participant in all phases of the SDLC ranging from requirement analysis design development and testing unit system integration Regression Product  Ability to design database schemas performance tunes SQLs troubleshoot improve logic and move data through the enterprise  Experience in writing complex SQL queries involving multiple tables inner and outer joins  Experience in Creating Tables Views Triggers Stored Procedures User Defined Functions and other SQL statements for various applications  Streamlined SQL improved speed via query optimization modified tables and enhanced metrics Maintain accurate and organized notes on the status of development projects  Experienced in Application development  Implementation  Agile  Waterfall Software Development Model Clientonsite Client  Offshore and OnsiteOffshore model Change Management Incident  Problem Management Supply Chain Management System  Transportation and Warehousing Media Research  Nielsen Media research  Project management Agile Scrum Master           Team LeadSenior Developer Data Analyst       052012      052014     Perspecta Inc    –    Miami     FL     India       Solution Environment Oracle 11g Netezza 702 TIBCO UNIX Responsibilities Analysis and Design Development  Implementation of Software solutions Team Handling Software Estimation  Impact Analysis  Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Involved in loading data from UNIX file system to Netezza Analyzed the data by performing Netezza queries and reviewed the results with data science team  Revised procedures that solve complex business problems with due considerations for hardwaresoftware capacity and limitations operating times and desired results  Analyzed large amounts of data sets to determine optimal way to aggregate and report on it  Provided quick response to ad hoc internal and external client requests for data and experienced in creating ad hoc reports  RFCs Change Management and Version control  Review of code and Test scenarios  Maintenance  Support of Existing Software applications Status reporting 04  IKEA Cargo Network Systems           Service Delivery Manager       052009      042012     IKEA IT    –    City          Germany       Oracle 10g D2K Forms 10g Open VMS Responsibilities Service Delivery Management Incident Problem and Change Management  Analysis and Design  Implementation of Changes identified in support  Ensuring Ontime delivery OTD First Time Right FTR  Ensuring E2E application is up and Running 247 Status reporting 05  JobJar GE Oil  Gas           Software Developer       052007      042009     GE Oil  Gas    –    City          India       Oracle 10g D2K UNIX Apps 11 Responsibilities Analysis and Design Development  Implementation of Change Requests  RFCs Change Management and Version Control  Software Estimation  Impact Analysis  Ensuring Ontime delivery OTD First Time Right FTR  Requirement analysis and technical design document creation  Involved in developingchanging modules as per Technical design documents  SQLQuery Tuning Created indexes on selective columns to speed up queries and Enhanced performance using optimization techniquesnormalization indexing and Transaction Isolation levels monitoring logs to ensure the completeness of batch processes and scripts  Designed and developed SQL stored procedures to replace existing business logic  Development of indexing schemes for most efficient insert update and select Involved in system testing  Integration testing Status reporting          Education and Training       Bachelor of technology       Computer Science and Engineering       Expected in                                   GPA        Status         Computer Science and Engineering         BTech  CSE              Expected in   2006                Mahatma Gandhi University           Kerala     GPA        Status                  Degree              Expected in   2001                Mahatma Gandhi University Board of Secondary Education           Kerala     GPA        Status                 Skills     ad Agile Analyst Application development Architect automation Big Data business analysis C Change Management CMS hardware Network Systems Client Version Control Data Analyst Data Analysis data modeling Databases Data base Database Delivery Eclipse XML Forms FTP HP IBM indexing Informatica Java leadership logic notes Managing Meetings MS Access MS Excel Excel MS Office MS Project Migration Mill enterprise Oil Open VMS optimization Oracle DB Developer Oracle PLSQL PLSQL processes Coding Programming Project lead Project management Python Quality QA Quest quick reporting Requirement Research Scrum SDLC Set top Box scripts Software Development SQL strategy structured Supply Chain Management system analysis Tables team player TIBCO TOAD Transportation troubleshoot UNIX Visio Visual Basic VBA Visualizer VMS Warehousing written written communication skills       Additional Information       Personal Details Date Of Birth 	 29Dec1983 Nationality		 Indian Marital Status 	 Married Visa Status		 H1B</data><data key="id">8411936289748720167168135634778463630</data><data key="url">https://www.livecareer.com/resume-search/r/team-lead-senior-developer-architect-data-analyst-42a746f6dd5c4ad19f2bdf63a59afc75</data></node>
<node id="n2083" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Career Overview     Experienced Data Analyst with extensive skill in Data Modelling Analysis and Reporting using SSMS SSRS Crystal  Reports and CRM applications Detailoriented professional with proficient technical documentation and project management skills           Education and Training       University of Central Oklahoma    Edmond     OK      Expected in   2014     –      –       BBA        Management Information System          GPA                    Q Tech Solutions         New Jersey      Expected in   2015     –      –       Certification        Business Analyst          GPA                     Work Experience       Addepar      Data Operations Analyst   New York     NY                   102015      082016     Contributed to the Development and Maintenance of data quality standards on Millennium a CRM application  Designed Excel Applications for automated processes using Excel Macro and Visual Basic Editor to analyze evaluate processes and improve divisional efficiency   Provided technical documentation for organizational reports and project designs  Delivered selfservice reports through SSRS  Designed forms and edited reports using Crystal Reports  Generated the Fiscal Year Reports for the Annual Funds division by sorting using aggregate functions Pivot table and VLOOKUP formula in Excel            Saic      Data Analyst Contract   Cape Canaveral     FL                   2015      042015     Committed to multiple projects and tasks during Scrum meetings and decreased 40 of teams workload  Designed business rules for incoming vendors files to designated folders  Analyzed and validated Data using SSMS Designed projects Test Cases tracked and conducted UAT   Produced internal and regulatory reports on regular or adhoc basis on SharePoint           Lockheed Martin Corporation      Technical Support   Vandenberg Air Force Base     CA                   2013      102014     Report track and fix defects on existing PC and Internet connections  Recommending the right Operating System Hard Drives and Disk for compatible assembly and installation  Provide technical assistance with several computer assembly  Managed and assembled the Internet connections through the gateway router to multiple PC both wireless and cabled connection           MYASales      Data Analyst   City     STATE                   2009      032013     Forecast future of business sales by querying and analyzing data for the past and present business activities Reported analytical information for business intelligence that highly impact business decisions  Analyzed daily transactions using financial functions Pivot Table Charts and VLOOKUP with Microsoft Excel   Designed relational database for enforcing data integrity and tracking inventory with Microsoft Access  Managed storage for user and systems databases including database files          Additional Skills       Proficient Technical Documentation skills  Adapts fast to Business Rules   Ability to read and Design Charts and Diagrams UML DFD ERD    Great Team Player and also ability to work independently</data><data key="id">33330076930037456402499499363323372399</data><data key="url">https://www.livecareer.com/resume-search/r/data-operations-analyst-15d1386b5dbe4a6ea5180e060cfae87c</data></node>
<node id="n2084" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-consultant-a4fb979e4586489cae15c4dea7a242aa</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market       Highlights         STATA programming R programming SAS programming GIS NVIVO Microsoft Office                       Accomplishments              Experience       Data Analyst Consultant       092014      012015     CH Robinson Worldwide Inc    –                     Conducting research and assisting with technical analyses of continued issues related to human exposure to contaminants in the environment researching and understanding pollution control operations and assessing the risks to human health and the environment from the application of wastewater effluent in wetland areas conducting human bioassays and other laboratory bench work to determine the estrogenicity of wastewater effluent evaluating pollution control technologies evaluating alternatives to wastewater chlorination and overseeing the training of other laboratory assistants  Primary duties included Provided analytical research and technical support for the Zimbabwe Health Results Based Financing Project Impact Evaluation  Duties include Management of data received in the field building statistical models from the data and assisting with preparation  EXPERIENCES of summary reports and working papers           Research Assistant       062014      Present     St Josephs Healthcare System    –    Paramus     NJ            Primary duties include Applying epidemiologic methods to understand DNA abnormalities in male reproductive health using fluorescence in situ hybridization FISH techniques in the GW Mens Health Study MHS population Evaluating data collection conducting data quality control and utilizing statistical methods to study epidemiological problems in reproductive public health Applying epidemiologic methods to identify and interpret associations between FISH data urinary biomarker data and selfreported questionnaire data from the MHS population sample collection and analysis critically reviewing abstracting environmental health research Overseeing research proposals grants and manuscript preparation Designing planning and initiating epidemiologic studies using available sources of public health data Overseeing the recruitment of study participants and data collection in the MHS and managing lab associates and graduate students in epidemiologic study protocols and laboratory experiment protocols           Environmental Health and Safety Intern       062013      082013     Alamo Group    –                     Primary duties included Critically reviewed environmental safety and health regulations devel oped Board procedural programs and operating guidance on healthenvironmental risks assessment reviewing workplace projects and providing training in health and safety procedures to Federal Reserve Board employees reviewing and interpreting federal hazard communication legislation to assure Board compliance wrote evaluation reports documenting compliance and recommendations writing program documents identifying roles and responsibilities in areas such as hearing conserva tion respiratory protection etc           Graduate Teaching Assistant       082012      122015     Ohsu    –                     Primary duties included Assisting with various environmental and occupational health courses including Environmental and Occupational Epidemiology and Toxicology Implications for Public Health Policy managing students over multiple courses leading organized classroom discussions serving as liaison between instructors and students and grading of assignments and exams          Education       Doctor of Public Health              Expected in   Present                George Washington University Milken Institute School of Public Health           Washington DC     GPA        Status                  Master of Science       Public Health       Expected in   May 2012                Tulane University School of Public Health      New Orleans     LA     GPA        Status         Public Health         Bachelor of Science       Biology       Expected in   May 2010                Tougaloo College                GPA        Status         Biology                Health Science       Expected in   May 2012                Tulane University School of Public Health                GPA        Status         Health Science        Work History                       Department of Environmental and Occupational Health   –                    Publications     Grant Reviewer          August 2014 Reviewed health program grants for the District of Columbia Department of Health DOH HIVAIDS Hepatitis STD and TB Administrations HASTA 2014 HIV Prevention Special Programs and Needle Exchange Program Claire F Shimony M  Perry M J Urinary Heavy Metals and Associations with Body Mass RECENT CONFERENCE Index and Waist Circumference Among Adolescents and Adults National Health and Nutrition    PRESENTATIONS Examination Survey 20112012 Poster presentation at the American College of Epidemiology Conference September 2015 Decatur GA Claire F Woodruff T J  Zota A R Vaginal douching and racialethnic disparities in phthalates exposures among reproductiveaged women Oral presentation at the 142nd APHA Annual Meeting and Exposition November 2014 New Orleans LA    PUBLICATIONS SOFTWARE Claire F Woodruff T J Mitro S D  Zota A R 2015 Vaginal douching and racialethnic disparities in phthalates exposures among reproductiveaged women National Health and Nutrition Examination Survey 2001­2004 Environmental Health 141 57       Skills     data collection Designing DNA environmental health Epidemiology experiment Financing GIS grants managing Microsoft Office occupational health programming proposals protocols Public Health quality control recruitment researching research safety sample collection SAS STATA technical support</data><data key="id">310342702130426494587696481019184318546</data></node>
<node id="n2085" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary      Human Resources Professional with 11year background in SAPHR experience emphasis on employee organizational and payroll data and training Highly detailoriented and organized Advanced analytical and resolution skills Excellent presentation and organizational skills  Demonstrate effective written and verbal communication sound judgment and work ethics  Ability to supervise staff members establish good working relationships with clients and colleagues        Experience       International Data Analyst       022011   to   112011     Chevron Corporation    –    City     STATE            Implemented expatriate policies and business processes in Global Advantage IHR GA IHR optimized the system to facilitate the expatriate administration processes conducted business process reviews and utilized knowledge sharing and standardization to create expatriate administration best practices    Processed semi monthly payrolls for 3135 expatriates to and from the United States including created maintained audited remuneration templates as well as researched and corrected variances   Provided leadership in the use of the software and continually searched out and implemented business process improvements   Maintained data integrity inbound interfaces system knowledge and functionality   Collaborated with Human Resources Information Management Group Human Resources Service Center Expat Team Payroll Operations on SAPHR updates and answered employee questions regarding elements of overbase compensation and hypothetical taxes           Organizational Data Analyst       092006   to   022011     Chevron Corporation    –    City     STATE            Serviced a population in the SAP  HR system of 28965 employees in domestic and international locations     Processed merit increases and other SAP actions using the Compensation Tool and the Salary Planner for the respective populations   Managed batch updates for uniongeneral increases Cost of Living Adjustments market adjustments and 3x3 ratings  Conducted analysis and executed retroactive processing  Responded to detailed and complex inquiries from the Manager Human Resources Information Management telephoneemail requests from Line Managers and Human Resource Representatives           Team Leader Human Resources Service Center       012009   to   072009     Chevron United Kingdom Limited    –    City     STATE            Led the daytoday activities of the Human Resources Service Center HRSC for nine HRSC Administrators   Allocated workload using Transaction Management System and ensured that service center adhered to servicelevel agreements Actively used sound judgment by facilitating complex customer service issues and produced systematic tangible resolutions   Produced budget analysis for the 2009 financial plan comprised of scenarios ranging from 5 10 and 20   Constructed a detailed resource analysis report using Crystal Ball software to substantiate the number of skilled Full Time Employees FTEs essential to operate and maintain the stability of the HR Service Center results 525 FTEs   Reestablished and strengthened partnerships with third party vendors and empowered team members to make linelevel decisions           Senior Personnel Data Maintainer       102000   to   082006     Chevron Corporation    –    City     STATE            Started as a Personnel Data Maintainer and quickly promoted to Sr Data Maintainer position leading the daytoday activities of the Human Resources Information Management US Group for 27 Personnel Data Maintainers PDMs that included   Participated in compiling and analyzing data developed recommendations for process improvement and project enhancement and project development efforts Provided support and problem resolution for managers and employees on SAP tools i e Manager’s Desktop and Employee Self Service  Responded to initial systems startup complexities and variable processes acted as a troubleshooter and mentored Junior PDMs   Accurately analyzed and entered management requests into SAPHR for transactions such as hiringrehiring transfers promotions salary changes leaves of absence terminations payroll taxes etc  Assisted in creatingmaintaining positions and organizational structure within SAPHR including generation of reports and analysis of position data  Performed data audits and clean up  Completed work required to accomplish reorganizations within SAP          Skills       HRIS Software  SAP – Human Resources Module Business Warehouse  Global Advantage  MicroSoft Office Products Excel Word PowerPoint Outlook Visio Access  Ariba Virtual Edge Org Publisher Lotus Notes KnowledgePlanet Learning Management System LMS Transaction Management System TMS Security Watchdog         Education       Associate of Science     Business Administration     Expected in   2014     Merritt College      Oakland     CA     GPA        367 GPA          Certificate of Completion     PICXEROX Word Processing Program     Expected in   1987     John Adams Community College      San Francisco     CA     GPA</data><data key="id">54369516354309618230740753789996227753</data><data key="url">https://www.livecareer.com/resume-search/r/international-data-analyst-8cec99a374a3408496f0173a40d50a17</data></node>
<node id="n2086" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/health-data-analyst-8c95efed056d435c945df8ee2bd3b9dc</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Professional Summary      Experienced Data Analyst with over 17 years of experience in Health Informatics Excellent reputation for resolving problems improving customer satisfaction and driving overall operational improvements Expertise in policy development and staff management procedures positively impacting overall morale and productivity        Skills           Electronic health records  Data privacy applications  Applicationdatabase design      Data management  Database and user interface software  Research and data analysis                       Work History        092001   to   072019   Health Data Analyst    Logistics Management Institute         Chantilly     VA            Clinic EMREHR Administrator Duties included consensus building vetting software solutions to meet changing needs and arranging demonstrations  Worked with users prioritizing their requirements then creating Purchasing Request for Proposals  Led users through the scoring of Proposals to determine a clear cut software solution for the Clinic  Collaborated with Purchasing and Law to create a contract that met all departmental needs  Extractedscrubbed data to meet conversion requirements  Implemented kick off and go live dates arranged cloud and server based software installation with Networking planned onsite user training and ongoing training meetings  Led similar processes for Laboratory Information Management Systems interfaces with Orchard Harvest and HL7 with ShowMeVax  Responsible for monitoring troubleshooting contract compliance software customization and training of all Clinic programsemployees  Successfully completed these steps converting EHR to Cerner Netsmart Insight Patagonia Health EHR  Conducted statistical analysis of health data  including interpretation and reporting of results to various levels of management  Ensured integrity of data collection data entry and statistical analysis  Administered departmental databases Maintained securityconfidentiality of data  Upheld departmental surveillance capabilities  Developed and analyzed complex project activities such as algorithms and workflow diagrams  Conducted all HIPAA training for new and existing employees as the HIPAA Compliance Officer  Audited investigated and reported findings of HIPAA complaints  Assured all medical records met with HIPAA security regulations  Provided Help Desk functions to 130 users Supported software included Windows XP 2010 Office 1997360 Adobe ReaderDeluxe and Google DocsSheets  Installed computers monitors printers scanners faxes copiers modems and routers  Promoted use of video cameras cell phones WiFi portable printers and IPads to achieve paperless processes          Education        Expected in      MBA       Business Administration    Alabama A  M University     Normal     AL      GPA         Graduated summa cum laude            Expected in      Bachelor of Science       BankingFinance    Missouri State University     Springfield     MO      GPA               Affiliations       Member Emergency Response Team  2002 to Current         Skills       Electronic health records  Data privacy applications  Applicationdatabase design    Data management  Database and user interface software  Research and data analysis         Work History        092001   to   072019   Health Data Analyst    SpringfieldGreene County Health Depatment       Springfield     MO     Clinic EMREHR Administrator Duties included consensus building vetting software solutions to meet changing needs and arranging demonstrations  Worked with users prioritizing their requirements then creating Purchasing Request for Proposals  Led users through the scoring of Proposals to determine a clear cut software solution for the Clinic  Collaborated with Purchasing and Law to create a contract that met all departmental needs  Extractedscrubbed data to meet conversion requirements  Implemented kick off and go live dates arranged cloud and server based software installation with Networking planned onsite user training and ongoing training meetings  Led similar processes for Laboratory Information Management Systems interfaces with Orchard Harvest and HL7 with ShowMeVax  Responsible for monitoring troubleshooting contract compliance software customization and training of all Clinic programsemployees  Successfully completed these steps converting EHR to Cerner Netsmart Insight Patagonia Health EHR  Conducted statistical analysis of health data  including interpretation and reporting of results to various levels of management  Ensured integrity of data collection data entry and statistical analysis  Administered departmental databases Maintained securityconfidentiality of data  Upheld departmental surveillance capabilities  Developed and analyzed complex project activities such as algorithms and workflow diagrams  Conducted all HIPAA training for new and existing employees as the HIPAA Compliance Officer  Audited investigated and reported findings of HIPAA complaints  Assured all medical records met with HIPAA security regulations  Provided Help Desk functions to 130 users Supported software included Windows XP 2010 Office 1997360 Adobe ReaderDeluxe and Google DocsSheets  Installed computers monitors printers scanners faxes copiers modems and routers  Promoted use of video cameras cell phones WiFi portable printers and IPads to achieve paperless processes</data><data key="id">135985736398681668928822491880823880091</data></node>
<node id="n2087" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-quality-analyst-9bca8405034641f2a82029632b8736f3</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Professional Summary      Data Quality Analyst  Data Enhancement Strategist  Efficiency Specialist  Goal Oriented  Market Research  Executive Assistant  Organizationally Inclined  Strategic Planning  Superior Attention to Detail        Core Qualifications         Oracle Forms Google Drive Google Docs Google Sheets Microsoft Office Made to Manage                       Experience        072014   to   Current   Data Quality Analyst    Vanguard         Washington     MN            Analyzing and improving data from multiple sources including State Licensing Boards National Provider Identifier NPI Drug Enforcement Agency DEA and American Medical Association AMA among others  Managing and scheduling all NPI AMA HIN CPI DEA Chemical HCO and NCPDP data  Supporting customer driven data analysis for Pharma and device companies relating to Health Care Practitioner HCP and Health Care Organization HCO data  Assisting Customer Service in resolving data inquires discovered by customers  Creating developing and diligently completing adhoc data projects internally and for customers such as Regeneron Shire ABSG Merck Endo Boehringer Ingelheim and Astra Zeneca  Establishing data quality best practices for data matching rules address verification and methods to link various sources of data together  Formulating data rules based on observed data patterns to increase data linking efficiency rates  Assisting PASID team in completing yearly Aggregate Spend reports and files  Quality checking all company SOPs for audits done by customers and clients  Executing and refining test scripts to validate new and updated MedPro system functionality            082013   to   072014   Executive Assistant to CEO    Kronospan Holdings Ltd         Anniston     AL            Provided exemplary efficient and dynamic administrative support including coordination and scheduling all onoffsite meetings and events  Planned and executed all travel arrangements for the owner and all other employees  Confidentially assisted the owner with personal and business related matters  Created a new inventory system managed all incoming and outgoing inventory and related data entry  Produced and generated daily weekly and monthly sales reports  Drafted documents ensured deposit amounts were on track to meet weekly and monthly forecasts  Conducted extensive research on emerging markets  Recorded transcribed and distributed meeting minutes  Completed monthly expense and budget reports  Built and maintained reference files retrieved documents and reference materials  Answered and managed incoming calls and client relations  Assisted project managers in meeting project deadlines and sales goals  Received and interacted with clients and other tasking as may be needed            122011   to   032013   Executive Assistant to Chief Executive Officer CEO    Iheartmedia Inc         Mansfield     OH            Conducted extensive research on emerging markets providing superior efficient and dynamic administrative  support to include coordination and scheduling all onoffsite meetings and events planned and executed all travel arrangements  Maintained confidentiality in personal and business related matters for CEO COO and executive team  Recorded transcribed and distributed meeting minutes completing monthly expense reports  Ensured deposit amounts on track to meet weekly and monthly forecasts  Received and interacted with clients monitored managed and marketed companys social media regular maintenance in updating files inventory controlmanagement and other tasking as needed  Compiled and researched all background information for meeting preparation  Retrieved documents and reference materials  Built and maintained ever expanding client profiles  Successfully aided in moving the corporate headquarters from Chicago IL to            012011   to   092011   Market Research Analyst    Emerge Tampa Bay Tampa Bay Chamber Of Commerce         City     STATE            Researched career development of young professionals in the Tampa Bay area  Lead focus groups comprised of current memberships and conducted indepth interviews  Compiled and entered data for SWOT analysis and eventual use in determining future of organization            012009   to   012011   Personal Assistant to Chief Operating Officer COO    PGA Tour         City     STATE            Maintained office records for personal inhome office  Filing and other administrative work  Developed an organization system for monthly expenses and records  Handled and coordinated all related correspondence with professional demeanor and maintained strict confidentiality          Education        Expected in   2012   Bachelor of Science       Business and Marketing International Studies International Business    The University of Tampa     Tampa     FL      GPA       Business and Marketing International Studies International Business          Expected in   2008   Diploma           Paris Sorbonne University                GPA                 Expected in                 Episcopal School of Jacksonville     Jacksonville     FL      GPA               Professional Affiliations              Accomplishments       Strategized and implemented multiple data enhancements internally to decrease the amount of time necessary to link data subset information  Developed and standardized core QC protocol for all internal data quality checks  Increased data linkage rates by a minimum of 30 for every weekly data cross reference  Exceeded every numerical data target set by manager weekly and quarterly         Skills     administrative administrative support administrative  support ad Agency budget CPI client clients client relations Customer Service data analysis data entry expense reports Filing focus Forms home office inventory inventory control Managing materials meetings Microsoft Office office Oracle Quality updating files research sales sales reports scheduling scripts SOP travel arrangements</data><data key="id">326780800171399887218060523612121117149</data></node>
<node id="n2088" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/program-specialist-data-analyst-9d5202e0e49d469dbfeb41c5f95e3bde</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional Summary     Healthcare Informatics Specialist professional with experience managing enterprise implementations of healthcare information System Master Trainer of multiple VISTA packages Experience in gathering analyzing and defining data and functional requirements and designingreengineering processes workflows and technology solutions for healthcare system Proven ability to lead seamless implementations and deliver technical support and solutions improving workplace productivity           Skills         Guest services  Inventory control procedures  Merchandising expertise      Loss prevention  Cash register operations  Product promotions                     Education       Beville State College    Sumiton     AL      Expected in   1997     –      –       LPN        Nursing          GPA        2007   received the Birmingham VAMC Woman of the Year Award   Inducted in the National Honor Society          Work History       Veterans Health Administration      Program SpecialistData Analyst   Boise     ID                   012013      2014     Acute and Specialty Care Developed a Share point to assist the service with managing and monitoring processes such as  provider DEA BLS and ACLS renewal requirements Coordinate all ConsultAccess NATS Developed an excel report tool for 14 services to provide weekly updates on access Oversight and Coordination of all data by developing and running reports utilizing multiple resources such as VSSC Cooperate Data Warehouse File Man Reminder Patient List TIU Wait List Ad Hoc and other Vista Tools for 14 services to review monitor and improves processes and access Provided handson and written training to service leadership and staff regarding Consult Management and Tracking Recall Reminder Management Inpatient Workload Credit EConsults NonVA Care Templates Encounter Forms  and others as requested  Collaborate with several multidisciplinary teams to assist with improving processes and access  Provide advice to Specialty Care Team Nurse Practitioners on identifying annual goals and provide tools to measure progress  Designed Sharepoint masterpage and page layouts serving as companys main Sharepoint support for all technical complications  Consistently met deadlines and requirements for all production work orders  Aligned office departments and increased interdepartment communication and data sharing           Birmingham VAMC      Clinical Application Coordinator   City     STATE                   102007      012013     Manage the Computerized Patient Record System CPRS infrastructure and content plan and implement clinical software applications and patches in accordance with VHA VISN medical Center and service goals and initiatives  Collaborate with multidisciplinary teams for the management of the Computerized Patient Record System and implementation of various informatics projects within the Medical Center  Teach entire BVAMC staff students and residents on the use of the Computerized Patient Record System and create training materials using Microsoft Word Microsoft PowerPoint and Microsoft Publisher  Developed and maintained Share Point Database for employee training  Provide status reports using Microsoft Excel on Computer Applications to various Departments  Create and run reports using File man Reminder Extracts and Reminder Due Reports for service line departments within the Medical Center  Currently involved with several process improvement teams to assist service line departments with developing new processes and systems to meet facility goals  Manage Appointment Recall Reminder Software Facility Lead Reminder Builder Manage Womens Health Software Manage Mental Health Assistance Tool Developed a detailed educational power point tool that is currently being used nationally to teach Clinical Application Coordinators how to build a complex order set for the Department of Palliative Care  Coordinated and lead multiple small learning sessions via live meeting regarding the comfort care order set  Developed a survey tool via share point to assess multiple facilities implementation progress  Collaborate with Computer Specialist programmers clinicians and other users on the development of functional clinical systems and to ensure that automated clinical procedures are maximized for efficiency and ease of operation  Developed and run File man and Reminder reports analyzing data to identify processes specific patient populations and opportunities for improvements  Facility Lead Reminder Builder           Birmingham VAMC      Websites Content Manager   City     STATE                   062004      102007     Building and updating content  Trained and developed training resources for multidiscipline on the use of CPRS Recall Reminder Software Vista Bed Management System Medical Support  Primary Care Coordinated patient notification and followup for Pap smear Mammogram and Fecal Occult Blood Test Results for Facility and CBOCs  Developed Excel Database to tract patient results and followup for Mammograms Pap Smears and Fecal Occult Blood Tests Implemented the Womens Health Software Developed Womens Clinic instructions guidelines for procedures performed in clinic Coordinated Breast Cancer Awareness Month activities LPN  Medical Surgical Unit           Birmingham VAMC      Health System Specialist   City     STATE                   2014      Current     Designed Sharepoint masterpage and page layouts serving as companys main Sharepoint support for all technical complications  Consistently met deadlines and requirements for all production work orders  Aligned office departments and increased interdepartment communication and data sharing          Accomplishments     Certified Master Project Manager 2014 5 STAR AWARD 2014 Certified COTR 2013 Woman of the Year 2007       Skills     Adobe Photoshop Ad Computer Applications content Credit Data Warehouse Database Forms functional GUI leadership Managing Mental Health Access Microsoft Excel Excel Microsoft Office Outlook Microsoft PowerPoint Power Point Microsoft Publisher Word Microsoft Word NATS process improvement processes progress Project Management Project Leadership Six Sigma employee training training materials VISIO Vista written</data><data key="id">288790633340663193176927144292896565239</data></node>
<node id="n2089" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-intern-99eee796dba740228bf75fb6a11030ec</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    QUALIFICATIONS                      •   Over two years experience of data analysis at Loyola Chicago University providing statistical analysis plan and generating report tables graphs proficient in R and SAS   •   Proficient in Microsoft Office Word Excel and Power Point   •   Ability to effectively organize and manage multiple assignments with hard deadlines   •   Great team player and extensive experience and skills in collaboration and communication          PROFESSIONAL EXPERIENCE       Data Analyst Intern       092016      Current     Envestnet    –    Secaucus     NJ                         •    Merge spreadsheets from different sources to create a more compact and centralized data platform for easy updating and data analysis using MS Excel advanced functions and formulas                •    Data cleaning and data validation of existing spreadsheets to promote a robust data management platform for accurate data entry and data analysis                •    Train employees on the use of pivot tables and tips and tricks in MS Excel for efficient data entry and analysis          Research Assistant       072013      042014     Suny Upstate Medical University    –    Syracuse     NY                             Supervisor Drs Michael Crum and David Rayome  Paper Selfemployment and subjective wellbeing A multicountry analysis using the world values survey International Journal of Entrepreneurship Published in 2014  •    Statistical analysis of elements influencing selfemployment and life happiness using R studio  •    Literature research on topics of selfemployment and satisfaction              Accounting Internship       042012      072012     Amica Mutual Insurance    –    Park Ridge                                  •    Provide data management for Accounting Journals using governmentsecured software   •    Prepare written report of Asset inspection and sorted the documents             Finance Internship       042011      072011     Siemens Corporation    –    Wichita               •    Sales consultant for financial products and services        EDUCATION       Master of Science       Applied Statistics       Expected in   2016                Loyola Chicago University      Chicago     IL     GPA        Status                  Bachelor of Science       Finance and risk management       Expected in   2013                Northern Michigan University      Marquette     MI     GPA        Status                 CERTIFICATION     SAS Certified Base Programmer for SAS 9 Issued Jul 15 2016 No BP061446v9       SKILLS                    Computer Software MS Word PowerPoint Excel Access Statistical  Software R Studio SAS Minitab SQL  Language Mandarin and English</data><data key="id">254685534432433690779542140002605509988</data></node>
<node id="n2090" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/business-data-analyst-95fdcb6a01db4b688f7117d917c807ea</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Profile      To creatively contribute my positive quota to enhancing viable growth in an organization where individuals are not limited career wise Highly talented dynamic knowledgeable and experienced savvy competent with professional engineering and management experience Seeks to obtain a challenging position where I can utilize my skills both in public and private sectors government agencies foundations corporations and local firms Possess an excellent organizational and analytical skills and the ability to get results Proven ability to manage with consistent results Adopts a systematic approach to problem solving and effectively analyze results to implement solutions Proficient at making quick and effective decisions in stressful situations Dedicated and committed willing to take on challenging roles tough assignments and work to tight deadlines A selfmotivated professional with an excellent track record in a high volume position A strong commitment to quality customer service coupled with firstrate communication skills builds significant customer loyalty Proven ability to handle a diverse customer base resolve problems and process transactions quickly and accurately ensures increased productivity and efficiency 10 years experience in the Customer Service Management Project Management and Quality Control Helped company attain the highest customer service ratings as determined by external auditors Led a team that increased CAP Plcs turnover in the regional office by over 300 within 2 years by creating new opportunities for the company Recommended Service Level Agreements to enhance customer experience for new and existing customers which led to a 3 increase in renewals and a 25 increase in value of units sold Implemented and enforced ISO 9001 engineering procedures Developed and administered engineering concept preliminary and final design review processes Good command over English Extensive knowledge of computer Pleasing personality     COMMUNICATION SKILLS    Excellent communication skills teamed with the ability to develop rapport with employees peers and clients Confident in dealing with individuals of all levels to provide quality client services    PERSONAL ATTRIBUTES    Highly organized and efficient flexible and versatile team player who works hard to excel in all environments Impressive work ethic reliable dependable and conscious of duties and responsibilities A positive approach to all tasks and pride in achievements has resulted in many successes Ability to form good working relationships with both my peers and staff             Areas of Expertise        Microsoft Word Access Excel and PowerPoint                      Education                      Expected in   2005     –      –       MBA        BScChemical Engineering          GPA           BScChemical Engineering                        Expected in   1998     –      –       Senior School Certification Examination   GCE O Level                  GPA                                   Expected in   1993     –      –       ITIL								Certified Customer Service professional Fellow LAGOS BUSINESS SCHOOL LBS             			Improve Personal Effectiveness TOM ASSOCIATE       						Customer Service Excellence EXCELLENCE MGT SUPPORT LTD      			Customer Relation Management HIGHWAY LIMITED                      				Effective Relationship Selling Skill NIGERIA CORROSION ASSOCIATION     			Corrosion Control of Facility PROSELL CONSULTING					Advance Selling Skill GREAT ORANGE VALUES CONSULT			Relationship Marketing  Customer Service Research                   GPA                                   Expected in        –      –       MASTERS        BUSINESS ADMINISTRATION          GPA           BUSINESS ADMINISTRATION          Professional Experience       Affinity Solutions      BusinessData Analyst   San Jose     CA                   012009      011     Process and audit charges  Billing analysis and evaluation  Monitor trends and applicability  Evaluate and assess client needs and product  Generate reports creates spreadsheets and develop databases and enter new data information Training and mentoring staff           Champion Windows      Human Resources Specialist Motivational Learners        OH                   012007      012009     Responsible for staff orientation programs  Managed training materials Scheduled training sessions  Responsible for departmental and unit support activities  Administrative and secretarial duties as needed Evaluated training programs for effectiveness           Chemical  Allied Products Plc      Customer Service Manager   City          Nigeria              012001      012007     Developed Managed and Marketed paint sales Managed the regional office profitably while ensuring the smooth running of sales activities Ensure consistency in provision of excellent ambience for our customers as it relates to companys actual standard  Implemented the Customer Relationships Management strategies and the Customer Service Policy Offer continual professional advice to the Dulux Agent in achieving goals and objectives Ensures that at all times Dulux exceeds the expectation of its customers          Affiliations               Skills      Administrative Billing C CONSULTING client Customer Service databases ITIL mentoring Access Excel office PowerPoint Microsoft Word paint Relationship Marketing Research Selling sales secretarial spreadsheets training materials training programs</data><data key="id">166735091700759635850696736844214280442</data></node>
<node id="n2091" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/epidemiologist-data-analyst-95d4faa01e614c3fa3fbcf4a23b5bd79</data><data key="resume">Jessica    Claire                                 609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor   Home   555 4321000        Cell           resumesampleexamplecom                  Summary      Detail oriented and team focused epidemiologist with 8 years of experience working as an epidemiologist and statistician for public health agencies        Skills           Statistical Analysis and Coding SAS R SQL EpiInfo  Data Visualization Tableau Arc GIS      Microsoft Suite Excel Access Word                       Experience      072021   to   Current     EpidemiologistData Analyst       California Department Of Public Health    –    City     STATE             Current epidemiologist on the Epidemiology and Surveillance Team within the COVID19 California Department of Public Health CDPH response responsible for maintaining and providing casebased surveillance and epidemiologic data for COVID19 cases in the state of California  Generated daily and weekly reports identifying COVID19 trends including case rates death rates test positivity and health equity measures  Monitored data quality of death duplicate and reinfection data and worked with individual counties and other partners  Led efforts to create a process for deduplicating COVID19 reinfection data by working closely with the data team and responding to feedback from individual counties  Assigned to the MPOX emergency response for 8 weeks and produced daily reports and visualizations using SAS and R Led efforts to improve on LGBTQIA representation in data  Created a staff roster using SAS from several different teams on the emergency COVID19 response Combined multiple datasets and created a new dataset with unique variable key  Assisted with coding of COVID19 racial equity analyses in SAS          122017   to   072021     Research Statistician IV      Hawaii Department Of Health    –    City     STATE             Developmental Disabilities Divisions research statistician responsible for submitting annual reports to national agencies consulting on data needs assisting in divisions new IT project and providing data quality control  Analyzed variety of data sources including fiscal data Medicaid claims and demographic data  Coordinated efforts to transition entire division to new database system Trained staff on data analysis concepts and methods and use of Microsoft Excel and new system  Recommended data quality improvement policies and standards  Assisted with legislative reports and generated frequent data reports to Division Director  Lead of PPE Distribution efforts to providers across the state of Hawaii during COVID19 pandemic when the division was assigned responsibility for allocating donated PPE Allocated millions of units of PPE across hundreds of providers by analyzing needs versus requests Coordinated hundreds of allocations with donors distribution centers and organizations          072017   to   122017     Research Statistician III      Hawaii State Hospital    –    City     STATE             Research statistician for the Departments Maternal and Child Health Branch  Collected and analyzed data from reproductive health care providers around the state and produced weekly reports using Microsoft Access  Excel  Assisted supervisor in providing data for annual grants  Submitted a poster that was accepted for a national maternal and child health conference          072015   to   072017     Epidemiology Fellow      CSTEHawaii Department Of Health    –    City     STATE             Two year epidemiology fellowship through the Council of State and Territorial Epidemiologists funded through CDC and assigned to Hawaii Department of Health  Provided epidemiological support for maternal and child health capacity in Hawaii  Developed multiple research projects for adolescent health injury prevention teen births and infant mortality  Proficient analysis of multiple data sources Behavioral Risk Factor Surveillance System Youth Risk Behavior Survey Pregnancy Risk Assessment Monitoring System Vital Statistics Medicaid claims data using SAS  Produced reports for grant applications and legislative briefings  Developed surveys for different audiences and evaluated surveillance systems  Participated in Hawaii Dengue outbreak investigation in 2015  Presented oral sessions and posters at national and statewide conferences trainings and workshops  Mentored graduate student interns working on oral health  Evaluated programs and projects and received opportunity to attend national program evaluation training          Education and Training      Expected in   052015     Masters of Public Health     Epidemiology     University of Georgia      Athens GA          GPA         UGA Dual Degree Program obtaining masters and bachelors degrees within 4 years  Coursework clinical epidemiology modeling infectious disease PhD course biostatistics environmental health health policy clinical trials and health promotion and behavior  39 GPA          Expected in   052015     Bachelor of Arts     International Affairs     Univeristy of Georgia      Athens GA          GPA         Honors International Scholar 2014  Zell B Miller Scholarship Recipient 20112015  39 GPA          Languages       English Mandarin Chinese            Native Bilingual      Negotiated                     Activities and Honors       Most outstanding scientific writing and data poster runner up City Match 2016 conference</data><data key="id">255542278549317475944547931663777430557</data></node>
<node id="n2092" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                      San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home           Cell           resumesampleexamplecom                  Professional Summary     I have a thirtyyear long career in higher education The focus of my expertise is data analysis with focuses on visualization and predictive modeling techniques I have also developed a skill set in statistical software packages such as SPSS Tableau EXCEL etc I have working knowledge in SAS and Python I now intend to switch my career which has the primary focus in data analysis visualization andor research with a remote option My main goal is to actualize my potentiality in data analysis field which is my main area of interest ie primarily visualization and advanced quantitative techniques I seek a part or fulltime position that offers professional challenges utilizing my data analysis and visualization skills       Skills           Data analytics 10 years  Microsoft Excel  SPSS  Tableau  ArcGIS  Some Knowledge Python  Working Knowledge SAS  Literature Review  Regression Analysis  Analysis Skills  Microsoft Office  Academic Trend  Analyst Tutoring  Curriculum Development Author  Claire  DATA ANALYSIS  Data collection  Database      Delivery  Forecasting  Grant writing  Grant proposals  Grants  Instruction  Interpretation  Materials  EXCEL  Microsoft Excel  Microsoft Office  Pp  Lisa  Modeling  Presenting  Presentations  PUBLICATIONS  Python  Research  SAS  SPSS  Statistics                       Work History         to        Lead Data Analyst      Advantagecare Physicians    –                      Assisting team members to clean data  Analyze data by utilizing visualization techniques on STEM education  Forecasting the trends  Presenting findings to the project team             to        Professor   Homeland      Elizabeth City State University    –    City     STATE             Used variety of learning modalities and support materials to facilitate learning process and accentuate presentations  Encouraged class discussions by building discussions into lessons actively soliciting input asking openended questions and using techniques to track student participation  Analyzed data on student retention completion employment and success on national certification exam  Developed and implemented grants for program funding  Generated grant proposals to gain funding for redesigning courses incorporating STEM component in a social science course and instructional development in 2021 winning 22000  Participated in grant endeavors for more than two million dollars  Collaborated with tutoring enrollment testing and advising services to support students  Developed and implemented multidisciplinary research teams  Developed and taught undergraduate courses in for department offerings serving major minor and general students  As the coordinator for two programs and their assessments conducted ongoing program assessment enrollment and retention tracking  Contributed to planning appropriate and engaging lessons for both classroom and distance learning applications  Created materials and exercises to illustrate application of course concepts  Assisted and identified atrisk students to eliminate student barriers to learning  Taught diverse student population by employing various learning styles and abilities  Incorporated instructional technologies in course delivery for both inclass and online instruction             to        Assessment Coordinator  Data Analyst      Homeland Security Elizabeth City State University    –    City     STATE             Accelerated annual assessment data collection and data analysis  Visualization via SPSS EXCEL Tableau  Reduced errors in assessment data  Developed quantitative metrics for the department and identified areas for students strengths and weaknesses  Optimized and streamlined existing data models  Increased efficiency for analysis in Tableau  Create annual assessment reports based on visualization  Synthesizing and summarizing findings for presentations to communicate assessment results to larger audience  Collaboration with Assessment Research team members both at program and university levels             to        Lead Data Analyst  Coordinator      Elizabeth    –         STATE             Grant MidAtlantic Consortium For Academic Excellence Developed questions for satisfaction surveys of students  Data cleaning and finalizing for analysis  Utilizing data to establish and foster security knowledge among students at higher education  Create and establish programs to encourage growth and professional development among college students  Assisted project team members in finalizing reports with data visualization and interpretation          Education      Expected in   2020               Minot State University      Dakota     North     GPA       Feasibility Study Survey on launching Community Studies program North Dakota  “Relative Effects of Modernity Survival and Tradition Selfexpression Values on the Attitudes About Environmental Philanthropy A Multilevel CrossNational Analysis” Journal of International Education  Human Development 62 Data Analysis Contribution Utilizing World Values Surveys for the waves from        Expected in   2015                               GPA        “Peoples Perceptions about Modernity and Self Expression Values as Determinants of the Beliefs in Islamism in Pakistan” Journal of Education  Social Justice 31 5774 Data Analysis Contribution Utilizing World Values Surveys for the waves from        Expected in   2014                               GPA        “CrossNational Comparison of the Roles that Economic Reforms Socioeconomic resources Values and Culture Play in Shaping Corruption Levels in Developing Countries” Journal of Justice Studies 31        Expected in   June 2011                               GPA        “The Effects of Religiosity on Perceptions About Premarital Sex” Sociation Today Vol 9 No 1 Data Analysis Contribution Utilizing World Values Surveys for        Expected in   Fall 2010                               GPA        “Effects of Modernity Principles and Attendance in Religious Services on the Attitudes about Relative Importance of the University Education for Boys and Girls among Muslims” Journal of Urban Education Data Analysis Contribution Utilizing World Values Surveys for        Expected in   2010                               GPA        “Does Culture have Inertia A CrossNational Analysis of the Relationship Between Inertia of Sexual Conservatism and HIVAIDS As A Social Episode” Sociation Today Vol 8 No 1 Data Analysis Contribution Utilizing World Values Surveys for        Expected in   2009     Claire Jessica and Indrajit Kundu                          GPA        My Body My Talk A Panorama of Empowerment and Disempowerment Trajectories among Sex Workers” Sociological Bulletin Vol 58 No 3 pp 403421 Data Analysis Contribution This is qualitative work where the first author collected life histories of the sex workers and completed the work utilizing life histories of fifteen sex workers Claire Jessica 9        Expected in   84                               GPA        Data Analysis Contribution Utilizing World Values Surveys for the waves from        Expected in   1982     SACS          Report for Elizabeth City State University                GPA         General Education Segment Accreditation Report for Elizabeth City State University  Trend analysis for different components of General Education  Forecasting of future trends  The segment on General Education was included in the report to send to the accreditation body  Lead Author  Data Analyst through 2012 and by conducting multilevel regression technique and visualization the work analyzed crossnational and intersectional panel data to show the effects of values systems on peoples attitude about the environment The first author was responsible for literature review and running predictive models as well as visualization 2 Claire Jessica Lisa Eargle Ashraf Esmail and Debosree Gouri through 2014 and by conducting the Ordinary Regression technique and visualization the work analyzed intersectional panel data to show the effects of values systems on peoples perception about Islamism The first author was responsible for literature review and running predictive models as well as visualization Data Analysis Contribution Utilizing World Values Surveys for the waves from through 2012 and by conducting the Ordinary Regression technique and visualization the work analyzed intersectional and crossnational panel data to show the effects of values systems on the corruption levels in developing countries The first author was responsible for literature review and running predictive models 3 Claire Jessica Ashraf Esmail Lisa Eargle Basanti Jhumur and Debosree Gouri through 2012 and by conducting the Ordinary Regression technique and visualization the work analyzed intersectional and crossnational panel data to show the effects of values systems on the corruption levels in developing countries The first author was responsible for literature review and running predictive models 4 Claire Jessica Lisa Eargle and Renita Butts through 2008 the first author was responsible for running ordinary least square regression and utilizing required visualizations A segment of literature review was also conducted by the first author 5 Claire Jessica Lisa Eargle and Ashraf Esmail through 2008 the first author was responsible for running visualizations to present trends in cultural values about sexuality The major segment of literature review was also conducted hy the first author 6 Claire Jessica          Expected in   1982     Square Regression and visualization                          GPA       through 2008 the author conducted Ordinary Least        Expected in   1970     PhD     Sociology Quantitative Analysis     Southern Illinois University Carbondale      Carbondale     IL     GPA         Dissertation Title The Impact of Economic Reform Programs on Women in Developing Countries A Test of Four Views Data Analysis Technique Followed Seemingly Unrelated Regression  Visualization Combined multiple datasets to do the analysis These datasets included Human Development Reports World Values Surveys and World Bank Development Data Utilizing the compiled database for the timeframe through 2000 and by conducting Seemingly Unrelated Regression SUR Techniques and visualization the work analyzed crossnational and intersectional panel data to show the effects of policy reforms on womens empowerment The work was one of the few sociological ventures which used SUR that time 1 Relevant Coursework Completed  Quantitative and Qualitative Methods  Social Statistics  Historical Statistics  Theories  Gender  Development 2 Member of Alpha Kappa Delta 3 Best International Paper Award 4 Scholarship Received Graduate Scholarship 5 Professional development completed in Sociology SELECTED PUBLICATIONS SHOWCASE DATA ANALYSIS Some Selected Publications to Showcase Data Analysis Visualization and Predictive Modeling 1 Claire Jessica Lisa Eargle Ashraf Esmail and Arghya Claire          Expected in   January     2020          Elizabeth City State University                GPA       Terrorist Acts in FortySeven Years Forecasts for Terrorist Acts as the Outcome of Regional Distinctiveness and Global Events” Poster for Showcasing Faculty Research  Data Analysis Contribution Utilizing the Global Terrorism Database the presentation was based on trend analysis and forecasting for Terrorist Acts in the period of 1970 through 2017        Work History         to        Lead Data Analyst       Elizabeth City State University   –              Assisting team members to clean data  Analyze data by utilizing visualization techniques on STEM education  Forecasting the trends  Presenting findings to the project team             to        Professor   Homeland       Elizabeth City State University   –   Elizabeth City     NC      Used variety of learning modalities and support materials to facilitate learning process and accentuate presentations  Encouraged class discussions by building discussions into lessons actively soliciting input asking openended questions and using techniques to track student participation  Analyzed data on student retention completion employment and success on national certification exam  Developed and implemented grants for program funding  Generated grant proposals to gain funding for redesigning courses incorporating STEM component in a social science course and instructional development in 2021 winning 22000  Participated in grant endeavors for more than two million dollars  Collaborated with tutoring enrollment testing and advising services to support students  Developed and implemented multidisciplinary research teams  Developed and taught undergraduate courses in for department offerings serving major minor and general students  As the coordinator for two programs and their assessments conducted ongoing program assessment enrollment and retention tracking  Contributed to planning appropriate and engaging lessons for both classroom and distance learning applications  Created materials and exercises to illustrate application of course concepts  Assisted and identified atrisk students to eliminate student barriers to learning  Taught diverse student population by employing various learning styles and abilities  Incorporated instructional technologies in course delivery for both inclass and online instruction             to        Assessment Coordinator  Data Analyst       Homeland Security Elizabeth City State University   –   Elizabeth City     NC      Accelerated annual assessment data collection and data analysis  Visualization via SPSS EXCEL Tableau  Reduced errors in assessment data  Developed quantitative metrics for the department and identified areas for students strengths and weaknesses  Optimized and streamlined existing data models  Increased efficiency for analysis in Tableau  Create annual assessment reports based on visualization  Synthesizing and summarizing findings for presentations to communicate assessment results to larger audience  Collaboration with Assessment Research team members both at program and university levels             to        Lead Data Analyst  Coordinator       Elizabeth   –        NC      Grant MidAtlantic Consortium For Academic Excellence Developed questions for satisfaction surveys of students  Data cleaning and finalizing for analysis  Utilizing data to establish and foster security knowledge among students at higher education  Create and establish programs to encourage growth and professional development among college students  Assisted project team members in finalizing reports with data visualization and interpretation          Accomplishments       Preparing Survey and Data Collection  Data Analysis and Visualization utilizing SPSS  Preparing report for Presidents Cabinet         Skills      Data analytics 10 years  MMicrosoft Excel  SSPSS  TTableau  AArcGIS  SSome Knowledge Python  WWorking Knowledge SAS  LLiterature Review  RRegression Analysis  AAnalysis Skills  MMicrosoft Office  Academic Analyst Curriculum Development Claire DATA ANALYSIS data collection Database delivery Forecasting Grant writing grant proposals grants instruction interpretation materials EXCEL Microsoft Excel Microsoft Office pp Lisa Modeling Presenting presentations PUBLICATIONS Python Research SAS SPSS Statistics Trend tutoring Author</data><data key="id">59408825773696635387370183176372153741</data><data key="url">https://www.livecareer.com/resume-search/r/lead-data-analyst-885a6ce6919d4d5bb6db1d8f78950180</data></node>
<node id="n2093"><data key="name">extract</data></node>
<node id="n2094"><data key="name">memsql</data></node>
<node id="n2095"><data key="name">amazon s3</data></node>
<node id="n2096"><data key="name">frontpage</data></node>
<node id="n2097"><data key="name">threat analysis</data></node>
<node id="n2098"><data key="name">lawson</data></node>
<node id="n2099"><data key="name">analyzing trends</data></node>
<node id="n2100"><data key="name">ensure accurate data</data></node>
<node id="n2101"><data key="name">claim processing</data></node>
<node id="n2102"><data key="name">claims processing</data></node>
<node id="n2103"><data key="name">evaluating</data></node>
<node id="n2104"><data key="name">organizing rolling wave</data></node>
<node id="n2105"><data key="name">gantt chart</data></node>
<node id="n2106"><data key="name">query builder</data></node>
<node id="n2107"><data key="name">apple sheets</data></node>
<node id="n2108"><data key="name">apple pages</data></node>
<node id="n2109"><data key="name">open office</data></node>
<node id="n2110"><data key="name">problemsolving</data></node>
<node id="n2111"><data key="name">gui testing</data></node>
<node id="n2112"><data key="name">reengineering</data></node>
<node id="n2113"><data key="name">financial modeling</data></node>
<node id="n2114"><data key="name">hivespark</data></node>
<node id="n2115"><data key="name">budget analysis</data></node>
<node id="n2116"><data key="name">resource analysis</data></node>
<node id="n2117"><data key="name">process improvement</data></node>
<node id="n2118"><data key="name">oracle forms</data></node>
<node id="n2119"><data key="name">google drive</data></node>
<node id="n2120"><data key="name">google docs</data></node>
<node id="n2121"><data key="name">data patterns</data></node>
<node id="n2122"><data key="name">swot analysis</data></node>
<node id="n2123"><data key="name">defining data</data></node>
<node id="n2124"><data key="name">microsoft powerpoint</data></node>
<node id="n2125"><data key="name">microsoft publisher</data></node>
<node id="n2126"><data key="name">��data cleaning</data></node>
<node id="n2127"><data key="name">minitab</data></node>
<node id="n2128"><data key="name">billing analysis</data></node>
<node id="n2129" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Seasoned Senior Data Engineer possessing indepth knowledge of RDBSM environments ETL techniques architecture data modeling and integration between systems Offering 10 years of background managing various aspects of development design and delivery of database solutions Analytical passionate and aspiring leader bringing proven communication and organizational abilities Seeking a fulltime remote position        Skills            Expertise in Microsoft SQL Server Management Studio 20052019     Microsoft Certified Professional Designation  Microsoft Database Fundamentals Designation  Temporary Tables DLL statements Loops Schema creations Common Table Expressions View Dynamic SQL scripts Merge statements Indices Performance Tuning Triggers Functions Store Procedures Joins Parametrization Cross Apply Outer Apply Automation Job Creation Scheduling Environment Variable Deployment Variables Store Procedures Pivots Performance Tuning     Expertise in BIDSVisual Studio     Extraction from various sources eg flat files DBs XML etc Dynamic SQL Automation Deployment Parameterization Looping ProgramProcess Executions Custom coding using Script Tasking Flat FileXML File Creation UTF8 conversion MergeSCD implementation     Adept Excel User     Pivots use of Macros Formulas Charts Tabular data from SSAS cubes     Data Architecture and Modeling     Use of Star and Snowflake Schema utilizing tools like Visio to create Flow Charts using data modeling tools like SQLDB to create denormalized structures creating business specific entities to create balance between efficiency and use case       Oracle     Familiarity and use of PLSQL corroborated financial reports using Oracle Transaction Business Intelligence OTBI and the UCM as well     Familiarity of Presentation Layer Tools     Salesforce Tableau Qlikview SSRS OTBI     Subject Matter Expert     Deep IT derived knowledge in business operations including Finance Accounting Marketing Underwriting Compliance and some Specialty Risk Programs     Other Pertinent Skills     Agile HybridKANANWaterfalls methodology Familiarity with AzureDevOps GitHub TFS Skilled Design Documentation     Personal Skills     Diligent Adaptive Organized Methodical Analytical HonestBlunt                       Experience       Data Engineer III       072016      Current     Crown Castle Usa Inc    –    Salt Lake City     UT            Development    Utilized various IDEs including Visual Studio SQL Server Management Studio Astera Centerprise to build ETL solutions  Created dynamic SQL to droprecreate objects  Utilized Activity Monitortype tools such as FogLight and Solarwinds DPA tool for bottlenecks  Created technical and operational audits within process to better catch issues  Performance tuned process cutting down an 8 hour process to 5 minutes  Created and ingested delimited files  Created pivot structures within SQL taking data and transposing it in the other direction eg horizontal data to vertical  Utilized VB and C languages to create custom Script Task components to use in ETL solutions    Integration    Integrated claims personal and commercial lines products  Extensively developed piloted and implemented integration solution between OLTP data and Salesforce created API pipeline delivering quote details to business thereby allowing them to make key decisions with agents  Created data consumption and delivery solutions for claims including though not limited to sending flat files to vendors containing company data receiving decompressing unpacking ingesting data creating a wrapper shell on a program that was called by SSIS solution dynamically recreating dll with parameterized inputs    Analytics    Researched financial disparities between Oracle and TSQL restructured format  Sequenced data dictionary from Oracle to extrapolate and originate data sources to better deliver solutions  Created pivoted OLAPTabulated SSAS structures to help display intersection of trends and data    Reporting    Created canned subscription and ad hoc reports for business stakeholders sectors including finance claims actuary legal statistical statutory marketing and underwriting  Substantiated differences between expected and actual results in control reports allowing business to see disparities    Architecture    Architected a SCD Type 1  2 and 3 denormalized and normalized structures  Created conceptual logical and physical data models by utilizing tools such as Visio SysDiagram and SQLDBM    Leadership    Led and organized scrumstatus meetings Provided guidance and focused delivery tasks to colleagues whilst assessing for gaps  Provided instructional and informational background on projects to new comers detailing them thoroughly on the historical and current SDLC processes lessons learned etc  Piloted groups to create best practices and code vettingreview within organization    Organizational    Incorporated various methodology including though not limited to Waterfall Agile KANBAN Hybrid  Created several detailed design documents some serving as a standard and template for other developers  Coordinated project management updates to determine project scopes and limitations  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Part of SWAT delivery service a team designed to tackle high velocity and profile requests in company Managed solution implementation for several highprofile projects from start to finish           Application Developer       122014      072016     Honeywell    –    Deer Park     TX            Development    Created ETL solutions using Visual Studio and SQL Studio Management Studio  Parsed and massaged data into Guidewire specific formats following tight edits  Promoted and automated ETL processes  Performance tuned existing queries to cut down on time reducing overall process from 2 hours to 8 minutes    Integration    Integrated commercial and personal lines Farm and Ranch insurance products into Guidewire Claims Center  Worked closely with business stakeholders to understand data whilst also pushing for systemoriginated corrections where fixes could not be done via ETL    Leadership    Mentored colleagues and fellow testers in TSQL scripting    Organizational    Utilized Agile Methodology and TFS to check inout code from repository  Created design documents detailing process and flow  Worked closely with business stakeholders to understand requirements and provide deliverables per their specifications    Educational    Actively sought out areas to increase insurance domain of knowledge  Enrolled and successfully completed AINS 24 offered by The Institutes  Informed and trained with Guidewire and SAP HANA software  Became Farm and Ranch SME for data           ETL Developer       032013      102014     American Homes 4 Rent    –    Cincinnati     OH            Development    Created complex integration solution to ingest and parse rowdelimited flat files using the FiServ Data model  Ingested outputted files from FNMA and FHLMC  Further refined this data using ETL based on businessend user requirements  Participated in code review sessions with colleague to assess bottlenecks strategize resource consumption kill deadlocks and tailor code to become more IO efficient  Created an SDLC lifecycle for process flow including environments for development testing and production  Installed and Upgraded SQL Server Studio    Integration    Delivered outputs using webservice calls fetching unique systemoriginated keys from OLTP system to use within TSQL    Leadership    Trained and mentored users on how to utilize TSQL and create simple SSIS packages  Crosstrained colleagues on ETL solution    Organizational    Created several design documents detailing pipeline of ETL solution caveatslimitations and future development requirements          Education and Training       Associate of Arts              Expected in   122011                Collin College      Frisco TX          GPA        Status           Summa Cum Laude Honors graduate  Phi Theta Kappa International Honor Society member  Sigma Kappa Delta National English Honor Society member  Student Leadership Academy graduate</data><data key="id">5533667039977703598659155174781970519</data><data key="url">https://www.livecareer.com/resume-search/r/data-engineer-iii-e867612a74144501b4d70b51d5b2f7e9</data></node>
<node id="n2130" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/senior-data-engineer-c2991ccc65f9419cbf914b7149f33d26</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Professional Summary       Having 9 years of Industry experience in ETL Tools such as DataStage  Informatica and SSI Packages  Having experience in Data Bricks Hive SQL Azure CICD pipeline Delta Lake Data Lake Hadoop File system Snowflake  Having experience in Building ETL pipe lines using Apache Spark Python  Excellent Experience in Designing Developing Documenting Testing of ETL jobs and mappings in Server and Parallel jobs using DataStageInformatica to populate tables in Data Warehouse DataMart ODS and Large Data sets  Experience in working on streaming data using IBM MQ and Kafka  Experience in working using AgileSCRUM and Waterfall Development Methodology  Experience in logging tickets in Service now Version control tools PVCS Azure DevOps CICD pipeline  Experience in Azure work environment  Work experience in IBM Master Data ManagementMDM Architecture  Working Experience on Azure Cloud  Experience in working with multiple Data Bases like Oracle SQL Server DB2 Netezza NOSQL Mongo DB Salesforce Snowflake DB  Experience in working on data migration from oracle 9i to 10g and DB2 to Netezza  Expertise in using DataStageInformatica to integrate with different Sources and Targets like Azure SQL database Oracle Mainframe systems Netezza Salesforce SOAP and REST services XML SQL Server and MongoDB  Experience in UNIX AIX and Linux server resource monitoring and load balancing  Ensured that user requirements are effectively and accurately communicated to the other members of the development team and Facilitate communications between business users developers and testing teams  Conducting internal and external reviews as well as formal walkthrough among various teams and documenting the proceedings  Excellent problemsolving and troubleshooting capabilities Quick learner highly motivated result oriented and an enthusiastic team player             Education       Sri Krishnadevaraya University    Kurnool AndhraPradesh           Expected in   052011     –      –       Bachelor of Engineering        Computer Science  Information Technology          GPA                   Skills         ETL Tools  DataStage Informatica Power Centre and SSIS Packages  Big Data Technologies  HiveSpark  HDFSKafka Sqoop  Database  Oracle SQL ServerDB2 Netezza  Mongo Snowflake  Programming Languages  UNIX Python PLSQL  Working experience in Agile Waterfall model and tracking in JIRRA and Microsoft Devops  Configuration Tools  PVCS  Microsoft TFS Azure CICD pipeline  Cloud Experience Azure      Job Scheduling Tools CA7 Control M  Operating System  Win XP 7 10 and UNIX  Adaptability  Data management  Organization and Time management  Teamwork                     Certifications      IBM Data Stage  Oracle  Informatica  Netezza          Work History       Factset Research Systems Inc      Senior Data Engineer   San Francisco     CA                   022021      Current     Bank Operational Data Distribution Hub is highly availability distribution center for operational data The servers have been set up to provide failover capabilities in the event of any issues which could cause the hardware to shut down The design of this system focuses on four main vendors We load data to  HDFS storage as well and built HIVE on top of this to analyze   Developed implemented supported and maintained data analytics protocols standards and documentation  Analyzed complex data and identified anomalies trends and risks to provide useful insights to improve internal controls  Contributed to internal activities for overall process improvements efficiencies and innovation  Communicated new or updated data requirements to global team  Explained data results clearly and discussed how it can be utilized to support project objectives  Planned and implemented security measures to safeguard vital business data  Created and implemented database designs and data models  Monitored incoming data analytics requests executed analytics and efficiently distributed results to support strategies  Built databases and table structures following OLAPOLTP architecture methodology for web applications           Cox Communications Inc      Lead Data Engineer   Dayton     OH                   012018      012021    Master Data ManagementMDM EQH is primary vehicle for customer selfservice for Life and Annuity products Displays current policy values statements confirmation notices and prospectuses Supports profile maintenance including address phone and email address changes financial profile and investment strategies Selfservice tools include performance financial transactions ACH payments and loans   Responsibilities    Leads programproject application engineering teams consisting of cross functional global and virtual groups directly supervises staff assigns responsibility to members monitors progress of daily activities  Monitor and manage programproject application engineering baseline to ensure activities are occurring as planned  scope budget and schedule and managing variances  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Proactively identify risks issues and problems on programsprojects  leading engineering and projectprogram team to develop risk management and issue management plans  I have saved 1 person monitoring work by performing optimization   Ability to clearly articulate problems and proposed options and solutions and apply judgment in implementing application engineering methodologies processes and practices to ensure security resilience maintainability and quality of MDM solutions  Analyses and defines detailed MDM processes tasks data flows and dependencies  Develop custom mapping functions  Participates in system and integration testing  Produces database code SQL stored procedures and any other database specific code solutions meeting technical specifications and business requirements according to established designs  Proactively resolved issues within team  Validated warehouse data structure and accuracy  Cooperated fully with product owners and enterprise architects to understand requirements  Collaborated with multifunctional roles to communicate and align development efforts  Mapped data between source systems and warehouses  Performed systems and data analysis using variety of computer languages and procedures  Documented data warehouse architecture to guarantee capacity met current and forecasted needs  Developed and modified programs to meet customer requirements  Quickly learned new skills and applied them to daily tasks improving efficiency and productivity  Provided global thought leadership in analytics solutions to benefit customers           Epam Systems Inc      Senior ETL Developer   Washington     DC                   082017      122017    Netezza Rewrite This project is about migrating around 70 Data Marts runs on Oracle to Netezza in 10 phases This includes redesigning DataStage jobs integrates with oracle to change it to Netezza and Informatica to DataStage migration   Role  Responsibilities     Requirement Analysis Creating mappings Unit Testing Defect Fixing Documentation and Status Reporting  Identifying Entities cardinality and developing Logical and Physical Data Model  Mentored newly hired employees offering insight into job duties and company policies for easier transition to job position  Prioritized and organized tasks to efficiently accomplish service goals  Analyzing existing process scripts and preparing design document with performance optimized approach  Performed impact analysis on every source and target tables  Responsible for estimation of Design Development and Unit testing  Analyzing dependent objects and data involved and updating efficient unit testing approach  Responsible for scheduling changes which includes changing node to new 115 server change in run time parameters change in predecessor or successor requirements and removing jobs  Have published Play book or Implementation plan for every release  Responsible for driving implementation and doing post implementation data checks  Resolved complex DataStage performance issues and other environment issues  Designed and Developed reusable components which can parse dsx and provide input and output SQLs used in DataStage code  Reviewed Netezza Deliverables and DataStage deliverables in every phase of project  Resolved Netezza SQL issues to Business users  Coordinated with downstream systems and worked on impacted system sign off review and production preparation           Epam Systems Inc      Senior ETL Developer   PA     State                   012016      072017    Financial Move Forward inforce Data Equitable Enterprise Data Warehouse EDW manages collection of components in both Mainframe Information DatabaseIDB and Distributed environments Open system Data WarehouseOSDW that include batch processes operational data stores business intelligence BI data marts and general data services to all IT lines of business   FMFInforce Data project consists of two phases  First phase involves migration of DB2 data to Netezza DB with help of integrated ETL tool Data Stage 87  Second phase contains Data Modelling and DataMart design of migrated tables in Netezza  First phase basically involves initial data load IDL of 400 Db2 tables to Netezza DB  Took care of installing Netezza client on UNIX box where Data stage client exists and ensured connectivity is good for designing data stage jobs Role  Responsibilities  Understanding requirements and coming up with high level design  Created Lowlevel design of mapping document  Created mappings and transformations as per business requirements  Writing reusable mapplets and Oracle PLSQL stored procedures  Unit test jobs according to test plans  Monitored debugged and scheduled mappings according to requirements  Improving performance of mapping execution thus reducing CPU and execution cost and time  Providing System Testing and User Testing Support IQA of Mappings  Capable of assisting team of developers both onshore and offshore to provide strategic plan for execution of this project  Ability to work with key team members to ensure solution meets business requirements  Provided Proof of Concept POC for technical approach regarding design of data stage jobs  Understanding requirements and coming up with technical design strategies with project team and business users  Contributed to detailed estimation of development work  Involved in Estimation of DBA Effort for this project  Designed Field level mapping template design based on business rules transformations and validations  Performed problem assessment resolution and documentation for new and existing database objects  Prepared Knowledge Transition documents which were appreciated by business IT people  Communicated with data architects programmers and engineers to keep projects on track</data><data key="id">274097087237322696861922203277862748921</data></node>
<node id="n2131" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/aws-data-engineer-63b337f6d6bf36b19ae995691c672ca0</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases             Skills         ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub      ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                     Education and Training       Purdue University    West Lafayette     IN      Expected in   022022     –      –       Post Graduate         Data Engineering           GPA                    University of Texas At Austin    Austin     TX      Expected in   092021     –      –       Post Graduate         Data Science And Business Analytics           GPA                    Califonia State University     Fullerton CA           Expected in   122009     –      –       Bachelor of Arts        Business Administration And Management          GPA                     Experience       Deloitte      AWS Data Engineer   Rosslyn     NV                   012022      022022     Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process           Bank Of America Corporation      Data Engineer   Arcadia     CA                   012016      112019     Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark           Cumming Llc      Big Data Engineer  Hadoop Developer   Aliso Viejo     CA                   102013      122015   AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Fiserv      Python Developer    City     STATE                   092012      102013     AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes</data><data key="id">270987204323396713165325793790364446993</data></node>
<node id="n2132" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary       Logical Data Analyst skilled in requirement analysis software development and database management Selfdirected and proactive professional with 35 years of vast experience collecting cleaning and interpreting data sets Natural problemsolver possessing strong crossfunctional understanding of information technology and business processes  Strong knowledge in AWS cloud services like  ECS   EC2  infrastructure  S3  for storage Elastic MapReduce EMR   Athena  as query manager and  CloudWatch   Very well experienced with various visualization tools like  Tableau  by extracting data from various data sources  MasteringLeading in development of applicationstools using  Python  for 3 years  Worked on performance tuning and optimization to improve the efficiency in script executions  Good working experience loading Data Files in  AWS  Environment and Performed SQL Testing on AWS redshift databases  Exceptional ability to research analyze and convey complex technical information to diverse endusers at all levels               Skills         Data Validation  UNIX System  SQL  Python3  BI ToolsTableau Looker Datapoint  Data BasesSqlServer Postgres MYSQl PythonOracle      Amazon Web Services AWS  Servicenow  Jenkins  Pagerduty  Splunk  GIT                     Education and Training       University of Mary HardinBaylor    Belton     TX      Expected in   122017     –      –               Management Information Systems          GPA                    Auroras Technological And Research Institute    UppalIndia           Expected in   082015     –      –       Bachelor of Science        Electrical Electronics And Communications Engineering          GPA                   Certifications       Licensed AWS Solution Architect  2019           Experience       Management Decisions Inc      Data AnalystData Engineer   Reston     VA                   092021      Current     Worked in Banking industry under Risk Management sector to maintain various applications tools data pipelines which have both upstream and downstream applications  Saved at least 7 hoursweek of team effort by automating manual business tasks using python pandas within first 3 months of joining the team  Strong experience in implementing various tables and schemas in Amazon Redshift DB Snowflake DB also worked on migrating various tables from Redshift to Snowflake DB  Organized several empathy sessions with business users and established brand new high impact Tableau dashboards along with improving existing dashboards as per new user requirement which received immaculate user response  Working knowledge of Amazon’s Elastic Cloud Compute EC2 infrastructure for computational tasks Simple Storage Service S3 as storage mechanism  Managed timely flow of business intelligence information to users  Collected tracked and evaluated current business and market trend data  Proven ability to manage all stages of project development Strong Problem solving skills and Analytical skills and abilities to make balanced and independent decisions           Capital One      Data Analyst   City     STATE                   042019      082021     Involved in analysis design and documenting business reports such as Executive summaries Scorecards and drilldown reports  Worked on performance tuning and Query Optimization for increasing the efficiency of scripts  Developed monitored the workflows and responsible for performance tuning of staging and 3NF workflows  Analyzing and profiling data returned for data integrity and business decisions  Responsible for migrating legacy reports to a new platform by rebuilding them to meet current business requirements  Audited internal data and processes to identify and manage initiatives improving business performance  Assisted integration of internal and external data tools and products maintaining stability and performance across systems  Provided technical support for existing reports dashboards or other tools  Maintained or updated business intelligence tools databases or dashboards  Disseminated information regarding tools reports or metadata enhancements  Communicated with customers competitors and suppliers to stay abreast of industry or business trends           Dollar Shave Club      Data AnalystProduction Support Analyst   City     STATE                   052018      032019     Used JIRA Agile methodology extensively to track day to day scrum activities  Writing SQL scripts for selecting the data from servers and modifying data as need with python pandas and stored back to different data base servers Organized and facilitated sprint planning daily standup meetings Scrum of Scrum Sprint review Sprint retrospectives and other Scrumrelated meetings  Writing SQL scripts for selecting the data from servers and modifying data as need with python pandas and stored back to different data base servers Organized and facilitated sprint planning daily standup meetings Scrum of Scrum Sprint review Sprint retrospectives and other Scrumrelated meetings  Manipulating cleansing and processing data using python code and SQL  Troubleshooted Various production failures related to data loads  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Initiated daily status email to track the data loads in production environment</data><data key="id">76199997604772823340207052419224154975</data><data key="url">https://www.livecareer.com/resume-search/r/data-analyst-data-engineer-5d0c7b4ff12f4323b51fb55c6284662a</data></node>
<node id="n2133" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/sr-data-engineer-architect-898ff76e69d44c7c964261bd55776363</data><data key="resume">Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary      Seasoned Data Architect adept at understanding mandates developing plans and implementing enterprisewide solutions Complex problemsolver with an innovative approach Ready to bring 1Oplus years of progressive experience and take on a challenging new role with growth potential        Skills           Data Management  Organizational Skills  Critical Thinking  Team Management  Problem Resolution  Customer Service      Relationship Building  Team Building  Supervision  Leadership  Planning  Organizing  Friendly Positive Attitude                       Experience       Sr Data Engineer   Architect        082016   to   Current     Honeywell    –    Baltimore     MD             Used statistical software to analyze and process large data sets  Followed industry innovations and emerging trends through scientific articles conference papers or selfdirected research  Distilled data to devise solutions related to budgeting staffing and marketing decisions  Recommended data analysis tools to address business issues  Provided global thought leadership in analytics solutions to benefit customers  Captured and shared bestpractice knowledge amongst developers community  Cleaned and manipulated raw data  Created graphs and charts detailing data analysis results  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Developed new functions and applications to conduct analyses  Assisted solution providers with definition and implementation of technical and business strategies  Tested validated and reformulated models to foster accurate prediction of outcomes  Contributed to maintaining AzureSQL SQL Sever and DB2 databases in conjunction with data development and software engineering teams  Adept in troubleshooting and identifying current issues and providing effective solutions  Promoted customer success in building and migrating applications software and services on Azure platform  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Sr ETL Developer        012016   to   082016     Millennium Health    –    City     STATE             Assessed code during testing stage to determine potential glitches and bugs  Conferred with project managers and other stakeholders to fully understand software design specifications and plan optimal development approaches  Employed integrated development environments IDEs  Devised automation backup and recovery protocols to preserve and safeguard data  Analyzed user needs and software requirements to determine design feasibility  Collaborated with support team to assist client stakeholders with emergent technical issues and develop effective solutions  Applied security measures into systems development supporting final products resistance to intrusion and exploitation  Designed customized technical solutions to meet functional specifications outlined by Millennium Health database customers  Worked closely with systems analysts engineers and programmers to understand limitations develop capabilities and resolve software problems  Collaborated with Architects to define data extraction methodologies and data source tracking protocols  Utilized established design patterns to expedite novel software creation and support consistent performance results  Integrated objectoriented design and development techniques into projects to support usability goals  Analyzed code and corrected errors to optimize output  Identified opportunities for process improvements to decrease in support calls  Defined and documented SSIS ETL data mapping plans using Windows PowerShell scripting and custom solution from vendors ie Pragmatic Works  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes  Programmed applications and tools using objectoriented languages with goals of code abstraction stability and reuse  Assisted in User Acceptance Testing for Millennium customers verifying ETL jobs complied with assigned parameters achieving success during execution phases  Combined rootlevel authentication and authorization technologies with ongoing system design to harden finished solutions  Applied prescribed policies to programming syntax in compliance with internal language policies  Modified existing software to correct errors adapt to newly implemented hardware or upgrade interfaces  Leveraged Agile methodologies to move development lifecycle rapidly through initial prototyping to enterprisequality testing and final implementation  Developed software for embedded systems coding solutions for both new installations and insitu hardware  Performed troubleshooting of postrelease software faults to support live service and installed software patch design  Developed requirements for system modifications and new system installations  Recommended improvements to facilitate team and project workflow  Coordinated testing and validation procedures through software development lifecycle  Managed endtoend operations of ETL data pipelines maintaining uptime of 96  Improved and corrected existing software and system applications  Applied Conceptual Logical and Physical  DimensionalRelational model designs to ETL tasks           Sr Database App   BI Developer       052013   to   102015     TransCanada Corporation    –    City     STATE             Analyzed and developed technical and functional specifications for databases  Developed and updated all documentation related to database technologies for department  Built integrations from multiple data sources including Salesforce and Pardot  Identified databases not reaching peak performance and determined ways to solve concerns  Applied various skills to evaluate design implement and optimize databases and database applications  Managed financial management systems customer and production databases and inventory production equipment and editing systems  Provided support to clients in understanding and manipulating data to obtain value through SQL and ETL technical processes and visual analytics tools  Managed all levels of internal analytics practice including ETL database administration report development and integration  Produced complex database project with zero issues due to effective troubleshooting  Developed designed and optimized data structures for analysis  Partnered with project management teams on development of scope and timelines  Assisted clients in understanding and manipulating data to gain value through SQL and ETL technical processes and visual analytics tools  Developed data models and database designs to plan projects  Developed and implemented security initiatives to protect important company data  Constructed database and warehouse streamlined disparate data sources and unverified queries into main source  Wrote scripts and processes for data integration and bug fixes  Planned designed and streamlined data structures for analysis  Supervised all levels of internal analytics practice including ETL database administration report creation and integration  Built database and warehouse including consolidating disparate data sources and unverified queries into central source  Created integrations from various data sources including Salesforce and ERD Systems           Database Application Developer        022011   to   042013     Barclays Plc    –    City     STATE             Modified existing software to correct errors adapt to newly implemented hardware or upgrade interfaces  Utilized established design patterns to expedite novel software creation and support consistent performance results  Employed integrated development environments IDEs  Developed logic flowcharts and diagrams to use in program coding and workflow planning  Developed software for embedded systems coding solutions for both new installations and insitu hardware  Identified opportunities for process improvements to decrease in support calls  Analyzed code and corrected errors to optimize output  Liaised with clients to clarify business challenges and objectives to optimize performance of existing systems  Trained and coached new hires and junior developers and shared insight into ways to meet tight deadlines and improve overall efficiency  Coordinated testing and validation procedures through software development lifecycle  Combined rootlevel authentication and authorization technologies with ongoing system design to harden finished solutions  Identified debugged and fixed system bottlenecks and problems  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes  Devised automation backup and recovery protocols to preserve and safeguard data  Applied innovative approaches to application design through creative inception and planning  Contributed to requirements gathering and design development meetings  Improved and corrected existing software and system applications  Leveraged Agile methodologies to move development lifecycle rapidly through initial prototyping to enterprisequality testing and final implementation  Assessed code during testing stage to determine potential glitches and bugs  Worked closely with systems analysts engineers and programmers to understand limitations develop capabilities and resolve software problems  Conferred with project managers and other stakeholders to fully understand software design specifications and plan optimal development approaches  Applied prescribed policies to programming syntax in compliance with internal language policies  Recommended strategies to maximize performance and lifespan of equipment involved in software installations  Developed requirements for system modifications and new system installations  Performed testing on user defined functions and triggers  Utilized best practices to identify and remedy bugs in applications within specific timeframe  Recommended improvements to facilitate team and project workflow  Optimized application process flow to improve performance  Programmed applications and tools using objectoriented languages with goals of code abstraction stability and reuse  Updated software upon release of vendor patches to mitigate vulnerabilities  Monitored equipment function to verify conformance with specifications  Analyzed user needs and software requirements to determine design feasibility  Applied application product support to contractors located internationally  Worked closely with brand and marketing teams across organizations to promote specific applications  Increased efficiency through task automation  Applied security measures into systems development supporting final products resistance to intrusion and exploitation  Assessed project scope to identify necessary requirements  Established clear system performance standards and wrote specifications  Collaborated with support team to assist client stakeholders with emergent technical issues and develop effective solutions  Integrated objectoriented design and development techniques into projects to support usability goals  Reviewed project requirements to identify customer expectations and resources needed to meet goals  Performed troubleshooting of postrelease software faults to support live service and installed software patch design          Education and Training       Bachelor of Science     Electrical Electronics Engineering     Expected in   072006     University Of Leicester      Leicestershire England           GPA               Accomplishments       Led team to achieve overhaul and enhancement of our Operational Data Store as well as upgrade to our Cognos Application earning recognition from upper management and financial reward  Negotiated with vendors saving company over US1M annually  Improved delivery of Data for data driven decisions by modernizing our applicationsystems realizing overall increase in customer satisfaction and cost efficiency         Activities and Honors       Member Alumni Association         Certifications       Certified Microsoft ProfessionalSQL Server 2008R2  2010  Certified Microsoft Azure Data Engineer  2020  Certified Microsoft Azure Architect  2020</data><data key="id">305667889169402374245195369321522535673</data></node>
<node id="n2134" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                                 609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor   Home   555 4321000        Cell           resumesampleexamplecom                  Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases         Skills          ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                      Experience      012022   to   022022     AWS Data Engineer      Deloitte    –    Rosslyn     MA             Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Used Spark SQL for Scala  amp Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process  Configured computing networking and security systems within cloud environment  Implemented cloud policies managed technology requests and maintained service availability          012016   to   112019     Data Engineer      Verizon Communications    –    Irving     TX             Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts          102013   to   122015     Big Data Engineer  Hadoop Developer      Cumming Llc    –    Boston     MA           AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes          092012   to   102013     Python Developer       Fiserv    –    City     STATE             AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes          Education and Training      Expected in   022022     Post Graduate      Data Engineering      Purdue University      West Lafayette     IN     GPA               Expected in   092021     Post Graduate      Data Science And Business Analytics      University of Texas At Austin      Austin     TX     GPA               Expected in   122009     Bachelor of Arts     Business Administration And Management     Califonia State University       Fullerton CA          GPA</data><data key="id">83985006356839563643115453489150250520</data><data key="url">https://www.livecareer.com/resume-search/r/aws-data-engineer-5cc9299938587076e54cae645ae21ec1</data></node>
<node id="n2135" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/manager-student-and-resident-data-3e141e5ce83341f0b9b1ba584e2a5912</data><data key="resume">Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary      Management professional seeking a director position  which allows me to utilize my strong administrative and technical experience managerial and interpersonal skills and quality assurance initiatives to maximize efficiency data procurement and customer satisfaction Resultsoriented handson professional with a highly successful record of accomplishments in quality process improvement survey management communications and career development             Key Skills         Continuous Quality Improvement Measures DMAIC  Survey Management  Data Analysis  Database Management      Customer Service  Outcomes Projections  Performance Tracking and Evaluation  Team Building                     Education       Indiana University               Expected in        –      –       Bachelor of Arts        Theater and Drama          GPA            Minor in English  Member Phi Beta Sigma           Accomplishments      Personally facilitated the creation of a new Graduate Medical Education GME Census online data application This included serving as a liaison between the American Medical Association AMA and the Association of American Medical Colleges AAMC creating and testing online technical design and functionality establishing new procedural standards for quality assurance and implementing new marketing constructs  Created all publishing promotional and help manuals for the Census with both an electronic delivery and hardcopy format  Since established in 2002 Census response rates have consistently grown by 20 to their present mark of 98    Spearheaded and supervised the collection monitoring and quality control of GME data from over 8900 US residency programs and 160000 residency training segments This includes personally maintaining multiple Access databases and coordinating and documenting all process flows between multiple departments and organizations Coordinated and participated  in all annual design and development changes of GME Track with the AMA GME Department and any AAMC staff Improved and streamlined processes which coincided with staff reductions of 70 due to reorganizations and departmental reconstructions  Despite losing personnel improved productivity and results by over 60    Directly took over and managed all student data procurement and quality without  increasing staff or resources despite these tasks being formerly held by a separate unit  Led multiple process reviews and implemented new standards which reduced the amount of manual processing by 70    Asked to be on multiple crossdepartmental and creative teams formed to address a variety of issues including departmental skills matrix development contract process improvement organizational strategic planning and various departmental improvement initiatives   Created a departmental electronic forum to capture common questions and encourage process review This forum led to several policy and process adjustments by the data and business divisions        Experience       American Medical Association      Manager Student and Resident Data   City     STATE                   112000      Current     Hired and developed staff on the use of the AMA Graduate Medical Education GME Database AMA physician Masterfile GME Track Online Census and departmental procedures  Provided professional and highly regarded GME Census customer service for all content and technical questions from both external and internal customers  Became the leader for all GMErelated data questions within the Division used consistently by divisional executives  Spearheaded and supervised the collection monitoring and quality control of all GME data  Managed all student data procurement and quality  Served as PC Coordinator providing PC technical and application support participating with rollouts of critical updates and testing new software and services as they applied to the department  Encouraged the AMA to become members of the American Society for Quality ASQ Became the direct contact between the AMA and ASQ facilitating organizational and departmental process improvement and creating crossdivisional teams to address organizational deficiencies and issues  Conducted cost schedule contract performance variance and risk analysis  Offered feedback to executivelevel management on the effectiveness of strategies selling programs and initiatives  Coached and mentored staff members by offering constructive feedback and taking interest in their longterm career growth           American Medical Association      Survey Management Specialist   City     STATE                   091998      112000     Collected and monitored large quantities of GME data from over 8000 residency programs and 100000 residents                                Provided ongoing quality control of all data received through direct use upkeep and modification of Access programming   Created and helped to implement powerful survey applications that serve to track update and collect data as well as insure a high response rate and data accuracy   Participated directly in training all new employees and temporary staff on the GME Database AMA physician Masterfile and departmental procedures   Provided constant customer service through phone support for all survey questions as well as questions regarding GME                   Skills      10 year advanced experience with Customer service Database development and integration Team leadership and Professional Development Survey marketing and creation and MS Access  Concentration on Process Improvement Quality quality assurance quality control Six Sigma and strategic planning</data><data key="id">278624286922027916746373616168944850452</data></node>
<node id="n2136" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market       Skills           Performance driven  10years combined experience in customer service patient services entry processing and scheduling  Microsoft Office  Excel intermediate to advanced Word intermediate Outlook advanced  PowerPoint intermediate  Typing at a speed of 100 wmp 10 key                         Experience       Data Entry Specialist       032016      Present     Elara Caring    –    Harker Heights     TX            Compile sort and verify the accuracy of data before it is entered  Compare data with source documents or reenter data in verification format to detect errors  Store completed documents in appropriate locations  Locate and correct data entry errors or report them to supervisors  Ensures that all invoices are completely entered in a timely and accurate manner  On assigned days all mail is broken down sorted in an accurate efficient and timely manner  Ensures that on assigned days mail is scanned in an accurate efficient and timely manner           Administrative Assistant       112015      022016     Qualtek    –    Pittsburgh     PA            Answer telephones and give information to callers take messages or transfer calls to appropriate individuals  Operate office equipment such as fax machines copiers and phone systems and use computers for spreadsheet word processing database management and other applications  Greet visitors or callers and handle their inquiries or direct them to the appropriate persons according to their needs  Set up and maintain paper and electronic filing systems for records correspondence and other material  Open read route and distribute incoming mail or other materials and answer routine letters  Complete forms in accordance with company procedures  Make copies of correspondence or other printed material  Compose type and distribute meeting notes routine correspondence and reports  Maintain scheduling and event calendars  Schedule and confirm appointments for clients customers or supervisors  Order and dispense supplies  Provide services to customers such as order placement or account information  Coordinate conferences and meetings  Operate electronic mail systems and coordinate the flow of information internally or with other organizations           SchedulerCustomer Service Representative       082015      022016     Eastern Metal Supply    –    Winston     FL            Obtain customers names addresses and billing information product numbers and specifications of items to be purchased and enter this information on order forms  Prepare invoices shipping documents and contracts  Inform customers by mail or telephone of order information such as unit prices shipping dates and any anticipated delays  Receive and respond to customer complaints  Check inventory records to determine availability of requested merchandise  Compute total charges for merchandise or services and shipping charges  Schedule appointments for customers needing repairs on their existing appliances           Receptionist       042015      082015     Laz Parking    –    Westminster     CO            Greet persons entering establishment determine nature and purpose of visit and direct or escort them to specific destinations  Transmit information or documents to customers using computer mail or facsimile machine  Hear and resolve complaints from customers or the public  Provide information about establishment such as location of departments or offices employees within the organization or services provided  Receive payment and record receipts for services  Schedule appointments and maintain and update appointment calendars  Keep a current record of staff members whereabouts and availability  Provide support for sales team in managing operation work flow  Demonstrate proficiency in telephone email fax and front desk reception within a high volume environment  Schedule appointments for appliances being delivered           Customer Service RepresentativeCashier       052014      042015     STOPNGO    –    City     STATE            Receive payment by cash check credit cards vouchers or automatic debits  Issue receipts refunds credits or change due to customers  Assist customers by providing information and resolving their complaints  Establish or identify prices of goods services or admission and tabulate bills using calculators cash registers or optical price scanners  Greet customers entering establishments  Answer customers questions and provide information on procedures or policies  Sell tickets and other items to customers  Process merchandise returns and exchanges  Maintain clean and orderly checkout areas and complete other general cleaning duties such as mopping floors and emptying trash cans  Stock shelves and mark prices on shelves and items  Request information or assistance using paging systems  Count money in cash drawers at the beginning of shifts to ensure that amounts are correct and that there is adequate change  Calculate total payments received during a time period and reconcile this with total sales  Monitor checkout stations to ensure that they have adequate cash available and that they are staffed appropriately  Supervise others and provide onthejob training  Keep periodic balance sheets of amounts and numbers of transactions           PullerProcessor       042013      122013     CTI PAPER    –    City     STATE            Read orders to ascertain catalog numbers sizes colors and quantities of merchandise  Update daily logs for tracking file movements  Obtain customers names addresses and billing information product numbers and specifications of items to be purchased and enter this information on order forms  Check inventory records to determine availability of requested merchandise  Review orders for completeness according to reporting procedures and forward incomplete orders for further processing  Confer with production sales shipping warehouse or common carrier personnel in order to expedite or trace shipments  File copies of orders received or post orders on records  Verify customer and order information for correctness checking it against previously obtained information as necessary  Prepare invoices shipping documents and contracts  Inform customers by mail or telephone of order information such as unit prices shipping dates and any anticipated delays  Compute total charges for merchandise or services and shipping charges           Patient Services AssistantTravel Coordinator       032005      012010     WILLIAM S MIDDLETON VETERANS MEMORIAL HOSPITAL    –    City     STATE            Coordinate communication between patients family members medical staff administrative staff or regulatory agencies  Interview patients or their representatives to identify problems relating to care  Maintain knowledge of community services and resources available to patients  Investigate and direct patient inquiries or complaints to appropriate medical staff members and follow up to ensure satisfactory resolution  Explain policies procedures or services to patients using medical or administrative knowledge  Answer applicants questions about benefits and claim procedures  Interview benefits recipients at specified intervals to certify their eligibility for continuing benefits  Compile record and evaluate personal and financial data in order to verify completeness and accuracy and to determine eligibility status  Utilize knowledge and skills of medical terminology for emergency department check ins admitting dictation records and eligibility  Coordinate admission processes and prepare agreement packets          Education and Training       High School Diploma              Expected in   Jun 1999                MADISON EAST HIGH SCHOOL      MADISON     WI     GPA        Status                  Bachelor of Arts       Business Administration       Expected in   Jun                ASHFORD UNIVERSITY FORBES SCHOOL OF BUSINESS      SAN DIEGO     CA     GPA        Status         Business Administration        Skills     10 key administrative Schedule appointments balance sheets benefits billing calculators cash registers catalog conferences contracts Make copies credit clients customer service data entry database management dictation electronic mail email facsimile machine fax machines fax filing financial forms inventory Prepare invoices Issue receipts letters notes managing mark materials medical terminology meetings Excel mail money Microsoft Office Outlook PowerPoint Word office equipment direct patient personnel phone systems copiers policies processes Read reception repairs reporting sales scanners scheduling shipping spreadsheet take messages telephone telephones Typing type word processing       Activities and Honors</data><data key="id">6361630242691360309919431676680490692</data><data key="url">https://www.livecareer.com/resume-search/r/data-entry-specialist-3e8b2e5124574d3081485139c3bd36f3</data></node>
<node id="n2137" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/secretary-customer-service-sales-data-base-entry-46df311c156e477c8d7f5406e116de02</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Career Overview      Dedicated personable and motivated secretarycustomer service representative My focus is to maintain customer satisfaction and contribute to company success I have extensive work experience in a variety of customer service settings such as hospitals businesses and retail         Core Strengths           Strong organizational skills  Active listening skills  Sharp problem solver  Courteous demeanor  Energetic work attitude  Telephone inquiries specialist  Customer service expert  Invoice processing  Adaptive team player  Openingclosing procedures  Quick learner   Have worked with many business specific computer programs       Data collection  Data entry  Documentation  Email  Internet research  Speaking  Telephone skills  Multitasking                        Accomplishments       Customer Service    Researched calmed and rapidly resolved client conflicts to prevent loss of key accounts    Customer Interface    Greeted customers upon entrance and handled all cash and credit transactions  Assisted customers over the phone regarding store operations product promotions and orders     Database Maintenance    Assisted in the managing of the company database and verified edited and modified customers’ information         Work Experience       SecretaryCustomer ServiceSalesData Base Entry       092012   to   062015     Taco Bell    –    Gravette     AR             Responsible for daily operations and overall finances of a small but busy satellite company which includes but is not limited to billing budgeting customer invoicing through QuickBooks payroll quarterly payroll and company taxes Knowledge in word and excel spreadsheets Created customer accounts revising as necessary  Developed highly empathetic client relationships  Computed accurate sales prices for purchase transactions  Resolved product issues and shared benefits of new technology  Expressed appreciation and invited customers to return  Managed quality communication and customer support for each client  Interacted with customers to followup on shipping statuses and expedited orders  Promptly responded to general inquiries from members staff and clients via mail email and fax  Guaranteed positive customer experiences and resolved customer complaints  Executed outbound calls to existing customer            Manager       2011   to   042012     Smart Cow    –    City     STATE             Managed team of 34 employees  Served as mentor to junior team members  Took necessary steps to meet customer needs and effectively resolve food or service issues  Communicated clearly and positively with employees  Resolved customer complaints in a postive manner  Assisted in important decisions on new products and new employee hire   Quickly and efficiently processed payments and made accurate change  Closely followed standard procedures for safe food preparation assembly and presentation to ensure customer satisfaction  Performed general maintenance duties including mopping floors washing dishes wiping counter tops and emptying traps           SecretaryCNA in MomBaby Unit       082009   to   112011     Exempla Lutheran Medical Center    –    City     STATE              CNA     Took vital signs of mothers and newborns for a floor of up to 20 patients per shift  Took and recorded patients temperature pulse and blood pressure  Performed 24 hr infant testing including PKU hearing and jaundice  Worked as part of team to ensure proper care and safety of myself  and  patient and newborns  Assisted physicians with the circumcision of newborns  Accurately identified patients with patient chart by verbalizing and checking patient bracelet  Cleaned and sterilized instruments and disposed of contaminated supplies     Secretary     Accurately documented all elements of inpatient information discharge instructions and followup care  Managed smooth and effective communication among physicians patients families and staff  Handled incoming and outgoing correspondence including mail email and faxes  Screened telephone calls and inquiries and directed them as appropriate  Devised and maintained office systems to efficiently deal with paper flow  Organized personal and professional calendars and supplied reminders of upcoming meetings and events when necessary  Flexible and trained to fillin as secretary in sister units such as Labor and Delivery  and  NICU  Actively maintained strict confidentiality and safeguarded all patientrelated information with HIPPA knowledge    Well trained in hospital specific computer programs such as Epic and CPN            Educational Background       Obtained Cosmetology License      Cosmetology     Expected in   2012     Empire Beauty School      Arvada     CO     GPA                Obtained CNA License     Nursing     Expected in   2009     Front Range Community College      Denver     CO     GPA                           Expected in   2008     Community College of Denver      Denver     CO     GPA        Completed necessary courses that contributed to nursing career          Obtained high school diploma     Basic     Expected in   2004     Arvada High School      Denver     CO     GPA                Skills       Patientfocused care  Excellent interpersonal skills  Compassionate and trustworthy caregiver  Detailoriented  Effectively interacts with patients and families  Medical terminology  Charting and record keeping</data><data key="id">147203991935502946470515237354155400269</data></node>
<node id="n2138" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary      Meticulous and detailoriented Executive AssistantOffice Administrator adept at implementing innovative practices and procedures to improve efficiency Organized and implemented an efficient work flow system that resulted in 30 increase in productivity Established good working relationships with students faculty staff and members of the public Selected to assume additional duties as Recording Secretary for committee meetings averaging 20 attendees  Executive Assistant with management experience and exceptional people skills Versed in  communication  and  organization  Desires a challenging role as a teamplayer        Core Qualifications         Microsoft Office Suite Google Docs Datatel Excels in area pf details   Resultsoriented     Proficiency in supervision      Operations management   Clientfocused                       Experience       Data Entry Specialist       092015      102015     Elliot Davis    –    Nashville     TN            Prepared source data for computer entry by compiling and sorting information establishing entry priorities  Processed employee and account source documents by reviewing data for deficiencies resolving discrepancies by using standard procedures or returning incomplete documents to the team leader for resolution  Entered employee and account data by inputting alphabetic and numeric information on keyboard or optical scanner according to screen format  Maintained data entry requirements by following data program techniques and procedures           Executive Assistant       022015      072015     Xl Group    –    Miami     FL            Reviewed and provided comments on the adequacy of documents and took necessary steps to cure any deficiencies       Effectively controlled the release of proprietary and confidential information for general client lists  Represented Administrator through communication both verbal and electronic as well as in person  Extensive planning and preparation of meetings and events  Extensive creation and maintenance of records  Worked in liaison with College Deans offices organize faculty interviews  Organized coordinated and maintained Administrators calendar  Assisted prospective faculty with house hunting trips  Organized and coordinate prospective faculty moves  Arranged meetings with top level administrators  Coordinated travel arrangements for Administrator  Processed Sabbatical applications and various reports Processed faculty overloads and tuition reimbursements applications  Managed use and reconciled Administrators credit cards           Administrative AssistantOffice Administrator       2011      2015     Soriano Tax Services    –    City     STATE              Prepared correspondence accounting and financial documents for analysis  Provided onsite training      Provided tax preparation services represented clients before IRS resolved outstanding tax issues  Responded to inquiries maintain office database and files  Maintained and processed confidential information  Followed up on client inquires  Heavy telephone sales to secure clients           Administrative Secretary Vice President       082008      072011     California State University    –    City     STATE          Full time student at California State University Northridge majoring in Sociology         Office of Vice President       061997      032007     East Los Angeles College    –    City     STATE            Managed the daily operations of the Office of Academic Affairs  Prepared and distributed agendas of various committee meetings  Took transcribed and distributed minutes of various committee meetings  Maintained daily calendarschedule of administrator  Managed communication flow and work flow to administrators office  Established cross training system that improved efficiency of department by 50  Supervised trained clerical staff provided key input on performance evaluations Managed confidential personnel records  Served as official liaison between administrator and campus staff students and members of the public  Initiated coordinated and monitored hiring process for faculty and administrators  Composed correspondences reports presentations Proof read documents reports press releases for accuracy  Provided research support for reports and meetings  Communicated and implemented District policies  Monitored and tracked departments budget expenditures  Managed official travel system processed expense reports and reimbursements  Facilitated website updates on Office of Academic Affairs working in liaison with webmaster          Education       Bachelor of Arts       Sociology       Expected in   2011                California State		College of Behavioral Sciences      Northridge     CA     GPA        Status           Coursework in  Sociology      NonProfit  Humanitarian Work   Assisted in the coordination of fund raising campaigns for Doctors without Borders St Josephs Indian School and Oxfam International         Certifications     EDD Certification 73 words per minute with 98 accuracy Reeswood Secretarial College Certificate in Shorthand 120 wpm Graduated with honors in the program       Interests     Volunteered at Lighthouse for Women and Children a homeless shelter in Oxnard California       Languages     French Basic Spanish Basic       Skills     Academic budget clerical credit client clients data entry database expense reports French fund raising hiring keyboard team leader meetings Microsoft Office Suite Office 98 personnel policies presentations press releases Profit read research sales scanner Secretarial Shorthand sorting Spanish  tax preparation telephone travel arrangements website       Additional Information       Volunteered at Lighthouse for Women and Children a homeless shelter in Oxnard California</data><data key="id">97098914450695563702884660921271152925</data><data key="url">https://www.livecareer.com/resume-search/r/data-entry-specialist-36e2b426632b41d08cf146119467bc1c</data></node>
<node id="n2139" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-entry-operator-3586d5320e6848b198a737749a7eba1b</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary       Committed and motivated Administrative Assistant with exceptional customer service and decision making skills Strong work ethic professional demeanor and great initiative  Energetic and reliable Office Manager skilled with working with a diverse group of people  Executive Assistant who is skilled at multitasking and maintaining a strong attention to detail Employs professionalism and superior communication skills to meet client and company needs          Areas of Expertise           Word Excel Access Word Perfect  Operations management  Communication   Interpersonal       Time management  Flexible  Works well under pressure  Employee training and development                       Work Experience       Data Entry Operator       032014      082014     Iron Mountain Incorporated    –    Fort Myers     FL            Performed general data entry using SAP Microsoft Excel and Word    Performed a wide variety of secretarial tasks in support of the business   Answered phones and create notifications in the system   Contacted with internal and external customers  Collaborated with other administrative team members human resources and the finance department on special projects and events  Developed and managed thirdtier resolution process to resolve issues originating from the customer retention team           Secretary       2010      2013     Walt Disney Co    –    Auburn Hills     MI            Arranged appointments sales calendars trainings for the sales of department  Maintained the operations sales database  Customized sales reports and sales literature  Verified and logged in deadlines for responding to daily inquiries  Improved communication efficiency as primary liaison between departments clients and vendors           Assistant Manager       2008      2010     Sumitomo Electric Group    –    Mount Prospect     IL            Recruited hired scheduled and motivated a staff of up to 8 people  Adapted in communicating effectively with customers vendors and staff  Reached the monthly goals  Managed the daytoday tactical and longterm strategic activities within the business  Reduced and controlled expenses by improving resource allocation          Education       Associate       Arts       Expected in                   Community College of Philadelphia      Philadelphia     PA     GPA        Status          Arts          Certified with diploma              Expected in   1 2013                Notary Public                GPA        Status                 Professional Affiliations              Languages     English Spanish       Skills      Interpersonal data entry database English Languages Access Microsoft Excel Excel Word sales SAP secretarial Spanish phones Word Perfect</data><data key="id">138536846257210096623495337605137733129</data></node>
<node id="n2140" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-entry-customer-service-357f0974fe214cdfad27461bf1b36f96</data><data key="resume">JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary       Energetic and reliable Retail Sales Associate skilled in highend merchandise environments  Personable and responsible Cashier with 5 years in retail and customer service Solid team player with upbeat positive attitude  Resultsdriven with proven ability to establish rapport with clients  Dedicated Customer Service Representative motivated to maintain customer satisfaction and contribute to company success         Highlights          Problem resolution  Selfstarter  Deadlineoriented  Microsoft Office  Employee training and development  Customer service expert  Openingclosing procedures  Telecommunication skills      Strong organizational skills  Active listening skills  Large cashcheck deposits expert  Energetic work attitude  Top sales performer  Invoice processing  Courteous demeanor  Sharp problem solver                       Experience        072013   to   Current   Data Entry Customer Service    A Duie Pyle Inc         Stoughton     MA           •Performed general clerical duties as needed such as completing forms and reports  •Confered with Assembly Technician Lead and Manager to determine progress of work and to provide order status  •Assembled various components within established assembly time standards while adhering to procedures and set specifications  •Used Method Instructions to assemble equipment  •Inspect parts to ensure that quality standards are maintained  •Provide responsibility for the quality of work  •May cross train to perform other duties on the production lines  •Performs simple calculations  •Clean and maintain work area  •Will be required to wear personal protective equipment relevant to work area            Other duties as needed and assigned           072010   to   062013   Data Entry Clerk    Ferguson         Red Bank     NJ           •Received and scans large stacks of documents with excellent attention to quality  •Scan to file of hard copy job following standard operating procedures  •Inspect finished work for accuracy  •Conscientious and consistent effort to quickly and accurately complete each task andor job is the companys productivity standard  •File records as needed  •Destroy documents as stated within the policy           2010   to   062012   Front Desk ClerkNight Manager    Western Inn         City     STATE           • Process guest registrations including collecting payment   • Complete shift reports   • Respond to guest needs special requests and complaints alert the appropriate manager to potential issues as needed   • Assist customer with making room keys  • Assist customers in various assignments including finding nearby attractions restaurants and etc   • Prepare coffee for guest  • Transmit and receive messages via telephone and fax machine   • Sort and rack incoming messages and mail   • Assist guests with requests and problems related to their stay at the property   •Resolve guest complaints ensuring guest satisfaction  •Maintain complete knowledge of or where to access to following information a all hotel featuresservices hours of operation b all room types numbers layout decor appointments and location c all room rates special packages and promotions d daily house count and expected arrivalsdepartures e room availability status for any given day f scheduled daily group activities  •Pick up count and maintain bank Secure bank at all times  •Read the log book daily and record all pertinent information in the log book  •Process currency exchange and payments to guest accounts  •Process adjustments rebates paid outs and credits as required  •Verified that all checks are closed and closes and logs any open check in the POS Point of Sale system  •Run Room  Tax verifying that all room rates posted  •Verify Cashiers Report to drop log and paperwork  •Record room statistics  •Close POS after all work was balanced  •Run end of day program and close day  •Check that interfaces are up and running  •Run daily Flash Reports and distribute accordingly  •Run morning reports and backup reports and distribute accordingly  •Print express check out folios and distribute  •Sign out and brief relief  •Review Night Audit checklist and verify that all work has been completed  •Restock all printers  •Fill out and deposit payment and corresponding checks  •Review status of assignments and any followup action with oncoming Supervisor  •Document maintenance needs on work orders and submit to ManagerSupervisor             102008   to   2010   Retail Sales Consultant    Sprint         City     STATE           •Provided a total sales solution to the customer regarding their wirelessmobility needs that includes selling the value for Sprints devices accessories and service plans maximizing customer connections saving the customer money personalizing the customer experience protecting their investment  •Delivered an outstanding store experience that improves customer loyalty and strengthens the Sprint Brand  •Met key performance objectives that include sales and customer satisfaction goals  •Made certain accurate customer account setup so they are ready to use when leaving the store  •Identified the right solutions to customer billing technical and or account issues  •Completed all courses in your curriculum path with the required time frame  •Complied with all operational policies and procedures including the Sprint Code of Conduct  •Promote innovation and friendly competition to deliver unparalleled customer experience           112006   to   112008   ExpeditorCashier    Macys         City     STATE           •Assist customers in all aspects of service fulfillment by demonstrating proficient use of proprietary devices and applications proactively create enhanced shopping experiences through the heightened use of tools technology and collaboration  •Determined customer needs based on personal features and other customer preference related factors  •Demonstrated knowledge of store products and services to build sales and minimize returns  •Maintained a professional attitude with sincerity and enthusiasm reflecting Macy’s commitment to our customer – the most important person in our stores  •  POS procedures  •Assisted customers in all aspects of service fulfillment by demonstrating proficient use of proprietary devices and applications proactively create enhanced shopping experiences through the heightened use of tools technology and collaboration  •Recovered shoe sales floor and scan inventory back into stock  •Maintained integrity of shoe inventory by ensuring accuracy of scanning and placement  •Receive and process new merchandise          Education        Expected in   2008   Associates       Network Support System    Everest Institute     Houston     TX      GPA                 Expected in   2006   High School Diploma           United Christian School     Houston     TX      GPA               Accomplishments       Volunteer at Mustang Center teaching English to adults and teens 2010  until        Skills       10Key Account Management Active Learning Calendaring Client Relations Computer Proficiency Coordination Creative Problem Solving Critical Thinking Customer Needs Assessment Customer Service Data Collection Data Entry Documentation Email Executive Management Support Filing Grammar Internet Research Report Transcription Research Scheduling Service Orientation Speaking Spreadsheets Telephone Skills Time Management Travel Arrangements Travel Booking Travel Planning Type 3040 WPM Typing Writing Letters and Memos Lotus Notes Microsoft Excel Microsoft Office Suite Microsoft Outlook Microsoft PowerPoint Microsoft Word Minute Taking MultiTask Management Organizational Skills Prioritization Proofreading Reading ComprehensionCash handlingProfessional and friendly Careful and active listener Multitasking Production Mechanical Assembler Packager Labeling Inventory</data><data key="id">321556423433782918032074297826301460362</data></node>
<node id="n2141" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-entry-operator-381caf3c16b649328748d7e6c673a7d8</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Energetic worker with a broad range of customer service and team leader skills Data Entry specialist adept at developing and maintaining databases Highly skilled at creating effective organizational and filing systems Dynamic Data Analyst trained in an IT environment who goes above and beyond given job responsibilities to achieve superior results and maintain companywide integrity        Highlights           Certified in 10key  Time management  Meticulous attention to detail  Resultsoriented  Selfdirected  Excellent communication skills  Strong problem solver  Filing and data archiving  HIPAA compliance      Advanced MS Office Suite knowledge  Resourceful  Strong interpersonal skills  Pleasant demeanor  Understands grammar  Customer serviceoriented  Advanced clerical knowledge  Critical thinker                       Accomplishments       Data Entry    Preserved an accuracy of 98 during 5 years of employment    Training    Successfully trained staff in all office systems and databases policies and procedures while focusing on minimizing errors and generating superior results    Reporting    Maintained status reports to provide management with updated information for client projects    Administration    Performed administration tasks such as filing developing spreadsheets faxing reports photocopying collateral and scanning documents for externaldepartmental use    Multitasking    Demonstrated proficiencies in telephone email fax and frontdesk reception within highvolume environment   Implementation   Assisted in implementation of new tracking system that resulted in improved patient care    OSHA Compliance    Properly disposed of daily biohazard waste in compliance with federal and local regulations   Documentation   Drafted documents for internal meetings          Experience       Data Entry Operator       082009      Current     Iron Mountain Incorporated    –    Essex Junction     VT            8000 key strokes per hour  Trained staff to operate new environmental health technology  Verified that information in the computer system was uptodate and accurate  Processed confidential medical information  Identified and resolved system and account issues  Developed and created a more effective filing system to accelerate paperwork processing  Trained new employees and explained protocols clearly and efficiently  Provided base level IT support to company personnel  Troubleshot hardware issues and worked with service providers to facilitate repairs           Tutor       032012      062012     Arizona State University    –    Tempe     AZ            Tutored college level students in the fields of reading writing and computers  Routinely met with students regarding inclass issues and learning interruptions to discuss solutions  Performed student background reviews to develop tailored lessons based on student needs  Taught Creative writing to a diverse class of 20 students  Developed and implemented interesting and interactive learning mediums to increase student understanding of course materials           VolunteerMentor       082007      052008     Boys And Girls Club Of Northeast Florida    –    City     STATE            Coordinated after school tutoring hours to help students in need of extra attention  Received high remarks for the creativity of classroom lesson plans and instructional techniques from students parents and faculty  Created and enforced childbased handson curriculum to promote student interest and receptive learning  Designed lesson plans focused on age and levelappropriate material  Developed interesting course plans to meet academic intellectual and social needs of students  Consistently met schedules and deadlines for all illustration projects  Worked alongside the entire development team in an energetic and creative environment          Education       Associate of Arts       Business Administration       Expected in   2014                Florida State College at Jacksonville      Jacksonville     Florida     GPA        Status          360 GPA  Member of Phi Theta Kappa Honor Society  Recipient of 2014 Academic Achievement Award  Coursework in Marketing Public Relations and Business Management           Skills      10Key  Customer Service  Data Entry  Microsoft Office Suite</data><data key="id">283692866702135157759709078394118382144</data></node>
<node id="n2142" labels=":CV"><data key="labels">:CV</data><data key="url">https://www.livecareer.com/resume-search/r/data-management-services-coordinator-368330306fc74431af7e0137d3706178</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  9XXX5    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary     Administrator with over 15 years of professional experience Skilled in all aspects of office administration organization of filing systems use of electronic office equipment handling multiline phone systems reception data entry coordinating with staff scheduling appointments banking and accounts receivable and payable Communication skills demonstrated through verbal and writing abilities client relations marketing expertise customer service skills training new employees and the ability to produce indepth reports and correspondence       Highlights           Confidential Correspondence and Data  Microsoft Excel  Microsoft Word  Microsoft Outlook  Microsoft PowerPoint  Data Entry  Document Creation and Maintenance  Editing and Proofreading  Information Resource  Knowledge of Office Equipment CopierFax  10Key Calculator  Agenda and Event Coordination  Business Correspondence  Client Services  Call Screening  Mail Distribution  Stocking and Supplies  Typing  Data Entry  Billing Processes  Purchasing and Inventory  Payroll and Accounts Administration                         Experience       Data Management Services Coordinator       2009   to   Current     Penguin Random House    –    City     STATE             Performs highly accurate and detailed data entry for end of month invoicing  Performs data entry for business account orders in a timely manner  Responsible for several monthly reports submitted to management   Compiled statistical information for special reports     Organized billing and invoice data     Updated departmental standard operating procedures and database to accurately reflect the current practices       Identified and resolved system and account issues         Crosstrained and provided backup for other data management representatives when needed       Resolved spreadsheet issues and shared benefits of new technology         Interacted with customers to followup on shipping status and expedited orders           Promptly responded to general inquiries from members staff and clients via mail email and fax             Assisted customers in finding outofprint items             Kept abreast of rapidly evolving technology             Provided accurate and appropriate information in response to customer inquiries             Dispersed incoming mail to correct recipients throughout the office               Organized files developed spreadsheets faxed reports and scanned documents               Received and screened a high volume of internal and external communications including email and mail               Created and maintained spreadsheets using advanced Excel functions and calculations to develop reports and lists                  ReceptionistCashier Supervisor       2008   to   2009     Koons Of Westminster    –    City     STATE              Assessed customer needs and responded to questions       Organized register supplies      Worked with customer service to resolve issues        Provided professional and courteous service at all times       Worked overtime shifts during busy periods       Monitored a   team of  78  of professionals    Trained and mentored new cashiers      Hired 34 team members     Managed cashier shifts and breaks       Built and maintained productive relationships with employees       Greeted customers promptly and responded to questions       Documented performance issues       Counted and balanced cashier drawers       Worked in competitive team environment to exceed revenue quotas             OfficeProgram Assistant       2004   to   2008     General Dynamics Information    Technology    –    City     STATE             Maximized productivity by maintaining multiple calendars scheduling meetings tracking expenses and prioritizing phone calls for Program ManagersMaintained office equipment and ordered supplies  Prepared weekly spreadsheets monitoring more than 15 ongoing projectsOversaw status of projects by continually gathering information and followingup with Program Managers  Updated dynamic organizational charts and headcount spreadsheets  Answered multiline telephone system maintained appointment calendar filed personnel records and assisted Program Manager  Performed timely and highly accurate data entry to ensure fastest turnaround possible for end of month invoicing  Developed planned organized and administered policies and procedures for organization to ensure administrative and operational objectives were met  Implemented corrective action plan to solve problems  Established and maintained comprehensive and current record keeping system of activities and operational procedures in business office  Prepared reviewed and submitted reports concerning activities expenses budget government statutes and rulings and other items affecting business and program services  Consulted with staff and others in government business and private organizations to discuss issued coordinate activities and resolve problems  Prepared budget and directed and monitored expenditures of department funds  Directed and conducted studies and research and prepared reports and other publications relating to operational trends and program objectives and accomplishments           Loan Editor       2001   to   2004     BancFirst    –    City     STATE             Verified and examined information and accuracy of loan application and closing documents  Recorded applications for loan and credit loan information and disbursement of funds using computer  Accepted payment on accounts   Filed and maintained loan records    Presented loan and repayment schedule to customer   Calculated reviewed and corrected errors on interest principal payment and closing costs using computer and calculator    Contacted credit bureaus employers and other sources to check applicant credit and personal references    Assembled and compiled documents for closing such as title abstract insurance form loan form and tax receipt   Prepared and typed loan applications closing documents legal documents letters forms government notices and checks using computer    Interviewed loan applicant to obtain personal and financial data and to assist in filling out application    Complied with federal state and company policies procedures and regulations    Debited and credits accounts   Processed negotiable instruments such as checks and vouchers   Evaluated records for accuracy of balanced postings calculations and other records pertaining to business and operating transactions and reconciled and notes discrepancies   Recorded financial transactions and other account information to update and maintain accounting records          Education       Associate     Accounting     Expected in   2007     Ashworth University      Norcross     GA     GPA   GPA 355    Accounting GPA 355        Skills     10Key Calculator accounting administrative Billing budget Business Correspondence calculator charts closing credit Client Data Entry Editing Event Coordination Fax filling Financial forms funds government insurance Inventory invoicing legal documents letters notes meetings Microsoft Excel Mail office Microsoft Outlook Microsoft PowerPoint Microsoft Word Office Equipment Office Management organizational Payroll personnel Copier policies Processes Proofreading publications Purchasing record keeping research scheduling spreadsheets tax telephone phone Typing</data><data key="id">189067289296683940487677712533012870883</data></node>
<node id="n2143" labels=":CV"><data key="labels">:CV</data><data key="resume">Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary       Proactive performancedriven professional with 105  years of experience in IT industry in Investment Banking domain   Proficient in the Integration of various data sources with multiple relational databases like Oracle11g Oracle10g9i Sybase into the staging area Data Warehouse which includes developing PLSQL Procedures Packages Triggers Bulk collections Cursors Views Objects and Performance Tuning of Data Warehouse environment  Data Warehousing ETL experience of using Informatica PowerCenter Client tools  Mapping Designer Repository manager Workflow ManagerMonitor and Server tool Repository Server manager   Proficient in UNIX shell scripting in automation of various processes Hands on experience in server setup to host files to automate the database refresh with help of Perlcgi scripts Developing web page applications using HTML5 CSS3 and Javascript for form validations Also using Ajax and PHP scripting to get back data from Oracle DB     Experience in using Automation Scheduling tools like Autosys and ControlM     Knowledge and hands on in Informatica ETL Tool for loading huge data files     Handson experience across all stages of Software Development Life Cycle SDLC including business requirement analysis data mapping build unit testing systems integration and user acceptance testing     Experience working in OnShoreOffshore Development Models     Strong time management skills in leading multiple projects and deadlines under minimal supervision     Recognized and highly appreciated for consistent success knowledge and flexibility          Areas of Expertise           Windows NTXP2007 UNIX MSDOS  ETL Tools Informatica Power Center 918171 Designer Workflow Manager Workflow Monitor Repository manager and Informatica Server  Databases Oracle 11g10g9i8i Sybase  Languages SQL PLSQL UNIX Shell scripts Perl HTML5 Java Scripting PHP  Scheduling Tools Autosys ControlM                         Work Experience       Data Warehouse Developer       032015      Current     22Nd Century Technologies       Bothell     WA            Customer Warehouse Environment CWE is a group in Fidelity focusing on providing analytical data to various business groups  This data helps the business community in increasing the Fidelity business in Retail and Institutional areas  The data constitutes of various customers accumulated from various sources  Major responsibilities include impact analysis support production support activities install planning and help answer business adhoc requests  Role and Responsibilities Providing resolution to the issues raised by users  Monitoring and providing batch support for the daily and monthly batches  Conduct regular meetings to check for weekly and monthly install  Provide month end batch support and provide oncall support  Enhancement of the existing application to provide more functionality  Identify long running process and tune the same  Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  Used SQLDeveloperToad for creating all types of PLSQL objects like tables Indexes packages triggers sequences and stored procedures  Optimized the existing long running processes to run faster with usage of Bulk Loader Hints and limits on bulk loader Used Explain Plans to analyze long running queries and degree of Parallel to make queries run faster  Create new web applications from HTML5 and CSS  Javascript for form validation Host the web applications on windows server 2012 R2 machine Used PHP as the server side language to psftp the files from users machine  Call oracle client functions ODBC to connect to db and get back the results on HTML forms           ETLData Warehouse Developer       072012      032015     22Nd Century Technologies        Burkeville     VA            The Strategic Investment Product team works on providing investment plans for individuals investment plan and guidance tool for retailers retirement plans 401k for organizations  The tasks involved as part of this are loading and processing the stocks and mutual fund details from different sources categorize and customize such that retail and individual investors get the accurate details which would help in investments  Role and Responsibilities Understand and analyze requirements follow up with business analysts and subject matter expert team for any clarification if required  DesignReview the Test Cases for Integration testing System testing and User Acceptance testing Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  From staging area data is processed and modified as per the requirement and then loaded to integration area through stored procedures and ETL Informatica mappings  Use Toad for developing all types of PLSQL objects like tables Indexes packages triggers sequences and stored procedures  Developed and modified Oracle packages stored procedures functions Scripts synonyms tables and indexes to implement business requirements and rules  Applied optimizer hints to tune the queries for faster performance  Worked on Performance tuning by using Explain plans and various hints  Worked on Table Partitioning and deploying various methods of indexing like local Indexes and Global Indexes on partitioned tables  Organize and analyze the behavior of each Investment Instrument like Stocks Bonds etc for a certain period and provide the complete detailed analysis to investors which would help them in planning their portfolio and the investments  Evaluate the performance of different funds which would be ideal for long term investing which is the primary requirement of a retirement plan 401k aimed at institutions  Project Title PARA reporting for an Investment Bank           Data Warehouse Developer       2009      062012     22Nd Century Technologies        Columbia     MO            Location Bangalore Tokyo and NY Description The department division deals in reporting profit and loss to back to the central system  It also involves doing the daily adjustments and sending the feed to different streams which will do further processing at their end  The data involved capital markets data like Securities Bonds Funds Repos etc  Role and Responsibilities Handling all user support requests from across the regions APAC UK  USA  Automating frequent process with shell scripting and Autosys scheduler  Handling major UATs independently  representing client from offshore office  Responsible for all Production Release across regions  Generate reports required by Japanese and US Federal Gov  Developed and modified Packages Functions Synonyms tables and indexes to implement business requirements and rules  Bulk loading of data done using utilities like BCP and Sybase Central  Code tuning or Query Optimization done using Explain Plans Hints  Analyzing and monitoring system performances using DBCC Trace on commands query plan outputs system  Handling various enhancements to the system this entailed writing new codes and changing some existing codes  Wrote new complex stored procedures in Sybase and optimized the existing code written in Sybase  Debugging the scripts and jobs in Production environment written in Shell and PERL  Handling and creating various Autosys jobs in Production environment  Providing daily status reports to the clients  Provided production support and 247 support for resolving the critical production issues  Involved in the solving the tickets that are raised by the end users  Responsible for creating PLSQL Programs and UNIX Scripts for Data Validation and Data Conversion  Project Title Basel 2 Risk Platform UK based bank           Data Warehouse Developer       012006      112008     Royal Bank Of Scotland        City     STATE            Location Bangalore Description FMIT is a dedicated IT division within RBS and we provide a fully managed service for Finance IT Business As Usual BAU team  Designed the tables indexes triggers stored procedures functions and packages to implement the requirement  Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  Informatica Mappings are used in loading the target databases which is usually on different schema and the data is transformed in the process before loading to target  Analysis of Source data Oracle Source Objects and identifying the methods for loading data to target  Created and used External Tables for migrating flat file data into target  Responsible for the creation of Packages Functions and Procedures  Performed Testing of the Packages Procedures and Functions  Wrote scripts for creating tables Indexes Grants and Synonyms in different schemas and made modifications for the existing tables as per business logic  Preparation of test scripts for System and Integration Testing and Traceability Matrix for the assurance of complete coverage of system requirements  Develop bug fixes and enhancements  Production support of some of the applications of the bank which includes resolving issues of the overnight batch processes and daily users queries          Education       Bachelors       Telecommunication       Expected in   2005                VTU                GPA        Status         Telecommunication        Professional Affiliations              Skills     Bonds capital markets CSS client clients Data Conversion Data Validation Databases Debugging ETL Finance forms ftp Funds Grants HTML HTML5 PHP indexing Informatica investments Japanese Javascript Java Scripting logic meetings office Windows NT works MSDOS ODBC Operating Systems Optimization Oracle db Developer PLSQL PLSQL PERL processes profit and loss reporting requirement Retail Scheduling Securities Shell Scripts Shell scripts shell scripting SQL Strategic Sybase Tables user support Toad UNIX Unix scripts utilities validation web applications windows server Workflow written</data><data key="id">14259385793885776770164514431890372233</data><data key="url">https://www.livecareer.com/resume-search/r/data-warehouse-developer-35a285e749a74031816e915414d2ae8b</data></node>
<node id="n2144"><data key="name">evaluating trends</data></node>
<node id="n2145"><data key="name">adhoc analysis</data></node>
<node id="n2146"><data key="name">customized analysis</data></node>
<node id="n2147"><data key="name">loading data</data></node>
<node id="n2148"><data key="name">accurate data</data></node>
<node id="n2149"><data key="name">research data</data></node>
<node id="n2150"><data key="name">ms sharepoint</data></node>
<node id="n2151"><data key="name">ms onenote</data></node>
<node id="n2152"><data key="name">performance analysis</data></node>
<node id="n2153"><data key="name">claims analysis</data></node>
<node id="n2154"><data key="name">rapid application development</data></node>
<node id="n2155"><data key="name">adhoc testing</data></node>
<node id="n2156"><data key="name">backend testing</data></node>
<node id="n2157"><data key="name">data mismatch</data></node>
<node id="n2158"><data key="name">collecting data</data></node>
<node id="n2159"><data key="name">system analysis</data></node>
<node id="n2160"><data key="name">query optimization</data></node>
<node id="n2161"><data key="name">technical design</data></node>
<node id="n2162"><data key="name">erd</data></node>
<node id="n2163"><data key="name">nvivo</data></node>
<node id="n2164"><data key="name">analytical research</data></node>
<node id="n2165"><data key="name">data linking</data></node>
<node id="n2166" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: Administrators, industry professionals and AI enthusiasts are invited to join this session.</data><data key="data">Gender and AI a feminist perspective  For this session  Anastasia Karagianni  Doctoral Candidate at the LSTS Department of the Law and Criminology Faculty of VUB  will examine gender issues in AI from a feminist perspective Through constant interaction with the audience  key definitions of AI will be provided 1st part  examplescase studies of discriminatory treatment by AI systems will be referred 2nd part  while an analysis of gender equality ethics and feminist principles will be shared to address these issues A balanced mix of learning and relaxation Attendees will have the opportunity to delve into sustainable digitalisation with Anastasia  followed by a casual happy hour with snacks and beverages This event is free to attend  but registration is compulsory  More info  Share this course   httpswwwvubbeeneventaihappyhourgenderandaifeministperspective 25 Mar 2024 1730  1900 lectureBrusselsFARI</data><data key="annotated_by">manual</data><data key="sub_title">25 Mar 2024 17:30 - 19:00</data><data key="start_time">25 Mar 2024 17:30 - 19:00</data><data key="location_detail">FARI  Cantersteen 16, 1000 Brussels</data><data key="price">Gratis</data><data key="subscription_limit">NA</data><data key="intro">More info  Share this course  </data><data key="date">22/03/2024</data><data key="language">English</data><data key="course_info">lecture-Brussels-FARI</data><data key="full_body"> For this session  Anastasia Karagianni  Doctoral Candidate at the LSTS Department of the Law and Criminology Faculty of VUB  will examine gender issues in AI from a feminist perspective. Through constant interaction with the audience  key definitions of AI will be provided (1st part)  examples/case studies of discriminatory treatment by AI systems will be referred (2nd part)  while an analysis of gender equality ethics and feminist principles will be shared to address these issues. A balanced mix of learning and relaxation Attendees will have the opportunity to delve into sustainable digitalisation with Anastasia  followed by a casual happy hour with snacks and beverages! This event is free to attend  but registration is compulsory. </data><data key="url">https://www.vaia.be/en/courses/gender-and-ai-a-feminist-perspective</data><data key="title">Gender and AI, a feminist perspective</data><data key="constraints">NA</data><data key="details">https://www.vub.be/en/event/ai-happy-hour-gender-and-ai-feminist-perspective</data></node>
<node id="n2167" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: decision makers from public organizations</data><data key="data">Enabling the future of work  This course will be given by Prof Rob Heyman on Friday 22 March 2024 from 930 to 1630 CET at the FARI Test  Experience Center Cantersteen 16  1000  Brussels The problems or challenges we wish to address in this 1day course are Adopt AI or die there is a push to use a form of artificial  intelligence for each organization  and failure to do so seems to imply  the end of your organization's history But how do you ensure that your  investments in AI are sustainable and successfully adopted by your  colleagues and employees While you are adapting to this new technological context  you will also have to deal with new regulations How will you do this Lastly  adopting technological and legal change is not something you  do on your own  so what are methods to facilitate this change in your  organization While not absolutely necessary  it helps to have a clear challenge in  mind when joining this session Rob will invite you to work on your  innovation case or with someone in case you lack one Knowledge of legal frameworks or AI is not necessary Session 1 How to and how not to Introduce Automation on the Work Floor We will look at practical  theoretical concepts to understand  innovation on the work floor and the tension between users and  technology The aim is to understand all successful technology  adoptions as innovations that change users' daily practices Next  we discuss cases where AI or industry 40 applications were  introduced and what these meant for different stakeholders If you have a  case  you wish to discuss  be sure to contact Rob Heyman to discuss  this as part of the session Session 2 Boosting Operational Efficiency by Reducing Digital Friction and Increasing Agility If you believe innovation can only be successful if practices change  then it follows that regulations can only be successful if regulations change practices  too In this part we look at the particular challenges we experienced during the implementation of the GDPR and what this might mean for the implementation of the AI Act Session 3 Improving Information Protection and Governance During the last part of the day  we will use a case for innovation of your choosing and define a workshop or process to include colleagues and employees in this process In this interactive part  you will learn a few facilitation methods and the key ingredients to define cocreation projects on your work floor This training was designed for decision  makers from public organizations that want to fully empower workforces  with flexible digital workplaces  increased agility and stronger  information protection to address current business needs and evolving  challenges As FARI is supported by the European Resilience  Recovery Fund RRF  we can continue to offer free access to participants from public administrations public institutions and research institutions We can also provide a 50 discount to participants from the following target groupsEducational institutionsNGOs and nonprofit organisationsBrussels based companiesBrussels citizens  More info  Share this course   httpswwwfaribrusselseducationenablingthefutureofwork 22 Mar 2024 0930  1630 1day courseBrusselsFARI</data><data key="sub_title">22 Mar 2024 09:30 - 16:30</data><data key="start_time">22 Mar 2024 09:30 - 16:30</data><data key="location_detail">Be Central, FARI Experience Centre - Cantersteen 16, 1000 Bruxelles</data><data key="price">€250 excl. VAT</data><data key="intro">More info  Share this course  </data><data key="details">https://www.fari.brussels/education/enabling-the-future-of-work</data><data key="full_body"> This course will be given by Prof. Rob Heyman on Friday 22 March 2024 from 9.30 to 16:30 CET at the FARI Test &amp; Experience Center (Cantersteen 16  1000  Brussels). The problems or challenges we wish to address in this 1-day course are: Adopt AI or die: there is a push to use a form of artificial  intelligence for each organization  and failure to do so seems to imply  the end of your organization's history. But how do you ensure that your  investments in AI are sustainable and successfully adopted by your  colleagues and employees? While you are adapting to this new technological context  you will also have to deal with new regulations. How will you do this? Lastly  adopting technological and legal change is not something you  do on your own  so what are methods to facilitate this change in your  organization? While not absolutely necessary  it helps to have a clear challenge in  mind when joining this session. Rob will invite you to work on your  innovation case or with someone in case you lack one. Knowledge of legal frameworks or AI is not necessary. Session 1: How to and how not to Introduce Automation on the Work Floor We will look at practical  theoretical concepts to understand  innovation on the work floor and the tension between users and  technology. The aim is to understand all successful technology  adoptions as innovations that change users' daily practices. Next  we discuss cases where AI or industry 4.0 applications were  introduced and what these meant for different stakeholders. If you have a  case  you wish to discuss  be sure to contact Rob Heyman to discuss  this as part of the session. Session 2: Boosting Operational Efficiency by Reducing Digital Friction and Increasing Agility If you believe innovation can only be successful if practices change  then it follows that regulations can only be successful if regulations change practices  too. In this part we look at the particular challenges we experienced during the implementation of the GDPR and what this might mean for the implementation of the AI Act. Session 3: Improving Information Protection and Governance During the last part of the day  we will use a case for innovation of your choosing and define a workshop or process to include colleagues and employees in this process. In this interactive part  you will learn a few facilitation methods and the key ingredients to define co-creation projects on your work floor. This training was designed for decision  makers from public organizations that want to fully empower workforces  with flexible digital workplaces  increased agility and stronger  information protection to address current business needs and evolving  challenges. As FARI is supported by the European Resilience &amp; Recovery Fund (RRF)  we can continue to offer free access to participants from public administrations public institutions and research institutions. We can also provide a 50% discount to participants from the following target groups:Educational institutionsNGOs and non-profit organisationsBrussels based companiesBrussels citizens </data><data key="language">English</data><data key="course_info">1-day course-Brussels-FARI</data><data key="title">Enabling the future of work</data><data key="url">https://www.vaia.be/en/courses/enabling-the-future-of-work</data><data key="constraints">have a clear challenge in mind when joining this session. Knowledge of legal frameworks or AI is not necessary.</data><data key="date">21/03/2024</data></node>
<node id="n2168" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="sub_title">Seminar by Matías R. Bender, CMAP, École Polytechnique</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: researchers and academics with an interest in advancing the theoretical underpinning of AI algorithms</data><data key="data">A new symbolicnumeric method to solve the multiparameter eigenvalue problem  In this talk  we focus on polynomial systems coming from the multiparameter eigenvalue problem and certain generalizations Using the theory of resultants and Weyman complexes  we present new matrices of optimal size for solving these systems This talk is based on joint work with Jean Charles Faugère  Angelos Mantzaflaris  and Elias Tsigaridas  A classical approach to solving polynomial systems is to linearize the problem and reduce it to an eigenvalue calculation For this purpose certain families of special matrices are used eg Sylvester and Dixon matrices Their size and structure determine how far these methods can go therefore it is essential to construct better matrices for the specific systems that arise in practice  httpshomesesatkuleuvenbesistawwwbdmbacktotherootsindexphpseminars Seminar by Matías R Bender CMAP École Polytechnique seminarHeverlee LeuvenKU Leuven ESAT STADIUS</data><data key="start_time">21 Mar 2024 17:00 - 18:00</data><data key="language">English</data><data key="location_detail">KU Leuven, Department of Electrical Engineering (ESAT), Aula C (ELEC B91.300)</data><data key="price">free</data><data key="date">NA</data><data key="details">https://homes.esat.kuleuven.be/~sistawww/bdm/backtotheroots/index.php/seminars</data><data key="full_body"> In this talk  we focus on polynomial systems coming from the multiparameter eigenvalue problem and certain generalizations. Using the theory of resultants and Weyman complexes  we present new matrices of optimal size for solving these systems. This talk is based on joint work with Jean Charles Faugère  Angelos Mantzaflaris  and Elias Tsigaridas. </data><data key="intro">A classical approach to solving polynomial systems is to linearize the problem and reduce it to an eigenvalue calculation. For this purpose, certain families of special matrices are used, e.g., Sylvester and Dixon matrices. Their size and structure determine how far these methods can go; therefore, it is essential to construct better matrices for the specific systems that arise in practice. </data><data key="url">https://www.vaia.be/en/courses/a-new-symbolic-numeric-method-to-solve-the-multiparameter-eigenvalue-problem</data><data key="title">A new symbolic-numeric method to solve the multiparameter eigenvalue problem</data><data key="constraints">NA</data><data key="course_info">seminar-Heverlee; Leuven-KU Leuven ESAT STADIUS</data></node>
<node id="n2169" labels=":Course"><data key="labels">:Course</data><data key="data">How Do Large Language Models Learn  With Vincent Ginis and Andres Algaba Organized by Professor Vincent Ginis  VUB AI steward  and Guest Professor Andres Algaba  generative AI coordinator  this series aims to articulate the VUBs guidelines on the emergent technology Prepare for an indepth exploration of generative AIs mechanics  its practical applications  and the ethical considerations it raises  including data privacy and biases For the entire VUB Community  The Generative AI Conversations are open to the entire VUB community  including students  faculty  research  administrative  and technical staff Each session features a presentation complemented by testimonials and a panel discussion  fostering a rich dialogue on generative AI Accessible in person or online Held in building I  room I003 at the VUB Main Campus  the lectures offer the flexibility to participate in person or online  ensuring everyone has a chance to engage with these discussions  Join us on Thursday 21 March from 16h30 to 18h30 for the inaugural lecture of our series How do large language models learn Hosted in room I003 at the VUB Main Campus this event is the second of five compelling lectures in the series Generative AI Conversations designed to navigate the complexities and challenges of generative AI shedding light on its implications for our university and beyond Moer specifically well take a closer look at the learning process of LLMs and the development of their emergent properties httpseventsvubbeHowdolargelanguagemodelslearnen A series of 5 lectures about the best use of AI Lecture SeriesBrusselsVUB</data><data key="annotated_by">manual</data><data key="sub_title">A series of 5 lectures about the best use of AI</data><data key="subscription_limit">21 Mar 2024</data><data key="target_group">Target audience: VUB students, faculty, researchers, administrative, and technical staff</data><data key="start_time">21 Mar 2024 16:30 - 18:30</data><data key="language">English</data><data key="location_detail">VUB, Pleinlaan 2 1050 Ixelles , Building I room I.0.03</data><data key="price">Free</data><data key="date">21/03/2024</data><data key="full_body"> With Vincent Ginis and Andres Algaba Organized by Professor Vincent Ginis  VUB AI steward  and Guest Professor Andres Algaba  generative AI coordinator  this series aims to articulate the VUB's guidelines on the emergent technology. Prepare for an in-depth exploration of generative AI's mechanics  its practical applications  and the ethical considerations it raises  including data privacy and biases. For the entire VUB Community  The Generative AI Conversations are open to the entire VUB community  including students  faculty  research  administrative  and technical staff. Each session features a presentation complemented by testimonials and a panel discussion  fostering a rich dialogue on generative AI. Accessible in person or online Held in building I  room I.0.03 at the VUB Main Campus  the lectures offer the flexibility to participate in person or online  ensuring everyone has a chance to engage with these discussions. </data><data key="intro">Join us on Thursday 21 March, from 16h30 to 18h30 for the inaugural lecture of our series, "How do large language models learn." Hosted in room I.0.03 at the VUB Main Campus, this event is the second of five compelling lectures in the series 'Generative AI Conversations', designed to navigate the complexities and challenges of generative AI, shedding light on its implications for our university and beyond. Moer specifically, we'll take a closer look at the learning process of LLMs and the development of their emergent properties.</data><data key="course_info">Lecture Series-Brussels-VUB</data><data key="url">https://www.vaia.be/en/courses/generative-ai-conversations</data><data key="title">How Do Large Language Models Learn?</data><data key="constraints">NA</data><data key="details">https://events.vub.be/How_do_large_language_models_learn-en</data></node>
<node id="n2170" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: Academics, Research, L&amp;amp;D, Education Personnel</data><data key="data">Learning Bytes Festival  Explore the forefront of innovation as Learning Bytes Festival showcases the results of imecs research and innovation programmes on Smart Education and EdTech Whether youre in education  training  or corporate learning  discover how digital technology can drive innovation in your practices  Join us for several workshops and talks  delving into the realms of digital transformation  innovation  research  and technology in education and training Be inspired by captivating keynotes and participate in engaging panel discussions that promise to enrich your understanding of the evolving landscape  Proudly partnering with digiwijs for this unique edition  Learning Bytes Festival offers an interactive opportunity to explore personalized and mediasavvy learning Immerse yourself in handson workshops tailored for IT coordinators  teachers  and board members in kindergarten  primary  and secondary education  Experience a unique edition of the Learning Bytes Festival this year  seamlessly integrated into the Festival about Technology and Innovation FTI by Hangar K  an initiative of the Flemish Government  Explore innovation at Learning Bytes Festival showcasing imecs Smart Education and EdTech research Delve into digital technologys impact on education and corporate learning through keynotes workshops talks and panel discussions This edition focuses on AI and is integrated into FTI a Tech and Innovation Festival by Hangar K supported by the Flemish Government Welcoming keynote speakers Monica Arés former head of Meta Immersive Learning  Steven Latré head of AI at imec httpswwwlearningbytesfestivalbe Smart Learning in a Digital World eventKortrijkimec itec KU Leuven VUB UGent</data><data key="annotated_by">manual</data><data key="sub_title">Smart Learning in a Digital World</data><data key="start_time">20 Mar 2024 09:00 - 20:00</data><data key="location_detail">Budascoop Kortrijk, Kapucijnenstraat 10, 8500 Kortrijk</data><data key="price">€80,00</data><data key="subscription_limit">NA</data><data key="full_body"> Explore the forefront of innovation as Learning Bytes Festival showcases the results of imec's research and innovation programmes on Smart Education and EdTech. Whether you're in education  training  or corporate learning  discover how digital technology can drive innovation in your practices.  Join us for several workshops and talks  delving into the realms of digital transformation  innovation  research  and technology in education and training. Be inspired by captivating keynotes and participate in engaging panel discussions that promise to enrich your understanding of the evolving landscape.  Proudly partnering with digiwijs for this unique edition  Learning Bytes Festival offers an interactive opportunity to explore personalized and media-savvy learning. Immerse yourself in hands-on workshops tailored for IT coordinators  teachers  and board members in kindergarten  primary  and secondary education.  Experience a unique edition of the Learning Bytes Festival this year  seamlessly integrated into the Festival about Technology and Innovation: FTI by Hangar K  an initiative of the Flemish Government. </data><data key="intro">Explore innovation at Learning Bytes Festival showcasing imec's Smart Education and EdTech research. Delve into digital technology's impact on education and corporate learning through keynotes, workshops, talks, and panel discussions. This edition focuses on AI and is integrated into FTI, a Tech and Innovation Festival by Hangar K, supported by the Flemish Government. Welcoming keynote speakers Monica Arés (former head of Meta Immersive Learning) &amp; Steven Latré (head of AI at imec).</data><data key="language">English</data><data key="details">https://www.learningbytesfestival.be/</data><data key="constraints">NA</data><data key="course_info">event-Kortrijk-imec; itec; KU Leuven; VUB; UGent</data><data key="date">20/03/2024</data><data key="url">https://www.vaia.be/en/courses/learning-bytes-festival-2024</data><data key="title">Learning Bytes Festival</data></node>
<node id="n2171" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">15 Mar 2024</data><data key="target_group">Target audience: anyone interested in the impact of AI on women and gender</data><data key="data">The impact of AI on women and gender  Artificial intelligence AI is increasingly and rapidly making its way into our digitalised society Although AI can bring about benefits  such as optimising job search services for more equal work opportunities  it inherently comes with several risks  such as perpetuating and exacerbating harmful gender inequities and stereotypes in training data or generating hypersexualised content Join us for a panel discussion with four KU Leuvenexperts who will bring different perspectives to our understanding of the impact of AI on women and gender The event is open to anyone interested in this topic and requires no prior AI knowledge  1900 Welcoming by Lean In Leuven Law 1905 General introduction to AI by Tinne De Laet 1915 Panel discussion with Sien Moens Massimiliano Simons Bieke Zaman and Emine Ozge Yildirim Moderated by Tinne De Laet 2000 QA 2030 Reception   In light of the International Women's Day Lean In Leuven Law is therefore organising an interdisciplinary panel discussion that will focus on the impact of AI on women and gender from a computer science philosophical social sciences and legal perspective This event is organised in collaboration with LeuvenAI and DigiSoc  the KU Leuven Digital Society Institute httpswwwlawkuleuvenbeleaninLeanInEnglishactivitiesitems18march2024paneldiscussiononwomenandaitheimpactofaionwomenandgender Panel discussion on Women and AI panel discussiononline amp LeuvenKU Leuven CiTiP</data><data key="sub_title">Panel discussion on Women and AI</data><data key="start_time">18 Mar 2024 19:00 - 20:30</data><data key="language">English</data><data key="location_detail">De Valk 3, DV3 01.31 (Tiensestraat 41, 3000 Leuven) &amp;amp; livestream</data><data key="price">free</data><data key="date">18/03/2024</data><data key="full_body"> Artificial intelligence (AI) is increasingly and rapidly making its way into our digitalised society. Although AI can bring about benefits  such as optimising job search services for more equal work opportunities  it inherently comes with several risks  such as perpetuating and exacerbating harmful gender inequities and stereotypes in training data or generating hypersexualised content. Join us for a panel discussion with four KU Leuven-experts who will bring different perspectives to our understanding of the impact of AI on women and gender. The event is open to anyone interested in this topic and requires no prior AI knowledge.  19.00 Welcoming by Lean In Leuven Law 19.05 General introduction to AI by Tinne De Laet 19.15 Panel discussion with Sien Moens Massimiliano Simons Bieke Zaman and Emine Ozge Yildirim. Moderated by Tinne De Laet. 20.00 Q&amp;A 20.30 Reception  </data><data key="intro">In light of the International Women's Day, Lean In Leuven Law is therefore organising an interdisciplinary panel discussion that will focus on the impact of AI on women and gender from a computer science, philosophical, social sciences and legal perspective. This event is organised in collaboration with Leuven.AI and DigiSoc - the KU Leuven Digital Society Institute.</data><data key="title">The impact of AI on women and gender</data><data key="url">https://www.vaia.be/en/courses/the-impact-of-ai-on-women-and-gender</data><data key="constraints">NA</data><data key="course_info">panel discussion-online &amp;amp; Leuven-KU Leuven CiTiP</data><data key="details">https://www.law.kuleuven.be/leanin/Lean-In-English/activities-items/18-march-2024-panel-discussion-on-women-and-ai-the-impact-of-ai-on-women-and-gender</data></node>
<node id="n2172" labels=":Course"><data key="labels">:Course</data><data key="data">Get Ready to Comply with the New Regulations on Data amp Artificial Intelligence  This course will be given by Gloria Gonzalez Fuster  Gregory Lewkowicz and David Restrepo Amariles at the FARI Test  Experience Center Cantersteen 16  1000  Brussels This program is structured into four distinct modules and provides a  comprehensive exploration of Europe's evolving regulatory landscape  related to data and artificial intelligence It encompasses a range of  frameworks  operational tools  and best practices crucial for ensuring  compliance Designed for professionals in both the public and private  sectors  the course aims to guide participants in complying with  relevant European regulations  while also enabling them to capitalize on  the new opportunities emerging from the European regulatory framework The modules are structured to create a cohesive and comprehensive  program However  participants have the flexibility to choose individual  modules ‘à la carte' according to their interests or needs It's  important to note that we strongly recommend completing Module 3 before  proceeding to Module 4 This sequence ensures a more effective learning  experience  as the content of Module 3 lays the foundational knowledge  necessary for understanding Module 4 This program is tailored for professionals across various fields  However  it does not delve deeply into the unique aspects of specific  sectors such as banking and finance  health  work  employment   critical infrastructures  enforcement  justice  machinery  etc If you  are interested in a customized program that specifically addresses the  needs and nuances of your organization or company  please do not  hesitate to get in touch We are more than willing to collaborate with  you to develop a program that aligns perfectly with your sector's  specific requirements and challenges Module 1 The GDPR in the Light of the Evolving EU Data Law Landscape 400 pm  730 pm March 13  2024 in English This module focuses on analyzing the influence of the new EU data law  landscape in relation to interpreting and complying with the GDPR  General Data Protection Regulation Upon completion of this module   participants will possess an indepth understanding of how the new  European regulations interact with the GDPR and how to adapt their  practices Furthermore  they will gain insights into the practical  implications of these interactions from an operational standpointModule 2 Regulating Data Beyond the GDPR the Data Governance Act  the Data Act  More 400 pm  730 pm March 14  2024 in English This module provides an extensive overview of the latest European  regulations concerning data It includes an indepth examination of key  legislations such as the Data Governance Act and the Data Act  among  others By the end of this module  participants will have acquired a  profound understanding of these new regulations  including how to adhere  to the obligations they entail Additionally  they will get insights on  how to effectively leverage the new European regulatory framework for  enhancing their professional practicesModule 3 Get Ready to Comply with the AI Act I Principles and Frameworks 400 pm  730 pm March 19  2024 in English This module lays down the essential principles for complying with the  AI Act It provides a comprehensive overview of the regulation   emphasizing the new responsibilities it assigns to a range of AI system  operators  encompassing everyone from producers to deployers  across the  entire lifecycle of these systems Participants will explore various  frameworks that facilitate the creation and implementation of a  compliance roadmap  customized for their specific organizational needs  This includes aspects such as internal control  risk assessment and  fundamental rights impact assessment  governance framework  and  reporting mechanisms By the end of this module  participants will not  only be equipped to develop their own customized compliance roadmap for  the AI Act but will also be able to pinpoint the key action points  necessary for effective implementationModule 4 Get Ready to Comply with the AI Act II Operational Tools 400 pm  730 pm April 16  2024 in English Expanding upon the foundational knowledge acquired in Module 3  this  module delves deeply into the operational tools essential for compliance  with the AI Act It navigates through the array of existing technical  standards for risk management  outlines best practices  and examines  relevant metrics crucial for compliance with the AI Act  enriched with  practical case studies Upon completing this module  participants will  be equipped to identify key operational focus areas and navigate the  variety of tools available for effective compliance with the AI Act  Additionally  they will be encouraged to think critically about these  tools and their application in various contexts As FARI is supported by the European Resilience  Recovery Fund RRF  we can continue to offer free access to participants from public administrations public institutions and research institutions We can also provide a 50 discount to participants from the following target groups Educational institutionsNGOs and nonprofit organisationsBrussels based companiesBrussels citizens  More info  Share this course   httpswwwfaribrusselseducationnewregulationsdataartificialintelligence 13 Mar 2024  16 Apr 2024 courseBrusselsFARI</data><data key="annotated_by">manual</data><data key="sub_title">13 Mar 2024 - 16 Apr 2024</data><data key="start_time">13 Mar 2024 - 16 Apr 2024</data><data key="location_detail">Be Central, FARI Experience Centre - Cantersteen 16, 1000 Bruxelles</data><data key="price">€500 excl. VAT (all modules), €125 excl. VAT (per module)</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: professionals across various fields</data><data key="intro">More info  Share this course  </data><data key="date">13/03/2024</data><data key="language">English</data><data key="course_info">course-Brussels-FARI</data><data key="full_body"> This course will be given by Gloria Gonzalez Fuster  Gregory Lewkowicz and David Restrepo Amariles at the FARI Test &amp; Experience Center (Cantersteen 16  1000  Brussels). This program is structured into four distinct modules and provides a  comprehensive exploration of Europe's evolving regulatory landscape  related to data and artificial intelligence. It encompasses a range of  frameworks  operational tools  and best practices crucial for ensuring  compliance. Designed for professionals in both the public and private  sectors  the course aims to guide participants in complying with  relevant European regulations  while also enabling them to capitalize on  the new opportunities emerging from the European regulatory framework. The modules are structured to create a cohesive and comprehensive  program. However  participants have the flexibility to choose individual  modules ‘à la carte' according to their interests or needs. It's  important to note that we strongly recommend completing Module 3 before  proceeding to Module 4. This sequence ensures a more effective learning  experience  as the content of Module 3 lays the foundational knowledge  necessary for understanding Module 4. This program is tailored for professionals across various fields.  However  it does not delve deeply into the unique aspects of specific  sectors such as banking and finance  health  work &amp; employment   critical infrastructures  enforcement  justice  machinery  etc. If you  are interested in a customized program that specifically addresses the  needs and nuances of your organization or company  please do not  hesitate to get in touch. We are more than willing to collaborate with  you to develop a program that aligns perfectly with your sector's  specific requirements and challenges. Module 1: The GDPR in the Light of the Evolving EU Data Law Landscape 4:00 pm - 7:30 pm March 13  2024 (in English) This module focuses on analyzing the influence of the new EU data law  landscape in relation to interpreting and complying with the GDPR  (General Data Protection Regulation). Upon completion of this module   participants will possess an in-depth understanding of how the new  European regulations interact with the GDPR and how to adapt their  practices. Furthermore  they will gain insights into the practical  implications of these interactions from an operational standpoint.Module 2: Regulating Data Beyond the GDPR: the Data Governance Act  the Data Act &amp; More 4:00 pm - 7:30 pm March 14  2024 (in English) This module provides an extensive overview of the latest European  regulations concerning data. It includes an in-depth examination of key  legislations such as the Data Governance Act and the Data Act  among  others. By the end of this module  participants will have acquired a  profound understanding of these new regulations  including how to adhere  to the obligations they entail. Additionally  they will get insights on  how to effectively leverage the new European regulatory framework for  enhancing their professional practicesModule 3: Get Ready to Comply with the AI Act I: Principles and Frameworks 4:00 pm - 7:30 pm March 19  2024 (in English) This module lays down the essential principles for complying with the  AI Act. It provides a comprehensive overview of the regulation   emphasizing the new responsibilities it assigns to a range of AI system  operators  encompassing everyone from producers to deployers  across the  entire lifecycle of these systems. Participants will explore various  frameworks that facilitate the creation and implementation of a  compliance roadmap  customized for their specific organizational needs.  This includes aspects such as internal control  risk assessment and  fundamental rights impact assessment  governance framework  and  reporting mechanisms. By the end of this module  participants will not  only be equipped to develop their own customized compliance roadmap for  the AI Act but will also be able to pinpoint the key action points  necessary for effective implementationModule 4: Get Ready to Comply with the AI Act II: Operational Tools 4:00 pm - 7:30 pm April 16  2024 (in English) Expanding upon the foundational knowledge acquired in Module 3  this  module delves deeply into the operational tools essential for compliance  with the AI Act. It navigates through the array of existing technical  standards for risk management  outlines best practices  and examines  relevant metrics crucial for compliance with the AI Act  enriched with  practical case studies. Upon completing this module  participants will  be equipped to identify key operational focus areas and navigate the  variety of tools available for effective compliance with the AI Act.  Additionally  they will be encouraged to think critically about these  tools and their application in various contexts. As FARI is supported by the European Resilience &amp; Recovery Fund (RRF)  we can continue to offer free access to participants from public administrations public institutions and research institutions. We can also provide a 50% discount to participants from the following target groups: Educational institutionsNGOs and non-profit organisationsBrussels based companiesBrussels citizens </data><data key="url">https://www.vaia.be/en/courses/get-ready-to-comply-with-the-new-regulations-on-data-artificial-intelligence</data><data key="title">Get Ready to Comply with the New Regulations on Data &amp;amp; Artificial Intelligence</data><data key="constraints">NA</data><data key="details">https://www.fari.brussels/education/new-regulations-data-artificial-intelligence</data></node>
<node id="n2173" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: Profiles in IT, design, communication, and business. Civil servants, students or professionals in the field of law or healthcare.</data><data key="data">SmartNation AI Hackathon  During this hackathon  you will develop a fullyfledged innovation project in three days  You will do it in a team  under the guidance of an experienced coach  At the end of the third day  you will present the results to a jury  Why participate You work on a project with societal impact You develop new knowledge and technical skills You meet motivated and interesting experts in the sector  You can add relevant work experience to your CV  You enhance your communication skills in English   You have the opportunity to work on one of the challenges below    More info  Share this course   httpsbosabelgiumbeenAIhackathon 12 Mar 2024  14 Mar 2024 hackathonBrusselsFPS Policy and Support BOSA</data><data key="annotated_by">manual</data><data key="sub_title">12 Mar 2024 - 14 Mar 2024</data><data key="start_time">12 Mar 2024 - 14 Mar 2024</data><data key="location_detail">KVS (Royal Flemish Theatre) Rue de Laeken 146, 1000 Brussels</data><data key="price">free</data><data key="subscription_limit">23 Feb 2024</data><data key="intro">More info  Share this course  </data><data key="details">https://bosa.belgium.be/en/AIhackathon</data><data key="full_body"> During this hackathon  you will develop a fully-fledged innovation project in three days.  You will do it in a team  under the guidance of an experienced coach.  At the end of the third day  you will present the results to a jury.  Why participate? You work on a project with societal impact. You develop new knowledge and technical skills. You meet motivated and interesting experts in the sector.  You can add relevant work experience to your CV.  You enhance your communication skills in English.   You have the opportunity to work on one of the challenges below:   </data><data key="language">English</data><data key="constraints">NA</data><data key="course_info">hackathon-Brussels-FPS Policy and Support (BOSA)</data><data key="date">12/03/2024</data><data key="url">https://www.vaia.be/en/courses/smartnation-ai-hackathon</data><data key="title">SmartNation AI Hackathon</data></node>
<node id="n2174" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: anyone interested in psychetric medicine and AI</data><data key="data">Machine Learning and Artificial Intelligence in Psychiatric Medicine Trustworthiness Epistemic Justice and Empowerment  Medicine has a distinct set of moral responsibilities This is because the physicianpatient relationship contains inherent inequalities of knowledge  skills  and control of resources  and it treats matters both intimate and potentially of great importance to the patient Rhodes 2001 Nickel  Frank 2020 These responsibilities include physician's obligation to earn trust and be trustworthy Rhodes 2001 2020 The field of psychiatric medicine faces additional ethical challenges specifically related to patient trust As artificial intelligence and machine learning technologies in diagnosis  relapse prevention  and treatment begin to play a larger role in all medicine  and specifically psychiatry  these ethical challenges may be exacerbated or complicated In this talk I present three of these challenges relating to patient testimony  false empowerment  and harm prevention  More info  Share this course   httpsghumkuleuvenbeLCHHcalendarlectureseriesmachinelearningandartificialintelligenceinpsychiatricmedicinebylilyfrank 7 Mar 2024 1600  1730 lezingonline amp LeuvenLeuven Centre for Health Humanities LCH² KU Leuven</data><data key="sub_title">7 Mar 2024 16:00 - 17:30</data><data key="start_time">7 Mar 2024 16:00 - 17:30</data><data key="language">English</data><data key="location_detail">Aula Emma Vorlat (Leercentrum AGORA (114-01), Edward Van Evenstraat 4, 3000 Leuven) &amp;amp; online via Zoom</data><data key="price">free</data><data key="intro">More info  Share this course  </data><data key="date">7/03/2024</data><data key="details">https://ghum.kuleuven.be/LCHH/calendar/lecture-series-machine-learning-and-artificial-intelligence-in-psychiatric-medicine-by-lily-frank</data><data key="full_body"> Medicine has a distinct set of moral responsibilities. This is because the physician-patient relationship contains inherent inequalities of knowledge  skills  and control of resources  and it treats matters both intimate and potentially of great importance to the patient (Rhodes 2001; Nickel &amp; Frank 2020). These responsibilities include physician's obligation to earn trust and be trustworthy (Rhodes 2001; 2020). The field of psychiatric medicine faces additional ethical challenges specifically related to patient trust. As artificial intelligence and machine learning technologies (in diagnosis  relapse prevention  and treatment) begin to play a larger role in all medicine  and specifically psychiatry  these ethical challenges may be exacerbated or complicated. In this talk I present three of these challenges relating to patient testimony  false empowerment  and harm prevention. </data><data key="url">https://www.vaia.be/en/courses/machine-learning-and-artificial-intelligence-in-psychiatric-medicine-trustworthiness-epistemic-justice-and-empowerment</data><data key="title">Machine Learning and Artificial Intelligence in Psychiatric Medicine: Trustworthiness, Epistemic Justice, and Empowerment</data><data key="constraints">NA</data><data key="course_info">lezing-online &amp;amp; Leuven-Leuven Centre for Health Humanities (LCH²); KU Leuven</data></node>
<node id="n2175" labels=":Course"><data key="labels">:Course</data><data key="data">How Generative and Predictive AI is shaping the future in industry for Operations amp Maintenance    In this seminar  in cooperation with PwC and Microsoft  SymphonyAI Industrial will be showing you how you can leverage generative and predictive AI  not only for your maintenance department  but also for the organisation as a whole  thus connecting assets  processes and people                               We will expand on the role of generative AI in industrial settings and talk about AIDriven Quality Control  Generative AI Enabled Predictive Maintenance and Applications of Visionbased AI Furthermore  we deepdive into AIDriven Process Automation and Optimization and HumanAI Collaboration in Manufacturing Lastly  SymphonyAI brings it all together in several practical usecases and live demos on how to leverage the power of AI for your company What will you learn during this seminar Understanding the role and potential of AI in transforming industrial operationsInsights into current trends and future directions in asset management and AI applicationsPractical knowledge on implementing AI solutions for asset reliability  process optimization  workforce empowerment  and sustainability Why attend Learn from expert speakers how AI can help you shape the future of your industryNetwork with likeminded peers looking to accelerate their digital transformation journeysHear how AI is driving tangible value across the operations landscapeSee the latest use cases of new industrial specific Generative AI solutions  In this seminar in cooperation with PwC and Microsoft SymphonyAI Industrial will be showing you how you can leverage generative and predictive AI not only for your maintenance department but also for the organisation as a whole thus connecting assets processes and people httpswwweventbritecoukeseminarhowgenerativeandpredictiveaiisshapingthefutureinindustrytickets803795272297 Seminar by BEMAS amp SymphonyAI seminarBeverenBEMAS SymphonyAI</data><data key="annotated_by">manual</data><data key="start_time">7 Mar 2024 13:00 - 17:00</data><data key="price">€ 0</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: Professionals uit de industrie</data><data key="location_detail">Van der Valk hotel Beveren</data><data key="intro">In this seminar, in cooperation with PwC and Microsoft, SymphonyAI Industrial will be showing you how you can leverage generative and predictive AI, not only for your maintenance department, but also for the organisation as a whole, thus connecting assets, processes and people.</data><data key="language">English</data><data key="sub_title">Seminar by BEMAS &amp;amp; SymphonyAI</data><data key="date">7/03/2024</data><data key="details">https://www.eventbrite.co.uk/e/seminar-how-generative-and-predictive-ai-is-shaping-the-future-in-industry-tickets-803795272297</data><data key="full_body">   In this seminar  in cooperation with PwC and Microsoft  SymphonyAI Industrial will be showing you how you can leverage generative and predictive AI  not only for your maintenance department  but also for the organisation as a whole  thus connecting assets  processes and people.                               We will expand on the role of generative AI in industrial settings and talk about AI-Driven Quality Control  Generative AI Enabled Predictive Maintenance and Applications of Vision-based AI. Furthermore  we deep-dive into AI-Driven Process Automation and Optimization and Human-AI Collaboration in Manufacturing. Lastly  SymphonyAI brings it all together in several practical use-cases and live demos on how to leverage the power of AI for your company. What will you learn during this seminar? Understanding the role and potential of AI in transforming industrial operations.Insights into current trends and future directions in asset management and AI applications.Practical knowledge on implementing AI solutions for asset reliability  process optimization  workforce empowerment  and sustainability. Why attend? Learn from expert speakers how AI can help you shape the future of your industry.Network with likeminded peers looking to accelerate their digital transformation journeys.Hear how AI is driving tangible value across the operations landscape.See the latest use cases of new industrial specific Generative AI solutions. </data><data key="url">https://www.vaia.be/en/courses/how-generative-and-predictive-ai-is-shaping-the-future-in-industry-for-operations-maintenance</data><data key="title">How Generative and Predictive AI is shaping the future in industry for Operations &amp;amp; Maintenance</data><data key="constraints">geen</data><data key="course_info">seminar-Beveren-BEMAS; SymphonyAI</data></node>
<node id="n2176" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">29 Feb 2024</data><data key="data">DEEPK 2024 on deep learning and kernel machines  Major progress and impact has been achieved through deep learning architectures with many exciting applications such as by generative models and transformers At the same time it triggers new questions on the fundamental possibilities and limitations of the models  with respect to representations  scalability  learning and generalisation aspects Through kernelbased methods often a deeper understanding and solid foundations have been obtained  complementary to the powerful and flexible deep learning architectures Recent examples are understanding generalisation of overparameterised models in the double descent phenomenon and conceiving attention mechanisms in transformers as kernel machines   Topics include but are not limited to  Deep learning and generalisation  Double descent phenomenon and overparameterised models  Transformers and asymmetric kernels  Attention mechanisms  kernel singular value decomposition  Learning with asymmetric kernels  Duality and deep learning  Regularisation schemes  normalisation  Neural tangent kernel  Deep learning and Gaussian processes  Transformers  support vector machines and least squares support vector machines  Autoencoders  neural networks and kernel methods  Kernel methods in GANs  variational autoencoders  diffusion models  Generative Flow Networks  Generative kernel machines  Deep Kernel PCA  deep kernel machines  deep eigenvalues  deep eigenvectors  Restricted Boltzmann machines  Restricted kernel machines  deep learning  energy based models  Disentanglement and explainability  Tensors  kernels and deep learning  Convolutional kernels  Sparsity  robustness  lowrank representations  compression  Nystrom method  Nystromformer  Efficient training methods  Lagrange duality  Fenchel duality  estimation in Hilbert  spaces  reproducing kernel Hilbert spaces  vectorvalued reproducing  kernel Hilbert spaces  Krein spaces  Banach spaces  RKHS and Calgebra Applications  in progress  list to be completedMikhail Belkin University of California San DiegoVolkan Cevher EPFLFlorence dAlcheBuc Telecom Paris  Institut Polytechnique de ParisJulien Mairal INRIAMassimiliano Pontil IIT and University College LondonDingxuan Zhou University of Sydney to be announced The DEEPK 2024 program will include oral and poster sessions Interested participants are cordially invited to submit an extended abstract max 2 pages for their contribution See the timeline for extended abstract submissions Please prepare your extended abstract submission in LaTeX  according to the following stylefile LaTeX stylefile and submit it in PDF format max 2 pages on the following website submission website to be opened   Deadline extended abstract submission Feb 8  2024  Notification of acceptance and presentation format oralposter Feb 22  2024  Deadline for registration  Feb 29  2024   International Workshop DEEPK 2024  March 78  2024   Johan Suykens  KU Leuven  ChairAlex Lambert  KU Leuven Panos Patrinos  KU Leuven Qinghua Tao  KU Leuven Francesco Tonin  KU Leuven Please consult the DEEPK 2024 website httpswwwesatkuleuvenbestadiusEDEEPK2024 for info on the programme  registration  location and venue The event is cosponsored by ERC Advanced Grant EDUALITY and KU Leuven  The aim of DEEPK 2024 is to provide a multidisciplinary forum where researchers of different communities can meet to find new synergies between deep learning and kernel machines both at the level of theory and applications httpswwwesatkuleuvenbestadiusEDEEPK2024 International workshop on deep learning and kernel machines twoday workshopLeuvenERC KU Leuven</data><data key="start_time">7 Mar 2024 - 8 Mar 2024</data><data key="price">€230</data><data key="sub_title">International workshop on deep learning and kernel machines</data><data key="target_group">Target audience: researchers in deep learning and kernel machines</data><data key="intro">The aim of DEEPK 2024 is to provide a multi-disciplinary forum where researchers of different communities can meet, to find new synergies between deep learning and kernel machines, both at the level of theory and applications.</data><data key="language">English</data><data key="location_detail">Arenberg Castle - Kardinaal Mercierlaan 3001, Leuven</data><data key="date">7/03/2024</data><data key="details">https://www.esat.kuleuven.be/stadius/E/DEEPK2024</data><data key="full_body"> Major progress and impact has been achieved through deep learning architectures with many exciting applications such as by generative models and transformers. At the same time it triggers new questions on the fundamental possibilities and limitations of the models  with respect to representations  scalability  learning and generalisation aspects. Through kernel-based methods often a deeper understanding and solid foundations have been obtained  complementary to the powerful and flexible deep learning architectures. Recent examples are understanding generalisation of over-parameterised models in the double descent phenomenon and conceiving attention mechanisms in transformers as kernel machines.   Topics include but are not limited to:  Deep learning and generalisation  Double descent phenomenon and over-parameterised models  Transformers and asymmetric kernels  Attention mechanisms  kernel singular value decomposition  Learning with asymmetric kernels  Duality and deep learning  Regularisation schemes  normalisation  Neural tangent kernel  Deep learning and Gaussian processes  Transformers  support vector machines and least squares support vector machines  Autoencoders  neural networks and kernel methods  Kernel methods in GANs  variational autoencoders  diffusion models  Generative Flow Networks  Generative kernel machines  Deep Kernel PCA  deep kernel machines  deep eigenvalues  deep eigenvectors  Restricted Boltzmann machines  Restricted kernel machines  deep learning  energy based models  Disentanglement and explainability  Tensors  kernels and deep learning  Convolutional kernels  Sparsity  robustness  low-rank representations  compression  Nystrom method  Nystromformer  Efficient training methods  Lagrange duality  Fenchel duality  estimation in Hilbert  spaces  reproducing kernel Hilbert spaces  vector-valued reproducing  kernel Hilbert spaces  Krein spaces  Banach spaces  RKHS and C*-algebra Applications  (in progress - list to be completed)Mikhail Belkin (University of California San Diego)Volkan Cevher (EPFL)Florence d'Alche-Buc (Telecom Paris  Institut Polytechnique de Paris)Julien Mairal (INRIA)Massimiliano Pontil (IIT and University College London)Dingxuan Zhou (University of Sydney) to be announced The DEEPK 2024 program will include oral and poster sessions. Interested participants are cordially invited to submit an extended abstract (max. 2 pages) for their contribution. See the timeline for extended abstract submissions. Please prepare your extended abstract submission in LaTeX  according to the following stylefile [LaTeX stylefile] and submit it in PDF format (max. 2 pages) on the following website [submission website to be opened] .  Deadline extended abstract submission: Feb 8  2024  Notification of acceptance and presentation format (oral/poster): Feb 22  2024  Deadline for registration:  Feb 29  2024   International Workshop DEEPK 2024:  March 7-8  2024   Johan Suykens  KU Leuven  ChairAlex Lambert  KU Leuven Panos Patrinos  KU Leuven Qinghua Tao  KU Leuven Francesco Tonin  KU Leuven Please consult the DEEPK 2024 website https://www.esat.kuleuven.be/stadius/E/DEEPK2024 for info on the programme  registration  location and venue. The event is co-sponsored by ERC Advanced Grant E-DUALITY and KU Leuven. </data><data key="url">https://www.vaia.be/en/courses/deepk-2024</data><data key="title">DEEPK 2024, on deep learning and kernel machines</data><data key="constraints">NA</data><data key="course_info">two-day workshop-Leuven-ERC; KU Leuven</data></node>
<node id="n2177" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: anyone interested in psychology and artificial intelligence</data><data key="data">Minds and Machines A Dialogue Between Cognitive Science and AI  Marco Zorzi is a Professor of Cognitive Psychology and Artificial Intelligence at the University of Padova Italy His work is at the intersection of cognitive science computer science and neuroscience  with a primary focus on the computational bases of human cognition He received doctoral and postdoctoral training in Trieste  London UCL  and Padova  and had previous faculty positions in Milan and Padova Currently  he serves as the Deputy Director of the Padova Neuroscience Center at the University of Padova and as a Senior Researcher at San Camillo Research Hospital in Venice  with a focus on AI applied to neuroimaging and neurorehabilitation He is a past recipient of a prestigious grant from the European Research Council ERC and an elected Fellow of the Cognitive Science Society Inaugural lecture of the Kulak Francqui Chair 2024 by Prof Dr Marco Zorzi Deep learning in artificial neural networks is the driving force behind the contemporary revolution brought by AI technologies Deep neural networks  show impressive performance in a wide range of domains  from image  processing to natural language understanding What insights can we  derive from these “artificial brains” about the human mind and brain  Conversely  can we use behavioural and neuroscientific methods to test and challenge these AI systems In this talk  Professor Marco Zorzi will discuss interdisciplinary research that emphasizes a constant dialogue between cognitive neuroscience and AI Contrasting  humans and machines in terms of learning and performance  including  failures  is the key to deepen our understanding of human cognition and  to highlight necessary improvements in AI technologies 1830  Welcome 1900  Opening of the ceremony  Welcome by Professor Piet Desmet  vice rector KU Leuven Kulak Introduction by Professor Bert Reynvoet  promotor  Faculty of Psychology and Educational Sciences KU Leuven Kulak 1915  Inaugural lecture by Professor Marco Zorzi  Minds and Machines A Dialogue Between Cognitive Science and AI 2025  Closing remarks by Professor Fien Depaepe  Dean of the Faculty of Psychology and Educational Sciences KU Leuven Kulak 2030  Reception The inaugural lecture will be preceded and followed by a series of lectures at KU Leuven Kulak and Leuven by Prof Marco Zorzi  All lectures are free  but registration is required   More info  Share this course   httpskulakkuleuvenbenloverkulakfaculteitenppwonderzoekfrancquichair2024francquileerstoel2024professormarcozorzi 6 Mar 2024 1830  2200 seminarseminar seriesKortrijkKU Leuven Kulak</data><data key="sub_title">6 Mar 2024 18:30 - 22:00</data><data key="start_time">6 Mar 2024 18:30 - 22:00</data><data key="language">English</data><data key="location_detail">Auditorium Stijn Streuvels, A301 KU Leuven Kulak - Etienne Sabbelaan 53, 8500 Kortrijk</data><data key="price">free</data><data key="intro">More info  Share this course  </data><data key="date">6/03/2024</data><data key="details">https://kulak.kuleuven.be/nl/over_kulak/faculteiten/ppw/onderzoek/francqui-chair-2024/francqui-leerstoel-2024-professor-marco-zorzi</data><data key="full_body"> Marco Zorzi is a Professor of Cognitive Psychology and Artificial Intelligence at the University of Padova (Italy). His work is at the intersection of cognitive science computer science and neuroscience  with a primary focus on the computational bases of human cognition. He received doctoral and postdoctoral training in Trieste  London (UCL)  and Padova  and had previous faculty positions in Milan and Padova. Currently  he serves as the Deputy Director of the Padova Neuroscience Center at the University of Padova and as a Senior Researcher at San Camillo Research Hospital in Venice  with a focus on AI applied to neuroimaging and neurorehabilitation. He is a past recipient of a prestigious grant from the European Research Council (ERC) and an elected Fellow of the Cognitive Science Society. Inaugural lecture of the Kulak Francqui Chair 2024 by Prof. Dr. Marco Zorzi Deep learning in artificial neural networks is the driving force behind the contemporary revolution brought by AI technologies. Deep neural networks  show impressive performance in a wide range of domains  from image  processing to natural language understanding. What insights can we  derive from these “artificial brains” about the human mind and brain?  Conversely  can we use behavioural and neuroscientific methods to test and challenge these AI systems? In this talk  Professor Marco Zorzi will discuss interdisciplinary research that emphasizes a constant dialogue between cognitive (neuro)science and AI. Contrasting  humans and machines in terms of learning and performance  including  failures  is the key to deepen our understanding of human cognition and  to highlight necessary improvements in AI technologies. 18.30 | Welcome 19:00 | Opening of the ceremony  Welcome by Professor Piet Desmet  vice rector KU Leuven Kulak Introduction by Professor Bert Reynvoet  promotor  Faculty of Psychology and Educational Sciences KU Leuven Kulak 19:15 | Inaugural lecture by Professor Marco Zorzi  Minds and Machines: A Dialogue Between Cognitive Science and AI 20:25 | Closing remarks by Professor Fien Depaepe  Dean of the Faculty of Psychology and Educational Sciences KU Leuven Kulak 20:30 | Reception The inaugural lecture will be preceded and followed by a series of lectures at KU Leuven Kulak and Leuven by Prof. Marco Zorzi.  All lectures are free  but registration is required.  </data><data key="url">https://www.vaia.be/en/courses/minds-and-machines-a-dialogue-between-cognitive-science-and-ai</data><data key="title">Minds and Machines: A Dialogue Between Cognitive Science and AI</data><data key="constraints">NA</data><data key="course_info">seminar/seminar series-Kortrijk-KU Leuven Kulak</data></node>
<node id="n2178" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: anyone interested in generative AI, economics and business</data><data key="data">The Economic Imperative of Responsible Innovation  Responsible innovation is becoming an economic imperative  Not only must organizations meet  ethical and regulatory compliance requirements  but they must also  ensure profitability when it comes to data  cloud and AI investments  Similarly  generative AI is bringing  new urgency to striking this equilibrium as business leaders are looking  for innovative ways to make their organizations more productive without  compromising on security  privacy and trust Join KPMG and SAS experts for this webinar to hear lessons learned and best practices  How generative AI provides an unprecedented range of opportunities and risksHow  an effective AI governance framework can deliver trusted  ethical and  profitable outcomes for people  processes and technologyHow  ModelOps  along with an endtoend analytics platform  can boost  productivity and transparency in the analytics life cycle and automated  decision making  More info  Share this course   httpswwwsascomenuswebinarsresponsibleinnovationhtml 5 Mar 2024 1600  1800 webinaronlineSAS</data><data key="annotated_by">manual</data><data key="sub_title">5 Mar 2024 16:00 - 18:00</data><data key="start_time">5 Mar 2024 16:00 - 18:00</data><data key="location_detail">Online</data><data key="price">free</data><data key="subscription_limit">NA</data><data key="intro">More info  Share this course  </data><data key="date">5/03/2024</data><data key="language">English</data><data key="course_info">webinar-online-SAS</data><data key="full_body"> Responsible innovation is becoming an economic imperative.  Not only must organizations meet  ethical and regulatory compliance requirements  but they must also  ensure profitability when it comes to data  cloud and AI investments.  Similarly  generative AI is bringing  new urgency to striking this equilibrium as business leaders are looking  for innovative ways to make their organizations more productive without  compromising on security  privacy and trust. Join KPMG and SAS experts for this webinar to hear lessons learned and best practices.  How generative AI provides an unprecedented range of opportunities and risks.How  an effective AI governance framework can deliver trusted  ethical and  profitable outcomes for people  processes and technology.How  ModelOps  along with an end-to-end analytics platform  can boost  productivity and transparency in the analytics life cycle and automated  decision making. </data><data key="url">https://www.vaia.be/en/courses/the-economic-imperative-of-responsible-innovation</data><data key="title">The Economic Imperative of Responsible Innovation</data><data key="constraints">NA</data><data key="details">https://www.sas.com/en_us/webinars/responsible-innovation.html</data></node>
<node id="n2179" labels=":Course"><data key="labels">:Course</data><data key="data">Artificial Intelligence in Sports Setting a Research Agenda and Priorities  In 2020  the IDEALAB on AI  Sport was established and funded by KU Leuven YouReCa to assess some of these challenges During this webinar  several experts involved in the IDEALAB and many others will discuss how the use of AI is influencing their respective research field By doing so  the webinar will highlight some priorities and set a research agenda with some important questions that need to be addressed in the future Therefore  the conference's intended audience is very broad including practitioners  academic researchers  sport lawyers  business developers  sport scientists  consultants  policymakers  members of civil society organisations  AI practitioners  postdoctoral researchers  students and other professionals with an interest in sport Date Tuesday 5 March 2024 Timing 13001730 CEST Attendance Online link will be provided on Monday afternoon  4 March Free registration no costs involved but registration required Registration and more information via CiTiP Admin   13001310 Welcome Jan De Bruyne  Natalie Bertels  Centre for IT  IP Law  KU Leuven  RAILS   13151335 AI is changing the game Steven Verstockt  IDLab  Ghent University   13401400 Business perspective on innovation and sports the case of AI Kristof De Mey  Victoris  STRN  Ghent University   14051425 Sport science perspective on AI in sports Arne Jaspers  Royal Belgian Football Association  KU Leuven   14251440 Coffee break   14401505 AI and sports some social considerations Michiel van Oudheusden  VU Amsterdam   15101530 Ethicaldiversity perspective on AI and sports Ana Maria Correa  Centre for IT  IP Law  KU Leuven   15351555 Data protection  AI and sport Pierre Dewitte  Centre for IT  IP Law  KU Leuven   15551610 Coffee break   16101630 An international perspective on AI and sports Jessica Zendler  Rimkus   16351655 AI and sports in practice  Sofie Debaere  Joachim Taelman  Sport Vlaanderen   16551710 Closing remarks Mike McNamee  iCERIS  KU Leuven    More info  Share this course   httpswwwlawkuleuvenbecitipennewsitemartificialintelligenceinsportssettingaresearchagendaandpriorities 5 Mar 2024 1300  1700 webinaronlineKU Leuven CiTiP YouReCa RAILS Robotics amp AI Law Society</data><data key="annotated_by">manual</data><data key="sub_title">5 Mar 2024 13:00 - 17:00</data><data key="start_time">5 Mar 2024 13:00 - 17:00</data><data key="price">gratis</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: practitioners, academic researchers, sport lawyers, business developers, sport scientists, consultants, policymakers, members of civil society organisations, AI practitioners, (post)doctoral researchers, students and other professionals with an interest in sports</data><data key="language">English</data><data key="location_detail">Online</data><data key="intro">More info  Share this course  </data><data key="date">5/03/2024</data><data key="details">https://www.law.kuleuven.be/citip/en/news/item/artificial-intelligence-in-sports-setting-a-research-agenda-and-priorities</data><data key="full_body"> In 2020  the IDEALAB on AI &amp; Sport was established and funded by KU Leuven YouReCa to assess some of these challenges. During this webinar  several experts involved in the IDEALAB and many others will discuss how the use of AI is influencing their respective (research) field. By doing so  the webinar will highlight some priorities and set a research agenda with some important questions that need to be addressed in the future. Therefore  the conference's intended audience is very broad including practitioners  academic researchers  sport lawyers  business developers  sport scientists  consultants  policymakers  members of civil society organisations  AI practitioners  (post)doctoral researchers  students and other professionals with an interest in sport. Date: Tuesday 5 March 2024 Timing: 13.00-17.30 CEST Attendance: Online (link will be provided on Monday afternoon  4 March) Free registration: no costs involved but registration required Registration and more information via CiTiP Admin   13.00-13.10 Welcome (Jan De Bruyne &amp; Natalie Bertels  Centre for IT &amp; IP Law  KU Leuven  RAILS)   13.15-13.35 AI is changing the game (Steven Verstockt  IDLab  Ghent University)   13.40-14.00 Business perspective on innovation and sports: the case of AI (Kristof De Mey  Victoris  STRN  Ghent University)   14.05-14.25 Sport science perspective on AI in sports (Arne Jaspers  Royal Belgian Football Association - KU Leuven)   14.25-14.40 Coffee break   14.40-15.05 AI and sports: some social considerations (Michiel van Oudheusden  VU Amsterdam).   15.10-15.30 Ethical/diversity perspective on AI and sports (Ana Maria Correa  Centre for IT &amp; IP Law  KU Leuven)   15.35-15.55 Data protection  AI and sport (Pierre Dewitte  Centre for IT &amp; IP Law  KU Leuven)   15.55-16.10 Coffee break   16.10-16.30 An international perspective on AI and sports (Jessica Zendler  Rimkus)   16.35-16.55 AI and sports in practice  (Sofie Debaere &amp; Joachim Taelman  Sport Vlaanderen)   16.55-17.10 Closing remarks (Mike McNamee  iCERIS  KU Leuven)   </data><data key="url">https://www.vaia.be/en/courses/artificial-intelligence-in-sports-setting-a-research-agenda-and-priorities</data><data key="title">Artificial Intelligence in Sports: Setting a Research Agenda and Priorities</data><data key="constraints">NA</data><data key="course_info">webinar-online-KU Leuven CiTiP; YouReCa; RAILS (Robotics &amp;amp; AI Law Society)</data></node>
<node id="n2180" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="target_group">Target audience: researchers</data><data key="sub_title">4 Mar 2024 13:00 - 18:00</data><data key="start_time">4 Mar 2024 13:00 - 18:00</data><data key="location_detail">Irish College Leuven - Janseniusstraat 1, 3000 Leuven</data><data key="price">free</data><data key="subscription_limit">NA</data><data key="data">AI driven data science  13h00  13h30 Welcome Coffee  13h30  14h15 Plenary Talk “GC1 AIdriven Data Science”  14h15  15h00 Responsible AI  15h00  15h15 QA session  15h15 16h00 Poster Session  Coffee Break  16h00  17h15 Breakout sessions    BO1 Use cases  Health      BO2 Use cases  Health and Society      BO3 Use cases  Industry      BO4 Use cases  Planet  Energy  Society    17h15  18h30 Networking  Drinks Organized by  Bart De Moor and Mauricio Agudelo  More info  Share this course   httpswwwesatkuleuvenbestadiuseventphpid2396 4 Mar 2024 1300  1800 seminarLeuvenKU Leuven ESAT STADIUS</data><data key="intro">More info  Share this course  </data><data key="details">https://www.esat.kuleuven.be/stadius/event.php?id=2396</data><data key="full_body"> 13h00  13h30 Welcome Coffee  13h30  14h15 Plenary Talk: “GC1: AI-driven Data Science”  14h15  15h00 Responsible AI  15h00  15h15 Q&amp;A session  15h15 -16h00 Poster Session + Coffee Break  16h00  17h15 Breakout sessions:    BO1: Use cases - Health      BO2: Use cases - Health and Society      BO3: Use cases - Industry      BO4: Use cases  Planet  Energy &amp; Society    17h15  18h30 Networking + Drinks Organized by:  Bart De Moor and Mauricio Agudelo </data><data key="language">English</data><data key="constraints">NA</data><data key="course_info">seminar-Leuven-KU Leuven ESAT STADIUS</data><data key="date">4/03/2024</data><data key="url">https://www.vaia.be/en/courses/ai-driven-data-science</data><data key="title">AI driven data science</data></node>
<node id="n2181" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="start_time">29 Feb 2024 - 28 Mar 2024</data><data key="price">€495 - €1100</data><data key="data">Explaining and Predicting Outcomes with Linear Regression    Explaining and Predicting Outcomes with Linear Regression                               Linear regression addresses how a continuous dependent variable is associated by one or more predictors of any type The fact that many practical problems deal with continuous outcomes eg income  blood pressure  temperature  affect makes linear regression a popular tool  and most of us will be familiar with the concept of drawing a line through a cloud of data points The first two sessions of this module introduce the conceptual framework of this method using the simple case of a single predictor Formulas and technicalities are kept to a minimum and the main focus is on interpretation of results and assessing model validity This includes confidence statements on the predictor effect hypothesis tests and confidence intervals  using the regression model to predict future results and verification of model assumptions In session 3 and 4 we allow for more than one predictor leading to the multiple linear regression model We focus on either explanation or prediction How to come to a parsimonious model starting from a large number of predictors will be discussed in detail In these complex linear models special attention will be given to interpreting individual predictor effects  as they critically depend on other terms in the model and underlying relations between predictors confounding In the last session a more elaborate data analysis is discussed We touch on problems where linear regression is not appropriate and replaced by related approaches such as generalized linear models and mixed models Different features will be illustrated with case examples from the instructors practical experience  and participants are encouraged to bring examples from their own work Handson exercises are worked out behind the PC using the R software Target audience This course targets professionals and investigators from all areas who are involved in prediction problems or need to model the relationship between a dependent variable and one or more explanatory variables Fees The participation fee is 1100 EUR for participants from the private sector Reduced prices apply to students and staff from nonprofit  social profit  and government organizations An exam fee of 35 EUR will be applied Industry  private sector  profession € 1100Non profit  government  higher education staff € 825Doctoral students  unemployed € 495 If two or more employees from the same company enrol simultaneously for this course a reduction of 20 on the course fee is taken into account starting from the second enrolment Registration More information and registration on our BetaAcademy website  Explaining and Predicting Outcomes with Linear Regression httpsbetaacademyugentbeenprogramshortandlongrunninginitiatives202320242024m11lrmodule11explainingandpredicting Explaining and Predicting Outcomes with Linear Regression courseGhentUGent</data><data key="location_detail">Krijgslaan 281, 9000 Gent</data><data key="language">English</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: This course targets professionals and investigators from all areas who are involved in prediction problems or need to model the relationship between a dependent variable and one or more explanatory variables.</data><data key="date">29/02/2024</data><data key="intro">Explaining and Predicting Outcomes with Linear Regression</data><data key="sub_title">Explaining and Predicting Outcomes with Linear Regression</data><data key="course_info">course-Ghent-UGent</data><data key="full_body">   Explaining and Predicting Outcomes with Linear Regression                               Linear regression addresses how a continuous dependent variable is associated by one or more predictors of any type. The fact that many practical problems deal with continuous outcomes (e.g. income  blood pressure  temperature  affect) makes linear regression a popular tool  and most of us will be familiar with the concept of drawing a line through a cloud of data points. The first two sessions of this module introduce the conceptual framework of this method using the simple case of a single predictor. Formulas and technicalities are kept to a minimum and the main focus is on interpretation of results and assessing model validity. This includes confidence statements on the predictor effect (hypothesis tests and confidence intervals)  using the regression model to predict future results and verification of model assumptions. In session 3 and 4 we allow for more than one predictor leading to the multiple linear regression model. We focus on either explanation or prediction. How to come to a parsimonious model starting from a large number of predictors will be discussed in detail. In these complex linear models special attention will be given to interpreting individual predictor effects  as they critically depend on other terms in the model and underlying relations between predictors (confounding). In the last session a more elaborate data analysis is discussed. We touch on problems where linear regression is not appropriate and replaced by related approaches such as generalized linear models and mixed models. Different features will be illustrated with case examples from the instructors practical experience  and participants are encouraged to bring examples from their own work. Hands-on exercises are worked out behind the PC using the R software. Target audience This course targets professionals and investigators from all areas who are involved in prediction problems or need to model the relationship between a dependent variable and one or more explanatory variables. Fees The participation fee is 1100 EUR for participants from the private sector. Reduced prices apply to students and staff from non-profit  social profit  and government organizations. An exam fee of 35 EUR will be applied. Industry  private sector  profession*: € 1100Non profit  government  higher education staff: € 825(Doctoral) students  unemployed: € 495 *If two or more employees from the same company enrol simultaneously for this course a reduction of 20% on the course fee is taken into account starting from the second enrolment. Registration More information and registration on our Beta-Academy website. </data><data key="url">https://www.vaia.be/en/courses/explaining-and-predicting-outcomes-with-linear-regression</data><data key="title">Explaining and Predicting Outcomes with Linear Regression</data><data key="constraints">Participants are expected to have an active knowledge of the basic principles underlying statistical strategies, at a level equivalent to the Module 4 of this program.</data><data key="details">https://beta-academy.ugent.be/en/program/short-and-long-running-initiatives/2023-2024-2024m11lr-module-11-explaining-and-predicting</data></node>
<node id="n2182" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="sub_title">29 Feb 2024 12:00 - 13:00</data><data key="start_time">29 Feb 2024 12:00 - 13:00</data><data key="price">free</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: anyone interested in the EU AI Act</data><data key="data">Exploring 5 key topics under the EU AI Act  During this webinar  CiTiPresearchers will explore 5 key topics under the AI Act  Definition of AI what type of digital technology is impacted by the AI Act Scope of application what is the geographical  material and temporal scope of application of the AI Act Risk categories which risk categories are applied to AItechnology General Purpose AI Models what type of AImodels are considered to be general purpose Which obligations do they have Regulatory Sandboxes What to expect from regulatory sandboxes  Speakers will focus on clarifying the  various concepts and their potential impact on a variety of  stakeholders During a second webinar  we will build further on these  topics and take a more detailed look at the prohibited and high risk  AIsystems  conformity assessments  the fundamental rights impact  assessment and the supportive measures for SME's List of speakers Ivo Emanuilov Katerina Yordanova Matteo Frigeri Abdullah Elbi and Thomas Gils Practical  • Date 29 February  • Timing 1200  1300 CEST • Attendance Online Teams  • Register by emailing to CiTiP Admin Upon confirmation  you will receive a calendarinvite Dowload the pdf of this page here This webinar is an initiative of CiTiP and the Knowledge Centre Data  Society  with the support of the research projects iMARS and FAITH  More info  Share this course   httpswwwlawkuleuvenbecitipennewsitemcitipwebinarexploring5keytopicsundertheeuaiact 29 Feb 2024 1200  1300 webinaronlineKU Leuven CiTiP Kenniscentrum Data amp Maatschappij</data><data key="language">English</data><data key="location_detail">Online</data><data key="intro">More info  Share this course  </data><data key="date">29/02/2024</data><data key="details">https://www.law.kuleuven.be/citip/en/news/item/citip-webinar-exploring-5-key-topics-under-the-eu-ai-act</data><data key="full_body"> During this webinar  CiTiP-researchers will explore 5 key topics under the AI Act:  Definition of AI: what type of digital technology is impacted by the AI Act? Scope of application: what is the geographical  material and temporal scope of application of the AI Act? Risk categories: which risk categories are applied to AI-technology? General Purpose AI Models: what type of AI-models are considered to be general purpose? Which obligations do they have? Regulatory Sandboxes: What to expect from regulatory sandboxes?  Speakers will focus on clarifying the  various concepts and their potential impact on a variety of  stakeholders. During a second webinar  we will build further on these  topics and take a more detailed look at the prohibited and high risk  AI-systems  conformity assessments  the fundamental rights impact  assessment and the supportive measures for SME's. List of speakers: Ivo Emanuilov Katerina Yordanova Matteo Frigeri Abdullah Elbi and Thomas Gils. Practical:  • Date: 29 February  • Timing: 12:00  13:00 (CEST) • Attendance: Online (Teams)  • Register by e-mailing to: CiTiP Admin. Upon confirmation  you will receive a calendar-invite. Dowload the pdf of this page here. This webinar is an initiative of CiTiP and the Knowledge Centre Data &amp; Society  with the support of the research projects iMARS and FAITH. </data><data key="url">https://www.vaia.be/en/courses/exploring-5-key-topics-under-the-eu-ai-act</data><data key="title">Exploring 5 key topics under the EU AI Act</data><data key="constraints">NA</data><data key="course_info">webinar-online-KU Leuven CiTiP; Kenniscentrum Data &amp;amp; Maatschappij</data></node>
<node id="n2183" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="sub_title">29 Feb 2024 - 1 Mar 2024</data><data key="start_time">29 Feb 2024 - 1 Mar 2024</data><data key="price">€375 excl. VAT</data><data key="data">Introduction into AI for the Common Good  This course will be given by Andrés Cotorruelo Jiménez and Emilie Grégoire at the FARI Test  Experience Center Cantersteen 16  1000  Brussels Session 1 Initiation to AIDemystification of AI  for learners to build foundation in AI concepts and their applicationsCovers legal  ethical  and philosophical aspectsPractical insights into machine learning  data processing  and realworld AI implementationsSession 2 Ideation on AI opportunitiesWorkshop to explore AI solutions for business needsEncourages creative thinking and identifies challengesYields at least one tangible use case takeawaySession 3 Breaking down some use casesDeep dive into relevant use casesShowcases practical AI opportunities and solutionsTailored to participants for maximum relevance This course is also taught in Dutch 67 May and French 1112 April  34 June This training is tailored for  public sector and decisionmakers to grasp the practical value of AI in  their professional context  As FARI is supported by the European Resilience  Recovery Fund RRF  we can continue to offer free access to participants from public administrations public institutions and research institutions We can also provide a 50 discount to participants from the following target groupsEducational institutionsNGOs and nonprofit organisationsBrussels based companiesBrussels citizens  More info  Share this course   httpswwwfaribrusselseducationintroductionintoaiforthecommongood 29 Feb 2024  1 Mar 2024 training courseBrusselsFARI</data><data key="language">English</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: public sector and decision-makers interested in grasping the practical value of AI in their professional context</data><data key="intro">More info  Share this course  </data><data key="date">29/02/2024</data><data key="full_body"> This course will be given by Andrés Cotorruelo Jiménez and Emilie Grégoire at the FARI Test &amp; Experience Center (Cantersteen 16  1000  Brussels). Session 1: Initiation to AIDemystification of AI  for learners to build foundation in AI concepts and their applicationsCovers legal  ethical  and philosophical aspectsPractical insights into machine learning  data processing  and real-world AI implementationsSession 2: Ideation on AI opportunitiesWorkshop to explore AI solutions for business needsEncourages creative thinking and identifies challengesYields at least one tangible use case takeawaySession 3: Breaking down some use casesDeep dive into relevant use casesShowcases practical AI opportunities and solutionsTailored to participants for maximum relevance This course is also taught in Dutch (6-7 May) and French (11-12 April &amp; 3-4 June). This training is tailored for  public sector and decision-makers to grasp the practical value of AI in  their professional context.  As FARI is supported by the European Resilience &amp; Recovery Fund (RRF)  we can continue to offer free access to participants from public administrations public institutions and research institutions. We can also provide a 50% discount to participants from the following target groups:Educational institutionsNGOs and non-profit organisationsBrussels based companiesBrussels citizens </data><data key="location_detail">Be Central, FARI Experience Centre - Cantersteen 16, 1000 Bruxelles</data><data key="course_info">training course-Brussels-FARI</data><data key="url">https://www.vaia.be/en/courses/introduction-into-ai-for-the-common-good-en</data><data key="title">Introduction into AI for the Common Good</data><data key="constraints">NA</data><data key="details">https://www.fari.brussels/education/introduction-into-ai-for-the-common-good</data></node>
<node id="n2184" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: professionals looking to deepen their (technical) knowledge in various aspects of cybersecurity/a broad range of cybersecurity aspects</data><data key="data">Cybersecurity excellence  This overview course is structured into seven sessions We start with an overview of methodologies and technologies   beginning with a holistic view of cybersecurity in a digital ecosystem  session 1 Cybersecurity plays a critical role in ensuring data privacy   which involves complying with privacy laws and regulations like GDPR  session 2 The programme then progresses to handson workshops one  focused on network security session 3 and another on the protection of  the growing category of embedded ‘IoT' devices session 4 As standards and certifications also  play an increasing role in implementing cybersecurity measures  this is  the focus of the next session session 5 We learn how not to respond  when a cyberattack occurs  and hear the lessons learned from a victim Session 6 Finally  we look ahead to what the future holds  especially in the context of cybersecurity in the era of quantum computing Session 7 On the programme Session 1 Tackling cybersecurity challenges a complex security puzzle  28 February 2024  Vincent Naessens KU LeuvenSession 2 ‘Privacy by design a technical approach to privacy risk  26 March 2024  Kim Wuyts PwCSession 3 Efficient use of a network protocol analyzer in cyber threats workshop  24 April 2024  Tom Cordemans KU LeuvenSession 4 Hacking and protecting embedded devices workshop  29 May 2024  Jorn Lapon KU LeuvenSession 5 EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems  28 August 2024  Vincent Naessens KU LeuvenSession 6 Cyberattack response  25 September 2024  Tom Bauwens Eubelius  Kalman Tiboldi TVHSession 7 Postquantum cryptography  23 October 2024  Eric Michiels IBM  More info  Share this course   httpspuckuleuvenbenlopleidingcybersecurityexcellenceseries20244w6o7lx597g5dmr0 28 Feb 2024  23 Oct 2024 courseGhentKU Leuven Postuniversitair Centrum</data><data key="annotated_by">manual</data><data key="sub_title">28 Feb 2024 - 23 Oct 2024</data><data key="start_time">28 Feb 2024 - 23 Oct 2024</data><data key="location_detail">KU Leuven, Campus Rabot - Gebroeders de Smetstraat 1, 9000 Ghent</data><data key="price">&lt;p&gt;Price per session: €250&lt;/p&gt;</data><data key="subscription_limit">21 Feb 2024</data><data key="intro">More info  Share this course  </data><data key="details">https://puc.kuleuven.be/nl/opleiding/cybersecurity_excellence_series_2024-4w6o7lx597g5dmr0</data><data key="full_body"> This overview course is structured into seven sessions. We start with an overview of methodologies and technologies   beginning with a holistic view of cybersecurity in a digital ecosystem  (session 1). Cybersecurity plays a critical role in ensuring data privacy   which involves complying with privacy laws and regulations like GDPR  (session 2). The programme then progresses to hands-on workshops: one  focused on network security (session 3) and another on the protection of  the growing category of embedded ‘IoT' devices (session 4). As standards and certifications also  play an increasing role in implementing cybersecurity measures  this is  the focus of the next session (session 5). We learn how (not) to respond  when a cyberattack occurs  and hear the lessons learned from a 'victim' (Session 6). Finally  we look ahead to what the future holds  especially in the context of cybersecurity in the era of quantum computing (Session 7). On the programme: Session 1: Tackling cybersecurity challenges: a complex security puzzle | 28 February 2024 - Vincent Naessens (KU Leuven)Session 2: ‘Privacy by design': a technical approach to privacy risk | 26 March 2024 - Kim Wuyts (PwC)Session 3: Efficient use of a 'network protocol analyzer' in cyber threats (workshop) | 24 April 2024 - Tom Cordemans (KU Leuven)Session 4: Hacking and protecting embedded devices (workshop) | 29 May 2024 - Jorn Lapon (KU Leuven)Session 5: EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems | 28 August 2024 - Vincent Naessens (KU Leuven)Session 6: Cyberattack response | 25 September 2024 - Tom Bauwens (Eubelius)  Kalman Tiboldi (TVH)Session 7: Post-quantum cryptography | 23 October 2024 - Eric Michiels (IBM) </data><data key="language">English</data><data key="constraints">a minimal basic knowledge of IT is required, without having to be a specialist</data><data key="course_info">course-Ghent-KU Leuven Postuniversitair Centrum</data><data key="date">28/02/2024</data><data key="url">https://www.vaia.be/en/courses/cybersecurity-excellence-2024</data><data key="title">Cybersecurity excellence</data></node>
<node id="n2185" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: professionals looking to deepen their (technical) knowledge in various aspects of cybersecurity/a broad range of cybersecurity aspects</data><data key="data">Tackling cybersecurity challenges a complex security puzzle  This introductory session provides insight into the complexity of the security puzzle that needs to be solved A wide range of building blocks for developing and maintaining digital applications and infrastructure is presented Each of these puzzle pieces serves a specific purpose and addresses a portion of the cybersecurity challenges During the session  it becomes clear that cybersecurity is a concern throughout the entire lifecycle of digital systems Furthermore  a silver bullet is unfortunately not on the table We focus on methodologies and technologies for the development of secure digital applications This session is part of the series Cybersecurity excellence Session 1 Tackling cybersecurity challenges a complex security puzzle  28 February 2024  Vincent Naessens KU LeuvenSession 2 ‘Privacy by design a technical approach to privacy risk  26 March 2024  Kim Wuyts PwCSession 3 Efficient use of a network protocol analyzer in cyber threats workshop  24 April 2024  Tom Cordemans KU LeuvenSession 4 Hacking and protecting embedded devices workshop  29 May 2024  Jorn Lapon KU LeuvenSession 5 EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems  28 August 2024  Vincent Naessens KU LeuvenSession 6 Cyberattack response  25 September 2024  Tom Bauwens Eubelius  Kalman Tiboldi TVHSession 7 Postquantum cryptography  23 October 2024  Eric Michiels IBM  In an increasingly technologydriven world cybersecurity stands as the cornerstone of digital resilience In this programme we will explore the full spectrum of cybersecurity from prevention to response while gaining both immediate handson skills and a foresight for the future of cybersecurity This programme brings together academic researchers and industrial experts and thus provides a blend of lectures and use cases and practical testimonials httpspuckuleuvenbenlopleidingtacklingcybersecuritychallengesacomplexsecuritypuzzlev4pjylm0png850an Session 1  Cybersecurity excellence series 2024 trainingGhentKU Leuven Postuniversitair Centrum</data><data key="annotated_by">manual</data><data key="start_time">28 Feb 2024 14:00 - 17:00</data><data key="price">€250</data><data key="sub_title">Session 1 | Cybersecurity excellence series 2024</data><data key="subscription_limit">NA</data><data key="intro">In an increasingly technology-driven world, cybersecurity stands as the cornerstone of digital resilience. In this programme, we will explore the full spectrum of cybersecurity, from prevention to response, while gaining both immediate, hands-on skills and a foresight for the future of cybersecurity. This programme brings together academic researchers and industrial experts, and thus provides a blend of lectures and use cases and practical testimonials.</data><data key="language">English</data><data key="location_detail">KU Leuven, Campus Rabot - Gebroeders de Smetstraat 1, 9000 Ghent</data><data key="date">28/02/2024</data><data key="details">https://puc.kuleuven.be/nl/opleiding/tackling_cybersecurity_challenges_a_complex_security_puzzle-v4pjylm0png850an</data><data key="full_body"> This introductory session provides insight into the complexity of the security puzzle that needs to be solved. A wide range of building blocks for developing and maintaining digital applications and infrastructure is presented. Each of these puzzle pieces serves a specific purpose and addresses a portion of the cybersecurity challenges. During the session  it becomes clear that cybersecurity is a concern throughout the entire lifecycle of digital systems. Furthermore  a silver bullet is unfortunately not on the table. We focus on methodologies and technologies for the development of secure digital applications. This session is part of the series Cybersecurity excellence: Session 1: Tackling cybersecurity challenges: a complex security puzzle | 28 February 2024 - Vincent Naessens (KU Leuven)Session 2: ‘Privacy by design': a technical approach to privacy risk | 26 March 2024 - Kim Wuyts (PwC)Session 3: Efficient use of a 'network protocol analyzer' in cyber threats (workshop) | 24 April 2024 - Tom Cordemans (KU Leuven)Session 4: Hacking and protecting embedded devices (workshop) | 29 May 2024 - Jorn Lapon (KU Leuven)Session 5: EU cybersecurity standards and regulation for IoT ecosystems and Industrial Control Systems | 28 August 2024 - Vincent Naessens (KU Leuven)Session 6: Cyberattack response | 25 September 2024 - Tom Bauwens (Eubelius)  Kalman Tiboldi (TVH)Session 7: Post-quantum cryptography | 23 October 2024 - Eric Michiels (IBM) </data><data key="url">https://www.vaia.be/en/courses/tackling-cybersecurity-challenges-a-complex-security-puzzle</data><data key="title">Tackling cybersecurity challenges: a complex security puzzle</data><data key="constraints">no technical prior knowledge/background is needed</data><data key="course_info">training-Ghent-KU Leuven Postuniversitair Centrum</data></node>
<node id="n2186" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="sub_title">27 Feb 2024 16:00 - 19:30</data><data key="start_time">27 Feb 2024 16:00 - 19:30</data><data key="price">€125 excl. VAT</data><data key="data">GPTWork The responsible use of generative AI in organizations  This course will be given by Prof Gregory Lewkowicz and Prof David Restrepo Amariles at the FARI Test  Experience Center Cantersteen 16  1000  Brussels Since the release of ChatGPT  the spread of generative AI has made available to individuals and organizations an impressive technology However  it also presents numerous risks for public administrations and businesses  especially when this technology is adopted discreetly and without proper guidance within them This training aims to ensure a better understanding of how generative AI works and to examine the conditions for its responsible use in organizations considering best practices developed internationally and the latest scientific research Acquire knowledge about the functioning of large language models LLMs and generative AI to better understand their value proposition for organizationsIdentify risks and key elements to consider for ensuring a responsible adoption of generative AI in organizationsFormulate a policy regarding the use of generative AI at work  taking into account the best practices currently under development As FARI is supported by the European Resilience  Recovery Fund RRF  we can continue to offer free access to participants from public administrations public institutions and research institutions We can also provide a 50 discount to participants from the following target groups Educational institutionsNGOs and nonprofit organisationsBrussels based companiesBrussels citizens  More info  Share this course   httpswwwfaribrusselseducationchatgptwork 27 Feb 2024 1600  1930 training courseBrusselsFARI</data><data key="language">English</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: anyone working in public administrations and businesses</data><data key="intro">More info  Share this course  </data><data key="date">27/02/2024</data><data key="full_body"> This course will be given by Prof. Gregory Lewkowicz and Prof. David Restrepo Amariles at the FARI Test &amp; Experience Center (Cantersteen 16  1000  Brussels). Since the release of ChatGPT  the spread of generative AI has made available to individuals and organizations an impressive technology. However  it also presents numerous risks for public administrations and businesses  especially when this technology is adopted discreetly and without proper guidance within them. This training aims to ensure a better understanding of how generative AI works and to examine the conditions for its responsible use in organizations considering best practices developed internationally and the latest scientific research. Acquire knowledge about the functioning of large language models (LLMs) and generative AI to better understand their value proposition for organizations.Identify risks and key elements to consider for ensuring a responsible adoption of generative AI in organizations.Formulate a policy regarding the use of generative AI at work  taking into account the best practices currently under development. As FARI is supported by the European Resilience &amp; Recovery Fund (RRF)  we can continue to offer free access to participants from public administrations public institutions and research institutions. We can also provide a 50% discount to participants from the following target groups: Educational institutionsNGOs and non-profit organisationsBrussels based companiesBrussels citizens </data><data key="location_detail">Be Central, FARI Experience Centre - Cantersteen 16, 1000 Bruxelles</data><data key="course_info">training course-Brussels-FARI</data><data key="url">https://www.vaia.be/en/courses/gpt-work-the-responsible-use-of-generative-ai-in-organizations-en</data><data key="title">GPT@Work: The responsible use of generative AI in organizations</data><data key="constraints">NA</data><data key="details">https://www.fari.brussels/education/chat-gpt-work</data></node>
<node id="n2187" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">31 Jan 2024</data><data key="target_group">Target audience: Msc, Bsc, PhD students and researchers interested in deploy machine learning models in real scenarios after testing in academic enviromental.</data><data key="data">Transitioning ML Workflows from Academia to Practice  MSc  BSc  PhD students and researchers interested in deploying  ML models in real scenarios after testing in an academic environment Day 1 900 Overview of academic machine learning versus realworld deployment  Dr Dirk Valkenborg  Christopher Patzanovsky  UHasselt 1030 Break 1045 Mapping the gaps in order to deploy the machine learning models Dr Gökhan Ertaylan  VITO 1215 Lunch break 1315 Possibilities of the use of Vlaams Supercomputer Centrum in machine learning models  Dr Kenneth Hoste  Dr Lara Peeters  Ghent University 1445 Break 1500 What is important in order to deploy the models in edge devices Dr Peter Karsmakers  KU Leuven 1630 QA and DAY 1 conclusion Day 2 900 Tiny machine learning TinyML overviewDr Sam Leroux  Ghent University 1030 Break 1045  Model optimisation for deployment Dr Sam Leroux  Ghent University 1215 Lunch break 1315 Introduction to containerisation with DockerDr Bruna Piereck  Dr Alexander Botzki 1445 Break 1500 Handson Session Docker for machine learningDr Bruna Piereck  Dr Alexander Botzki 1630 Course conclusion  QA  and feedback session  This foundational course offers a comprehensive journey through  the various stages of developing and deploying machine learning models  Participants will gain insights into transitioning models from academic  research to practical applications They will understand the  significance of having a clear understanding of what to expect in  realworld scenarios when deploying a machine learning model The course  also delves into strategies for scalability and efficiency ensuring  participants are equipped to handle diverse realworld scenarios  Additionally it concludes with techniques for downscaling models to  edge devices for realtime processing Throughout the course  participants will acquire practical skills and knowledge essential for  navigating these transitions smoothly empowering them to face various  realworld challenges  httpswwwuhasseltbeenstudyprogrammesprofessionalstransitioningmlworkflowsfromacademiatopracticevaia Twoday master class twoday masterclassHasseltUHasselt amp VAIA</data><data key="start_time">26 Feb 2024 - 27 Feb 2024</data><data key="price">researchers: €242/day | students: €60.50/day | professionals : €484/day (TVA included)</data><data key="sub_title">Two-day master class</data><data key="intro">This foundational course offers a comprehensive journey through  the various stages of developing and deploying machine learning models.  Participants will gain insights into transitioning models from academic  research to practical applications. They will understand the  significance of having a clear understanding of what to expect in  real-world scenarios when deploying a machine learning model. The course  also delves into strategies for scalability and efficiency, ensuring  participants are equipped to handle diverse real-world scenarios.  Additionally, it concludes with techniques for downscaling models to  edge devices for real-time processing. Throughout the course,  participants will acquire practical skills and knowledge essential for  navigating these transitions smoothly, empowering them to face various  real-world challenges. </data><data key="language">English</data><data key="location_detail">Oude gevangenis, Martelarenlaan 42, 3500 Hasselt</data><data key="date">26/02/2024</data><data key="details">https://www.uhasselt.be/en/study/programmes-professionals/transitioning-ml-workflows-from-academia-to-practice-vaia</data><data key="full_body"> MSc  BSc  PhD students and researchers interested in deploying  ML models in real scenarios after testing in an academic environment Day 1 9:00: Overview of academic machine learning versus real-world deployment.  Dr Dirk Valkenborg &amp; Christopher Patzanovsky - UHasselt 10:30: Break 10:45: Mapping the gaps in order to deploy the machine learning models. Dr Gökhan Ertaylan - VITO 12:15: Lunch break 13:15: Possibilities of the use of Vlaams Supercomputer Centrum in machine learning models  Dr Kenneth Hoste &amp; Dr Lara Peeters - Ghent University 14:45: Break 15:00: What is important in order to deploy the models in edge devices Dr Peter Karsmakers - KU Leuven 16:30: Q&amp;A and DAY 1 conclusion Day 2 9:00: Tiny machine learning (TinyML) overviewDr Sam Leroux - Ghent University 10:30: Break 10:45:  Model optimisation for deployment Dr Sam Leroux - Ghent University 12:15: Lunch break 13:15: Introduction to containerisation with DockerDr Bruna Piereck &amp; Dr Alexander Botzki 14:45: Break 15:00: Hands-on Session: Docker for machine learningDr Bruna Piereck &amp; Dr Alexander Botzki 16:30: Course conclusion  Q&amp;A  and feedback session </data><data key="url">https://www.vaia.be/en/courses/transitioning-ml-workflows-from-academia-to-practice</data><data key="title">Transitioning ML Workflows from Academia to Practice</data><data key="constraints">programming skills (preferably Python), foundational understanding of machine learning concepts, laptop</data><data key="course_info">two-day masterclass-Hasselt-UHasselt &amp;amp; VAIA</data></node>
<node id="n2188" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="target_group">Target audience: AI researchers</data><data key="sub_title">26 Feb 2024 09:00 - 10:15</data><data key="start_time">26 Feb 2024 09:00 - 10:15</data><data key="price">free</data><data key="subscription_limit">NA</data><data key="data">KG3D Learning and Reasoning Webinar  by Simon Vandevelde Operational decisions are an important part of knowledgeintensive organizations  as these are taken in a high volume on a daily basis However  describing these decisions in a standardized format such as DMN is a timeconsuming task  as various textual sources need to be analyzed In this talk  we present the results of our experiments on an automated approach to generating decision tables from natural language based on the GPT3 LLM Through a total of 72 experiments over six problem descriptions  we evaluated GPT3's decision logic modeling and reasoning capabilities While GPT3 demonstrates promising abilities in extracting decision context and identifying relevant variables from natural language  further enhancements are needed to improve its decision table capabilities for efficient automation of DMN modeling by Ioannis Dasoulas An abundance of tabular data exists and is used by a wide range of applications However  a big portion of these data lack the semantic information necessary for users and machines to properly understand them This lack of table semantic understanding impedes their usage in data analytics pipelines Existing solutions are focused on specific annotation tasks and types of tables  and rely solely on large knowledge bases  making it difficult to reuse in realworld settings In this talk  we present TorchicTab  aversatile semantic table interpretation system able to annotate tables with varied structures by using either an external knowledge graph  such as Wikidata  or annotated tables with predefined terms for training The results demonstrate TorchicTab's ability to produce accurate annotations for different tasks across varied datasets  More info  Share this course   httpskg3dcskuleuvenbeevents 26 Feb 2024 0900  1015 webinaronlineKG3D KU Leuven</data><data key="language">English</data><data key="location_detail">Online</data><data key="intro">More info  Share this course  </data><data key="details">https://kg3d.cs.kuleuven.be/#events</data><data key="course_info">webinar-online-KG3D; KU Leuven</data><data key="date">26/02/2024</data><data key="full_body"> by Simon Vandevelde Operational decisions are an important part of knowledge-intensive organizations  as these are taken in a high volume on a daily basis. However  describing these decisions in a standardized format such as DMN is a time-consuming task  as various textual sources need to be analyzed. In this talk  we present the results of our experiments on an automated approach to generating decision tables from natural language based on the GPT-3 LLM. Through a total of 72 experiments over six problem descriptions  we evaluated GPT-3's decision logic modeling and reasoning capabilities. While GPT-3 demonstrates promising abilities in extracting decision context and identifying relevant variables from natural language  further enhancements are needed to improve its decision table capabilities for efficient automation of DMN modeling. by Ioannis Dasoulas An abundance of tabular data exists and is used by a wide range of applications. However  a big portion of these data lack the semantic information necessary for users and machines to properly understand them. This lack of table semantic understanding impedes their usage in data analytics pipelines. Existing solutions are focused on specific annotation tasks and types of tables  and rely solely on large knowledge bases  making it difficult to re-use in real-world settings. In this talk  we present TorchicTab  aversatile semantic table interpretation system able to annotate tables with varied structures by using either an external knowledge graph  such as Wikidata  or annotated tables with pre-defined terms for training. The results demonstrate TorchicTab's ability to produce accurate annotations for different tasks across varied datasets. </data><data key="title">KG3D Learning and Reasoning Webinar</data><data key="url">https://www.vaia.be/en/courses/kg3d-learning-and-reasoning-webinar</data><data key="constraints">registration is required</data></node>
<node id="n2189" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: all UAntwerp FBE alumni, students, staff and interested professionals are welcome</data><data key="data">The future is AI  Business Connect  Weve invited four expert business leaders from Google  Microsoft   DXC Technology and KPMG to discuss just that How you  as a  professional  can navigate AI and use new technologies to your  advantage  Join us on Thursday 22nd February for  the first edition of our Business Connect  The Future is AI This  event is cohosted by Antwerp Management School  IMBIT and the Faculty  of Business and Economics All alumni  students  staff and interested  professionals are welcome Entrance is free but registration is required 1900  1930 Welcome and registration 1930  2100 Panel discussion withThierry Geerts CEO Google Belgium  LuxembourgMarijke Schroos CEO Microsoft BelgiumPeter Van den Spiegel Head of KPMG LighthousePeter Van Tricht CEO of DXC Belgium 2100  2300 Network reception with drinks and small bites Thierry Geerts  Google Since 2011  Thierry Geerts has  been heading Google in Belgium and Luxembourg  the company that has  become much more than just a search engine He graduated from the VUB as  a Solvay Business Engineer and soon became general manager of an  industrial laundry company With the advent of the internet in the  mid1990s  he reoriented himself towards the media industry and held  various management positions at VUM now Mediahuis  publisher of  newspapers such as De Standaard and Het Nieuwsblad His book Digitalis  2018  in which he describes the possibilities of the digital world   has meanwhile sold more than 25 000 copies In 2021 he published his new  book  Homo Digitalis  about the impact of the digital revolution on  people and society   Marijke Schroos  Microsoft   Marijke Schroos  the General Manager of Microsoft Belgium and  Luxembourg  is a seasoned leader with a demonstrated history of working  in the computer industry She has a broad experience spanning  from marketing  sales and partner management to communication   organizational agility and change management She has a strong track  record in business development and growth strategies  both in Commercial  as in Public Sector environments  including EUi and NATO    Marijke  is passionate about empowering people to unlock their true potential by  understanding their strengths and needs  which she supports through  mentoring programs as well as by being the executive sponsor for  Diversity and Inclusion at Microsoft BeLux   Peter Van Tricht  DXC Technology   Peter Van  Tricht is an alumnus from the economy faculty at Antwerp University He  has over 25 years of experience leveraging the strength of technology to  improve operations within complex government and commercial  organizations Today he is the country director for DXC Technology  a  Fortune 500 global IT services leader helping global companies run their  missioncritical systems and operations while modernizing IT   optimizing data architectures and ensuring security and scalability  across public  private and hybrid clouds In Belgium  DXC serves  customers such as Flemish Government  FOD Justice  Baloise  Bpost   Beaulieu and Umicore Peter Van den Spiegel  KPMG Peter  has over 12 years of experience in both audit and advisory assignments   with a focus on managing and leveraging data as a driving force to  generate business value He has been involved in various business and IT  transformation projects and leads the Data  Analytics Competence  Center at KPMG Belgium In the context of large business transformation  projects  Peter has successfully assisted clients in implementing  recommended practices in business and IT  through standardization  the  use of leading practices  optimization  and automation In an  international context  he has conducted numerous workshops to drive  change in organizations The Business Connects are powered by both the Faculty of Business and  Economics and the Antwerp Management School By joining forces and  cohosting biannual alumni events  youll benefit from our shared  academic strengths and expand your professional network  times two This first edition was organised together with IMBIT   the student association linked to the Business Engineering programme  The association aims to bridge the gap between business management and  information technology  and encourages students to showcase their  entrepreneurial spirit In addition to organizing studentrelated  events  IMBIT hosts activities to support students in their career  development  The association played a key role in the  organisation of this event  by connecting the academic theory with  corporate practice Want to be first to know about future events Join  the IMBIT alumni group  Theres no denying AI is everywhere It creates many solutions but raises an equal amount of questions How do I optimize my daytoday by using AI What developments are on the horizon What about the ethical aspect of things privacy bias security httpswwwuantwerpenbeenaboutuantwerpfacultiesfacultyofbusinessandeconomicsalumnibusinessconnectthefutureisaimsdynttridIX5ZM70zuNmPC4Hn1ck3RnVc0lCXoFWCR2MfsBKgRs Reimagining tomorrow eventAntwerpUAntwerp Faculty of Business and Economics</data><data key="annotated_by">manual</data><data key="sub_title">Reimagining tomorrow</data><data key="subscription_limit">NA</data><data key="start_time">22 Feb 2024 19:00 - 23:00</data><data key="language">English</data><data key="location_detail">Aula Rector Dhanis, K.001 - Stadscampus, Prinsstraat 13, 2000 Antwerp</data><data key="price">free</data><data key="date">22/02/2024</data><data key="full_body"> We've invited four expert business leaders from Google  Microsoft   DXC Technology and KPMG to discuss just that: How you  as a  professional  can navigate AI and use new technologies to your  advantage.  Join us on Thursday 22nd February for  the first edition of our Business Connect  'The Future is AI'. This  event is co-hosted by Antwerp Management School  IMBIT and the Faculty  of Business and Economics. All alumni  students  staff and interested  professionals are welcome. Entrance is free but registration is required. 19:00 - 19:30 Welcome and registration 19:30 - 21:00 Panel discussion withThierry Geerts (CEO Google Belgium &amp; Luxembourg)Marijke Schroos (CEO Microsoft Belgium)Peter Van den Spiegel (Head of KPMG Lighthouse)Peter Van Tricht (CEO of DXC Belgium) 21:00 - 23:00 Network reception with drinks and small bites Thierry Geerts  Google Since 2011  Thierry Geerts has  been heading Google in Belgium and Luxembourg  the company that has  become much more than just a search engine. He graduated from the VUB as  a Solvay Business Engineer and soon became general manager of an  industrial laundry company. With the advent of the internet in the  mid-1990s  he reoriented himself towards the media industry and held  various management positions at VUM (now Mediahuis)  publisher of  newspapers such as De Standaard and Het Nieuwsblad. His book Digitalis  (2018)  in which he describes the possibilities of the digital world   has meanwhile sold more than 25 000 copies. In 2021 he published his new  book  Homo Digitalis  about the impact of the digital revolution on  people and society.   Marijke Schroos  Microsoft   Marijke Schroos  the General Manager of Microsoft Belgium and  Luxembourg  is a seasoned leader with a demonstrated history of working  in the computer industry. She has a broad experience spanning  from marketing  sales and partner management to communication   organizational agility and change management. She has a strong track  record in business development and growth strategies  both in Commercial  as in Public Sector environments  including EUi and NATO.    Marijke  is passionate about empowering people to unlock their true potential by  understanding their strengths and needs  which she supports through  mentoring programs as well as by being the executive sponsor for  Diversity and Inclusion at Microsoft BeLux.   Peter Van Tricht  DXC Technology   Peter Van  Tricht is an alumnus from the economy faculty at Antwerp University. He  has over 25 years of experience leveraging the strength of technology to  improve operations within complex government and commercial  organizations. Today he is the country director for DXC Technology  a  Fortune 500 global IT services leader helping global companies run their  mission-critical systems and operations while modernizing IT   optimizing data architectures and ensuring security and scalability  across public  private and hybrid clouds. In Belgium  DXC serves  customers such as Flemish Government  FOD Justice  Baloise  Bpost   Beaulieu and Umicore. Peter Van den Spiegel  KPMG Peter  has over 12 years of experience in both audit and advisory assignments   with a focus on managing and leveraging data as a driving force to  generate business value. He has been involved in various business and IT  transformation projects and leads the Data &amp; Analytics Competence  Center at KPMG Belgium. In the context of large business transformation  projects  Peter has successfully assisted clients in implementing  recommended practices in business and IT  through standardization  the  use of leading practices  optimization  and automation. In an  international context  he has conducted numerous workshops to drive  change in organizations. The Business Connects are powered by both the Faculty of Business and  Economics and the Antwerp Management School. By joining forces and  co-hosting bi-annual alumni events  you'll benefit from our shared  academic strengths and expand your professional network  times two. This first edition was organised together with IMBIT   the student association linked to the Business Engineering programme.  The association aims to bridge the gap between business management and  information technology  and encourages students to showcase their  entrepreneurial spirit. In addition to organizing student-related  events  IMBIT hosts activities to support students in their career  development.  The association played a key role in the  organisation of this event  by connecting the academic theory with  corporate practice. Want to be first to know about future events? Join  the IMBIT alumni group. </data><data key="intro">There's no denying, AI is everywhere. It creates many solutions, but raises an equal amount of questions. How do I optimize my day-to-day by using AI? What developments are on the horizon? What about the ethical aspect of things... privacy, bias, security?</data><data key="title">The future is AI | Business Connect</data><data key="url">https://www.vaia.be/en/courses/the-future-is-ai-business-connect</data><data key="constraints">NA</data><data key="course_info">event-Antwerp-UAntwerp Faculty of Business and Economics</data><data key="details">https://www.uantwerpen.be/en/about-uantwerp/faculties/faculty-of-business-and-economics/alumni/business-connect/the-future-is-ai/#msdynttrid=IX5ZM70zuNmPC_4Hn1ck3RnVc0lCXoFWCR2MfsBKgRs</data></node>
<node id="n2190" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: researchers and academics with an interest in system theory, algebraic geometry, polynomial optimization, numerical linear algebra, system identification</data><data key="data">The MomentSOS hierarchy  The lecture introduces the approach and describes its main milestones during the last two decades The focus is on the computational features of the MomentSOS hierarchy  its limitations and current efforts to overcome them  Polynomial optimization consists of minimizing a polynomial of many real variables subject to polynomial equality and inequality constraints Its special case is the problem of finding real solutions of a system of polynomial equations This difficult problem has many applications in fields such as statistics signal processing machine learning computer vision computational geometry and control engineering The MomentSOS hierarchy is an approach to polynomial optimization that solves it globally at the price of solving a family of convex semidefinite optimization problems of increasing size httpshomesesatkuleuvenbesistawwwbdmbacktotherootsindexphpseminars Seminar by Didier Henrion LAASCNRS Toulouse France seminarHeverlee LeuvenKU Leuven ESAT STADIUS</data><data key="start_time">20 Feb 2024 17:00 - 18:00</data><data key="language">English</data><data key="price">free</data><data key="sub_title">Seminar by Didier Henrion, LAAS-CNRS, Toulouse, France</data><data key="details">https://homes.esat.kuleuven.be/~sistawww/bdm/backtotheroots/index.php/seminars</data><data key="full_body"> The lecture introduces the approach and describes its main milestones during the last two decades. The focus is on the computational features of the Moment-SOS hierarchy  its limitations and current efforts to overcome them. </data><data key="intro">Polynomial optimization consists of minimizing a polynomial of many real variables subject to polynomial equality and inequality constraints. Its special case is the problem of finding real solutions of a system of polynomial equations. This difficult problem has many applications in fields such as statistics, signal processing, machine learning, computer vision, computational geometry, and control engineering. The Moment-SOS hierarchy is an approach to polynomial optimization that solves it globally at the price of solving a family of convex (semidefinite) optimization problems of increasing size.</data><data key="location_detail">KU Leuven Thermotechnisch Instituut Aula van de Tweede Hoofdwet (01.02)</data><data key="constraints">NA</data><data key="course_info">seminar-Heverlee; Leuven-KU Leuven ESAT STADIUS</data><data key="date">20/02/2024</data><data key="url">https://www.vaia.be/en/courses/the-moment-sos-hierarchy</data><data key="title">The Moment-SOS hierarchy</data></node>
<node id="n2191" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: lawyers, in-house legal counsels, board members, investment bankers, investors, academics, students, and everyone interested in artificial intelligence and business law.</data><data key="data">Artificial Intelligence and Business Law  The Antwerp Center on Responsible AI ACRAI and the Faculty of Law's  research group Business  Law therefore host this lecture series to  facilitate the understanding of the interplay between AI and business  law These lectures have an explicitly interdisciplinary focus  since  the issues raised require insights in the technical aspects of computers  and data science  in business economics  and in the relevant legal  framework   Five lectures are currently planned   Monday 19 February 2024 11h13h AI Bias and the European AI Act Toon Calders University of Antwerp Faculty of Science  Jan Blockx University of Antwerp Faculty of LawMonday 4 March 2024 11h13h AI Transparency and Explainability David Martens University of Antwerp Faculty of Business and Economics   Sylvie De Raedt University of Antwerp Faculty of LawMonday 18 March 2024 11h13h AI Friend or Foe of Fairness from the Tax Administration Anne Van de Vijver  Anouk Decuypere University of Antwerp Faculty of LawMonday 15 April 2024 11h13h TBC AI and Intellectual Property Esther van Zimmeren University of Antwerp Faculty of Law Monday 29 April 2024 11h13h AI and Transport Edwin Van Hassel University of Antwerp Faculty of Business and Economics  Wouter Verheyen University of Antwerp Faculty of Law   It  is possible to attend the lectures physically at the University of  Antwerp's City Campus or via the Livestream  Technological advances and datafication have propelled the development of artificial intelligence AI and spread its applications across society This evolution has many implications for businesses and the legal framework within which businesses operate At the same time legislators and other legal actors are increasingly trying to get to grips with this surge of AI applications httpswwwuantwerpenbeenresearchgroupsbusinessandlawnewsandeventseventslecture2024 5 lectures lecture seriesonline amp AntwerpACRAI UAntwerp</data><data key="annotated_by">manual</data><data key="start_time">19 Feb 2024 - 29 Apr 2024</data><data key="price">free for academics and students; €100 per lecture/€400 for all five lectures for non-academic participants</data><data key="sub_title">5 lectures</data><data key="subscription_limit">NA</data><data key="intro">Technological advances and datafication have propelled the development of artificial intelligence (AI) and spread its applications across society. This evolution has many implications for businesses and the legal framework within which businesses operate. At the same time, legislators and other legal actors are increasingly trying to get to grips with this surge of AI applications.</data><data key="language">English</data><data key="location_detail">Online &amp;amp; UAntwerp City Campus - room R.218, Rodestraat 14, 2000 Antwerp</data><data key="date">19/02/2024</data><data key="details">https://www.uantwerpen.be/en/research-groups/business-and-law/news-and-events/events/lecture-2024/</data><data key="full_body"> The Antwerp Center on Responsible AI (ACRAI) and the Faculty of Law's  research group Business &amp; Law therefore host this lecture series to  facilitate the understanding of the interplay between AI and business  law. These lectures have an explicitly interdisciplinary focus  since  the issues raised require insights in the technical aspects of computers  and data science  in business economics  and in the relevant legal  framework.   Five lectures are currently planned:   Monday 19 February 2024 11h-13h AI Bias and the European AI Act Toon Calders (University of Antwerp Faculty of Science) &amp; Jan Blockx (University of Antwerp Faculty of Law)Monday 4 March 2024 11h-13h AI Transparency and Explainability David Martens (University of Antwerp Faculty of Business and Economics)  &amp; Sylvie De Raedt (University of Antwerp Faculty of Law)Monday 18 March 2024 11h-13h AI: Friend or Foe of Fairness from the Tax Administration Anne Van de Vijver &amp; Anouk Decuypere (University of Antwerp Faculty of Law)Monday 15 April 2024 11h-13h [TBC] AI and Intellectual Property Esther van Zimmeren (University of Antwerp Faculty of Law) Monday 29 April 2024 11h-13h AI and Transport Edwin Van Hassel (University of Antwerp Faculty of Business and Economics) &amp; Wouter Verheyen (University of Antwerp Faculty of Law)   It  is possible to attend the lectures physically at the University of  Antwerp's City Campus or via the Livestream. </data><data key="url">https://www.vaia.be/en/courses/artificial-intelligence-and-business-law</data><data key="title">Artificial Intelligence and Business Law</data><data key="constraints">NA</data><data key="course_info">lecture series-online &amp;amp; Antwerp-ACRAI; UAntwerp</data></node>
<node id="n2192" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: academics and researchers</data><data key="data">Robotics in Agriculture  Where we are where we are going  Currently most agricultural control action fertilisation  irrigation  plant protections  weeding  harvesting  etc are based on scarcely sampled measurements and are performed homogeneously over large portions of the farm  resulting in unnecessary usage of agrochemicals and loss of yield “Precision Agriculture” aims at overcoming this limitation by performing actions only where and when they are effectively needed Robotics is a key technology to support the technical challenges of precision agriculture In this forum worldwide experts will approach the topic from different points of view  providing an overview of the current state of the art and future challenges of robotics in Agriculture Organised by Brussels Institute of Advanced Studies BrIAS Funded By BrIAS  FARI  FNRS  European Commission Marie SklodovskaCurie Grant N 101102281 Technical Support Food and Agriculture Organization FAOOrganizing Committee General Chair  Emanuele Garone ULB Program Chair Luca Rossini ULB Logistics Chair Anja Garone VUB Member of the Program Committee Frederic Francis ULg  Raphael Junkers UCL  Francesco Lamonaca University of Calabria  Mario Contarini University of Tuscia  Yael Edan BenGurion University of the Negev  Carl Morch FARI  More info  Share this course   httpswwwbriasbeenbriasforumonroboticsinagriculturewherewearewherewearegoing 15 Feb 2024  16 Feb 2024 seminaronline amp BrusselsBrussels Institute of Advanced Studies BrIAS FARI</data><data key="sub_title">15 Feb 2024 - 16 Feb 2024</data><data key="start_time">15 Feb 2024 - 16 Feb 2024</data><data key="language">English</data><data key="location_detail">Online &amp;amp; Cantersteen 16, 1000 Brussels</data><data key="price">free</data><data key="intro">More info  Share this course  </data><data key="date">15/02/2024</data><data key="details">https://www.brias.be/en/brias-forum-on-robotics-in-agriculture-where-we-are-where-we-are-going</data><data key="full_body"> Currently most agricultural control action (fertilisation  irrigation  plant protections  weeding  harvesting  etc.) are based on scarcely sampled measurements and are performed homogeneously over large portions of the farm  resulting in unnecessary usage of agrochemicals and loss of yield. “Precision Agriculture” aims at overcoming this limitation by performing actions only where and when they are effectively needed. Robotics is a key technology to support the technical challenges of precision agriculture. In this forum worldwide experts will approach the topic from different points of view  providing an overview of the current state of the art and future challenges of robotics in Agriculture. Organised by: Brussels Institute of Advanced Studies (BrIAS). Funded By: BrIAS  FARI  FNRS  European Commission (Marie Sklodovska-Curie Grant N. 101102281). Technical Support: Food and Agriculture Organization (FAO).Organizing Committee General Chair : Emanuele Garone (ULB) Program Chair: Luca Rossini (ULB) Logistics Chair: Anja Garone (VUB) Member of the Program Committee: Frederic Francis (ULg)  Raphael Junkers (UCL)  Francesco Lamonaca (University of Calabria)  Mario Contarini (University of Tuscia)  Yael Edan (Ben-Gurion University of the Negev)  Carl Morch (FARI). </data><data key="url">https://www.vaia.be/en/courses/robotics-in-agriculture-where-we-are-where-we-are-going</data><data key="title">Robotics in Agriculture - Where we are, where we are going</data><data key="constraints">NA</data><data key="course_info">seminar-online &amp;amp; Brussels-Brussels Institute of Advanced Studies (BrIAS); FARI</data></node>
<node id="n2193" labels=":Course"><data key="labels">:Course</data><data key="subscription_limit">11 Feb 2024</data><data key="annotated_by">manual</data><data key="sub_title">Microcredential</data><data key="data">Knowledge Graphs  Open Data Web ScrapingLegal aspects of data reuseFindable  Accessible  Interoperable and Reusable dataOpen Data portals The quest for the universal data model Knowledge Representations keyval  resourcebased  triplebasedLinked Data and the RDF data modelLinked Data and its serializationsProperty graphs and RDFLogic with N3 Data Architectures Linked Data FragmentsEvent sourcing and Linked Data Event StreamsRDF Stream ProcessingThe Open World AssumptionConway's law Web Querying An introduction to SPARQLQuerying endpointsLink TraversalHypermediabased queryingData summaries Building Linked Data spaces Data Spaces with IDSAMetadata management with DCATIdentity management with SolidOIDCAuthorization and policies with WAC  ACP  ODRL and N3 rulesPersonal data management with SolidCrossapp interoperability with SolidData provenance with PROVO  PPlan  SDSOntology engineering with SKOS  RDFS and OWLValidating RDF and building application profiles with SHACL and ShEx Guest Lectures from European data tech companies and data publishers Competences Initial competences Being able to read HTTP messages URL  method  body  response codes  headers…Executing HTTP requests via the browser and the commandlineReading and writing data fromin a CSVfile  a JSONfile and relational databasesMaking small JavaScript programs in the browser and Nodejs reading files  performing HTTP interactions Final competences Arguing the positioning  importance  and limitations of open dataChoosing the appropriate Web API to publish knowledge graphsModeling data as RDF graphsPublishing knowledge graph on the Web from raw dataDesigning a data architecture with fully automated data adoption and assessing tradeoffsBuilding a Linked Data vocabulary and application profile in RDFInterpreting and creating SKOS  RDFS  and OWL constraintsInterpreting provenance of RDF dataPerforming validation on RDF dataQuerying the Web of Linked Data using ComunicaPositioning the industry opportunities and challenges on graph data  Managing data on one machine for one specific kind of use is fairly straightforward It is from the moment that that initial dataset needs to be shared with more than one application and needs to be combined with other datasets managed by other organizations on different machines that more complex computer science and information technology problems arise In this course we will deepdive in the current state of the art in creating Knowledge on WebScale Your personal data data published publicly on the Web and data explicitly shared with you becomes your Knowledge Graph that applications and services can use to assist you in your day to day activities Data scientists and engineers today claim 80 of their time goes to preparing and integrating the data let us take you on a quest to fully automate data integration httpswwwugainugentbeMCKG2024htm Microcredential MicrocredentialGentUGent</data><data key="start_time">12 Feb 2024 - 28 Jun 2024</data><data key="location_detail">Gent</data><data key="price">322,60 euro</data><data key="target_group">Target audience: Several vacancies in Flanders require knowledge of the Flemish data space in order to share data interoperable. In this series of lessons, the professional learns to prioritize what is important when publishing data, and how he can contribute to a future in which data integration and negotiations over access to data can be fully automated.</data><data key="full_body"> Open Data Web ScrapingLegal aspects of data reuseFindable  Accessible  Interoperable and Reusable dataOpen Data portals The quest for the universal data model Knowledge Representations: keyval  resource-based  triple-basedLinked Data and the RDF data modelLinked Data and its serializationsProperty graphs and RDF*Logic with N3 Data Architectures Linked Data FragmentsEvent sourcing and Linked Data Event StreamsRDF Stream ProcessingThe Open World AssumptionConway's law Web Querying An introduction to SPARQLQuerying endpointsLink TraversalHypermedia-based queryingData summaries Building Linked Data spaces Data Spaces with IDSAMetadata management with DCATIdentity management with Solid-OIDCAuthorization and policies with WAC  ACP  ODRL and N3 rulesPersonal data management with SolidCross-app interoperability with SolidData provenance with PROV-O  P-Plan  SDSOntology engineering with SKOS  RDFS and OWLValidating RDF and building application profiles with SHACL and ShEx Guest Lectures from European data tech companies and data publishers Competences Initial competences Being able to read HTTP messages (URL  method  body  response codes  headers…)Executing HTTP requests via the browser and the command-lineReading and writing data from/in a CSV-file  a JSON-file and relational databasesMaking small JavaScript programs in the browser and Node.js (reading files  performing HTTP interactions) Final competences Arguing the positioning  importance  and limitations of open dataChoosing the appropriate Web API to publish knowledge graphsModeling data as RDF graphsPublishing knowledge graph on the Web from raw dataDesigning a data architecture with fully automated data adoption and assessing trade-offsBuilding a Linked Data vocabulary and application profile in RDFInterpreting and creating SKOS  RDFS  and OWL constraintsInterpreting provenance of RDF dataPerforming validation on RDF dataQuerying the Web of Linked Data using ComunicaPositioning the industry opportunities and challenges on graph data </data><data key="intro">Managing data on one machine for one specific kind of use is fairly straightforward. It is from the moment that that initial dataset needs to be shared with more than one application and needs to be combined with other datasets managed by other organizations on different machines, that more complex computer science and information technology problems arise. In this course we will deep-dive in the current state of the art in creating Knowledge on WebScale. Your personal data, data published publicly on the Web and data explicitly shared with you, becomes your Knowledge Graph that applications and services can use to assist you in your day to day activities. Data scientists and engineers today claim 80% of their time goes to preparing and integrating the data: let us take you on a quest to fully automate data integration.</data><data key="language">English</data><data key="course_info">Micro-credential-Gent-UGent</data><data key="constraints">A basic knowledge of JavaScript, HTTP, and command line is required.</data><data key="date">12/02/2024</data><data key="details">https://www.ugain.ugent.be/MCKG2024.htm</data><data key="url">https://www.vaia.be/en/courses/microcredential-knowledge-graphs</data><data key="title">Knowledge Graphs</data></node>
<node id="n2194" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: researchers in AI</data><data key="subscription_limit">NA</data><data key="data">Objectives and challenges of neuromorphic engineering amp Processing personal data  Guillaume Drion  Pierre Sacré  Alessio Franci Montefiore Institute  ULiège This course will provide an overview of the field of neuromorphic engineering The first part will start with a brief history map of the parallel evolution of artificial neural networks and neurophysiology  highlighting similarities and differences as well as attempts to bridge both fields over the last decades It will then review the particular objectives  methods  and challenges of neuromorphic computing in the broad context of artificial intelligence The second part will dive into the specific cases of eventbased sensing  eventbased actuation  and eventbased computing Each of these cases will be motivated by concrete examples  specifically focusing on eventbased cameras  neuromodulation control of locomotion  and spiking neural networks PierreFrançois Pirlet Data Protection Officer  ULiège The GDPR is European legislation organizing the collection and use of personal data This law organizes strict modalities for this use This presentation will be organized around the presentation of the GDPRs key legal concepts and the tangible situations encountered by researchers using personal data The right to image as interpreted in Belgium will also be briefly presented  The TRAIL doctoral seminars are afternoon sessions primarily targeting PhD students doing research in AI They are organized in the context of TRAIL and cosponsored by the FNRS CIL doctoral school and the SPWEER ARIAC project These events take place every month in a hybrid mode both onsite and online httpwwwciluliegebetraildoctoralseminarsindexhtml TRAIL doctoral seminar in February 2024 seminaronline amp LiègeTRAIL</data><data key="annotated_by">manual</data><data key="start_time">8 Feb 2024 14:00 - 16:45</data><data key="location_detail">Online &amp;amp; ULiège</data><data key="sub_title">TRAIL doctoral seminar in February 2024</data><data key="intro">The TRAIL doctoral seminars are afternoon sessions primarily targeting PhD students doing research in AI. They are organized in the context of TRAIL and co-sponsored by the FNRS CIL doctoral school and the SPW-EER ARIAC project. These events take place every month, in a hybrid mode (both on-site and online).</data><data key="language">English</data><data key="price">free</data><data key="date">8/02/2024</data><data key="details">http://www.cil.uliege.be/trail_doctoral_seminars/index.html</data><data key="full_body"> Guillaume Drion  Pierre Sacré  Alessio Franci (Montefiore Institute  ULiège) This course will provide an overview of the field of neuromorphic engineering. The first part will start with a brief history map of the parallel evolution of artificial neural networks and neurophysiology  highlighting similarities and differences as well as attempts to bridge both fields over the last decades. It will then review the particular objectives  methods  and challenges of neuromorphic computing in the broad context of artificial intelligence. The second part will dive into the specific cases of event-based sensing  event-based actuation  and event-based computing. Each of these cases will be motivated by concrete examples  specifically focusing on event-based cameras  neuromodulation control of locomotion  and spiking neural networks. Pierre-François Pirlet (Data Protection Officer  ULiège) The GDPR is European legislation organizing the collection and use of personal data. This law organizes strict modalities for this use. This presentation will be organized around the presentation of the GDPR's key legal concepts and the tangible situations encountered by researchers using personal data. The "right to image" (as interpreted in Belgium) will also be briefly presented. </data><data key="url">https://www.vaia.be/en/courses/objectives-and-challenges-of-neuromorphic-engineering-processing-personal-data</data><data key="title">Objectives and challenges of neuromorphic engineering &amp;amp; Processing personal data</data><data key="constraints">NA</data><data key="course_info">seminar-online &amp;amp; Liège-TRAIL</data></node>
<node id="n2195" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: This course targets professionals and investigators from all areas that are high-dimensional.</data><data key="data">High Dimensional Data Analysis    Methods for the analysis of high dimensional data                               Modern high throughput technologies easily generate data on thousands of variables eg health care data  genomics  chemometrics  environmental monitoring  web logs  movie ratings  … Conventional statistical methods are no longer suited for effectively analysing such highdimensional data Multivariate statistical methods may be used  but for often the dimensionality of the data set is much larger than the number of biological samples Modern advances in statistical data analyses allow for the appropriate analysis of such data Methods for the analysis of high dimensional data rely heavily on multivariate statistical methods Therefore a large part of the course content is devoted to multivariate methods  but with a focus on high dimensional settings and issues Multivariate statistical analysis covers many methods In this course a selection of techniques is covered based on our experience that they are frequently used in industry and research institutes The course is taught using case studies with applications from different fields analytical chemistry  ecology  biotechnology  genomics  … Target audience This course targets professionals and investigators from all areas that are highdimensional Fees The participation fee is 1320 EUR for participants from the private sector Reduced prices apply to students and staff from nonprofit  social profit  and government organizations Industry  private sector  profession € 1320Non profit  government  higher education staff € 990Doctoral students  unemployed € 595 If two or more employees from the same company enrol simultaneously for this course a reduction of 20 on the course fee is taken into account starting from the second enrolment Registration More information and registration on our BetaAcademy website  Methods for the analysis of high dimensional data httpsbetaacademyugentbeenprogramshortandlongrunninginitiatives202320242024m10hdmodule10highdimensionaldata Methods for the analysis of high dimensional data courseGhentUGent</data><data key="start_time">6 Feb 2024 - 22 Feb 2024</data><data key="price">€595 - €1320</data><data key="sub_title">Methods for the analysis of high dimensional data</data><data key="location_detail">Krijgslaan 281, 9000 Gent</data><data key="full_body">   Methods for the analysis of high dimensional data                               Modern high throughput technologies easily generate data on thousands of variables; e.g. health care data  genomics  chemometrics  environmental monitoring  web logs  movie ratings  … Conventional statistical methods are no longer suited for effectively analysing such high-dimensional data. Multivariate statistical methods may be used  but for often the dimensionality of the data set is much larger than the number of (biological) samples. Modern advances in statistical data analyses allow for the appropriate analysis of such data. Methods for the analysis of high dimensional data rely heavily on multivariate statistical methods. Therefore a large part of the course content is devoted to multivariate methods  but with a focus on high dimensional settings and issues. Multivariate statistical analysis covers many methods. In this course a selection of techniques is covered based on our experience that they are frequently used in industry and research institutes. The course is taught using case studies with applications from different fields (analytical chemistry  ecology  biotechnology  genomics  …). Target audience This course targets professionals and investigators from all areas that are high-dimensional. Fees The participation fee is 1320 EUR for participants from the private sector. Reduced prices apply to students and staff from non-profit  social profit  and government organizations. Industry  private sector  profession*: € 1320Non profit  government  higher education staff: € 990(Doctoral) students  unemployed: € 595 *If two or more employees from the same company enrol simultaneously for this course a reduction of 20% on the course fee is taken into account starting from the second enrolment. Registration More information and registration on our Beta-Academy website. </data><data key="intro">Methods for the analysis of high dimensional data</data><data key="language">English</data><data key="course_info">course-Ghent-UGent</data><data key="details">https://beta-academy.ugent.be/en/program/short-and-long-running-initiatives/2023-2024-2024m10hd-module-10-high-dimensional-data</data><data key="title">High Dimensional Data Analysis</data><data key="url">https://www.vaia.be/en/courses/high-dimensional-data-analysis</data><data key="constraints">Course prerequisites are ready at hand knowledge of basic statistics: data exploration and descriptive statistics, statistical modeling, and inference: linear models, confidence intervals, t-tests, F-tests, anova, chi-squared test, such as covered in Module 4 - Drawing Conclusions from Data: an Introduction, Module 8 - Exploiting Sources of Variation in your Data: the ANOVA Approach and Module 11 - Explaining and Predicting Outcomes with Linear Regression of this years' course program.</data><data key="date">6/02/2024</data></node>
<node id="n2196" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: researchers and academics with an interest in system theory, algebraic geometry, polynomial optimization, numerical linear algebra, system identification</data><data key="data">Fast and Stable Roots of Polynomials via Companion Matrices  To compactly store the matrix we will show that only 3n1 rotators are required  so the storage space is On In fact  these rotators only represent the unitary part  but we will show that we can retrieve the rankone part from the unitary part with a trick It is thus not necessary to store the rankone part explicitly Francis's algorithm tuned for working on this representation requires only On flops per iteration and thus On² flops in total The algorithm is normwise backward stable and is shown to be about as accurate as the slow Francis QR algorithm applied to the companion matrix without exploiting the structure It is also faster than other On² methods that have been proposed  and its accuracy is comparable or better The paper accompanying this research received SIAM's outstanding paper prize in 2017httpswwwsiamorgprizesrecognitionmajorprizeslecturesdetailsiamoutstandingpaperprizes  Raf Vandebril presents a fast and stable algorithm for computing roots of polynomials The roots are found by computing the eigenvalues of the associated companion matrix A companion matrix is an upper Hessenberg matrix that is of unitaryplusrankone form that is it is the sum of a unitary matrix and a rankone matrix When running Francis's implicitlyshifted QR algorithm this property is preserved and exactly that is exploited here httpshomesesatkuleuvenbesistawwwbdmbacktotherootsindexphpseminars Seminar by Raf Vandebril seminarHeverlee LeuvenKU Leuven ESAT STADIUS</data><data key="start_time">6 Feb 2024 17:00 - 18:00</data><data key="price">0</data><data key="sub_title">Seminar by Raf Vandebril</data><data key="intro">Raf Vandebril presents a fast and stable algorithm for computing roots of polynomials. The roots are found by computing the eigenvalues of the associated companion matrix. A companion matrix is an upper Hessenberg matrix that is of unitary-plus-rank-one form, that is, it is the sum of a unitary matrix and a rank-one matrix. When running Francis's implicitly-shifted QR algorithm this property is preserved, and exactly that is exploited here.</data><data key="language">English</data><data key="location_detail">KU Leuven, Aula Arenbergkasteel (01.07)</data><data key="date">6/02/2024</data><data key="details">https://homes.esat.kuleuven.be/~sistawww/bdm/backtotheroots/index.php/seminars</data><data key="full_body"> To compactly store the matrix we will show that only 3n-1 rotators are required  so the storage space is O(n). In fact  these rotators only represent the unitary part  but we will show that we can retrieve the rank-one part from the unitary part with a trick. It is thus not necessary to store the rank-one part explicitly. Francis's algorithm tuned for working on this representation requires only O(n) flops per iteration and thus O(n²) flops in total. The algorithm is normwise backward stable and is shown to be about as accurate as the (slow) Francis QR algorithm applied to the companion matrix without exploiting the structure. It is also faster than other O(n²) methods that have been proposed  and its accuracy is comparable or better. The paper accompanying this research received SIAM's outstanding paper prize in 2017.https://www.siam.org/prizes-recognition/major-prizes-lectures/detail/siam-outstanding-paper-prizes </data><data key="url">https://www.vaia.be/en/courses/fast-and-stable-roots-of-polynomials-via-companion-matrices</data><data key="title">Fast and Stable Roots of Polynomials via Companion Matrices</data><data key="constraints">NA</data><data key="course_info">seminar-Heverlee; Leuven-KU Leuven ESAT STADIUS</data></node>
<node id="n2197" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: The Winter School is open to people coming from academia and professionals active within the sports sector.</data><data key="data">Becoming an Olympian in Sports Analytics  During the first three days  BOSA participants will gain fundamental and practical skills from both disciplines sport and data science and combinemix them in the final hackathon project in which they will intensively work together in a multidisciplinary context  The hackathon will require participants to put together the diverse  skill sets they bring to the table During the hackathon  they will also  be in close contact with the end usersfederations that define the  hackathon topics Confirmed speakers  topics Cédric Van Branteghem BOIC  Team Belgium  Data  Ine Van Caekenberghe Ghent University  Biomechanics  Tyler Bosch Red Bull Athlete Performance Centers  Data Analytics  Stijn Corten Zwemfed  Automating Swimming Analysis  Jan De Bruyne KU Leuven  Sports Data Ethics  GDPR  Privacy  José José Antonio Oramas Mogrovejo Antwerp University  Explainable AI  Jan Vancompernolle Cycling Vlaanderen  Track Cycling Analysis  Dietrich Heiser iLudus  implementation in clubsteams  Dirk Vissers Antwerp University  Sports Physiology  Jan Boone Ghent University  PACE  Benedikt Fasel Archinisis  Wearable Sports Performance Analysis  Nico Van de Weghe Ghent University  geoAI  Tim Verdonck Antwerp University  Data Science  Xan Allaerts Sneeuwsport Vlaanderen  Ski monitoring  Ann Wauters Chicago Sky  the role of Data in elite female basketball Steven Verstockt Ghent University  imec  IDLab  Leonid Kholkine University of Antwerp  imec  IDLab  Femke De Backere Vlaamse AI Academie  VAIA  Bart Nonneman University of Antwerp  imec  IDLab  Robbe Decorte Ghent University  imec  IDLab  Maarten Slembrouck Ghent University  imec  IDLab  Joachim Taelman Ghent University  imec  IDLab  Sport Vlaanderen The IDLab sports data science team  the organiser of  BOSA 2024 and BOSA 2022  has a broad network and projectsresearch in  each of the 3 sports data science pillars performance analysis   storytelling and athlete safety that will be studied during the winter  school  Data analysis in sports is definitely changing the game  Data analysis helps sports entities evaluate the performance of their  athletes and assess the recruitment necessary to improve the teams  performance It also evaluates the strong and weak areas of their  opponents enabling coaches to make the right decision on their tactics  But data is not only used in performance optimization It is also used in the storytelling of the game and the safety monitoring of the athletevenue  The Becoming An Olympian In Sports Analytics BOSA Winter School organized by the IDLab Sports Data Science teams of Ghent University and the University of Antwerp focuses on each of these topics It is a weeklong intensive course bringing together junior and senior academics and practitioners in the field of sports data science Two tracks will be organized in parallel  with sufficient crosspollination i a track focused on sports  scientists that want to gather more AIdata science skills and ii a  more handson track for computer scientists to apply their  expertiseskills in a sports context The Winter School is open to people coming from academia and professionals active within the sports sector httpssportsdatasciencebeBOSA2024 A weeklong intensive course bringing together academics and practitioners in the field of sports data science winter schoolGhentVAIA UGent amp UAntwerp</data><data key="annotated_by">manual</data><data key="start_time">5 Feb 2024 - 9 Feb 2024</data><data key="price">TBA</data><data key="sub_title">A week-long intensive course bringing together academics and practitioners in the field of sports data science</data><data key="subscription_limit">10 Jan 2024</data><data key="intro">Data analysis in sports is definitely changing the game.  Data analysis helps sports entities evaluate the performance of their  athletes and assess the recruitment necessary to improve the team's  performance. It also evaluates the strong and weak areas of their  opponents, enabling coaches to make the right decision on their tactics.  But data is not only used in performance optimization. It is also used in the storytelling of the game and the safety monitoring of the athlete/venue.  The Becoming An Olympian In Sports Analytics (BOSA) Winter School, organized by the IDLab Sports Data Science teams of Ghent University and the University of Antwerp, focuses on each of these topics. It is a week-long intensive course bringing together junior and senior academics and practitioners in the field of sports data science. Two tracks will be organized in parallel  with sufficient cross-pollination: (i) a track focused on sports  scientists that want to gather more AI/data science skills, and (ii) a  more hands-on track for computer scientists to apply their  expertise/skills in a sports context. The Winter School is open to people coming from academia and professionals active within the sports sector.</data><data key="language">English</data><data key="location_detail">Sportverblijf Sport Vlaanderen, Watersportbaan, Gent</data><data key="date">5/02/2024</data><data key="details">https://sportsdatascience.be/BOSA2024/</data><data key="full_body"> During the first three days  BOSA participants will gain fundamental and practical skills from both disciplines (sport and data science) and combine/mix them in the final hackathon project in which they will intensively work together in a multidisciplinary context.  The hackathon will require participants to put together the diverse  skill sets they bring to the table. During the hackathon  they will also  be in close contact with the end users/federations that define the  hackathon topics. Confirmed speakers &amp; topics: Cédric Van Branteghem (BOIC  Team Belgium &amp; Data)  Ine Van Caekenberghe (Ghent University  Biomechanics)  Tyler Bosch (Red Bull Athlete Performance Centers  Data Analytics)  Stijn Corten (Zwemfed  Automating Swimming Analysis)  Jan De Bruyne (KU Leuven  Sports Data Ethics  GDPR &amp; Privacy)  José José Antonio Oramas Mogrovejo (Antwerp University  Explainable AI)  Jan Vancompernolle (Cycling Vlaanderen  Track Cycling Analysis)  Dietrich Heiser (i-Ludus  implementation in clubs/teams)  Dirk Vissers (Antwerp University  Sports Physiology)  Jan Boone (Ghent University  PACE)  Benedikt Fasel (Archinisis  Wearable Sports Performance Analysis)  Nico Van de Weghe (Ghent University  geoAI)  Tim Verdonck (Antwerp University  Data Science)  Xan Allaerts (Sneeuwsport Vlaanderen  Ski monitoring)  Ann Wauters (Chicago Sky  the role of Data in elite female basketball). Steven Verstockt (Ghent University  imec  IDLab)  Leonid Kholkine (University of Antwerp - imec  IDLab)  Femke De Backere (Vlaamse AI Academie  VAIA)  Bart Nonneman (University of Antwerp - imec  IDLab)  Robbe Decorte (Ghent University  imec  IDLab)  Maarten Slembrouck (Ghent University  imec  IDLab)  Joachim Taelman (Ghent University  imec  IDLab &amp; Sport Vlaanderen). The IDLab sports data science team  the organiser of  BOSA 2024 (and BOSA 2022)  has a broad network and projects/research in  each of the 3 sports data science pillars (performance analysis   storytelling and athlete safety) that will be studied during the winter  school. </data><data key="url">https://www.vaia.be/en/courses/becoming-an-olympian-in-sports-analytics-2024</data><data key="title">Becoming an Olympian in Sports Analytics</data><data key="constraints">Two tracks will be organized in parallel with sufficient cross-pollination: (i) a track focused on sports scientists that want to gather more AI/data science skills, and (ii) a more hands-on track for computer scientists to apply their expertise/skills in a sports context.</data><data key="course_info">winter school-Ghent-VAIA; UGent &amp;amp; UAntwerp</data></node>
<node id="n2198" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: researchers in natural language understanding</data><data key="data">CALCULUS on natural language understanding  The symposium aims to encourage discussion among researchers on  realizations and challenges  and will explore topics related but not  limited to  Anticipatory and causal representation learning Braininspired machine learning Commonsense reasoning Continual learning Multimodal representations grounded in the physical world Spatiotemporal reasoning Structured and compositional representation learning Texttoimage synthesis and control  The Symposium will feature 16 research talks from the CALCULUS team  and keynote lectures by Mrinmaya Sachan  James Henderson  Mariya Toneva  and Yuval Alaluf Additionally  we invite the submission of abstracts to be presented as posters at the Symposium Dr James Henderson Idiap Research Institute Bayesian Language Understanding with Nonparametric Variational TransformersProf Mariya Toneva Max Planck Institute for Software Systems Language modeling beyond language modelingProf Mrinmaya Sachan ETH Zürich Towards a more Literate AI  and AI powered Learning TechnologiesYuval Alaluf Tel Aviv University Exploring the Creative Possibilities of Generative Models Session 1 Inference and Learning of New Knowledge  Aristotelis Chrysakis “Continual Learning How Neural Networks Expand their Knowledge” Vladimir Araujo “Prediction and Integration for Natural Language Understanding” Ruben Cartuyvels “Explicitly Representing Syntax Improves Sentencetolayout Prediction of Unexpected Situations”  Session 2 Anticipation and Braininspired Representation Learning  Mingxiao Li “Controllable Text to Image and Video Generation and Its Application in Commonsense Reasoning” Florian Mai “Large Language Models with a Working Memory” Jingyuan Sun “Brain Encoding and Decoding for Visual and Language Perception”  Session 3 Multimodal Representation Learning  Damien Sileo “Visual Grounding Strategies for TextOnly Natural Language Processing” Nathan Cornille “Causality and Representation Learning” Graham Spinks “Generating Textual and Visual Explanations of Radiography Images”  Session 4 Multimodal Learning with Structures  Wolf Nuyts “Focus your Attention Improving TexttoImage Generation with Syntactical Restrictions” Victor Milewski “Structured Representations in Visual and Language Data  and Their Correlations” Maria Trusca “Textbased Control for Image Manipulation”  You can find more information about registration here  Florian Mai Jingyuan Sun Maria Trusca MarieFrancine Moens   More info  Share this course   httpscalculusprojecteusymposium 29 Jan 2024  30 Jan 2024 symposiumLeuvenERC KU Leuven</data><data key="annotated_by">manual</data><data key="sub_title">29 Jan 2024 - 30 Jan 2024</data><data key="start_time">29 Jan 2024 - 30 Jan 2024</data><data key="language">English</data><data key="price">€10 for Day 1, €20 for Day 2</data><data key="subscription_limit">NA</data><data key="intro">More info  Share this course  </data><data key="details">https://calculus-project.eu/symposium/</data><data key="date">29/01/2024</data><data key="full_body"> The symposium aims to encourage discussion among researchers on  realizations and challenges  and will explore topics related but not  limited to:  Anticipatory and causal representation learning Brain-inspired machine learning Commonsense reasoning Continual learning Multi-modal representations grounded in the physical world Spatiotemporal reasoning Structured and compositional representation learning Text-to-image synthesis and control  The Symposium will feature 16 research talks from the CALCULUS team  and keynote lectures by Mrinmaya Sachan  James Henderson  Mariya Toneva  and Yuval Alaluf. Additionally  we invite the submission of abstracts to be presented as posters at the Symposium. Dr. James Henderson (Idiap Research Institute): Bayesian Language Understanding with Nonparametric Variational TransformersProf. Mariya Toneva (Max Planck Institute for Software Systems): Language modeling beyond language modelingProf. Mrinmaya Sachan (ETH Zürich): Towards a more Literate AI  and AI powered Learning TechnologiesYuval Alaluf (Tel Aviv University): Exploring the Creative Possibilities of Generative Models Session 1: Inference and Learning of New Knowledge  Aristotelis Chrysakis: “Continual Learning: How Neural Networks Expand their Knowledge” Vladimir Araujo: “Prediction and Integration for Natural Language Understanding” Ruben Cartuyvels: “Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations”  Session 2: Anticipation and Brain-inspired Representation Learning  Mingxiao Li: “Controllable Text to Image and Video Generation and Its Application in Commonsense Reasoning” Florian Mai: “Large Language Models with a Working Memory” Jingyuan Sun: “Brain Encoding and Decoding for Visual and Language Perception”  Session 3: Multi-modal Representation Learning  Damien Sileo: “Visual Grounding Strategies for Text-Only Natural Language Processing” Nathan Cornille: “Causality and Representation Learning” Graham Spinks: “Generating Textual and Visual Explanations of Radiography Images”  Session 4: Multi-modal Learning with Structures  Wolf Nuyts: “Focus your Attention: Improving Text-to-Image Generation with Syntactical Restrictions” Victor Milewski: “Structured Representations in Visual and Language Data  and Their Correlations” Maria Trusca: “Text-based Control for Image Manipulation”  You can find more information about registration here.  Florian Mai Jingyuan Sun Maria Trusca Marie-Francine Moens  </data><data key="location_detail">Kasteel Arenberg - Kardinaal Mercierlaan 94, 3001 Leuven</data><data key="url">https://www.vaia.be/en/courses/calculus-on-natural-language-understanding</data><data key="title">CALCULUS, on natural language understanding</data><data key="constraints">NA</data><data key="course_info">symposium-Leuven-ERC; KU Leuven</data></node>
<node id="n2199" labels=":Course"><data key="labels">:Course</data><data key="data">AI for Forecasting and Demand Planning  In this seminar  three experts share their knowledge and  experience using AI in forecasting and demand planning We showcase the  effectiveness of AI forecasting using treebased machine learning and  outline the conditions in which these methods surpass popular  statistical approaches Next  we will explore the essential elements of a  good AI demandsales forecast beyond mere accuracy  delving into  probability forecasting  explainability  stability  and more Finally   we provide a guide for achieving demand planning excellence through  objectives  data  metrics  models  and enrichment We also discuss how  teams can enrich MLgenerated forecasts using the famous forecast  valueadded framework These three experts are Arnoud Wellens Postdoctoral researcher in forecasting  KU Leuven  Ruben Crevits Lead data scientist  OMP  PhD in forecasting and Nicolas Vandeput Supply chain data scientist  CEO  founder  Author 1830 Registration and sandwiches1900 Seminar2100 Networking drink Registration Ekonomika AlumniPICSmembers € 30Nonmembers € 45 Registration is possible until Thursday 11 January Location Faculteit FEB  entrance hall of Huis van t Sestich  HOGS 0010 reception and closing network drink afterwardsFaculteit FEB  Huis De Munter  HOG 0085 presentation  Accessibility By train 15minute walk from Leuven train station via StadsparkBy car parking spaces on Vlamingenstraat  30m from the intersection with Parkstraat Contact Do you have a question or would you like to have more information Send an email to ekonomikaalumnikuleuvenbe  We face a remarkable digital transformation creating new  opportunities through big data and artificial intelligence AI This is  particularly true for demand forecasting and planning  where the recent surge in data has prompted a paradigm shift  Historically forecasting was dominated by simple statistical  forecasting methods However machine learningbased forecasting  has recently outperformed all sales forecasting competitions with  superior performance Despite these advancements most companies still  rely heavily on simple statistical forecasting techniques httpsappekonomikaalumnibeappeventsjanuari2024aiforforecastinganddemandplanning Discover the experiences and knowledge of three experts on AI for forecasting and demand planning seminarLeuvenEkonomika Alumni</data><data key="annotated_by">manual</data><data key="sub_title">Discover the experiences and knowledge of three experts on AI for forecasting and demand planning</data><data key="subscription_limit">11 Jan 2024</data><data key="target_group">Target audience: anyone interested in forecasting and demand planning</data><data key="start_time">17 Jan 2024 18:30 - 22:00</data><data key="language">English</data><data key="location_detail">Faculty FEB, HOG 00.85  Naamsestraat 69, 3000 Leuven</data><data key="price">€30€45 (see Additional information below)</data><data key="date">17/01/2024</data><data key="full_body"> In this seminar  three experts share their knowledge and  experience using AI in forecasting and demand planning. We showcase the  effectiveness of AI forecasting using tree-based machine learning and  outline the conditions in which these methods surpass popular  statistical approaches. Next  we will explore the essential elements of a  good AI demand/sales forecast beyond mere accuracy  delving into  probability forecasting  explainability  stability  and more. Finally   we provide a guide for achieving demand planning excellence through  objectives  data  metrics  models  and enrichment. We also discuss how  teams can enrich ML-generated forecasts using the famous forecast  value-added framework. These three experts are Arnoud Wellens (Postdoctoral researcher in forecasting  KU Leuven)  Ruben Crevits (Lead data scientist  OMP  PhD in forecasting) and Nicolas Vandeput (Supply chain data scientist  CEO &amp; founder  Author). 18:30 Registration and sandwiches19:00 Seminar21:00 Networking drink Registration Ekonomika Alumni/PICS-members: € 30Non-members: € 45 Registration is possible until Thursday 11 January. Location Faculteit FEB  entrance hall of Huis van 't Sestich  HOGS 00.10 (reception and closing network drink afterwards)Faculteit FEB  Huis De Munter  HOG 00.85 (presentation)  Accessibility By train: 15-minute walk from Leuven train station (via Stadspark)By car: parking spaces on Vlamingenstraat  30m from the intersection with Parkstraat Contact Do you have a question or would you like to have more information? Send an e-mail to ekonomika-alumni@kuleuven.be </data><data key="intro">We face a remarkable digital transformation, creating new  opportunities through big data and artificial intelligence (AI). This is  particularly true for demand forecasting and planning,  where the recent surge in data has prompted a paradigm shift.  Historically, forecasting was dominated by simple statistical  forecasting methods. However, machine learning-based forecasting  has recently outperformed all sales forecasting competitions with  superior performance. Despite these advancements, most companies still  rely heavily on simple statistical forecasting techniques.</data><data key="course_info">seminar-Leuven-Ekonomika Alumni</data><data key="url">https://www.vaia.be/en/courses/ai-for-forecasting-and-demand-planning</data><data key="title">AI for Forecasting and Demand Planning</data><data key="constraints">NA</data><data key="details">https://app.ekonomika-alumni.be/app/events/januari-2024/ai-for-forecasting-and-demand-planning</data></node>
<node id="n2200" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: everyone interested in responisible artifical intelligence</data><data key="data">The Power of Prediction and Moral Responsibility amp Fair Machine Learning how to get there  1800 The Power of Prediction  Moral Responsibility  a lecture by Prof Mayli Mertens University of Antwerp  Faculty of Arts 1830 discussion and QA1845 Fair Machine Learning how to get there  a lecture by prof Toon Calders University of Antwerp  Faculty of Science1915 discussion and QA 1930  networking drinks   ACRAI the Antwerp Center for Responsible AI is a cuttingedge research center dedicated to advancing the field of responsible AI Located in the vibrant city of Antwerp Belgium ACRAI is at the forefront of the rapidly growing field of responsible AI bringing together researchers to tackle some of the most pressing issues related to AIs impact on society ACRAI regularly organises events and workshops httpswwwuantwerpenbeenresearchgroupsantwerpcenterresponsibleainewslectures ACRAI lectures lectureonline amp AntwerpACRAI</data><data key="annotated_by">manual</data><data key="start_time">16 Jan 2024 18:00 - 20:00</data><data key="price">free</data><data key="sub_title">ACRAI lectures</data><data key="subscription_limit">NA</data><data key="intro">ACRAI, the Antwerp Center for Responsible AI, is a cutting-edge research center dedicated to advancing the field of responsible AI. Located in the vibrant city of Antwerp, Belgium, ACRAI is at the forefront of the rapidly growing field of responsible AI, bringing together researchers to tackle some of the most pressing issues related to AI's impact on society. ACRAI regularly organises events and workshops.</data><data key="language">English</data><data key="location_detail">Online &amp;amp; UAntwerp Stadscampus, room S.C002 - Prinsstraat 13, 2000 Antwerp</data><data key="date">16/01/2024</data><data key="details">https://www.uantwerpen.be/en/research-groups/antwerp-center-responsible-ai/news/lectures/</data><data key="full_body"> 18.00: "The Power of Prediction &amp; Moral Responsibility"  a lecture by Prof. Mayli Mertens (University of Antwerp  Faculty of Arts) 18.30: discussion and Q&amp;A18.45: "Fair Machine Learning: how to get there?"  a lecture by prof. Toon Calders (University of Antwerp  Faculty of Science)19.15: discussion and Q&amp;A 19.30 : networking drinks  </data><data key="url">https://www.vaia.be/en/courses/acrai-lectures-january-2024</data><data key="title">The Power of Prediction and Moral Responsibility &amp;amp; Fair Machine Learning: how to get there?</data><data key="constraints">NA</data><data key="course_info">lecture-online &amp;amp; Antwerp-ACRAI</data></node>
<node id="n2201" labels=":Course"><data key="labels">:Course</data><data key="data">Exploiting Sources of Variation in your Data the ANOVA Approach    In this course we will focus on correct execution of data analysis and understanding its results We pay attention to expressing these conclusions in a correct and understandable way                               To emphasize the practical approach in this course all classes will take place in a pc room Analysis of variance ANOVA is a statistical tool used in the comparison of means of a random variable over populations that differ in one or more characteristics factors  eg treatment  age  sex  subject  etc First  we cover oneway ANOVA  where only one factor is of concern Depending on the type of the factor  the conclusions pertain to just those factor levels included in the study fixed factor model  or to a population of factor levels of which we observed a sample random effects model In twoway and multiway ANOVA where populations differ in more than one characteristic  the effects of factors are studied simultaneously This yields information about the main effects of each of the factors as well as about any special joint effects factorial design We also consider nested designs  where each level of a second mostly random factor occurs in conjunction with only one level of the first factor One special challenge in multiway ANOVA lies in verifying the assumptions that must be satisfied In this course we will focus on correct execution of data analysis and understanding its results We pay attention to expressing these conclusions in a correct and understandable way The different methods will be extensively illustrated with examples from scientific studies in a variety of fields Exercises are worked out behind PC using the R software Target audience This course targets professionals and investigators from diverse areas  who need to use statistical methods in the collection and handling of data in their research  in particular for assessing the effect of eg different treatments Fees The participation fee is 1100 EUR for participants from the private sector Reduced prices apply to students and staff from nonprofit  social profit  and government organizations An exam fee of 35 EUR will be applied Industry  private sector  profession € 1100Non profit  government  higher education staff € 825Doctoral students  unemployed € 495 If two or more employees from the same company enrol simultaneously for this course a reduction of 20 on the course fee is taken into account starting from the second enrolment Registration More information and registration on our BetaAcademy website  In this course we will focus on correct execution of data analysis and understanding its results We pay attention to expressing these conclusions in a correct and understandable way httpsbetaacademyugentbeenprogramshortandlongrunninginitiatives202320242024m8anmodule8exploitingsourcesof ANOVA Approach courseGhentUGent</data><data key="annotated_by">manual</data><data key="start_time">9 Jan 2024 - 6 Feb 2024</data><data key="price">€495 - €1100</data><data key="target_group">Target audience: This course targets professionals and investigators from diverse areas, who need to use statistical methods in the collection and handling of data in their research, in particular for assessing the effect of e.g. different treatments.</data><data key="location_detail">Krijgslaan 281, 9000 Gent</data><data key="language">English</data><data key="sub_title">ANOVA Approach</data><data key="date">9/01/2024</data><data key="intro">In this course we will focus on correct execution of data analysis and understanding its results. We pay attention to expressing these conclusions in a correct and understandable way.</data><data key="subscription_limit">NA</data><data key="course_info">course-Ghent-UGent</data><data key="full_body">   In this course we will focus on correct execution of data analysis and understanding its results. We pay attention to expressing these conclusions in a correct and understandable way.                               To emphasize the practical approach in this course all classes will take place in a pc room. Analysis of variance (ANOVA) is a statistical tool used in the comparison of means of a random variable over populations that differ in one or more characteristics (factors)  e.g. treatment  age  sex  subject  etc. First  we cover one-way ANOVA  where only one factor is of concern. Depending on the type of the factor  the conclusions pertain to just those factor levels included in the study (fixed factor model)  or to a population of factor levels of which we observed a sample (random effects model). In two-way and multi-way ANOVA where populations differ in more than one characteristic  the effects of factors are studied simultaneously. This yields information about the main effects of each of the factors as well as about any special joint effects (factorial design). We also consider nested designs  where each level of a second (mostly random) factor occurs in conjunction with only one level of the first factor. One special challenge in multi-way ANOVA lies in verifying the assumptions that must be satisfied. In this course we will focus on correct execution of data analysis and understanding its results. We pay attention to expressing these conclusions in a correct and understandable way. The different methods will be extensively illustrated with examples from scientific studies in a variety of fields. Exercises are worked out behind PC using the R software. Target audience This course targets professionals and investigators from diverse areas  who need to use statistical methods in the collection and handling of data in their research  in particular for assessing the effect of e.g. different treatments. Fees The participation fee is 1100 EUR for participants from the private sector. Reduced prices apply to students and staff from non-profit  social profit  and government organizations. An exam fee of 35 EUR will be applied. Industry  private sector  profession*: € 1100Non profit  government  higher education staff: € 825(Doctoral) students  unemployed: € 495 *If two or more employees from the same company enrol simultaneously for this course a reduction of 20% on the course fee is taken into account starting from the second enrolment. Registration More information and registration on our Beta-Academy website. </data><data key="url">https://www.vaia.be/en/courses/exploiting-sources-of-variation-in-your-data-the-anova-approach</data><data key="title">Exploiting Sources of Variation in your Data: the ANOVA Approach</data><data key="constraints">Participants are expected to have an active knowledge of the basic principles underlying statistical strategies, at a level equivalent to Module 4 of this year's program. Some R skills are advised consistent with the course content of Module 2 of this year's program.</data><data key="details">https://beta-academy.ugent.be/en/program/short-and-long-running-initiatives/2023-2024-2024m8an-module-8-exploiting-sources-of</data></node>
<node id="n2202" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">13 Dec 2023</data><data key="target_group">Target audience: people working in AI (including in academia, policy and industry)</data><data key="data">Symposium on AI and Society    The goal of the symposium is to tighten the link between technical viewpoints on the societal risks of AI and those from industry  law and policy Top experts in Belgium representing these diverse viewpoints will present their work and participate in a panel discussion   Following the symposium  Maarten Buyl will hold his public PhD defense on the topic of AI fairness                               Over the past decade  concerns about the societal risks posed by AI systems have been steadily growing AI systems have already been known to reinforce discriminatory biases  violate privacy  and manipulate public discourse A wealth of technical work has been produced to assess and mitigate those risks  yet it is often based on simplistic assumptions of the role that AI plays in the real world  The goal of this symposium is to tighten the link between technical viewpoints on the societal risks of AI and those from industry  law and policy Hence  top experts in Belgium representing these diverse viewpoints  from within academia and without  will present their work and the challenges they face in addressing the societal risks of AI Afterwards  they will participate in a panel discussion The invited speakers are Toon Calders University of AntwerpJelle Hoedemaekers AgoriaDavid Martens University of AntwerpSofie Serwir Permanent Representation of Belgium to the EUKatrien Verbert KU Leuven Please refer to the website for details on the programme and registration  Following the symposium  Maarten Buyl will hold his public PhD defense on the topic of AI fairness His thesis proposes technical tools to mitigate algorithmic bias and discusses the limitations of technical tools in truly achieving fair decision processes The presentation is aimed at a nonexpert audience  Please note that registration free  but mandatory  Organized by Maarten Buyl  MaryBeth Defrance and Tijl De Bie at Ghent University Supported by the Flanders AI Academy VAIA and the Flanders AI Research Program FAIR  The goal of the symposium is to tighten the link between technical viewpoints on the societal risks of AI and those from industry law and policy Top experts in Belgium representing these diverse viewpoints will present their work and participate in a panel discussion   Following the symposium Maarten Buyl will hold his public PhD defense on the topic of AI fairness httpssitesgooglecomviewaiandsocietyghentoverview Bridging the gap between technical and nontechnical viewpoints on AI and society symposiumGhentUGent VAIA FAIR</data><data key="start_time">22 Dec 2023 09:15 - 14:30</data><data key="location_detail">Aula Academica of Ghent University</data><data key="language">English</data><data key="price">free</data><data key="sub_title">Bridging the gap between technical and non-technical viewpoints on AI and society.</data><data key="date">22/12/2023</data><data key="full_body">   The goal of the symposium is to tighten the link between technical viewpoints on the societal risks of AI and those from industry  law and policy. Top experts in Belgium representing these diverse viewpoints will present their work and participate in a panel discussion.   Following the symposium  Maarten Buyl will hold his public PhD defense on the topic of AI fairness.                               Over the past decade  concerns about the societal risks posed by AI systems have been steadily growing. AI systems have already been known to reinforce discriminatory biases  violate privacy  and manipulate public discourse. A wealth of technical work has been produced to assess and mitigate those risks  yet it is often based on simplistic assumptions of the role that AI plays in the real world.  The goal of this symposium is to tighten the link between technical viewpoints on the societal risks of AI and those from industry  law and policy. Hence  top experts in Belgium representing these diverse viewpoints  from within academia and without  will present their work and the challenges they face in addressing the societal risks of AI. Afterwards  they will participate in a panel discussion. The invited speakers are: Toon Calders (University of Antwerp)Jelle Hoedemaekers (Agoria)David Martens (University of Antwerp)Sofie Serwir (Permanent Representation of Belgium to the EU)Katrien Verbert (KU Leuven) Please refer to the website for details on the programme and registration.  Following the symposium  Maarten Buyl will hold his public PhD defense on the topic of AI fairness. His thesis proposes technical tools to mitigate algorithmic bias and discusses the limitations of technical tools in truly achieving fair decision processes. The presentation is aimed at a non-expert audience.  Please note that registration free  but mandatory.  Organized by Maarten Buyl  MaryBeth Defrance and Tijl De Bie at Ghent University. Supported by the Flanders AI Academy (VAIA) and the Flanders AI Research Program (FAIR). </data><data key="intro">The goal of the symposium is to tighten the link between technical viewpoints on the societal risks of AI and those from industry, law and policy. Top experts in Belgium representing these diverse viewpoints will present their work and participate in a panel discussion.   Following the symposium, Maarten Buyl will hold his public PhD defense on the topic of AI fairness.</data><data key="title">Symposium on AI and Society</data><data key="url">https://www.vaia.be/en/courses/symposium-on-ai-and-society</data><data key="constraints">a strong background in AI (technical, legal or policy)</data><data key="course_info">symposium-Ghent-UGent; VAIA; FAIR</data><data key="details">https://sites.google.com/view/ai-and-society-ghent/overview</data></node>
<node id="n2203" labels=":Course"><data key="labels">:Course</data><data key="data">Dynamic Report Generation with R Markdown and Quarto    R offers many first class features for statistics and data science One of these  is certainly Rmarkdown  that allows seamless integration of analysis code and text This greatly improves reproducibility  reduces copypaste and others errors and enhances possibilities for automation                               R offers many first class features for statistics and data science One of these  is certainly Rmarkdown  that allows seamless integration of analysis code and text This greatly improves reproducibility  reduces copypaste and others errors and enhances possibilities for automation R markdown offers three main types of output pdf  html and docx The first session introduces the basic framework  the outputspecific possibilities  the bookdownextension and the easy and rewarding move to its recent less platform dependent successor Quarto The second session explores some general approaches for automation using selfbuilt templates for reportsections or complete reports and presents Officedown The latter is less flexible than Rmarkdown  but offers more options for docxoutput Target audience This module targets anyone who wants to produce professional reports using R Fees The participation fee is 330 EUR for participants from the private sector Reduced prices apply to students and staff from nonprofit  social profit  and government organizations An exam fee of 35 EUR will be applied Industry  private sector  profession € 330Non profit  government  higher education staff € 250Doctoral students  unemployed € 150 If two or more employees from the same company enrol simultaneously for this course a reduction of 20 on the course fee is taken into account starting from the second enrolment Registration More information and registration on our BetaAcademy website  R offers many first class features for statistics and data science One of these is certainly Rmarkdown that allows seamless integration of analysis code and text This greatly improves reproducibility reduces copypaste and others errors and enhances possibilities for automation httpsbetaacademyugentbeenprogramshortandlongrunninginitiatives202320242023m7rmmodule7dynamicreportgeneration Rmarkdown courseGhentUGent</data><data key="annotated_by">manual</data><data key="start_time">22 Dec 2023 09:00 - 16:00</data><data key="price">€150 - €330</data><data key="target_group">Target audience: This module targets anyone who wants to produce professional reports using R.</data><data key="location_detail">Krijgslaan 281, 9000 Gent</data><data key="language">English</data><data key="sub_title">Rmarkdown</data><data key="date">22/12/2023</data><data key="intro">R offers many first class features for statistics and data science. One of these, is certainly Rmarkdown, that allows seamless integration of analysis (code) and text. This greatly improves reproducibility, reduces copy-paste- and others errors and enhances possibilities for automation.</data><data key="subscription_limit">NA</data><data key="course_info">course-Ghent-UGent</data><data key="full_body">   R offers many first class features for statistics and data science. One of these  is certainly Rmarkdown  that allows seamless integration of analysis (code) and text. This greatly improves reproducibility  reduces copy-paste- and others errors and enhances possibilities for automation.                               R offers many first class features for statistics and data science. One of these  is certainly Rmarkdown  that allows seamless integration of analysis (code) and text. This greatly improves reproducibility  reduces copy-paste- and others errors and enhances possibilities for automation. R markdown offers three main types of output: pdf  html and docx. The first session introduces the basic framework  the output-specific possibilities  the bookdown-extension and the easy and rewarding move to its recent less platform dependent successor Quarto. The second session explores some general approaches for automation (using self-built templates for report-sections or complete reports) and presents Officedown. The latter is less flexible than Rmarkdown  but offers more options for docx-output. Target audience This module targets anyone who wants to produce professional reports using R. Fees The participation fee is 330 EUR for participants from the private sector. Reduced prices apply to students and staff from non-profit  social profit  and government organizations. An exam fee of 35 EUR will be applied. Industry  private sector  profession*: € 330Non profit  government  higher education staff: € 250(Doctoral) students  unemployed: € 150 *If two or more employees from the same company enrol simultaneously for this course a reduction of 20% on the course fee is taken into account starting from the second enrolment. Registration More information and registration on our Beta-Academy website. </data><data key="url">https://www.vaia.be/en/courses/dynamic-report-generation-with-r-markdown-and-quarto</data><data key="title">Dynamic Report Generation with R Markdown and Quarto</data><data key="constraints">Basic knowledge of R as provided in Module 2 of this year's program is required. Knowledge of tidyverse as provided in Module 8 is helpful.</data><data key="details">https://beta-academy.ugent.be/en/program/short-and-long-running-initiatives/2023-2024-2023m7rm-module-7-dynamic-report-generation</data></node>
<node id="n2204" labels=":Course"><data key="labels">:Course</data><data key="data">DutchBelgian DataBase Day DBDBD 2023  At DBDBD 2023  junior and senior researchers from the Netherlands and Belgium can present their recent results  and meet fellow researchers in the field of data management It is an excellent opportunity to meet up with your BelgianDutch colleagues  and to get informed about the recent databaserelated research performed in BelgianDutch universities The workshop welcomes nonBelgianDutch participants presentations are in English DBDBD has a tradition of favouring presentations by junior researchers The workshop consists of oral and poster presentations There are no printed proceedings Abstracts of oral and poster presentations will be published on the DBDBD 2023 website We invite submissions 1 page abstract on a broad range of database and databaserelated topics   including but not limited to data storage and management  theoretical  database issues  database performance  data mining  artificial  intelligence in data management  information retrieval  data semantics   querying  ontologies  etc Based on the submissions  the workshop will be  organized in different sessions  each covering a particular topic See  the Submission Guidelines on our website for further details  More info  Share this course   httpsdbdbd2023ugentbe 21 Dec 2023 0900  1700 oneday workshopGhentDDCM UGent VAIA</data><data key="annotated_by">manual</data><data key="sub_title">21 Dec 2023 09:00 - 17:00</data><data key="start_time">21 Dec 2023 09:00 - 17:00</data><data key="price">€ 40 for PhD students, €50 for all other participants</data><data key="subscription_limit">15 Dec 2023</data><data key="target_group">Target audience: Database researchers and enthusiasts</data><data key="intro">More info  Share this course  </data><data key="details">https://dbdbd2023.ugent.be/</data><data key="date">21/12/2023</data><data key="language">English</data><data key="constraints">an interest in databases</data><data key="location_detail">Sint-Pietersnieuwstraat 41, 9000 Ghent</data><data key="url">https://www.vaia.be/en/courses/dutch-belgian-database-day-dbdbd-2023</data><data key="title">Dutch-Belgian DataBase Day (DBDBD) 2023</data><data key="course_info">one-day workshop-Ghent-DDCM; UGent; VAIA</data><data key="full_body"> At DBDBD 2023  junior and senior researchers from the Netherlands and Belgium can present their recent results  and meet fellow researchers in the field of data management. It is an excellent opportunity to meet up with your Belgian/Dutch colleagues  and to get informed about the (recent) database-related research performed in Belgian/Dutch universities. The workshop welcomes non-Belgian/Dutch participants (presentations are in English). DBDBD has a tradition of favouring presentations by junior researchers. The workshop consists of oral and poster presentations. There are no printed proceedings. Abstracts of oral and poster presentations will be published on the DBDBD 2023 website. We invite submissions (1 page abstract) on a broad range of database and database-related topics   including but not limited to data storage and management  theoretical  database issues  database performance  data mining  artificial  intelligence in data management  information retrieval  data semantics   querying  ontologies  etc. Based on the submissions  the workshop will be  organized in different sessions  each covering a particular topic. See  the Submission Guidelines on our website for further details. </data></node>
<node id="n2205" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="start_time">18 Dec 2023 - 21 Dec 2023</data><data key="price">€225 - €495</data><data key="data">Leverage your R Skills Data Wrangling amp Plotting with Tidyverse    Data wrangling and visualization challenges  using the best parts of R tidyverse                               Tidyverse is a collection of Rpackages used for data wrangling and visualization that share a common design philosophy The goal of this course is to get you up to speed with the most uptodate and essential tidyverse tools for data exploration After attending this course  you'll have the tools to tackle a wide variety of data wrangling and visualization challenges  using the best parts of R tidyverse This course covers the most essential tools from 3 main R tidyverse packages that are frequently used in general data analysis procedure Lectures with R code demonstrations are blended with handson exercises which allows you to try out the tools you've seen in the class under guides What you will learn Data transforming and summarizing with dplyr narrowing in on observations of interest  creating new variables that are functions of existing variables  and calculating a set of summary statistics like counts or meansData visualization with ggplot2 creating more informative graphs eg  scatter plot  bar plot  histogram  smootherregression line  … in an elegant and efficient way Arranging multiple plots on a gridData ingest and tidying with tidyr storing it in a consistent form that matches the semantics of the dataset with the way it is storedExtra tools for programming Merging and comparing two datasets based on various matching or filtering criterion Other useful tools for R programming Not included in this course A systematic training guide in basics of R If you never used R or RStudio before  we highly recommend you to take Module 2 of this years program which will guide you to be familiar with the R environment for the implementation of data management and exploration tasksBig data This course focuses on small  inmemory datasets as you can't tackle big data easily unless you have experience with small dataStatistics Although you will see many basic statistics in this course  the main focus is on R and the tidyverse tools instead of explaining the statistical concepts Target audience This course targets anyone who wants to use R for data processing and needs to produce professional looking graphs andor summary statistics Fees The participation fee is 495 EUR for participants from the private sector Reduced prices apply to students and staff from nonprofit  social profit  and government organizations An exam fee of 35 EUR will be applied Industry  private sector  profession € 495Non profit  government  higher education staff € 370Doctoral students  unemployed € 225 If two or more employees from the same company enrol simultaneously for this course a reduction of 20 on the course fee is taken into account starting from the second enrolment Registration More information and registration on our BetaAcademy website  Data wrangling and visualization challenges using the best parts of R tidyverse httpsbetaacademyugentbeenprogramshortandlongrunninginitiatives202320242023m6rsmodule6leverageyourrskillsdata Data wrangling and visualization challenges using the best parts of R tidyverse courseGhentUGent</data><data key="location_detail">Krijgslaan 281, 9000 Gent</data><data key="language">English</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: This course targets anyone who wants to use R for data processing and needs to produce professional looking graphs and/or summary statistics.</data><data key="date">18/12/2023</data><data key="intro">Data wrangling and visualization challenges, using the best parts of R tidyverse.</data><data key="sub_title">Data wrangling and visualization challenges, using the best parts of R tidyverse.</data><data key="course_info">course-Ghent-UGent</data><data key="full_body">   Data wrangling and visualization challenges  using the best parts of R tidyverse.                               Tidyverse is a collection of R-packages used for data wrangling and visualization that share a common design philosophy. The goal of this course is to get you up to speed with the most up-to-date and essential tidyverse tools for data exploration. After attending this course  you'll have the tools to tackle a wide variety of data wrangling and visualization challenges  using the best parts of R tidyverse. This course covers the most essential tools from 3 main R tidyverse packages that are frequently used in general data analysis procedure. Lectures with R code demonstrations are blended with hands-on exercises which allows you to try out the tools you've seen in the class under guides. What you will learn: Data transforming and summarizing with dplyr: narrowing in on observations of interest  creating new variables that are functions of existing variables  and calculating a set of summary statistics (like counts or means)Data visualization with ggplot2: creating more informative graphs (e.g.  scatter plot  bar plot  histogram  smoother/regression line  …) in an elegant and efficient way. Arranging multiple plots on a gridData ingest and tidying with tidyr: storing it in a consistent form that matches the semantics of the dataset with the way it is stored.Extra tools for programming: Merging and comparing two datasets based on various matching or filtering criterion. Other useful tools for R programming. Not included in this course: A systematic training guide in basics of R. If you never used R or RStudio before  we highly recommend you to take Module 2 of this year's program which will guide you to be familiar with the R environment for the implementation of data management and exploration tasks.Big data. This course focuses on small  in-memory datasets as you can't tackle big data easily unless you have experience with small data.Statistics. Although you will see many basic statistics in this course  the main focus is on R and the tidyverse tools instead of explaining the statistical concepts. Target audience This course targets anyone who wants to use R for data processing and needs to produce professional looking graphs and/or summary statistics. Fees The participation fee is 495 EUR for participants from the private sector. Reduced prices apply to students and staff from non-profit  social profit  and government organizations. An exam fee of 35 EUR will be applied. Industry  private sector  profession*: € 495Non profit  government  higher education staff: € 370(Doctoral) students  unemployed: € 225 *If two or more employees from the same company enrol simultaneously for this course a reduction of 20% on the course fee is taken into account starting from the second enrolment. Registration More information and registration on our Beta-Academy website. </data><data key="url">https://www.vaia.be/en/courses/leverage-your-r-skills-data-wrangling-plotting-with-tidyverse</data><data key="title">Leverage your R Skills: Data Wrangling &amp;amp; Plotting with Tidyverse</data><data key="constraints">The course is open to all interested persons. Basic R skills as provided in Module 2 of this year's program are strongly advised.</data><data key="details">https://beta-academy.ugent.be/en/program/short-and-long-running-initiatives/2023-2024-2023m6rs-module-6-leverage-your-r-skills-data</data></node>
<node id="n2206" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: Everyone interested in the use, development, or the impact of the use of recommender systems</data><data key="data">Recommender Systems DBWRS 2023    DBWRS is the first DutchBelgian Workshop on Recommender Systems In todays digital age  where information overload and an abundance of choices are the norm  recommender systems play a vital role in helping users navigate through vast amounts of data These systems have become ubiquitous across various domains  including ecommerce  social media  entertainment  and many more  shaping the way we discover new products  engage with content  and connect with others                               DBWRS will bring together researchers and practitioners from diverse fields to explore and discuss the latest advancements  challenges  and opportunities in this dynamic field By fostering collaboration and exchange of ideas among experts in computer science  data science  artificial intelligence  psychology  communication  and beyond  this workshop aims to unlock new frontiers and push the boundaries of recommender systems We invite you to join us in this exciting journey as we explore the interdisciplinary nature of recommender systems and work together to uncover innovative approaches that enhance user experiences  foster personalized decisionmaking  and contribute to the everevolving landscape of digital recommendations DBWRS is a twoday event filled with inspiring talks and numerous networking opportunities Attendees are warmly invited to submit their own work to bring to the stage There will be opportunities to discuss published work as well as work in progress DBWRS will not publish proceedings More details on the submission process will be announced soon  DBWRS is the first DutchBelgian Workshop on Recommender Systems In todays digital age where information overload and an abundance of choices are the norm recommender systems play a vital role in helping users navigate through vast amounts of data These systems have become ubiquitous across various domains including ecommerce social media entertainment and many more shaping the way we discover new products engage with content and connect with others httpssitesgooglecomviewdbwrs2023home RecSys crossing boundaries WorkshopAntwerpenVAIA amp Doctoral Schools of UAntwerpen amp VUB</data><data key="sub_title">RecSys crossing boundaries</data><data key="start_time">14 Dec 2023 - 15 Dec 2023</data><data key="language">English</data><data key="location_detail">Zaha Hadidplein 1, 2030 Antwerp</data><data key="price">Tbd</data><data key="date">14/12/2023</data><data key="full_body">   DBWRS is the first Dutch-Belgian Workshop on Recommender Systems. In today's digital age  where information overload and an abundance of choices are the norm  recommender systems play a vital role in helping users navigate through vast amounts of data. These systems have become ubiquitous across various domains  including e-commerce  social media  entertainment  and many more  shaping the way we discover new products  engage with content  and connect with others.                               DBWRS will bring together researchers and practitioners from diverse fields to explore and discuss the latest advancements  challenges  and opportunities in this dynamic field. By fostering collaboration and exchange of ideas among experts in computer science  data science  artificial intelligence  psychology  communication  and beyond  this workshop aims to unlock new frontiers and push the boundaries of recommender systems. We invite you to join us in this exciting journey as we explore the interdisciplinary nature of recommender systems and work together to uncover innovative approaches that enhance user experiences  foster personalized decision-making  and contribute to the ever-evolving landscape of digital recommendations. DBWRS is a two-day event filled with inspiring talks and numerous networking opportunities. Attendees are warmly invited to submit their own work to bring to the stage. There will be opportunities to discuss published work as well as work in progress. DBWRS will not publish proceedings. More details on the submission process will be announced soon. </data><data key="intro">DBWRS is the first Dutch-Belgian Workshop on Recommender Systems. In today's digital age, where information overload and an abundance of choices are the norm, recommender systems play a vital role in helping users navigate through vast amounts of data. These systems have become ubiquitous across various domains, including e-commerce, social media, entertainment, and many more, shaping the way we discover new products, engage with content, and connect with others.</data><data key="constraints">An interest in recommender systems</data><data key="url">https://www.vaia.be/en/courses/dutch-belgian-workshop-on-recommender-systems-dbwrs-2023</data><data key="title">Recommender Systems (DBWRS) 2023</data><data key="course_info">Workshop-Antwerpen-VAIA &amp;amp; Doctoral Schools of UAntwerpen &amp;amp; VUB</data><data key="details">https://sites.google.com/view/dbwrs2023/home</data></node>
<node id="n2207" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: higher education lecturers</data><data key="data">ChatGPT and Co in higher education to be cheered or feared    Artificial Intelligence takes many forms One of those of widest  relevance for our universities is the passionately discussed chatbot  application ChatGPT  launched in November 2022 by OpenAI In a matter of  seconds  it can produce texts that sound like wellinformed answers to  any possible question  react to users' objections and summarize long  texts Its potential is evident in many fields  including all aspects of  university life    But it also constitutes  along with related AI applications  a  formidable challenge  not least for the way we teach and evaluate our  students The texts ChatGPT generates  whether concise or richly  developed  often seem to provide students with all they need to perform  many tasks currently assigned to them as homework  tests or exams  Should we regulate  restrict or even forbid their use of ChatGPT    Should we on the contrary encourage our students to use it and  radically redesign how we evaluate them Should we even rethink in depth  what we need to teach them why bother trying to teach them to write  texts in their own and other languages if ChatGPT can do that far better  for them Should we concentrate instead on teaching our students  critical thinking skills    Or should we rather be primarily worried about the unwarranted trust  which our students will unavoidably nourish for ChatGPT's answers  or  about the many biases that affect unwittingly the information that is  reaching them  or about the huge ecological cost of the worldwide  operation of AI applications  Organizing committee  P Van Parijs coordinator  JM Chaumont  E De Keuleneer  MC de Marneffe  B De Munck   H Garmyn  P Goethals  JP Lambert  Q Michel  B Seghers  D Willems  J Willems  The potential of artificial intelligence applications like ChatGPT is evident in many fields including all aspects of university life but it also constitutes a formidable challenge not least for the way we teach and evaluate our students Should we encourage our students to use it and radically redesign how we evaluate them Or should we rather be worried about the unwarranted trust in it or about the many biases that affect the information that is reaching them httpswwwethicalforumbe Should academics fear or cheer ChatGPT and its cognates seminarBrusselsKU Leuven UGent UCLL Ethical Forum</data><data key="start_time">7 Dec 2023 14:00 - 18:00</data><data key="language">English</data><data key="price">free</data><data key="sub_title">Should academics fear or cheer ChatGPT and its cognates?</data><data key="details">https://www.ethicalforum.be/</data><data key="location_detail">Rue d'Egmont 11, 1000 Brussels</data><data key="date">7/12/2023</data><data key="full_body">   Artificial Intelligence takes many forms. One of those of widest  relevance for our universities is the passionately discussed chatbot  application ChatGPT  launched in November 2022 by OpenAI. In a matter of  seconds  it can produce texts that sound like well-informed answers to  any possible question  react to users objections and summarize long  texts. Its potential is evident in many fields  including all aspects of  university life.    But it also constitutes  along with related AI applications  a  formidable challenge  not least for the way we teach and evaluate our  students. The texts ChatGPT generates  whether concise or richly  developed  often seem to provide students with all they need to perform  many tasks currently assigned to them as homework  tests or exams.  Should we regulate  restrict or even forbid their use of ChatGPT?    Should we on the contrary encourage our students to use it and  radically redesign how we evaluate them? Should we even rethink in depth  what we need to teach them: why bother trying to teach them to write  texts in their own and other languages if ChatGPT can do that far better  for them? Should we concentrate instead on teaching our students  critical thinking skills?    Or should we rather be primarily worried about the unwarranted trust  which our students will unavoidably nourish for ChatGPT's answers  or  about the many biases that affect unwittingly the information that is  reaching them  or about the huge ecological cost of the worldwide  operation of AI applications?  Organizing committee  P. Van Parijs (coordinator)  J.M. Chaumont  E. De Keuleneer  M.C. de Marneffe  B. De Munck   H. Garmyn  P. Goethals  J.P. Lambert  Q. Michel  B. Seghers  D. Willems  J. Willems. </data><data key="intro">The potential of artificial intelligence applications like ChatGPT is evident in many fields, including all aspects of university life, but it also constitutes, a formidable challenge, not least for the way we teach and evaluate our students. Should we encourage our students to use it and radically redesign how we evaluate them? Or should we rather be worried about the unwarranted trust in it, or about the many biases that affect the information that is reaching them?</data><data key="url">https://www.vaia.be/en/courses/chatgpt-and-co-in-higher-education-to-be-cheered-or-feared</data><data key="title">ChatGPT and Co in higher education: to be cheered or feared?</data><data key="constraints">NA</data><data key="course_info">seminar-Brussels-KU Leuven; UGent; UCLL; Ethical Forum</data></node>
<node id="n2208" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: professionals and investigators from diverse areas with little to no Python-programming experience who wish to start using Python for their data manipulation, data exploration or statistical analysis.</data><data key="data">Getting Started with Python for Data Scientists    Data management  visualization and analysis in Python                               Description Python started off as a generalpurpose programming language  but in the last decade it has become a popular environment for data science The reason is that the community of Python users have recently created useful addon packages which are suitable for data manipulation  preparation  visualization and analysis This practical course introduces both base Python and the most important packages in a handson way with many exercises The contents of the course are Introduction Python and the Anaconda distributionData types numbers  strings  lists  tuples  sets and dictionariesAutomation control flow and selfdefined functionsImporting data and exporting resultsManaging data with NumPy and pandasGraphs with matplotlib and seabornStatistical analysis with statsmodelsThe objective of the course is that you are capable of doing data management  visualization and analysis in Python on your own Python is an opensource programming language which you can freely download ie the Anaconda distribution Python version 3 or higher is recommended Target audience This course targets professionals and investigators from diverse areas with little to no Pythonprogramming experience who wish to start using Python for their data manipulation  data exploration or statistical analysis Fees The participation fee is 1000 EUR for participants from the private sector Reduced prices apply to students and staff from nonprofit  social profit  and government organizations Industry  private sector  profession € 1000Non profit  government  higher education staff € 750Doctoral students  unemployed € 450If two or more employees from the same company enrol simultaneously for this course a reduction of 20 on the course fee is taken into account starting from the second enrolment Registration More information and registration on our BetaAcademy website  More info  Share this course   httpsbetaacademyugentbeenprogramshortandlongrunninginitiatives202320242023m5pymodule5gettingstartedwithpython 4 Dec 2023  18 Dec 2023 courseGhentUGent</data><data key="annotated_by">manual</data><data key="sub_title">4 Dec 2023 - 18 Dec 2023</data><data key="start_time">4 Dec 2023 - 18 Dec 2023</data><data key="price">€400 - €1000</data><data key="subscription_limit">NA</data><data key="location_detail">Krijgslaan 281, 9000 Gent</data><data key="intro">More info  Share this course  </data><data key="date">4/12/2023</data><data key="language">English</data><data key="course_info">course-Ghent-UGent</data><data key="full_body">   Data management  visualization and analysis in Python.                               Description Python started off as a general-purpose programming language  but in the last decade it has become a popular environment for data science. The reason is that the community of Python users have recently created useful add-on packages which are suitable for data manipulation  preparation  visualization and analysis. This practical course introduces both base Python and the most important packages in a hands-on way with many exercises. The contents of the course are: Introduction: Python and the Anaconda distributionData types: numbers  strings  lists  tuples  sets and dictionariesAutomation: control flow and self-defined functionsImporting data and exporting resultsManaging data with NumPy and pandasGraphs with matplotlib and seabornStatistical analysis with statsmodelsThe objective of the course is that you are capable of doing data management  visualization and analysis in Python on your own. Python is an open-source programming language which you can freely download (i.e. the Anaconda distribution). Python version 3 or higher is recommended. Target audience This course targets professionals and investigators from diverse areas with little to no Python-programming experience who wish to start using Python for their data manipulation  data exploration or statistical analysis. Fees The participation fee is 1000 EUR for participants from the private sector. Reduced prices apply to students and staff from non-profit  social profit  and government organizations. Industry  private sector  profession*: € 1000Non profit  government  higher education staff: € 750(Doctoral) students  unemployed: € 450*If two or more employees from the same company enrol simultaneously for this course a reduction of 20% on the course fee is taken into account starting from the second enrolment. Registration More information and registration on our Beta-Academy website. </data><data key="url">https://www.vaia.be/en/courses/getting-started-with-python-for-data-scientists</data><data key="title">Getting Started with Python for Data Scientists</data><data key="constraints">The course is open to all interested persons. Knowledge of basic statistical concepts and experience with other programming languages are considered advantages, but not required for learning the Python language.</data><data key="details">https://beta-academy.ugent.be/en/program/short-and-long-running-initiatives/2023-2024-2023m5py-module-5-getting-started-with-python</data></node>
<node id="n2209" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="sub_title">A workshop part of the 'Back to the Roots' seminar series</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: researchers and academics with an interest in advancing the theoretical underpinning of AI algorithms</data><data key="data">Back to the numerical linear algebra roots of polynomials and nonlinear eigenvalue problems  12001245 Informal walking lunch in ESAT B0035 12451330 Presentation by prof dr Fatemeh Mohammadi “Polynomial systems arising in the formal verification of programs” 13301415 Presentation by prof dr Bor Plestenjak “Numerical methods for rectangular multiparameter eigenvalue problems” 14151430 Coffee break 14301515 Presentation by prof dr Karl Meerbergen “Linearizations for NEPv nonlinear eigenvalue problems with eigenvector nonlinearity” 15151600 Presentation by prof dr Bernard Mourrain “Linear algebra for nonlinear problems”  This event features presentations of four experts in algebraic geometry and numerical linear algebra delving into topics such as multivariate polynomials and nonlinearmultiparameter eigenvalue problems  httpshomesesatkuleuvenbesistawwwbdmbacktotherootsindexphpseminars A workshop part of the Back to the Roots seminar series workshopLeuvenKU Leuven ESAT</data><data key="start_time">1 Dec 2023 12:00 - 16:00</data><data key="language">English</data><data key="location_detail">ESAT B91.100 (Dept. of Electrical Engineering - Kasteelpark Arenberg 10, 3001 Leuven)</data><data key="price">free</data><data key="date">1/12/2023</data><data key="full_body"> 12:00-12:45 Informal walking lunch (in ESAT B00.35) 12:45-13:30 Presentation by prof. dr. Fatemeh Mohammadi “Polynomial systems arising in the formal verification of programs” 13:30-14:15 Presentation by prof. dr. Bor Plestenjak “Numerical methods for rectangular multiparameter eigenvalue problems” 14:15-14:30 Coffee break 14:30-15:15 Presentation by prof. dr. Karl Meerbergen “Linearizations for NEPv: nonlinear eigenvalue problems with eigenvector nonlinearity” 15:15-16:00 Presentation by prof. dr. Bernard Mourrain “Linear algebra for non-linear problems” </data><data key="intro">This event features presentations of four experts in algebraic geometry and numerical linear algebra, delving into topics such as multivariate polynomials and nonlinear/multiparameter eigenvalue problems. </data><data key="course_info">workshop-Leuven-KU Leuven ESAT</data><data key="url">https://www.vaia.be/en/courses/back-to-the-numerical-linear-algebra-roots-of-polynomials-and-nonlinear-eigenvalue-problems</data><data key="title">Back to the numerical linear algebra roots of polynomials and nonlinear eigenvalue problems</data><data key="constraints">foundational understanding of artificial intelligence, numerical optimization, and mathematical concepts</data><data key="details">https://homes.esat.kuleuven.be/~sistawww/bdm/backtotheroots/index.php/seminars</data></node>
<node id="n2210" labels=":Course"><data key="labels">:Course</data><data key="annotated_by">manual</data><data key="subscription_limit">NA</data><data key="target_group">Target audience: Belgian doctoral students in Systems, Optimization, Control and Networks</data><data key="data">Mixed feedback systems  Eventbased technology is developing at a fast pace eg via  neuromorphic computing and eventbased cameras but we currently lack a  control theory of eventbased systems The general aim is to conceive  physical machines that combine the reliability of discrete automata and  the robustness and adaptation of physical control systems The theory of mixed feedback systems aims at leveraging the existing  theory of control  optimization  and learning while acknowledging the  inherent outofequilibrium nature of eventbased machines The theory is grounded in the operatortheoretic framework of maximal  monotonicity in Reproducing Kernel Hilbert Spaces The course will  introduce those mathematical tools at a basic level and present how they  can be used to analyze and design neuromorphic eventbased machines The course consists of six sessions over three days Each session will cover one of the following modules Module1 Feedback and memory RS 11 Examples of mixed feedback systems Motivation Current  limitations of control theory  machine learning  and neuromorphic  engineering 12 Static mixed feedback 13 Dynamic mixed feedback Questions and challenges Module 2 Feedback system analysis TC 21 Systems as operators Phase and Gain Monotonicity and Lipschitz operators The passivity and the smallgain theorems 22 Graphical representations Nyquist plots Scaled Relative Graphs 23 Mixed feedback systems Mixed gain and mixed phase feedback systems Module 3 Feedback algorithms TC 31 Fixed point algorithms and zero finding algorithms 32 Splitting algorithms and circuit representations 33 Algorithmic solutions of mixed feedback systems Module 4 Modelling at scale RS 41 Reproducing Kernel Hilbert Spaces 42 Gradient systems in Reproducing Kernel Hilbert Spaces 43 Feedback neural networks at scale Module 5 Tuning at scale RS 51 Recursive least square estimation 52 A robust internal model principle for mixed feedback systems 53 Adaptive control and online learning at scale Module 6 Neuromorphic control RS 61 Physical Spiking Neural Networks 62 Control as neuromodulation at scale 63 Neuromorphic eventbased machines Course material  Slides  exercises  and references will be available  Evaluation  An assignment that will be released during the course  The SOCN Graduate School gathers Belgian teams UAntwerp  UNamur  KUL  UCLouvain  UGent  ULB  ULg  UMONS  VUB active in the area of Systems  Optimization  Control and Networks  or have strong interest in topics related to it The main objective of the Graduate School is to provide a high quality scientific environment and a wellbalanced graduatelevel program for the Belgian doctoral students in Systems  Optimization  Control and Networks A common research theme pursued by the participating teams consists in the modelling of certain systems and phenomena with the purpose of controlling or optimizing their behavior and performance These models are based on dynamical systems  differential or difference equations and mathematical programming formulations Research of the participating teams focusses on the identification  analysis  control and optimization of these models  including the corresponding theory  algorithmic aspects and numerical resolution techniques This area of growing interest finds numerous applications in various fields such as automatic control  biotechnology and biomedical engineering  mechanical and electrical engineering  production and supply chain management and planning  process engineering  robotics  routing  traffic or service networks and signal processing  Todays lecturers of this SOCN seminar are Rodolphe Sepulchre KU Leuven  University of CambridgeTom Chaffey University of Cambridge httpssitesuclouvainbesocndrupalsocnnode363 A seminar out of the SOCN seminar series on systems optimization control and networks seminarLeuvenSOCN Graduate School</data><data key="start_time">29 Nov 2023 - 1 Dec 2023</data><data key="location_detail">KU Leuven</data><data key="price">free</data><data key="sub_title">A seminar out of the SOCN seminar series on systems, optimization, control and networks</data><data key="full_body"> Event-based technology is developing at a fast pace (e.g. via  neuromorphic computing and event-based cameras) but we currently lack a  control theory of event-based systems. The general aim is to conceive  physical machines that combine the reliability of discrete automata and  the robustness and adaptation of physical control systems. The theory of mixed feedback systems aims at leveraging the existing  theory of control  optimization  and learning while acknowledging the  inherent out-of-equilibrium nature of event-based machines. The theory is grounded in the operator-theoretic framework of maximal  monotonicity in Reproducing Kernel Hilbert Spaces. The course will  introduce those mathematical tools at a basic level and present how they  can be used to analyze and design neuromorphic event-based machines. The course consists of six sessions over three days. Each session will cover one of the following modules Module1: Feedback and memory (RS) 1.1. Examples of mixed feedback systems. Motivation. Current  limitations of control theory  machine learning  and neuromorphic  engineering. 1.2. Static mixed feedback 1.3. Dynamic mixed feedback. Questions and challenges Module 2: Feedback system analysis (TC) 2.1. Systems as operators. Phase and Gain. Monotonicity and Lipschitz operators. The passivity and the small-gain theorems. 2.2. Graphical representations. Nyquist plots. Scaled Relative Graphs. 2.3. Mixed feedback systems. Mixed gain and mixed phase feedback systems. Module 3: Feedback algorithms (TC) 3.1. Fixed point algorithms and zero finding algorithms. 3.2. Splitting algorithms and circuit representations 3.3. Algorithmic solutions of mixed feedback systems Module 4: Modelling at scale (RS) 4.1. Reproducing Kernel Hilbert Spaces 4.2. Gradient systems in Reproducing Kernel Hilbert Spaces 4.3. Feedback neural networks at scale Module 5: Tuning at scale (RS) 5.1. Recursive least square estimation 5.2. A robust internal model principle for mixed feedback systems 5.3 Adaptive control and online learning at scale Module 6: Neuromorphic control (RS) 6.1. Physical Spiking Neural Networks 6.2. Control as neuromodulation at scale 6.3. Neuromorphic event-based machines Course material  Slides  exercises  and references will be available  Evaluation  An assignment that will be released during the course  The SOCN Graduate School gathers Belgian teams (UAntwerp  UNamur  KUL  UCLouvain  UGent  ULB  ULg  UMONS  VUB) active in the area of Systems  Optimization  Control and Networks  or have strong interest in topics related to it. The main objective of the Graduate School is to provide a high quality scientific environment and a well-balanced graduate-level program for the Belgian doctoral students in Systems  Optimization  Control and Networks. A common research theme pursued by the participating teams consists in the modelling of certain systems and phenomena with the purpose of controlling or optimizing their behavior and performance. These models are based on dynamical systems  differential (or difference) equations and mathematical programming formulations. Research of the participating teams focusses on the identification  analysis  control and optimization of these models  including the corresponding theory  algorithmic aspects and numerical resolution techniques. This area of growing interest finds numerous applications in various fields such as automatic control  biotechnology and biomedical engineering  mechanical and electrical engineering  production and supply chain management and planning  process engineering  robotics  routing  traffic or service networks and signal processing. </data><data key="intro">Today's lecturers of this SOCN seminar are: Rodolphe Sepulchre, KU Leuven &amp; University of CambridgeTom Chaffey, University of Cambridge</data><data key="language">English</data><data key="course_info">seminar-Leuven-SOCN Graduate School</data><data key="constraints">NA</data><data key="date">29/11/2023</data><data key="details">https://sites.uclouvain.be/socn/drupal/socn/node/363</data><data key="url">https://www.vaia.be/en/courses/mixed-feedback-systems</data><data key="title">Mixed feedback systems</data></node>
<node id="n2211" labels=":Course"><data key="labels">:Course</data><data key="target_group">Target audience: Geo-ICT, Theme courses</data><data key="data">Artificial Intelligence for Geodata  These days  you cant escape the terms artificial intelligence  machine learning  deep learning  computer vision  NLP  data science   But what do these terms mean exactly and how can these techniques be applied to geodata to solve your spatial problems How do you determine if AI is the right solution for your problem Get started during this oneday session that teaches you the concepts  the tools  the typical project workflow and the pitfalls In this course you will learn The basic concepts  tools  development process and criteria that will  allow you to evaluate how GeoAI can help solve your spatial problems Topics Concepts of artificial intelligence  machine learning  GeoAITaxonomy of AI techniquesBasic concepts such as supervised  unsupervised and reinforcement learning methodsMachine learning tools frameworks  applications and servercloud infrastructureDevelopment and evaluation of processesMachine learning canvas to evaluate the feasibility of your project and plan the projectGeoAI use cases in areas such as computer vision  planning  prediction and pattern recognitionGeoAI in practice using an extensive set of use cases  These days you cant escape the terms artificial intelligence machine learning deep learning computer vision NLP data science  But what do these terms mean exactly and how can these techniques be applied to geodata to solve your spatial problems How do you determine if AI is the right solution for your problem Get started during this oneday session that teaches you the concepts the tools the typical project workflow and the pitfalls httpswwwgimbenltrainingartificieleintelligentievoorgeodata Geodata science machine learning artificial intelligence courseLeuvenGeographic Information Management</data><data key="annotated_by">manual</data><data key="start_time">28 Nov 2023 09:00 - 16:30</data><data key="price">€ 520.00 excl. 21% VAT</data><data key="location_detail">UBIcenter D, Philipssite 5 bus 27, 3001 Leuven</data><data key="sub_title">Geodata science, machine learning, artificial intelligence</data><data key="subscription_limit">28 Nov 2023</data><data key="full_body"> These days  you can't escape the terms artificial intelligence  machine learning  deep learning  computer vision  NLP  data science  ... But what do these terms mean exactly and how can these techniques be applied to geodata to solve your spatial problems? How do you determine if AI is the right solution for your problem? Get started during this one-day session that teaches you the concepts  the tools  the typical project workflow and the pitfalls. In this course you will learn: The basic concepts  tools  development process and criteria that will  allow you to evaluate how GeoAI can help solve your spatial problems. Topics: Concepts of artificial intelligence  machine learning  GeoAI.Taxonomy of AI techniques.Basic concepts such as supervised  unsupervised and reinforcement learning methods.Machine learning tools: frameworks  applications and server/cloud infrastructure.Development and evaluation of processes.Machine learning canvas to evaluate the feasibility of your project and plan the project.GeoAI use cases in areas such as computer vision  planning  prediction and pattern recognition.GeoAI in practice using an extensive set of use cases. </data><data key="intro">These days, you can't escape the terms artificial intelligence, machine learning, deep learning, computer vision, NLP, data science, ... But what do these terms mean exactly and how can these techniques be applied to geodata to solve your spatial problems? How do you determine if AI is the right solution for your problem? Get started during this one-day session that teaches you the concepts, the tools, the typical project workflow and the pitfalls.</data><data key="language">English</data><data key="course_info">course-Leuven-Geographic Information Management</data><data key="date">28/11/2023</data><data key="details">https://www.gim.be/nl/training/artificiele-intelligentie-voor-geodata</data><data key="title">Artificial Intelligence for Geodata</data><data key="url">https://www.vaia.be/en/courses/artificial-intelligence-for-geodata-november2023</data><data key="constraints">Basic knowledge of GIS</data></node>
<node id="n2212" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">understand innovation</data></node>
<node id="n2213" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">boosting operational efficiency</data></node>
<node id="n2214" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">workshop</data></node>
<node id="n2215" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">facilitation methods</data></node>
<node id="n2216" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">eigenvalue</data></node>
<node id="n2217" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">generalizations</data></node>
<node id="n2218" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">weyman complexes</data></node>
<node id="n2219" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">dixon matrices</data></node>
<node id="n2220" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">training data</data></node>
<node id="n2221" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">regulations on data</data></node>
<node id="n2222" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">european regulations</data></node>
<node id="n2223" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">eu data law</data></node>
<node id="n2224" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">data act</data></node>
<node id="n2225" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">ai act</data></node>
<node id="n2226" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">compliance</data></node>
<node id="n2227" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">technical skills</data></node>
<node id="n2228" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">microsoft symphonyai</data></node>
<node id="n2229" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">kernel machines</data></node>
<node id="n2230" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">transformers</data></node>
<node id="n2231" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">scalability learning</data></node>
<node id="n2232" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">generalisation aspects</data></node>
<node id="n2233" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">overparameterised models</data></node>
<node id="n2234" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">attention mechanisms</data></node>
<node id="n2235" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">double descent phenomenon</data></node>
<node id="n2236" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">neural tangent kernel</data></node>
<node id="n2237" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">gaussian processes</data></node>
<node id="n2238" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">least squares support vector machines</data></node>
<node id="n2239" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">deep kernel machines</data></node>
<node id="n2240" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">deep eigenvalues</data></node>
<node id="n2241" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">deep eigenvectors</data></node>
<node id="n2242" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">restricted boltzmann machines</data></node>
<node id="n2243" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">restricted kernel machines</data></node>
<node id="n2244" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">tensors kernels</data></node>
<node id="n2245" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">convolutional kernels</data></node>
<node id="n2246" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">artificial neural networks</data></node>
<node id="n2247" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">deep neural networks</data></node>
<node id="n2248" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">image processing</data></node>
<node id="n2249" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">regulatory compliance</data></node>
<node id="n2250" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">data science�</data></node>
<node id="n2251" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">predicting outcomes</data></node>
<node id="n2252" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">verification</data></node>
<node id="n2253" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">ai models</data></node>
<node id="n2254" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">conformity assessments</data></node>
<node id="n2255" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">�privacy by design</data></node>
<node id="n2256" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">edge devices</data></node>
<node id="n2257" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">annotation tasks</data></node>
<node id="n2258" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">torchictab</data></node>
<node id="n2259" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">knowledge graph</data></node>
<node id="n2260" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">annotated tables</data></node>
<node id="n2261" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">hybrid clouds</data></node>
<node id="n2262" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">it transformation projects</data></node>
<node id="n2263" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">momentsos hierarchy</data></node>
<node id="n2264" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">polynomial optimization</data></node>
<node id="n2265" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">signal processing</data></node>
<node id="n2266" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">computational geometry</data></node>
<node id="n2267" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">control engineering</data></node>
<node id="n2268" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">convex semidefinite optimization</data></node>
<node id="n2269" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">responsible ai</data></node>
<node id="n2270" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">interoperable</data></node>
<node id="n2271" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">reusable</data></node>
<node id="n2272" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">automated data adoption</data></node>
<node id="n2273" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">linked data vocabulary</data></node>
<node id="n2274" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">graph data</data></node>
<node id="n2275" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">neuromorphic computing</data></node>
<node id="n2276" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">eventbased sensing</data></node>
<node id="n2277" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">eventbased actuation</data></node>
<node id="n2278" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">eventbased computing</data></node>
<node id="n2279" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">multivariate statistical methods</data></node>
<node id="n2280" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">statistical data</data></node>
<node id="n2281" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">high dimensional data</data></node>
<node id="n2282" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">francis qr algorithm</data></node>
<node id="n2283" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">companion matrix</data></node>
<node id="n2284" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">eigenvalues</data></node>
<node id="n2285" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">hessenberg matrix</data></node>
<node id="n2286" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">unitary matrix</data></node>
<node id="n2287" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">rankone matrix</data></node>
<node id="n2288" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">practical skills</data></node>
<node id="n2289" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">explainable ai</data></node>
<node id="n2290" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">safety monitoring</data></node>
<node id="n2291" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">inference and learning of new knowledge</data></node>
<node id="n2292" labels=":Skill"><data key="labels">:Skill</data><data key="name">anticipation and braininspired representation</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n2293" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">�large language models</data></node>
<node id="n2294" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">multimodal representation learning</data></node>
<node id="n2295" labels=":Skill"><data key="labels">:Skill</data><data key="name">natural language processing�</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n2296" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">correlations�</data></node>
<node id="n2297" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">manipulation�</data></node>
<node id="n2298" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">risks of ai</data></node>
<node id="n2299" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">ai fairness</data></node>
<node id="n2300" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">r markdown</data></node>
<node id="n2301" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">information retrieval</data></node>
<node id="n2302" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">data semantics</data></node>
<node id="n2303" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">querying ontologies</data></node>
<node id="n2304" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">scatter plot</data></node>
<node id="n2305" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">bar plot</data></node>
<node id="n2306" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">histogram</data></node>
<node id="n2307" labels=":Skill"><data key="labels">:Skill</data><data key="name">tidyr</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n2308" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">chatgpt</data></node>
<node id="n2309" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">numerical linear algebra</data></node>
<node id="n2310" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">nonlinear eigenvalue problems</data></node>
<node id="n2311" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">�polynomial systems</data></node>
<node id="n2312" labels=":Skill"><data key="labels">:Skill</data><data key="name">rectangular multiparameter eigenvalue problems�</data><data key="category">BD-ML-AI-Courses</data></node>
<node id="n2313" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">�linear algebra</data></node>
<node id="n2314" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">algebraic geometry</data></node>
<node id="n2315" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">multivariate polynomials</data></node>
<node id="n2316" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">nonlinearmultiparameter eigenvalue problems</data></node>
<node id="n2317" labels=":Skill"><data key="labels">:Skill</data><data key="category">BD-ML-AI-Courses</data><data key="name">neuromorphic engineering</data></node>
<edge id="e0" source="n460" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1" source="n460" target="n499" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2" source="n460" target="n500" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3" source="n460" target="n501" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4" source="n460" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5" source="n460" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6" source="n460" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7" source="n460" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8" source="n460" target="n421" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9" source="n460" target="n502" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e10" source="n460" target="n503" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e11" source="n460" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e12" source="n460" target="n504" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e13" source="n461" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e14" source="n461" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e15" source="n461" target="n505" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e16" source="n461" target="n506" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e17" source="n461" target="n507" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e18" source="n461" target="n508" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e19" source="n461" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e20" source="n461" target="n510" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e21" source="n461" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e22" source="n461" target="n48" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e23" source="n461" target="n511" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e24" source="n461" target="n512" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e25" source="n461" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e26" source="n461" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e27" source="n461" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e28" source="n461" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e29" source="n462" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e30" source="n462" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e31" source="n462" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e32" source="n462" target="n514" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e33" source="n462" target="n515" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e34" source="n462" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e35" source="n462" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e36" source="n462" target="n517" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e37" source="n462" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e38" source="n462" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e39" source="n462" target="n520" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e40" source="n462" target="n521" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e41" source="n462" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e42" source="n462" target="n523" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e43" source="n462" target="n524" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e44" source="n462" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e45" source="n462" target="n526" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e46" source="n462" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e47" source="n462" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e48" source="n462" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e49" source="n462" target="n348" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e50" source="n462" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e51" source="n462" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e52" source="n462" target="n528" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e53" source="n462" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e54" source="n462" target="n529" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e55" source="n463" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e56" source="n463" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e57" source="n463" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e58" source="n463" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e59" source="n463" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e60" source="n463" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e61" source="n463" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e62" source="n463" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e63" source="n463" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e64" source="n463" target="n532" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e65" source="n463" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e66" source="n463" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e67" source="n463" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e68" source="n463" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e69" source="n463" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e70" source="n463" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e71" source="n464" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e72" source="n464" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e73" source="n464" target="n534" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e74" source="n464" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e75" source="n464" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e76" source="n464" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e77" source="n464" target="n535" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e78" source="n464" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e79" source="n464" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e80" source="n464" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e81" source="n464" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e82" source="n464" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e83" source="n464" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e84" source="n464" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e85" source="n464" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e86" source="n464" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e87" source="n464" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e88" source="n464" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e89" source="n464" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e90" source="n464" target="n539" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e91" source="n464" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e92" source="n464" target="n540" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e93" source="n465" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e94" source="n465" target="n541" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e95" source="n465" target="n542" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e96" source="n465" target="n543" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e97" source="n465" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e98" source="n465" target="n71" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e99" source="n465" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e100" source="n465" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e101" source="n465" target="n545" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e102" source="n465" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e103" source="n465" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e104" source="n465" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e105" source="n465" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e106" source="n465" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e107" source="n465" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e108" source="n466" target="n546" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e109" source="n466" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e110" source="n466" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e111" source="n466" target="n547" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e112" source="n466" target="n548" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e113" source="n466" target="n549" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e114" source="n466" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e115" source="n467" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e116" source="n467" target="n550" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e117" source="n467" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e118" source="n467" target="n551" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e119" source="n467" target="n552" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e120" source="n467" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e121" source="n467" target="n553" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e122" source="n467" target="n554" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e123" source="n467" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e124" source="n467" target="n555" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e125" source="n467" target="n556" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e126" source="n467" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e127" source="n467" target="n150" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e128" source="n467" target="n557" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e129" source="n467" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e130" source="n467" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e131" source="n467" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e132" source="n467" target="n559" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e133" source="n467" target="n560" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e134" source="n467" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e135" source="n467" target="n224" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e136" source="n468" target="n561" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e137" source="n468" target="n562" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e138" source="n468" target="n563" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e139" source="n468" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e140" source="n468" target="n564" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e141" source="n468" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e142" source="n468" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e143" source="n468" target="n80" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e144" source="n468" target="n566" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e145" source="n469" target="n567" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e146" source="n469" target="n568" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e147" source="n469" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e148" source="n469" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e149" source="n469" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e150" source="n469" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e151" source="n469" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e152" source="n469" target="n569" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e153" source="n470" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e154" source="n470" target="n570" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e155" source="n470" target="n571" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e156" source="n470" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e157" source="n471" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e158" source="n471" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e159" source="n471" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e160" source="n471" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e161" source="n471" target="n573" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e162" source="n471" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e163" source="n471" target="n575" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e164" source="n471" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e165" source="n471" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e166" source="n471" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e167" source="n471" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e168" source="n471" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e169" source="n471" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e170" source="n471" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e171" source="n471" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e172" source="n471" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e173" source="n472" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e174" source="n472" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e175" source="n472" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e176" source="n472" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e177" source="n472" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e178" source="n472" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e179" source="n472" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e180" source="n472" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e181" source="n473" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e182" source="n473" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e183" source="n473" target="n579" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e184" source="n473" target="n580" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e185" source="n473" target="n581" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e186" source="n473" target="n582" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e187" source="n473" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e188" source="n473" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e189" source="n473" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e190" source="n473" target="n583" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e191" source="n473" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e192" source="n473" target="n584" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e193" source="n473" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e194" source="n473" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e195" source="n473" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e196" source="n474" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e197" source="n474" target="n545" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e198" source="n474" target="n585" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e199" source="n474" target="n586" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e200" source="n474" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e201" source="n474" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e202" source="n474" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e203" source="n474" target="n434" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e204" source="n474" target="n428" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e205" source="n474" target="n587" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e206" source="n474" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e207" source="n474" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e208" source="n474" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e209" source="n474" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e210" source="n474" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e211" source="n474" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e212" source="n474" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e213" source="n474" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e214" source="n474" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e215" source="n474" target="n590" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e216" source="n474" target="n591" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e217" source="n474" target="n592" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e218" source="n474" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e219" source="n474" target="n579" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e220" source="n474" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e221" source="n474" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e222" source="n474" target="n593" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e223" source="n474" target="n137" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e224" source="n474" target="n594" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e225" source="n474" target="n118" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e226" source="n474" target="n595" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e227" source="n474" target="n596" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e228" source="n474" target="n597" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e229" source="n474" target="n191" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e230" source="n474" target="n248" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e231" source="n474" target="n217" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e232" source="n475" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e233" source="n475" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e234" source="n475" target="n599" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e235" source="n475" target="n600" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e236" source="n475" target="n590" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e237" source="n475" target="n601" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e238" source="n475" target="n602" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e239" source="n475" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e240" source="n475" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e241" source="n475" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e242" source="n475" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e243" source="n475" target="n603" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e244" source="n475" target="n604" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e245" source="n476" target="n605" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e246" source="n476" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e247" source="n476" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e248" source="n476" target="n606" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e249" source="n476" target="n183" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e250" source="n476" target="n607" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e251" source="n476" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e252" source="n476" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e253" source="n476" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e254" source="n476" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e255" source="n476" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e256" source="n476" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e257" source="n476" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e258" source="n476" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e259" source="n477" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e260" source="n477" target="n608" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e261" source="n477" target="n609" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e262" source="n477" target="n610" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e263" source="n477" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e264" source="n477" target="n20" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e265" source="n477" target="n611" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e266" source="n477" target="n612" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e267" source="n477" target="n613" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e268" source="n477" target="n614" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e269" source="n477" target="n615" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e270" source="n477" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e271" source="n477" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e272" source="n477" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e273" source="n477" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e274" source="n477" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e275" source="n477" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e276" source="n477" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e277" source="n477" target="n617" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e278" source="n477" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e279" source="n477" target="n618" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e280" source="n477" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e281" source="n477" target="n619" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e282" source="n477" target="n620" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e283" source="n478" target="n579" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e284" source="n478" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e285" source="n478" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e286" source="n478" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e287" source="n478" target="n561" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e288" source="n478" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e289" source="n478" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e290" source="n478" target="n550" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e291" source="n478" target="n621" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e292" source="n478" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e293" source="n478" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e294" source="n479" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e295" source="n479" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e296" source="n479" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e297" source="n479" target="n623" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e298" source="n479" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e299" source="n479" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e300" source="n479" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e301" source="n479" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e302" source="n479" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e303" source="n479" target="n346" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e304" source="n479" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e305" source="n479" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e306" source="n479" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e307" source="n479" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e308" source="n479" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e309" source="n479" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e310" source="n479" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e311" source="n480" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e312" source="n480" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e313" source="n480" target="n626" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e314" source="n480" target="n627" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e315" source="n480" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e316" source="n480" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e317" source="n480" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e318" source="n480" target="n569" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e319" source="n480" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e320" source="n480" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e321" source="n480" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e322" source="n480" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e323" source="n480" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e324" source="n480" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e325" source="n480" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e326" source="n480" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e327" source="n480" target="n346" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e328" source="n480" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e329" source="n481" target="n630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e330" source="n481" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e331" source="n481" target="n64" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e332" source="n481" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e333" source="n481" target="n631" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e334" source="n481" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e335" source="n481" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e336" source="n481" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e337" source="n482" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e338" source="n482" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e339" source="n482" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e340" source="n482" target="n114" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e341" source="n482" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e342" source="n483" target="n633" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e343" source="n483" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e344" source="n483" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e345" source="n483" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e346" source="n483" target="n499" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e347" source="n483" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e348" source="n483" target="n634" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e349" source="n483" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e350" source="n483" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e351" source="n484" target="n635" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e352" source="n484" target="n636" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e353" source="n484" target="n592" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e354" source="n484" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e355" source="n484" target="n637" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e356" source="n484" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e357" source="n484" target="n638" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e358" source="n484" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e359" source="n484" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e360" source="n484" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e361" source="n485" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e362" source="n485" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e363" source="n485" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e364" source="n485" target="n640" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e365" source="n485" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e366" source="n485" target="n4" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e367" source="n485" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e368" source="n485" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e369" source="n485" target="n642" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e370" source="n485" target="n643" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e371" source="n485" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e372" source="n485" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e373" source="n485" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e374" source="n485" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e375" source="n485" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e376" source="n485" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e377" source="n485" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e378" source="n485" target="n644" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e379" source="n485" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e380" source="n485" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e381" source="n485" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e382" source="n485" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e383" source="n485" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e384" source="n485" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e385" source="n485" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e386" source="n485" target="n646" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e387" source="n485" target="n500" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e388" source="n486" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e389" source="n486" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e390" source="n486" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e391" source="n486" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e392" source="n486" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e393" source="n486" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e394" source="n486" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e395" source="n486" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e396" source="n486" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e397" source="n487" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e398" source="n487" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e399" source="n487" target="n650" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e400" source="n487" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e401" source="n487" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e402" source="n487" target="n652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e403" source="n487" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e404" source="n487" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e405" source="n488" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e406" source="n488" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e407" source="n488" target="n653" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e408" source="n488" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e409" source="n488" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e410" source="n488" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e411" source="n488" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e412" source="n489" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e413" source="n489" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e414" source="n489" target="n654" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e415" source="n489" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e416" source="n489" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e417" source="n489" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e418" source="n489" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e419" source="n489" target="n656" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e420" source="n489" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e421" source="n489" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e422" source="n489" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e423" source="n489" target="n657" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e424" source="n489" target="n658" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e425" source="n489" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e426" source="n489" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e427" source="n489" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e428" source="n489" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e429" source="n489" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e430" source="n489" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e431" source="n489" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e432" source="n490" target="n659" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e433" source="n490" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e434" source="n490" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e435" source="n490" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e436" source="n490" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e437" source="n490" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e438" source="n490" target="n660" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e439" source="n490" target="n661" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e440" source="n490" target="n662" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e441" source="n490" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e442" source="n490" target="n535" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e443" source="n490" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e444" source="n490" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e445" source="n490" target="n665" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e446" source="n490" target="n666" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e447" source="n490" target="n667" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e448" source="n490" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e449" source="n490" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e450" source="n490" target="n40" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e451" source="n490" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e452" source="n490" target="n626" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e453" source="n490" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e454" source="n490" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e455" source="n490" target="n668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e456" source="n490" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e457" source="n490" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e458" source="n490" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e459" source="n490" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e460" source="n490" target="n30" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e461" source="n491" target="n633" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e462" source="n491" target="n669" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e463" source="n491" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e464" source="n491" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e465" source="n491" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e466" source="n491" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e467" source="n491" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e468" source="n491" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e469" source="n491" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e470" source="n491" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e471" source="n492" target="n671" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e472" source="n492" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e473" source="n492" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e474" source="n492" target="n672" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e475" source="n492" target="n673" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e476" source="n492" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e477" source="n492" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e478" source="n492" target="n674" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e479" source="n492" target="n675" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e480" source="n492" target="n114" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e481" source="n493" target="n676" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e482" source="n493" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e483" source="n493" target="n677" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e484" source="n493" target="n592" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e485" source="n493" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e486" source="n493" target="n678" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e487" source="n493" target="n679" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e488" source="n493" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e489" source="n493" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e490" source="n493" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e491" source="n493" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e492" source="n493" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e493" source="n493" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e494" source="n493" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e495" source="n493" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e496" source="n493" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e497" source="n493" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e498" source="n493" target="n680" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e499" source="n494" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e500" source="n494" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e501" source="n494" target="n344" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e502" source="n494" target="n681" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e503" source="n494" target="n682" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e504" source="n494" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e505" source="n494" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e506" source="n494" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e507" source="n494" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e508" source="n495" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e509" source="n495" target="n636" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e510" source="n495" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e511" source="n495" target="n683" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e512" source="n495" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e513" source="n495" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e514" source="n495" target="n685" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e515" source="n495" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e516" source="n495" target="n687" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e517" source="n495" target="n688" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e518" source="n495" target="n689" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e519" source="n495" target="n690" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e520" source="n495" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e521" source="n495" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e522" source="n495" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e523" source="n495" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e524" source="n496" target="n691" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e525" source="n496" target="n692" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e526" source="n496" target="n535" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e527" source="n496" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e528" source="n496" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e529" source="n496" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e530" source="n496" target="n693" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e531" source="n496" target="n694" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e532" source="n496" target="n695" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e533" source="n496" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e534" source="n496" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e535" source="n496" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e536" source="n496" target="n696" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e537" source="n496" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e538" source="n496" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e539" source="n496" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e540" source="n496" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e541" source="n496" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e542" source="n496" target="n683" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e543" source="n496" target="n638" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e544" source="n496" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e545" source="n496" target="n697" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e546" source="n496" target="n698" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e547" source="n496" target="n699" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e548" source="n496" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e549" source="n496" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e550" source="n496" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e551" source="n496" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e552" source="n496" target="n700" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e553" source="n496" target="n701" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e554" source="n496" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e555" source="n497" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e556" source="n497" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e557" source="n498" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e558" source="n498" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e559" source="n498" target="n702" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e560" source="n498" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e561" source="n498" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e562" source="n498" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e563" source="n498" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e564" source="n498" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e565" source="n498" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e566" source="n498" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e567" source="n498" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e568" source="n498" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e569" source="n498" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e570" source="n498" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e571" source="n498" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e572" source="n498" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e573" source="n498" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e574" source="n498" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e575" source="n498" target="n703" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e576" source="n498" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e577" source="n460" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e578" source="n461" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e579" source="n462" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e580" source="n463" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e581" source="n464" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e582" source="n465" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e583" source="n466" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e584" source="n467" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e585" source="n468" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e586" source="n469" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e587" source="n470" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e588" source="n471" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e589" source="n472" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e590" source="n473" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e591" source="n474" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e592" source="n475" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e593" source="n476" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e594" source="n477" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e595" source="n478" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e596" source="n479" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e597" source="n480" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e598" source="n481" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e599" source="n482" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e600" source="n483" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e601" source="n484" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e602" source="n485" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e603" source="n486" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e604" source="n487" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e605" source="n488" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e606" source="n489" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e607" source="n490" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e608" source="n491" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e609" source="n492" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e610" source="n493" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e611" source="n494" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e612" source="n495" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e613" source="n496" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e614" source="n497" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e615" source="n498" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e616" source="n452" target="n11" label="HAS"><data key="label">HAS</data><data key="score">16.216216216216218</data></edge>
<edge id="e617" source="n452" target="n499" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e618" source="n452" target="n500" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e619" source="n452" target="n501" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e620" source="n452" target="n279" label="HAS"><data key="label">HAS</data><data key="score">16.216216216216218</data></edge>
<edge id="e621" source="n452" target="n278" label="HAS"><data key="label">HAS</data><data key="score">21.62162162162162</data></edge>
<edge id="e622" source="n452" target="n69" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e623" source="n452" target="n63" label="HAS"><data key="label">HAS</data><data key="score">10.81081081081081</data></edge>
<edge id="e624" source="n452" target="n421" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e625" source="n452" target="n502" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e626" source="n452" target="n503" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e627" source="n452" target="n55" label="HAS"><data key="label">HAS</data><data key="score">15.315315315315313</data></edge>
<edge id="e628" source="n452" target="n504" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e629" source="n452" target="n53" label="HAS"><data key="label">HAS</data><data key="score">39.63963963963964</data></edge>
<edge id="e630" source="n452" target="n121" label="HAS"><data key="label">HAS</data><data key="score">9.90990990990991</data></edge>
<edge id="e631" source="n452" target="n505" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e632" source="n452" target="n506" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e633" source="n452" target="n507" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e634" source="n452" target="n508" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e635" source="n452" target="n509" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e636" source="n452" target="n510" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e637" source="n452" target="n50" label="HAS"><data key="label">HAS</data><data key="score">19.81981981981982</data></edge>
<edge id="e638" source="n452" target="n48" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e639" source="n452" target="n511" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e640" source="n452" target="n512" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e641" source="n452" target="n277" label="HAS"><data key="label">HAS</data><data key="score">44.14414414414414</data></edge>
<edge id="e642" source="n452" target="n43" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e643" source="n452" target="n513" label="HAS"><data key="label">HAS</data><data key="score">14.414414414414415</data></edge>
<edge id="e644" source="n452" target="n54" label="HAS"><data key="label">HAS</data><data key="score">18.01801801801802</data></edge>
<edge id="e645" source="n452" target="n514" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e646" source="n452" target="n515" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e647" source="n452" target="n516" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e648" source="n452" target="n5" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e649" source="n452" target="n517" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e650" source="n452" target="n518" label="HAS"><data key="label">HAS</data><data key="score">15.315315315315313</data></edge>
<edge id="e651" source="n452" target="n519" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e652" source="n452" target="n520" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e653" source="n452" target="n521" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e654" source="n452" target="n522" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e655" source="n452" target="n523" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e656" source="n452" target="n524" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e657" source="n452" target="n525" label="HAS"><data key="label">HAS</data><data key="score">17.117117117117118</data></edge>
<edge id="e658" source="n452" target="n526" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e659" source="n452" target="n527" label="HAS"><data key="label">HAS</data><data key="score">46.846846846846844</data></edge>
<edge id="e660" source="n452" target="n188" label="HAS"><data key="label">HAS</data><data key="score">27.027027027027028</data></edge>
<edge id="e661" source="n452" target="n338" label="HAS"><data key="label">HAS</data><data key="score">9.90990990990991</data></edge>
<edge id="e662" source="n452" target="n348" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e663" source="n452" target="n528" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e664" source="n452" target="n39" label="HAS"><data key="label">HAS</data><data key="score">18.01801801801802</data></edge>
<edge id="e665" source="n452" target="n529" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e666" source="n452" target="n286" label="HAS"><data key="label">HAS</data><data key="score">14.414414414414415</data></edge>
<edge id="e667" source="n452" target="n336" label="HAS"><data key="label">HAS</data><data key="score">22.52252252252252</data></edge>
<edge id="e668" source="n452" target="n530" label="HAS"><data key="label">HAS</data><data key="score">22.52252252252252</data></edge>
<edge id="e669" source="n452" target="n531" label="HAS"><data key="label">HAS</data><data key="score">22.52252252252252</data></edge>
<edge id="e670" source="n452" target="n532" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e671" source="n452" target="n52" label="HAS"><data key="label">HAS</data><data key="score">9.00900900900901</data></edge>
<edge id="e672" source="n452" target="n149" label="HAS"><data key="label">HAS</data><data key="score">14.414414414414415</data></edge>
<edge id="e673" source="n452" target="n3" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e674" source="n452" target="n533" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e675" source="n452" target="n534" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e676" source="n452" target="n535" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e677" source="n452" target="n148" label="HAS"><data key="label">HAS</data><data key="score">23.423423423423422</data></edge>
<edge id="e678" source="n452" target="n21" label="HAS"><data key="label">HAS</data><data key="score">28.82882882882883</data></edge>
<edge id="e679" source="n452" target="n84" label="HAS"><data key="label">HAS</data><data key="score">13.513513513513514</data></edge>
<edge id="e680" source="n452" target="n536" label="HAS"><data key="label">HAS</data><data key="score">11.711711711711711</data></edge>
<edge id="e681" source="n452" target="n537" label="HAS"><data key="label">HAS</data><data key="score">17.117117117117118</data></edge>
<edge id="e682" source="n452" target="n349" label="HAS"><data key="label">HAS</data><data key="score">13.513513513513514</data></edge>
<edge id="e683" source="n452" target="n538" label="HAS"><data key="label">HAS</data><data key="score">10.81081081081081</data></edge>
<edge id="e684" source="n452" target="n241" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e685" source="n452" target="n41" label="HAS"><data key="label">HAS</data><data key="score">10.81081081081081</data></edge>
<edge id="e686" source="n452" target="n539" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e687" source="n452" target="n351" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e688" source="n452" target="n540" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e689" source="n452" target="n8" label="HAS"><data key="label">HAS</data><data key="score">24.324324324324326</data></edge>
<edge id="e690" source="n452" target="n541" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e691" source="n452" target="n542" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e692" source="n452" target="n543" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e693" source="n452" target="n9" label="HAS"><data key="label">HAS</data><data key="score">9.00900900900901</data></edge>
<edge id="e694" source="n452" target="n71" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e695" source="n452" target="n544" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e696" source="n452" target="n545" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e697" source="n452" target="n352" label="HAS"><data key="label">HAS</data><data key="score">14.414414414414415</data></edge>
<edge id="e698" source="n452" target="n546" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e699" source="n452" target="n547" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e700" source="n452" target="n548" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e701" source="n452" target="n549" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e702" source="n452" target="n550" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e703" source="n452" target="n551" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e704" source="n452" target="n552" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e705" source="n452" target="n553" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e706" source="n452" target="n554" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e707" source="n452" target="n555" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e708" source="n452" target="n556" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e709" source="n452" target="n150" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e710" source="n452" target="n557" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e711" source="n452" target="n558" label="HAS"><data key="label">HAS</data><data key="score">12.612612612612612</data></edge>
<edge id="e712" source="n452" target="n559" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e713" source="n452" target="n560" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e714" source="n452" target="n107" label="HAS"><data key="label">HAS</data><data key="score">13.513513513513514</data></edge>
<edge id="e715" source="n452" target="n224" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e716" source="n452" target="n561" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e717" source="n452" target="n562" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e718" source="n452" target="n563" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e719" source="n452" target="n564" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e720" source="n452" target="n565" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e721" source="n452" target="n80" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e722" source="n452" target="n566" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e723" source="n452" target="n567" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e724" source="n452" target="n568" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e725" source="n452" target="n193" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e726" source="n452" target="n256" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e727" source="n452" target="n569" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e728" source="n452" target="n570" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e729" source="n452" target="n571" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e730" source="n452" target="n572" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e731" source="n452" target="n573" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e732" source="n452" target="n574" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e733" source="n452" target="n575" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e734" source="n452" target="n47" label="HAS"><data key="label">HAS</data><data key="score">10.81081081081081</data></edge>
<edge id="e735" source="n452" target="n576" label="HAS"><data key="label">HAS</data><data key="score">9.90990990990991</data></edge>
<edge id="e736" source="n452" target="n577" label="HAS"><data key="label">HAS</data><data key="score">21.62162162162162</data></edge>
<edge id="e737" source="n452" target="n578" label="HAS"><data key="label">HAS</data><data key="score">25.225225225225223</data></edge>
<edge id="e738" source="n452" target="n579" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e739" source="n452" target="n580" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e740" source="n452" target="n581" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e741" source="n452" target="n582" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e742" source="n452" target="n583" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e743" source="n452" target="n584" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e744" source="n452" target="n178" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e745" source="n452" target="n585" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e746" source="n452" target="n586" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e747" source="n452" target="n280" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e748" source="n452" target="n175" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e749" source="n452" target="n282" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e750" source="n452" target="n434" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e751" source="n452" target="n428" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e752" source="n452" target="n587" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e753" source="n452" target="n359" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e754" source="n452" target="n342" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e755" source="n452" target="n347" label="HAS"><data key="label">HAS</data><data key="score">8.108108108108109</data></edge>
<edge id="e756" source="n452" target="n56" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e757" source="n452" target="n588" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e758" source="n452" target="n589" label="HAS"><data key="label">HAS</data><data key="score">11.711711711711711</data></edge>
<edge id="e759" source="n452" target="n590" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e760" source="n452" target="n591" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e761" source="n452" target="n592" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e762" source="n452" target="n284" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e763" source="n452" target="n593" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e764" source="n452" target="n137" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e765" source="n452" target="n594" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e766" source="n452" target="n118" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e767" source="n452" target="n595" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e768" source="n452" target="n596" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e769" source="n452" target="n597" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e770" source="n452" target="n191" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e771" source="n452" target="n248" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e772" source="n452" target="n217" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e773" source="n452" target="n598" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e774" source="n452" target="n599" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e775" source="n452" target="n600" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e776" source="n452" target="n601" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e777" source="n452" target="n602" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e778" source="n452" target="n603" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e779" source="n452" target="n604" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e780" source="n452" target="n605" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e781" source="n452" target="n606" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e782" source="n452" target="n183" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e783" source="n452" target="n607" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e784" source="n452" target="n333" label="HAS"><data key="label">HAS</data><data key="score">11.711711711711711</data></edge>
<edge id="e785" source="n452" target="n608" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e786" source="n452" target="n609" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e787" source="n452" target="n610" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e788" source="n452" target="n20" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e789" source="n452" target="n611" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e790" source="n452" target="n612" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e791" source="n452" target="n613" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e792" source="n452" target="n614" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e793" source="n452" target="n615" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e794" source="n452" target="n616" label="HAS"><data key="label">HAS</data><data key="score">9.00900900900901</data></edge>
<edge id="e795" source="n452" target="n617" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e796" source="n452" target="n381" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e797" source="n452" target="n618" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e798" source="n452" target="n619" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e799" source="n452" target="n620" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e800" source="n452" target="n621" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e801" source="n452" target="n622" label="HAS"><data key="label">HAS</data><data key="score">18.01801801801802</data></edge>
<edge id="e802" source="n452" target="n623" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e803" source="n452" target="n346" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e804" source="n452" target="n624" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e805" source="n452" target="n625" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e806" source="n452" target="n626" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e807" source="n452" target="n627" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e808" source="n452" target="n628" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e809" source="n452" target="n629" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e810" source="n452" target="n630" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e811" source="n452" target="n64" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e812" source="n452" target="n631" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e813" source="n452" target="n632" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e814" source="n452" target="n114" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e815" source="n452" target="n633" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e816" source="n452" target="n634" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e817" source="n452" target="n635" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e818" source="n452" target="n636" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e819" source="n452" target="n637" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e820" source="n452" target="n638" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e821" source="n452" target="n639" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e822" source="n452" target="n640" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e823" source="n452" target="n641" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e824" source="n452" target="n4" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e825" source="n452" target="n642" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e826" source="n452" target="n643" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e827" source="n452" target="n247" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e828" source="n452" target="n76" label="HAS"><data key="label">HAS</data><data key="score">9.90990990990991</data></edge>
<edge id="e829" source="n452" target="n644" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e830" source="n452" target="n385" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e831" source="n452" target="n645" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e832" source="n452" target="n646" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e833" source="n452" target="n647" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e834" source="n452" target="n648" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e835" source="n452" target="n337" label="HAS"><data key="label">HAS</data><data key="score">14.414414414414415</data></edge>
<edge id="e836" source="n452" target="n649" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e837" source="n452" target="n650" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e838" source="n452" target="n651" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e839" source="n452" target="n652" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e840" source="n452" target="n341" label="HAS"><data key="label">HAS</data><data key="score">10.81081081081081</data></edge>
<edge id="e841" source="n452" target="n653" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e842" source="n452" target="n654" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e843" source="n452" target="n655" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e844" source="n452" target="n656" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e845" source="n452" target="n320" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e846" source="n452" target="n657" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e847" source="n452" target="n658" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e848" source="n452" target="n355" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e849" source="n452" target="n659" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e850" source="n452" target="n354" label="HAS"><data key="label">HAS</data><data key="score">14.414414414414415</data></edge>
<edge id="e851" source="n452" target="n660" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e852" source="n452" target="n661" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e853" source="n452" target="n662" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e854" source="n452" target="n663" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e855" source="n452" target="n664" label="HAS"><data key="label">HAS</data><data key="score">18.91891891891892</data></edge>
<edge id="e856" source="n452" target="n665" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e857" source="n452" target="n666" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e858" source="n452" target="n667" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e859" source="n452" target="n40" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e860" source="n452" target="n668" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e861" source="n452" target="n30" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e862" source="n452" target="n669" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e863" source="n452" target="n670" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e864" source="n452" target="n671" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e865" source="n452" target="n34" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e866" source="n452" target="n672" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e867" source="n452" target="n673" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e868" source="n452" target="n674" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e869" source="n452" target="n675" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e870" source="n452" target="n676" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e871" source="n452" target="n677" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e872" source="n452" target="n678" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e873" source="n452" target="n679" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e874" source="n452" target="n265" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e875" source="n452" target="n680" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e876" source="n452" target="n344" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e877" source="n452" target="n681" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e878" source="n452" target="n682" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e879" source="n452" target="n383" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e880" source="n452" target="n683" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e881" source="n452" target="n684" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e882" source="n452" target="n685" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e883" source="n452" target="n686" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e884" source="n452" target="n687" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e885" source="n452" target="n688" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e886" source="n452" target="n689" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e887" source="n452" target="n690" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e888" source="n452" target="n24" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e889" source="n452" target="n691" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e890" source="n452" target="n692" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e891" source="n452" target="n693" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e892" source="n452" target="n694" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e893" source="n452" target="n695" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e894" source="n452" target="n696" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e895" source="n452" target="n697" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e896" source="n452" target="n698" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e897" source="n452" target="n699" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e898" source="n452" target="n326" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e899" source="n452" target="n700" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e900" source="n452" target="n701" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e901" source="n452" target="n702" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e902" source="n452" target="n703" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e903" source="n704" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e904" source="n704" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e905" source="n704" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e906" source="n704" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e907" source="n705" target="n745" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e908" source="n705" target="n746" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e909" source="n705" target="n747" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e910" source="n705" target="n748" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e911" source="n705" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e912" source="n705" target="n679" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e913" source="n705" target="n749" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e914" source="n705" target="n750" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e915" source="n706" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e916" source="n706" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e917" source="n706" target="n753" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e918" source="n706" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e919" source="n706" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e920" source="n706" target="n755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e921" source="n706" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e922" source="n706" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e923" source="n706" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e924" source="n706" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e925" source="n706" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e926" source="n706" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e927" source="n706" target="n758" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e928" source="n706" target="n759" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e929" source="n706" target="n760" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e930" source="n706" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e931" source="n706" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e932" source="n706" target="n761" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e933" source="n706" target="n762" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e934" source="n706" target="n763" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e935" source="n706" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e936" source="n706" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e937" source="n706" target="n764" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e938" source="n706" target="n765" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e939" source="n706" target="n766" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e940" source="n706" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e941" source="n706" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e942" source="n706" target="n569" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e943" source="n706" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e944" source="n706" target="n768" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e945" source="n706" target="n769" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e946" source="n706" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e947" source="n706" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e948" source="n706" target="n771" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e949" source="n706" target="n688" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e950" source="n706" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e951" source="n706" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e952" source="n706" target="n772" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e953" source="n706" target="n773" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e954" source="n706" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e955" source="n706" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e956" source="n706" target="n364" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e957" source="n706" target="n774" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e958" source="n706" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e959" source="n707" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e960" source="n707" target="n775" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e961" source="n707" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e962" source="n707" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e963" source="n707" target="n776" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e964" source="n707" target="n777" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e965" source="n707" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e966" source="n707" target="n778" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e967" source="n707" target="n779" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e968" source="n707" target="n780" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e969" source="n707" target="n781" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e970" source="n707" target="n782" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e971" source="n707" target="n783" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e972" source="n707" target="n784" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e973" source="n707" target="n785" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e974" source="n707" target="n786" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e975" source="n707" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e976" source="n708" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e977" source="n708" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e978" source="n708" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e979" source="n708" target="n788" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e980" source="n708" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e981" source="n708" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e982" source="n708" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e983" source="n708" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e984" source="n708" target="n789" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e985" source="n708" target="n790" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e986" source="n708" target="n791" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e987" source="n709" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e988" source="n709" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e989" source="n709" target="n793" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e990" source="n709" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e991" source="n709" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e992" source="n709" target="n794" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e993" source="n709" target="n583" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e994" source="n709" target="n795" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e995" source="n709" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e996" source="n709" target="n599" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e997" source="n709" target="n796" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e998" source="n709" target="n210" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e999" source="n709" target="n797" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1000" source="n709" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1001" source="n709" target="n798" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1002" source="n709" target="n799" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1003" source="n709" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1004" source="n709" target="n30" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1005" source="n709" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1006" source="n709" target="n800" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1007" source="n709" target="n801" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1008" source="n709" target="n802" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1009" source="n709" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1010" source="n709" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1011" source="n709" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1012" source="n709" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1013" source="n710" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1014" source="n710" target="n803" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1015" source="n710" target="n804" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1016" source="n710" target="n805" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1017" source="n710" target="n806" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1018" source="n710" target="n807" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1019" source="n710" target="n808" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1020" source="n710" target="n809" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1021" source="n710" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1022" source="n710" target="n810" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1023" source="n710" target="n811" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1024" source="n710" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1025" source="n710" target="n812" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1026" source="n710" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1027" source="n710" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1028" source="n710" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1029" source="n710" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1030" source="n710" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1031" source="n710" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1032" source="n710" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1033" source="n710" target="n815" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1034" source="n710" target="n816" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1035" source="n710" target="n817" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1036" source="n711" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1037" source="n711" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1038" source="n711" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1039" source="n711" target="n818" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1040" source="n711" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1041" source="n711" target="n819" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1042" source="n711" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1043" source="n711" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1044" source="n711" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1045" source="n711" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1046" source="n711" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1047" source="n711" target="n369" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1048" source="n711" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1049" source="n711" target="n820" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1050" source="n711" target="n239" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1051" source="n711" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1052" source="n711" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1053" source="n711" target="n680" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1054" source="n711" target="n821" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1055" source="n711" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1056" source="n711" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1057" source="n711" target="n559" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1058" source="n711" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1059" source="n711" target="n299" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1060" source="n711" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1061" source="n711" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1062" source="n711" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1063" source="n711" target="n410" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1064" source="n711" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1065" source="n711" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1066" source="n711" target="n822" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1067" source="n711" target="n823" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1068" source="n711" target="n824" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1069" source="n711" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1070" source="n711" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1071" source="n711" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1072" source="n711" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1073" source="n711" target="n388" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1074" source="n711" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1075" source="n711" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1076" source="n712" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1077" source="n712" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1078" source="n712" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1079" source="n712" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1080" source="n712" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1081" source="n712" target="n825" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1082" source="n712" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1083" source="n712" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1084" source="n712" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1085" source="n712" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1086" source="n712" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1087" source="n712" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1088" source="n712" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1089" source="n712" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1090" source="n712" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1091" source="n712" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1092" source="n712" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1093" source="n712" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1094" source="n712" target="n826" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1095" source="n712" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1096" source="n712" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1097" source="n712" target="n769" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1098" source="n712" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1099" source="n712" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1100" source="n712" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1101" source="n712" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1102" source="n713" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1103" source="n713" target="n827" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1104" source="n713" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1105" source="n713" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1106" source="n713" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1107" source="n713" target="n828" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1108" source="n713" target="n829" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1109" source="n713" target="n830" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1110" source="n713" target="n831" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1111" source="n713" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1112" source="n714" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1113" source="n714" target="n832" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1114" source="n714" target="n109" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1115" source="n714" target="n154" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1116" source="n714" target="n131" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1117" source="n714" target="n113" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1118" source="n714" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1119" source="n714" target="n101" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1120" source="n714" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1121" source="n714" target="n296" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1122" source="n714" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1123" source="n714" target="n187" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1124" source="n714" target="n833" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1125" source="n714" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1126" source="n714" target="n261" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1127" source="n714" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1128" source="n714" target="n86" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1129" source="n715" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1130" source="n715" target="n834" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1131" source="n715" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1132" source="n715" target="n64" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1133" source="n715" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1134" source="n716" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1135" source="n716" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1136" source="n716" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1137" source="n717" target="n835" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1138" source="n717" target="n836" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1139" source="n717" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1140" source="n717" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1141" source="n717" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1142" source="n717" target="n837" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1143" source="n717" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1144" source="n717" target="n838" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1145" source="n717" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1146" source="n717" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1147" source="n717" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1148" source="n717" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1149" source="n717" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1150" source="n717" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1151" source="n717" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1152" source="n717" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1153" source="n717" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1154" source="n717" target="n840" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1155" source="n717" target="n644" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1156" source="n717" target="n698" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1157" source="n717" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1158" source="n717" target="n650" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1159" source="n717" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1160" source="n717" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1161" source="n717" target="n344" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1162" source="n718" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1163" source="n718" target="n841" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1164" source="n718" target="n842" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1165" source="n718" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1166" source="n718" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1167" source="n718" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1168" source="n718" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1169" source="n718" target="n844" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1170" source="n718" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1171" source="n718" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1172" source="n718" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1173" source="n718" target="n114" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1174" source="n718" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1175" source="n718" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1176" source="n718" target="n845" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1177" source="n718" target="n846" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1178" source="n718" target="n17" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1179" source="n718" target="n847" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1180" source="n718" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1181" source="n718" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1182" source="n718" target="n848" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1183" source="n719" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1184" source="n719" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1185" source="n719" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1186" source="n719" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1187" source="n719" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1188" source="n719" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1189" source="n719" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1190" source="n719" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1191" source="n719" target="n849" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1192" source="n719" target="n850" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1193" source="n719" target="n851" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1194" source="n719" target="n239" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1195" source="n719" target="n852" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1196" source="n719" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1197" source="n720" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1198" source="n720" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1199" source="n720" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1200" source="n720" target="n853" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1201" source="n720" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1202" source="n720" target="n855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1203" source="n720" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1204" source="n720" target="n856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1205" source="n720" target="n857" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1206" source="n720" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1207" source="n720" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1208" source="n720" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1209" source="n720" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1210" source="n720" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1211" source="n720" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1212" source="n720" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1213" source="n720" target="n859" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1214" source="n720" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1215" source="n720" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1216" source="n720" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1217" source="n720" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1218" source="n720" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1219" source="n720" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1220" source="n720" target="n618" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1221" source="n720" target="n861" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1222" source="n720" target="n862" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1223" source="n720" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1224" source="n720" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1225" source="n721" target="n835" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1226" source="n721" target="n863" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1227" source="n721" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1228" source="n721" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1229" source="n721" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1230" source="n721" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1231" source="n721" target="n867" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1232" source="n721" target="n777" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1233" source="n721" target="n868" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1234" source="n721" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1235" source="n721" target="n869" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1236" source="n721" target="n366" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1237" source="n721" target="n870" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1238" source="n721" target="n871" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1239" source="n721" target="n872" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1240" source="n721" target="n873" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1241" source="n721" target="n874" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1242" source="n721" target="n875" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1243" source="n721" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1244" source="n721" target="n877" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1245" source="n721" target="n878" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1246" source="n721" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1247" source="n721" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1248" source="n721" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1249" source="n721" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1250" source="n721" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1251" source="n721" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1252" source="n721" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1253" source="n721" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1254" source="n721" target="n698" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1255" source="n721" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1256" source="n721" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1257" source="n721" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1258" source="n721" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1259" source="n721" target="n690" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1260" source="n721" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1261" source="n721" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1262" source="n721" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1263" source="n721" target="n292" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1264" source="n721" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1265" source="n721" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1266" source="n721" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1267" source="n721" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1268" source="n722" target="n881" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1269" source="n722" target="n882" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1270" source="n722" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1271" source="n722" target="n683" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1272" source="n722" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1273" source="n722" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1274" source="n722" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1275" source="n722" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1276" source="n722" target="n883" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1277" source="n722" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1278" source="n722" target="n884" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1279" source="n722" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1280" source="n722" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1281" source="n722" target="n885" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1282" source="n722" target="n886" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1283" source="n722" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1284" source="n722" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1285" source="n722" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1286" source="n722" target="n887" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1287" source="n722" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1288" source="n722" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1289" source="n722" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1290" source="n722" target="n888" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1291" source="n722" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1292" source="n722" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1293" source="n722" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1294" source="n722" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1295" source="n722" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1296" source="n722" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1297" source="n722" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1298" source="n722" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1299" source="n722" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1300" source="n722" target="n861" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1301" source="n722" target="n889" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1302" source="n722" target="n890" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1303" source="n722" target="n891" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1304" source="n722" target="n892" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1305" source="n722" target="n893" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1306" source="n722" target="n894" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1307" source="n722" target="n895" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1308" source="n723" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1309" source="n723" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1310" source="n723" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1311" source="n723" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1312" source="n723" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1313" source="n723" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1314" source="n723" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1315" source="n723" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1316" source="n723" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1317" source="n723" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1318" source="n723" target="n85" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1319" source="n723" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1320" source="n723" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1321" source="n723" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1322" source="n723" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1323" source="n723" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1324" source="n724" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1325" source="n724" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1326" source="n724" target="n897" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1327" source="n724" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1328" source="n724" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1329" source="n724" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1330" source="n724" target="n898" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1331" source="n724" target="n899" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1332" source="n724" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1333" source="n724" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1334" source="n724" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1335" source="n724" target="n810" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1336" source="n724" target="n900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1337" source="n724" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1338" source="n724" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1339" source="n724" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1340" source="n724" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1341" source="n724" target="n902" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1342" source="n724" target="n690" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1343" source="n725" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1344" source="n725" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1345" source="n725" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1346" source="n725" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1347" source="n725" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1348" source="n725" target="n386" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1349" source="n725" target="n903" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1350" source="n725" target="n904" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1351" source="n725" target="n905" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1352" source="n725" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1353" source="n725" target="n42" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1354" source="n725" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1355" source="n725" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1356" source="n725" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1357" source="n725" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1358" source="n725" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1359" source="n725" target="n906" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1360" source="n725" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1361" source="n725" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1362" source="n725" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1363" source="n725" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1364" source="n725" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1365" source="n725" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1366" source="n725" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1367" source="n725" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1368" source="n725" target="n907" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1369" source="n725" target="n908" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1370" source="n725" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1371" source="n725" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1372" source="n725" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1373" source="n725" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1374" source="n725" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1375" source="n725" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1376" source="n725" target="n690" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1377" source="n725" target="n411" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1378" source="n725" target="n909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1379" source="n726" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1380" source="n726" target="n910" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1381" source="n726" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1382" source="n726" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1383" source="n726" target="n911" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1384" source="n726" target="n912" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1385" source="n726" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1386" source="n726" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1387" source="n726" target="n51" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1388" source="n726" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1389" source="n726" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1390" source="n726" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1391" source="n726" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1392" source="n726" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1393" source="n726" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1394" source="n726" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1395" source="n727" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1396" source="n727" target="n913" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1397" source="n727" target="n914" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1398" source="n727" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1399" source="n727" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1400" source="n727" target="n917" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1401" source="n727" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1402" source="n727" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1403" source="n727" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1404" source="n727" target="n918" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1405" source="n727" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1406" source="n728" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1407" source="n728" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1408" source="n729" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1409" source="n729" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1410" source="n729" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1411" source="n729" target="n920" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1412" source="n729" target="n921" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1413" source="n729" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1414" source="n729" target="n206" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1415" source="n729" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1416" source="n729" target="n922" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1417" source="n729" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1418" source="n729" target="n923" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1419" source="n729" target="n924" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1420" source="n729" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1421" source="n729" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1422" source="n730" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1423" source="n730" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1424" source="n730" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1425" source="n730" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1426" source="n730" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1427" source="n730" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1428" source="n730" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1429" source="n730" target="n926" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1430" source="n730" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1431" source="n730" target="n85" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1432" source="n730" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1433" source="n730" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1434" source="n730" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1435" source="n730" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1436" source="n730" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1437" source="n730" target="n927" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1438" source="n730" target="n928" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1439" source="n730" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1440" source="n730" target="n929" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1441" source="n730" target="n930" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1442" source="n730" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1443" source="n731" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1444" source="n731" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1445" source="n731" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1446" source="n731" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1447" source="n731" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1448" source="n731" target="n26" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1449" source="n731" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1450" source="n731" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1451" source="n731" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1452" source="n731" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1453" source="n731" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1454" source="n731" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1455" source="n732" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1456" source="n732" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1457" source="n732" target="n933" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1458" source="n732" target="n934" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1459" source="n732" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1460" source="n732" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1461" source="n732" target="n935" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1462" source="n732" target="n936" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1463" source="n732" target="n937" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1464" source="n732" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1465" source="n732" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1466" source="n732" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1467" source="n732" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1468" source="n733" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1469" source="n733" target="n938" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1470" source="n733" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1471" source="n733" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1472" source="n733" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1473" source="n733" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1474" source="n733" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1475" source="n733" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1476" source="n733" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1477" source="n733" target="n939" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1478" source="n733" target="n940" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1479" source="n733" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1480" source="n733" target="n620" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1481" source="n733" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1482" source="n733" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1483" source="n733" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1484" source="n733" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1485" source="n733" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1486" source="n733" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1487" source="n733" target="n825" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1488" source="n733" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1489" source="n733" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1490" source="n733" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1491" source="n733" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1492" source="n733" target="n103" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1493" source="n733" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1494" source="n733" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1495" source="n734" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1496" source="n734" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1497" source="n734" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1498" source="n734" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1499" source="n734" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1500" source="n734" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1501" source="n734" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1502" source="n734" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1503" source="n734" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1504" source="n734" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1505" source="n734" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1506" source="n734" target="n941" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1507" source="n734" target="n298" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1508" source="n734" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1509" source="n734" target="n883" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1510" source="n734" target="n840" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1511" source="n734" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1512" source="n734" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1513" source="n734" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1514" source="n734" target="n942" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1515" source="n734" target="n943" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1516" source="n734" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1517" source="n734" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1518" source="n734" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1519" source="n734" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1520" source="n734" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1521" source="n734" target="n382" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1522" source="n734" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1523" source="n734" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1524" source="n734" target="n944" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1525" source="n734" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1526" source="n734" target="n344" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1527" source="n734" target="n945" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1528" source="n734" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1529" source="n734" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1530" source="n734" target="n343" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1531" source="n734" target="n946" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1532" source="n734" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1533" source="n734" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1534" source="n734" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1535" source="n734" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1536" source="n734" target="n680" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1537" source="n734" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1538" source="n735" target="n17" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1539" source="n735" target="n948" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1540" source="n735" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1541" source="n735" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1542" source="n735" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1543" source="n735" target="n950" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1544" source="n735" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1545" source="n735" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1546" source="n735" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1547" source="n735" target="n702" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1548" source="n735" target="n951" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1549" source="n735" target="n868" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1550" source="n735" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1551" source="n735" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1552" source="n735" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1553" source="n735" target="n680" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1554" source="n735" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1555" source="n735" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1556" source="n735" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1557" source="n735" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1558" source="n735" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1559" source="n735" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1560" source="n735" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1561" source="n735" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1562" source="n735" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1563" source="n735" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1564" source="n735" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1565" source="n735" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1566" source="n735" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1567" source="n735" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1568" source="n735" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1569" source="n735" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1570" source="n735" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1571" source="n735" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1572" source="n735" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1573" source="n735" target="n296" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1574" source="n735" target="n364" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1575" source="n735" target="n115" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1576" source="n736" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1577" source="n736" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1578" source="n736" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1579" source="n736" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1580" source="n736" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1581" source="n736" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1582" source="n736" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1583" source="n736" target="n954" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1584" source="n736" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1585" source="n736" target="n867" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1586" source="n736" target="n868" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1587" source="n736" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1588" source="n736" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1589" source="n736" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1590" source="n736" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1591" source="n736" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1592" source="n736" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1593" source="n736" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1594" source="n736" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1595" source="n736" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1596" source="n736" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1597" source="n736" target="n955" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1598" source="n736" target="n956" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1599" source="n736" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1600" source="n736" target="n957" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1601" source="n736" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1602" source="n736" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1603" source="n736" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1604" source="n736" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1605" source="n737" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1606" source="n737" target="n618" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1607" source="n737" target="n958" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1608" source="n737" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1609" source="n737" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1610" source="n737" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1611" source="n737" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1612" source="n737" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1613" source="n737" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1614" source="n737" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1615" source="n737" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1616" source="n737" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1617" source="n737" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1618" source="n737" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1619" source="n737" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1620" source="n737" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1621" source="n737" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1622" source="n737" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1623" source="n737" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1624" source="n737" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1625" source="n737" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1626" source="n737" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1627" source="n737" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1628" source="n737" target="n644" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1629" source="n738" target="n959" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1630" source="n738" target="n960" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1631" source="n738" target="n79" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1632" source="n738" target="n961" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1633" source="n738" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1634" source="n738" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1635" source="n738" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1636" source="n738" target="n499" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1637" source="n738" target="n315" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1638" source="n738" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1639" source="n738" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1640" source="n738" target="n305" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1641" source="n738" target="n408" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1642" source="n738" target="n410" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1643" source="n738" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1644" source="n739" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1645" source="n739" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1646" source="n739" target="n364" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1647" source="n739" target="n963" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1648" source="n739" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1649" source="n739" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1650" source="n739" target="n964" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1651" source="n739" target="n965" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1652" source="n739" target="n848" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1653" source="n739" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1654" source="n739" target="n941" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1655" source="n739" target="n966" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1656" source="n739" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1657" source="n739" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1658" source="n739" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1659" source="n739" target="n322" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1660" source="n739" target="n967" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1661" source="n739" target="n968" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1662" source="n740" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1663" source="n740" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1664" source="n740" target="n17" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1665" source="n740" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1666" source="n740" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1667" source="n740" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1668" source="n740" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1669" source="n741" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1670" source="n741" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1671" source="n741" target="n969" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1672" source="n741" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1673" source="n741" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1674" source="n741" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1675" source="n741" target="n825" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1676" source="n741" target="n970" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1677" source="n741" target="n348" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1678" source="n741" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1679" source="n741" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1680" source="n741" target="n972" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1681" source="n741" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1682" source="n741" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1683" source="n741" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1684" source="n742" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1685" source="n743" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1686" source="n743" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1687" source="n743" target="n973" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1688" source="n743" target="n974" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1689" source="n743" target="n975" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1690" source="n743" target="n976" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1691" source="n743" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1692" source="n743" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1693" source="n743" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e1694" source="n704" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1695" source="n705" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1696" source="n706" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1697" source="n707" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1698" source="n708" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1699" source="n709" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1700" source="n710" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1701" source="n711" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1702" source="n712" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1703" source="n713" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1704" source="n714" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1705" source="n715" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1706" source="n716" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1707" source="n717" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1708" source="n718" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1709" source="n719" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1710" source="n720" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1711" source="n721" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1712" source="n722" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1713" source="n723" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1714" source="n724" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1715" source="n725" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1716" source="n726" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1717" source="n727" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1718" source="n728" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1719" source="n729" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1720" source="n730" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1721" source="n731" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1722" source="n732" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1723" source="n733" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1724" source="n734" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1725" source="n735" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1726" source="n736" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1727" source="n737" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1728" source="n738" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1729" source="n739" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1730" source="n740" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1731" source="n741" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1732" source="n742" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1733" source="n743" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e1734" source="n450" target="n25" label="HAS"><data key="label">HAS</data><data key="score">17.307692307692307</data></edge>
<edge id="e1735" source="n450" target="n55" label="HAS"><data key="label">HAS</data><data key="score">19.230769230769234</data></edge>
<edge id="e1736" source="n450" target="n47" label="HAS"><data key="label">HAS</data><data key="score">20.192307692307693</data></edge>
<edge id="e1737" source="n450" target="n744" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1738" source="n450" target="n745" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1739" source="n450" target="n746" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1740" source="n450" target="n747" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1741" source="n450" target="n748" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1742" source="n450" target="n11" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1743" source="n450" target="n679" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1744" source="n450" target="n749" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1745" source="n450" target="n750" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1746" source="n450" target="n751" label="HAS"><data key="label">HAS</data><data key="score">9.615384615384617</data></edge>
<edge id="e1747" source="n450" target="n752" label="HAS"><data key="label">HAS</data><data key="score">10.576923076923077</data></edge>
<edge id="e1748" source="n450" target="n753" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1749" source="n450" target="n531" label="HAS"><data key="label">HAS</data><data key="score">25.0</data></edge>
<edge id="e1750" source="n450" target="n754" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1751" source="n450" target="n755" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1752" source="n450" target="n756" label="HAS"><data key="label">HAS</data><data key="score">10.576923076923077</data></edge>
<edge id="e1753" source="n450" target="n525" label="HAS"><data key="label">HAS</data><data key="score">23.076923076923077</data></edge>
<edge id="e1754" source="n450" target="n757" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1755" source="n450" target="n333" label="HAS"><data key="label">HAS</data><data key="score">33.65384615384615</data></edge>
<edge id="e1756" source="n450" target="n663" label="HAS"><data key="label">HAS</data><data key="score">20.192307692307693</data></edge>
<edge id="e1757" source="n450" target="n8" label="HAS"><data key="label">HAS</data><data key="score">25.0</data></edge>
<edge id="e1758" source="n450" target="n758" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1759" source="n450" target="n759" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1760" source="n450" target="n760" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1761" source="n450" target="n277" label="HAS"><data key="label">HAS</data><data key="score">59.61538461538461</data></edge>
<edge id="e1762" source="n450" target="n684" label="HAS"><data key="label">HAS</data><data key="score">39.42307692307692</data></edge>
<edge id="e1763" source="n450" target="n761" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1764" source="n450" target="n762" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1765" source="n450" target="n763" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1766" source="n450" target="n107" label="HAS"><data key="label">HAS</data><data key="score">25.0</data></edge>
<edge id="e1767" source="n450" target="n764" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1768" source="n450" target="n765" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1769" source="n450" target="n766" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1770" source="n450" target="n76" label="HAS"><data key="label">HAS</data><data key="score">20.192307692307693</data></edge>
<edge id="e1771" source="n450" target="n278" label="HAS"><data key="label">HAS</data><data key="score">49.03846153846153</data></edge>
<edge id="e1772" source="n450" target="n569" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1773" source="n450" target="n767" label="HAS"><data key="label">HAS</data><data key="score">14.423076923076922</data></edge>
<edge id="e1774" source="n450" target="n768" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1775" source="n450" target="n769" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1776" source="n450" target="n378" label="HAS"><data key="label">HAS</data><data key="score">22.115384615384613</data></edge>
<edge id="e1777" source="n450" target="n770" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1778" source="n450" target="n771" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1779" source="n450" target="n688" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1780" source="n450" target="n686" label="HAS"><data key="label">HAS</data><data key="score">9.615384615384617</data></edge>
<edge id="e1781" source="n450" target="n34" label="HAS"><data key="label">HAS</data><data key="score">29.807692307692307</data></edge>
<edge id="e1782" source="n450" target="n772" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1783" source="n450" target="n773" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1784" source="n450" target="n399" label="HAS"><data key="label">HAS</data><data key="score">17.307692307692307</data></edge>
<edge id="e1785" source="n450" target="n398" label="HAS"><data key="label">HAS</data><data key="score">23.076923076923077</data></edge>
<edge id="e1786" source="n450" target="n364" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1787" source="n450" target="n774" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1788" source="n450" target="n161" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1789" source="n450" target="n647" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1790" source="n450" target="n775" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1791" source="n450" target="n41" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1792" source="n450" target="n776" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1793" source="n450" target="n777" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1794" source="n450" target="n778" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1795" source="n450" target="n779" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1796" source="n450" target="n780" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1797" source="n450" target="n781" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1798" source="n450" target="n782" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1799" source="n450" target="n783" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1800" source="n450" target="n784" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1801" source="n450" target="n785" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1802" source="n450" target="n786" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1803" source="n450" target="n787" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1804" source="n450" target="n577" label="HAS"><data key="label">HAS</data><data key="score">14.423076923076922</data></edge>
<edge id="e1805" source="n450" target="n537" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1806" source="n450" target="n788" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1807" source="n450" target="n527" label="HAS"><data key="label">HAS</data><data key="score">19.230769230769234</data></edge>
<edge id="e1808" source="n450" target="n530" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1809" source="n450" target="n349" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1810" source="n450" target="n670" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1811" source="n450" target="n789" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1812" source="n450" target="n790" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1813" source="n450" target="n791" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1814" source="n450" target="n792" label="HAS"><data key="label">HAS</data><data key="score">10.576923076923077</data></edge>
<edge id="e1815" source="n450" target="n793" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1816" source="n450" target="n509" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1817" source="n450" target="n544" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1818" source="n450" target="n794" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1819" source="n450" target="n583" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1820" source="n450" target="n795" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1821" source="n450" target="n599" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1822" source="n450" target="n796" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1823" source="n450" target="n210" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1824" source="n450" target="n797" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1825" source="n450" target="n24" label="HAS"><data key="label">HAS</data><data key="score">10.576923076923077</data></edge>
<edge id="e1826" source="n450" target="n798" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1827" source="n450" target="n799" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1828" source="n450" target="n30" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1829" source="n450" target="n800" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1830" source="n450" target="n801" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1831" source="n450" target="n802" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1832" source="n450" target="n32" label="HAS"><data key="label">HAS</data><data key="score">13.461538461538462</data></edge>
<edge id="e1833" source="n450" target="n56" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1834" source="n450" target="n379" label="HAS"><data key="label">HAS</data><data key="score">20.192307692307693</data></edge>
<edge id="e1835" source="n450" target="n803" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1836" source="n450" target="n804" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1837" source="n450" target="n805" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1838" source="n450" target="n806" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1839" source="n450" target="n807" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1840" source="n450" target="n808" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1841" source="n450" target="n809" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1842" source="n450" target="n810" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1843" source="n450" target="n811" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1844" source="n450" target="n812" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1845" source="n450" target="n813" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1846" source="n450" target="n814" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1847" source="n450" target="n815" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1848" source="n450" target="n816" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1849" source="n450" target="n817" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1850" source="n450" target="n818" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1851" source="n450" target="n69" label="HAS"><data key="label">HAS</data><data key="score">13.461538461538462</data></edge>
<edge id="e1852" source="n450" target="n819" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1853" source="n450" target="n148" label="HAS"><data key="label">HAS</data><data key="score">42.30769230769231</data></edge>
<edge id="e1854" source="n450" target="n284" label="HAS"><data key="label">HAS</data><data key="score">37.5</data></edge>
<edge id="e1855" source="n450" target="n335" label="HAS"><data key="label">HAS</data><data key="score">10.576923076923077</data></edge>
<edge id="e1856" source="n450" target="n360" label="HAS"><data key="label">HAS</data><data key="score">19.230769230769234</data></edge>
<edge id="e1857" source="n450" target="n369" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1858" source="n450" target="n362" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e1859" source="n450" target="n820" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1860" source="n450" target="n239" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1861" source="n450" target="n334" label="HAS"><data key="label">HAS</data><data key="score">15.384615384615385</data></edge>
<edge id="e1862" source="n450" target="n265" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1863" source="n450" target="n680" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1864" source="n450" target="n821" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1865" source="n450" target="n65" label="HAS"><data key="label">HAS</data><data key="score">16.346153846153847</data></edge>
<edge id="e1866" source="n450" target="n33" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e1867" source="n450" target="n559" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1868" source="n450" target="n280" label="HAS"><data key="label">HAS</data><data key="score">16.346153846153847</data></edge>
<edge id="e1869" source="n450" target="n299" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1870" source="n450" target="n290" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1871" source="n450" target="n282" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1872" source="n450" target="n410" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1873" source="n450" target="n37" label="HAS"><data key="label">HAS</data><data key="score">13.461538461538462</data></edge>
<edge id="e1874" source="n450" target="n822" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1875" source="n450" target="n823" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1876" source="n450" target="n824" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1877" source="n450" target="n72" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1878" source="n450" target="n178" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1879" source="n450" target="n565" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1880" source="n450" target="n350" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1881" source="n450" target="n388" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1882" source="n450" target="n193" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e1883" source="n450" target="n21" label="HAS"><data key="label">HAS</data><data key="score">12.5</data></edge>
<edge id="e1884" source="n450" target="n376" label="HAS"><data key="label">HAS</data><data key="score">23.076923076923077</data></edge>
<edge id="e1885" source="n450" target="n323" label="HAS"><data key="label">HAS</data><data key="score">18.269230769230766</data></edge>
<edge id="e1886" source="n450" target="n54" label="HAS"><data key="label">HAS</data><data key="score">15.384615384615385</data></edge>
<edge id="e1887" source="n450" target="n336" label="HAS"><data key="label">HAS</data><data key="score">23.076923076923077</data></edge>
<edge id="e1888" source="n450" target="n825" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1889" source="n450" target="n279" label="HAS"><data key="label">HAS</data><data key="score">8.653846153846153</data></edge>
<edge id="e1890" source="n450" target="n286" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1891" source="n450" target="n287" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1892" source="n450" target="n641" label="HAS"><data key="label">HAS</data><data key="score">15.384615384615385</data></edge>
<edge id="e1893" source="n450" target="n826" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1894" source="n450" target="n60" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e1895" source="n450" target="n121" label="HAS"><data key="label">HAS</data><data key="score">18.269230769230766</data></edge>
<edge id="e1896" source="n450" target="n365" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e1897" source="n450" target="n39" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1898" source="n450" target="n827" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1899" source="n450" target="n828" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1900" source="n450" target="n829" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1901" source="n450" target="n830" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1902" source="n450" target="n831" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1903" source="n450" target="n832" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1904" source="n450" target="n109" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1905" source="n450" target="n154" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1906" source="n450" target="n131" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1907" source="n450" target="n113" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1908" source="n450" target="n101" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1909" source="n450" target="n296" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1910" source="n450" target="n187" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1911" source="n450" target="n833" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1912" source="n450" target="n84" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1913" source="n450" target="n261" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1914" source="n450" target="n86" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1915" source="n450" target="n188" label="HAS"><data key="label">HAS</data><data key="score">17.307692307692307</data></edge>
<edge id="e1916" source="n450" target="n834" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1917" source="n450" target="n64" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1918" source="n450" target="n384" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1919" source="n450" target="n93" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1920" source="n450" target="n835" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1921" source="n450" target="n836" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1922" source="n450" target="n533" label="HAS"><data key="label">HAS</data><data key="score">10.576923076923077</data></edge>
<edge id="e1923" source="n450" target="n837" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1924" source="n450" target="n43" label="HAS"><data key="label">HAS</data><data key="score">17.307692307692307</data></edge>
<edge id="e1925" source="n450" target="n838" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1926" source="n450" target="n381" label="HAS"><data key="label">HAS</data><data key="score">22.115384615384613</data></edge>
<edge id="e1927" source="n450" target="n839" label="HAS"><data key="label">HAS</data><data key="score">8.653846153846153</data></edge>
<edge id="e1928" source="n450" target="n840" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1929" source="n450" target="n644" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1930" source="n450" target="n698" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1931" source="n450" target="n338" label="HAS"><data key="label">HAS</data><data key="score">15.384615384615385</data></edge>
<edge id="e1932" source="n450" target="n650" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1933" source="n450" target="n351" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1934" source="n450" target="n344" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1935" source="n450" target="n841" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1936" source="n450" target="n842" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1937" source="n450" target="n843" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e1938" source="n450" target="n247" label="HAS"><data key="label">HAS</data><data key="score">8.653846153846153</data></edge>
<edge id="e1939" source="n450" target="n844" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1940" source="n450" target="n114" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1941" source="n450" target="n9" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1942" source="n450" target="n845" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1943" source="n450" target="n846" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1944" source="n450" target="n17" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1945" source="n450" target="n847" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1946" source="n450" target="n848" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1947" source="n450" target="n285" label="HAS"><data key="label">HAS</data><data key="score">27.884615384615387</data></edge>
<edge id="e1948" source="n450" target="n849" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1949" source="n450" target="n850" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1950" source="n450" target="n851" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1951" source="n450" target="n852" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1952" source="n450" target="n385" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e1953" source="n450" target="n853" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1954" source="n450" target="n854" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1955" source="n450" target="n855" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1956" source="n450" target="n629" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1957" source="n450" target="n856" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1958" source="n450" target="n857" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1959" source="n450" target="n858" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1960" source="n450" target="n859" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1961" source="n450" target="n860" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e1962" source="n450" target="n618" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1963" source="n450" target="n861" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1964" source="n450" target="n862" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1965" source="n450" target="n75" label="HAS"><data key="label">HAS</data><data key="score">17.307692307692307</data></edge>
<edge id="e1966" source="n450" target="n863" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1967" source="n450" target="n864" label="HAS"><data key="label">HAS</data><data key="score">17.307692307692307</data></edge>
<edge id="e1968" source="n450" target="n865" label="HAS"><data key="label">HAS</data><data key="score">9.615384615384617</data></edge>
<edge id="e1969" source="n450" target="n866" label="HAS"><data key="label">HAS</data><data key="score">8.653846153846153</data></edge>
<edge id="e1970" source="n450" target="n867" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1971" source="n450" target="n868" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1972" source="n450" target="n519" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1973" source="n450" target="n869" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1974" source="n450" target="n366" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1975" source="n450" target="n870" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1976" source="n450" target="n871" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1977" source="n450" target="n872" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1978" source="n450" target="n873" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1979" source="n450" target="n874" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1980" source="n450" target="n875" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1981" source="n450" target="n876" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1982" source="n450" target="n877" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1983" source="n450" target="n878" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1984" source="n450" target="n645" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1985" source="n450" target="n879" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e1986" source="n450" target="n880" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e1987" source="n450" target="n396" label="HAS"><data key="label">HAS</data><data key="score">29.807692307692307</data></edge>
<edge id="e1988" source="n450" target="n50" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1989" source="n450" target="n690" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1990" source="n450" target="n292" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e1991" source="n450" target="n881" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1992" source="n450" target="n882" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1993" source="n450" target="n683" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e1994" source="n450" target="n883" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e1995" source="n450" target="n884" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1996" source="n450" target="n885" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e1997" source="n450" target="n886" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e1998" source="n450" target="n589" label="HAS"><data key="label">HAS</data><data key="score">16.346153846153847</data></edge>
<edge id="e1999" source="n450" target="n887" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2000" source="n450" target="n888" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2001" source="n450" target="n400" label="HAS"><data key="label">HAS</data><data key="score">9.615384615384617</data></edge>
<edge id="e2002" source="n450" target="n889" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2003" source="n450" target="n890" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2004" source="n450" target="n891" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2005" source="n450" target="n892" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2006" source="n450" target="n893" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2007" source="n450" target="n894" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2008" source="n450" target="n895" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2009" source="n450" target="n397" label="HAS"><data key="label">HAS</data><data key="score">20.192307692307693</data></edge>
<edge id="e2010" source="n450" target="n896" label="HAS"><data key="label">HAS</data><data key="score">10.576923076923077</data></edge>
<edge id="e2011" source="n450" target="n85" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2012" source="n450" target="n361" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e2013" source="n450" target="n897" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2014" source="n450" target="n341" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2015" source="n450" target="n898" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2016" source="n450" target="n899" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2017" source="n450" target="n900" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2018" source="n450" target="n901" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e2019" source="n450" target="n664" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2020" source="n450" target="n902" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e2021" source="n450" target="n386" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2022" source="n450" target="n903" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2023" source="n450" target="n904" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2024" source="n450" target="n905" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2025" source="n450" target="n42" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2026" source="n450" target="n906" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2027" source="n450" target="n53" label="HAS"><data key="label">HAS</data><data key="score">29.807692307692307</data></edge>
<edge id="e2028" source="n450" target="n907" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2029" source="n450" target="n908" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2030" source="n450" target="n411" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2031" source="n450" target="n909" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e2032" source="n450" target="n910" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2033" source="n450" target="n5" label="HAS"><data key="label">HAS</data><data key="score">8.653846153846153</data></edge>
<edge id="e2034" source="n450" target="n639" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2035" source="n450" target="n911" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2036" source="n450" target="n912" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2037" source="n450" target="n52" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2038" source="n450" target="n51" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2039" source="n450" target="n913" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2040" source="n450" target="n914" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2041" source="n450" target="n915" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e2042" source="n450" target="n916" label="HAS"><data key="label">HAS</data><data key="score">14.423076923076922</data></edge>
<edge id="e2043" source="n450" target="n917" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2044" source="n450" target="n918" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2045" source="n450" target="n919" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2046" source="n450" target="n920" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2047" source="n450" target="n921" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2048" source="n450" target="n206" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e2049" source="n450" target="n922" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2050" source="n450" target="n649" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e2051" source="n450" target="n923" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2052" source="n450" target="n924" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2053" source="n450" target="n925" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2054" source="n450" target="n926" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2055" source="n450" target="n927" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2056" source="n450" target="n928" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2057" source="n450" target="n648" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2058" source="n450" target="n929" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2059" source="n450" target="n930" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2060" source="n450" target="n628" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e2061" source="n450" target="n26" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2062" source="n450" target="n931" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2063" source="n450" target="n221" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2064" source="n450" target="n932" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e2065" source="n450" target="n933" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2066" source="n450" target="n934" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2067" source="n450" target="n935" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2068" source="n450" target="n936" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2069" source="n450" target="n937" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2070" source="n450" target="n938" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2071" source="n450" target="n518" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2072" source="n450" target="n939" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2073" source="n450" target="n940" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2074" source="n450" target="n293" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e2075" source="n450" target="n620" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2076" source="n450" target="n578" label="HAS"><data key="label">HAS</data><data key="score">21.153846153846153</data></edge>
<edge id="e2077" source="n450" target="n103" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2078" source="n450" target="n82" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e2079" source="n450" target="n941" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2080" source="n450" target="n298" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2081" source="n450" target="n942" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2082" source="n450" target="n943" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2083" source="n450" target="n382" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2084" source="n450" target="n944" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2085" source="n450" target="n945" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2086" source="n450" target="n343" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e2087" source="n450" target="n946" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2088" source="n450" target="n947" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e2089" source="n450" target="n948" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2090" source="n450" target="n949" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2091" source="n450" target="n950" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2092" source="n450" target="n702" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2093" source="n450" target="n951" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2094" source="n450" target="n952" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e2095" source="n450" target="n953" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e2096" source="n450" target="n123" label="HAS"><data key="label">HAS</data><data key="score">8.653846153846153</data></edge>
<edge id="e2097" source="n450" target="n115" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e2098" source="n450" target="n954" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2099" source="n450" target="n955" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e2100" source="n450" target="n956" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2101" source="n450" target="n957" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2102" source="n450" target="n377" label="HAS"><data key="label">HAS</data><data key="score">17.307692307692307</data></edge>
<edge id="e2103" source="n450" target="n958" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2104" source="n450" target="n3" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e2105" source="n450" target="n380" label="HAS"><data key="label">HAS</data><data key="score">8.653846153846153</data></edge>
<edge id="e2106" source="n450" target="n959" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2107" source="n450" target="n960" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2108" source="n450" target="n79" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2109" source="n450" target="n961" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2110" source="n450" target="n962" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e2111" source="n450" target="n499" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2112" source="n450" target="n315" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2113" source="n450" target="n305" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2114" source="n450" target="n408" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2115" source="n450" target="n963" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2116" source="n450" target="n964" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2117" source="n450" target="n965" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2118" source="n450" target="n966" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2119" source="n450" target="n149" label="HAS"><data key="label">HAS</data><data key="score">9.615384615384617</data></edge>
<edge id="e2120" source="n450" target="n322" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2121" source="n450" target="n967" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2122" source="n450" target="n968" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2123" source="n450" target="n111" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2124" source="n450" target="n969" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2125" source="n450" target="n970" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2126" source="n450" target="n348" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2127" source="n450" target="n971" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e2128" source="n450" target="n972" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e2129" source="n450" target="n973" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2130" source="n450" target="n974" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2131" source="n450" target="n975" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e2132" source="n450" target="n976" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e2133" source="n977" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2134" source="n977" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2135" source="n977" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2136" source="n977" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2137" source="n977" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2138" source="n977" target="n1009" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2139" source="n977" target="n1010" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2140" source="n977" target="n1011" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2141" source="n977" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2142" source="n977" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2143" source="n977" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2144" source="n977" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2145" source="n977" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2146" source="n978" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2147" source="n978" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2148" source="n978" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2149" source="n978" target="n1012" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2150" source="n978" target="n1013" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2151" source="n978" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2152" source="n978" target="n1014" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2153" source="n978" target="n1015" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2154" source="n978" target="n1016" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2155" source="n978" target="n1017" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2156" source="n978" target="n1018" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2157" source="n978" target="n1019" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2158" source="n978" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2159" source="n978" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2160" source="n978" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2161" source="n978" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2162" source="n978" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2163" source="n978" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2164" source="n978" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2165" source="n978" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2166" source="n978" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2167" source="n978" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2168" source="n978" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2169" source="n979" target="n1020" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2170" source="n979" target="n1021" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2171" source="n979" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2172" source="n979" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2173" source="n979" target="n1022" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2174" source="n979" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2175" source="n979" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2176" source="n979" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2177" source="n979" target="n1023" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2178" source="n979" target="n614" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2179" source="n979" target="n1024" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2180" source="n979" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2181" source="n979" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2182" source="n979" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2183" source="n979" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2184" source="n980" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2185" source="n980" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2186" source="n980" target="n1025" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2187" source="n980" target="n1026" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2188" source="n980" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2189" source="n980" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2190" source="n980" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2191" source="n980" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2192" source="n980" target="n1027" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2193" source="n980" target="n1028" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2194" source="n980" target="n614" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2195" source="n980" target="n1029" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2196" source="n980" target="n1030" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2197" source="n980" target="n1031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2198" source="n980" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2199" source="n980" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2200" source="n980" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2201" source="n980" target="n1032" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2202" source="n980" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2203" source="n980" target="n429" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2204" source="n980" target="n433" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2205" source="n980" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2206" source="n980" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2207" source="n980" target="n88" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2208" source="n981" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2209" source="n981" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2210" source="n981" target="n17" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2211" source="n981" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2212" source="n981" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2213" source="n981" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2214" source="n981" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2215" source="n981" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2216" source="n981" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2217" source="n981" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2218" source="n981" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2219" source="n981" target="n1033" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2220" source="n981" target="n1034" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2221" source="n981" target="n1035" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2222" source="n981" target="n1028" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2223" source="n981" target="n1036" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2224" source="n982" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2225" source="n982" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2226" source="n982" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2227" source="n982" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2228" source="n982" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2229" source="n982" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2230" source="n982" target="n366" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2231" source="n982" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2232" source="n982" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2233" source="n982" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2234" source="n982" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2235" source="n982" target="n1037" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2236" source="n982" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2237" source="n982" target="n1038" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2238" source="n982" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2239" source="n982" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2240" source="n982" target="n1039" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2241" source="n982" target="n291" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2242" source="n983" target="n1040" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2243" source="n983" target="n630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2244" source="n983" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2245" source="n983" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2246" source="n983" target="n805" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2247" source="n983" target="n1041" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2248" source="n983" target="n856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2249" source="n983" target="n683" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2250" source="n983" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2251" source="n983" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2252" source="n983" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2253" source="n983" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2254" source="n983" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2255" source="n983" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2256" source="n983" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2257" source="n983" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2258" source="n983" target="n1042" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2259" source="n983" target="n1043" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2260" source="n983" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2261" source="n983" target="n1045" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2262" source="n983" target="n1046" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2263" source="n983" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2264" source="n983" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2265" source="n983" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2266" source="n983" target="n1047" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2267" source="n983" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2268" source="n983" target="n1048" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2269" source="n983" target="n1049" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2270" source="n984" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2271" source="n984" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2272" source="n984" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2273" source="n984" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2274" source="n984" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2275" source="n984" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2276" source="n984" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2277" source="n984" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2278" source="n984" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2279" source="n984" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2280" source="n984" target="n386" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2281" source="n984" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2282" source="n984" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2283" source="n984" target="n903" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2284" source="n984" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2285" source="n984" target="n1051" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2286" source="n984" target="n772" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2287" source="n984" target="n905" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2288" source="n984" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2289" source="n984" target="n930" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2290" source="n984" target="n343" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2291" source="n984" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2292" source="n984" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2293" source="n984" target="n328" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2294" source="n984" target="n1052" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2295" source="n984" target="n371" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2296" source="n984" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2297" source="n984" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2298" source="n984" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2299" source="n985" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2300" source="n985" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2301" source="n985" target="n505" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2302" source="n985" target="n1053" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2303" source="n985" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2304" source="n985" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2305" source="n985" target="n48" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2306" source="n985" target="n511" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2307" source="n985" target="n512" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2308" source="n985" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2309" source="n985" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2310" source="n985" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2311" source="n985" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2312" source="n986" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2313" source="n986" target="n6" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2314" source="n986" target="n1054" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2315" source="n986" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2316" source="n986" target="n941" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2317" source="n986" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2318" source="n986" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2319" source="n986" target="n1055" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2320" source="n986" target="n1056" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2321" source="n987" target="n1057" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2322" source="n987" target="n1058" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2323" source="n987" target="n1059" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2324" source="n987" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2325" source="n987" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2326" source="n987" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2327" source="n987" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2328" source="n987" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2329" source="n987" target="n48" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2330" source="n987" target="n49" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2331" source="n987" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2332" source="n987" target="n1060" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2333" source="n987" target="n1030" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2334" source="n987" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2335" source="n987" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2336" source="n987" target="n1061" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2337" source="n987" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2338" source="n987" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2339" source="n987" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2340" source="n987" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2341" source="n987" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2342" source="n987" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2343" source="n987" target="n1062" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2344" source="n987" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2345" source="n987" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2346" source="n987" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2347" source="n987" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2348" source="n987" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2349" source="n987" target="n1063" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2350" source="n987" target="n1064" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2351" source="n987" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2352" source="n987" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2353" source="n987" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2354" source="n987" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2355" source="n987" target="n1065" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2356" source="n987" target="n1066" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2357" source="n987" target="n224" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2358" source="n987" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2359" source="n988" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2360" source="n988" target="n1023" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2361" source="n988" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2362" source="n988" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2363" source="n988" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2364" source="n988" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2365" source="n988" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2366" source="n988" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2367" source="n988" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2368" source="n988" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2369" source="n988" target="n1067" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2370" source="n988" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2371" source="n988" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2372" source="n988" target="n1068" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2373" source="n988" target="n412" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2374" source="n988" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2375" source="n988" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2376" source="n988" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2377" source="n988" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2378" source="n988" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2379" source="n989" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2380" source="n989" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2381" source="n989" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2382" source="n989" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2383" source="n989" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2384" source="n989" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2385" source="n989" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2386" source="n989" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2387" source="n989" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2388" source="n989" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2389" source="n989" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2390" source="n989" target="n1068" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2391" source="n989" target="n412" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2392" source="n989" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2393" source="n989" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2394" source="n989" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2395" source="n989" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2396" source="n989" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2397" source="n989" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2398" source="n989" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2399" source="n990" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2400" source="n990" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2401" source="n990" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2402" source="n990" target="n298" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2403" source="n990" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2404" source="n990" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2405" source="n990" target="n411" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2406" source="n990" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2407" source="n990" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2408" source="n990" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2409" source="n990" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2410" source="n990" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2411" source="n990" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2412" source="n990" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2413" source="n990" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2414" source="n990" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2415" source="n990" target="n1070" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2416" source="n990" target="n903" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2417" source="n990" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2418" source="n990" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2419" source="n990" target="n343" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2420" source="n990" target="n407" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2421" source="n990" target="n57" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2422" source="n991" target="n1071" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2423" source="n991" target="n1072" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2424" source="n991" target="n803" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2425" source="n991" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2426" source="n991" target="n1073" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2427" source="n991" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2428" source="n991" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2429" source="n991" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2430" source="n991" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2431" source="n991" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2432" source="n991" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2433" source="n992" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2434" source="n992" target="n807" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2435" source="n992" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2436" source="n992" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2437" source="n992" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2438" source="n992" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2439" source="n992" target="n1067" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2440" source="n992" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2441" source="n992" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2442" source="n993" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2443" source="n993" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2444" source="n993" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2445" source="n993" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2446" source="n993" target="n51" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2447" source="n993" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2448" source="n993" target="n1074" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2449" source="n993" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2450" source="n993" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2451" source="n993" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2452" source="n993" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2453" source="n993" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2454" source="n994" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2455" source="n994" target="n1076" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2456" source="n994" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2457" source="n994" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2458" source="n994" target="n1077" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2459" source="n994" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2460" source="n994" target="n1078" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2461" source="n994" target="n861" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2462" source="n994" target="n248" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2463" source="n994" target="n418" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2464" source="n994" target="n177" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2465" source="n994" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2466" source="n994" target="n1079" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2467" source="n995" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2468" source="n995" target="n1080" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2469" source="n995" target="n1081" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2470" source="n995" target="n1082" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2471" source="n995" target="n1083" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2472" source="n995" target="n1084" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2473" source="n995" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2474" source="n995" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2475" source="n995" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2476" source="n995" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2477" source="n995" target="n348" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2478" source="n995" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2479" source="n996" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2480" source="n996" target="n1085" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2481" source="n996" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2482" source="n996" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2483" source="n996" target="n1086" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2484" source="n996" target="n1087" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2485" source="n996" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2486" source="n996" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2487" source="n996" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2488" source="n996" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2489" source="n996" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2490" source="n996" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2491" source="n996" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2492" source="n996" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2493" source="n996" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2494" source="n996" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2495" source="n996" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2496" source="n996" target="n40" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2497" source="n996" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2498" source="n997" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2499" source="n997" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2500" source="n997" target="n1067" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2501" source="n997" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2502" source="n997" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2503" source="n997" target="n45" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2504" source="n997" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2505" source="n997" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2506" source="n997" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2507" source="n997" target="n1088" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2508" source="n997" target="n1089" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2509" source="n997" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2510" source="n997" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2511" source="n997" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2512" source="n998" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2513" source="n998" target="n1081" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2514" source="n998" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2515" source="n998" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2516" source="n998" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2517" source="n998" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2518" source="n998" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2519" source="n998" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2520" source="n998" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2521" source="n998" target="n535" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2522" source="n998" target="n345" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2523" source="n998" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2524" source="n998" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2525" source="n998" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2526" source="n998" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2527" source="n998" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2528" source="n999" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2529" source="n999" target="n1091" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2530" source="n999" target="n1092" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2531" source="n999" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2532" source="n999" target="n1093" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2533" source="n999" target="n630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2534" source="n999" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2535" source="n1000" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2536" source="n1000" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2537" source="n1000" target="n45" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2538" source="n1000" target="n1094" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2539" source="n1000" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2540" source="n1000" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2541" source="n1000" target="n1095" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2542" source="n1000" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2543" source="n1000" target="n416" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2544" source="n1000" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2545" source="n1000" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2546" source="n1000" target="n1096" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2547" source="n1000" target="n668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2548" source="n1000" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2549" source="n1000" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2550" source="n1000" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2551" source="n1000" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2552" source="n1000" target="n406" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2553" source="n1000" target="n1097" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2554" source="n1000" target="n1098" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2555" source="n1000" target="n1099" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2556" source="n1000" target="n1100" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2557" source="n1000" target="n1101" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2558" source="n1000" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2559" source="n1001" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2560" source="n1001" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2561" source="n1001" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2562" source="n1001" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2563" source="n1001" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2564" source="n1001" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2565" source="n1001" target="n406" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2566" source="n1001" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2567" source="n1001" target="n45" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2568" source="n1001" target="n1102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2569" source="n1002" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2570" source="n1002" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2571" source="n1002" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2572" source="n1002" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2573" source="n1002" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2574" source="n1002" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2575" source="n1002" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2576" source="n1002" target="n1103" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2577" source="n1002" target="n1063" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2578" source="n1002" target="n26" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2579" source="n1002" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2580" source="n1002" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2581" source="n1003" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2582" source="n1003" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2583" source="n1003" target="n45" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2584" source="n1003" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2585" source="n1003" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2586" source="n1003" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2587" source="n1003" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2588" source="n1003" target="n1104" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2589" source="n1003" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2590" source="n1003" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2591" source="n1003" target="n1087" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2592" source="n1003" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2593" source="n1003" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2594" source="n1003" target="n1105" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2595" source="n1003" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2596" source="n1003" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2597" source="n1003" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2598" source="n1003" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2599" source="n1003" target="n1106" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2600" source="n1003" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2601" source="n1004" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2602" source="n1005" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2603" source="n1005" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2604" source="n1005" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2605" source="n1005" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2606" source="n1005" target="n1107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2607" source="n1005" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2608" source="n1005" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2609" source="n1006" target="n64" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2610" source="n1006" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2611" source="n1006" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2612" source="n1006" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2613" source="n1006" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2614" source="n1006" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2615" source="n1006" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2616" source="n1006" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2617" source="n1006" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2618" source="n1006" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2619" source="n1006" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2620" source="n1006" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2621" source="n1007" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2622" source="n1007" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2623" source="n1007" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2624" source="n1007" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2625" source="n1007" target="n48" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2626" source="n1007" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2627" source="n1007" target="n1034" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2628" source="n1007" target="n1102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2629" source="n1007" target="n1108" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2630" source="n1007" target="n1109" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2631" source="n1008" target="n45" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2632" source="n1008" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2633" source="n1008" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2634" source="n1008" target="n6" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2635" source="n1008" target="n1110" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2636" source="n1008" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2637" source="n1008" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2638" source="n1008" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2639" source="n1008" target="n1111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2640" source="n977" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2641" source="n978" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2642" source="n979" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2643" source="n980" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2644" source="n981" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2645" source="n982" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2646" source="n983" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2647" source="n984" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2648" source="n985" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2649" source="n986" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2650" source="n987" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2651" source="n988" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2652" source="n989" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2653" source="n990" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2654" source="n991" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2655" source="n992" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2656" source="n993" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2657" source="n994" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2658" source="n995" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2659" source="n996" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2660" source="n997" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2661" source="n998" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2662" source="n999" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2663" source="n1000" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2664" source="n1001" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2665" source="n1002" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2666" source="n1003" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2667" source="n1004" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2668" source="n1005" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2669" source="n1006" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2670" source="n1007" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2671" source="n1008" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e2672" source="n451" target="n565" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2673" source="n451" target="n43" label="HAS"><data key="label">HAS</data><data key="score">50.588235294117645</data></edge>
<edge id="e2674" source="n451" target="n111" label="HAS"><data key="label">HAS</data><data key="score">14.117647058823529</data></edge>
<edge id="e2675" source="n451" target="n37" label="HAS"><data key="label">HAS</data><data key="score">35.294117647058826</data></edge>
<edge id="e2676" source="n451" target="n925" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2677" source="n451" target="n1009" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2678" source="n451" target="n1010" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2679" source="n451" target="n1011" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2680" source="n451" target="n148" label="HAS"><data key="label">HAS</data><data key="score">31.76470588235294</data></edge>
<edge id="e2681" source="n451" target="n278" label="HAS"><data key="label">HAS</data><data key="score">68.23529411764706</data></edge>
<edge id="e2682" source="n451" target="n41" label="HAS"><data key="label">HAS</data><data key="score">15.294117647058824</data></edge>
<edge id="e2683" source="n451" target="n279" label="HAS"><data key="label">HAS</data><data key="score">38.82352941176471</data></edge>
<edge id="e2684" source="n451" target="n277" label="HAS"><data key="label">HAS</data><data key="score">45.88235294117647</data></edge>
<edge id="e2685" source="n451" target="n1012" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2686" source="n451" target="n1013" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2687" source="n451" target="n149" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2688" source="n451" target="n1014" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2689" source="n451" target="n1015" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2690" source="n451" target="n1016" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2691" source="n451" target="n1017" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2692" source="n451" target="n1018" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2693" source="n451" target="n1019" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2694" source="n451" target="n525" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2695" source="n451" target="n531" label="HAS"><data key="label">HAS</data><data key="score">20.0</data></edge>
<edge id="e2696" source="n451" target="n21" label="HAS"><data key="label">HAS</data><data key="score">27.058823529411764</data></edge>
<edge id="e2697" source="n451" target="n56" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2698" source="n451" target="n34" label="HAS"><data key="label">HAS</data><data key="score">17.647058823529413</data></edge>
<edge id="e2699" source="n451" target="n396" label="HAS"><data key="label">HAS</data><data key="score">17.647058823529413</data></edge>
<edge id="e2700" source="n451" target="n377" label="HAS"><data key="label">HAS</data><data key="score">14.117647058823529</data></edge>
<edge id="e2701" source="n451" target="n50" label="HAS"><data key="label">HAS</data><data key="score">32.94117647058823</data></edge>
<edge id="e2702" source="n451" target="n1020" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2703" source="n451" target="n1021" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2704" source="n451" target="n1022" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2705" source="n451" target="n518" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e2706" source="n451" target="n663" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2707" source="n451" target="n1023" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2708" source="n451" target="n614" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2709" source="n451" target="n1024" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2710" source="n451" target="n336" label="HAS"><data key="label">HAS</data><data key="score">22.35294117647059</data></edge>
<edge id="e2711" source="n451" target="n54" label="HAS"><data key="label">HAS</data><data key="score">21.176470588235293</data></edge>
<edge id="e2712" source="n451" target="n53" label="HAS"><data key="label">HAS</data><data key="score">36.470588235294116</data></edge>
<edge id="e2713" source="n451" target="n1025" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2714" source="n451" target="n1026" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2715" source="n451" target="n1027" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2716" source="n451" target="n1028" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2717" source="n451" target="n1029" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2718" source="n451" target="n1030" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2719" source="n451" target="n1031" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e2720" source="n451" target="n285" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e2721" source="n451" target="n46" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2722" source="n451" target="n1032" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2723" source="n451" target="n39" label="HAS"><data key="label">HAS</data><data key="score">15.294117647058824</data></edge>
<edge id="e2724" source="n451" target="n429" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2725" source="n451" target="n433" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2726" source="n451" target="n335" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2727" source="n451" target="n32" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2728" source="n451" target="n88" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2729" source="n451" target="n350" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2730" source="n451" target="n17" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2731" source="n451" target="n5" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2732" source="n451" target="n402" label="HAS"><data key="label">HAS</data><data key="score">15.294117647058824</data></edge>
<edge id="e2733" source="n451" target="n403" label="HAS"><data key="label">HAS</data><data key="score">10.588235294117647</data></edge>
<edge id="e2734" source="n451" target="n1033" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2735" source="n451" target="n1034" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2736" source="n451" target="n1035" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2737" source="n451" target="n1036" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2738" source="n451" target="n399" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2739" source="n451" target="n398" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2740" source="n451" target="n362" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2741" source="n451" target="n366" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2742" source="n451" target="n684" label="HAS"><data key="label">HAS</data><data key="score">12.941176470588237</data></edge>
<edge id="e2743" source="n451" target="n107" label="HAS"><data key="label">HAS</data><data key="score">23.52941176470588</data></edge>
<edge id="e2744" source="n451" target="n1037" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2745" source="n451" target="n1038" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2746" source="n451" target="n1039" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2747" source="n451" target="n291" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2748" source="n451" target="n1040" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2749" source="n451" target="n630" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2750" source="n451" target="n629" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e2751" source="n451" target="n805" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2752" source="n451" target="n1041" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2753" source="n451" target="n856" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2754" source="n451" target="n683" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2755" source="n451" target="n280" label="HAS"><data key="label">HAS</data><data key="score">17.647058823529413</data></edge>
<edge id="e2756" source="n451" target="n284" label="HAS"><data key="label">HAS</data><data key="score">24.705882352941178</data></edge>
<edge id="e2757" source="n451" target="n282" label="HAS"><data key="label">HAS</data><data key="score">12.941176470588237</data></edge>
<edge id="e2758" source="n451" target="n221" label="HAS"><data key="label">HAS</data><data key="score">10.588235294117647</data></edge>
<edge id="e2759" source="n451" target="n1042" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2760" source="n451" target="n1043" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e2761" source="n451" target="n1044" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e2762" source="n451" target="n1045" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2763" source="n451" target="n1046" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2764" source="n451" target="n52" label="HAS"><data key="label">HAS</data><data key="score">18.823529411764707</data></edge>
<edge id="e2765" source="n451" target="n1047" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2766" source="n451" target="n55" label="HAS"><data key="label">HAS</data><data key="score">17.647058823529413</data></edge>
<edge id="e2767" source="n451" target="n1048" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2768" source="n451" target="n1049" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2769" source="n451" target="n839" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2770" source="n451" target="n860" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2771" source="n451" target="n386" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2772" source="n451" target="n903" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2773" source="n451" target="n1050" label="HAS"><data key="label">HAS</data><data key="score">11.76470588235294</data></edge>
<edge id="e2774" source="n451" target="n1051" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2775" source="n451" target="n772" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2776" source="n451" target="n905" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2777" source="n451" target="n930" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2778" source="n451" target="n343" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2779" source="n451" target="n558" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2780" source="n451" target="n519" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2781" source="n451" target="n328" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2782" source="n451" target="n1052" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2783" source="n451" target="n371" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2784" source="n451" target="n24" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2785" source="n451" target="n333" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2786" source="n451" target="n121" label="HAS"><data key="label">HAS</data><data key="score">10.588235294117647</data></edge>
<edge id="e2787" source="n451" target="n505" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2788" source="n451" target="n1053" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2789" source="n451" target="n509" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2790" source="n451" target="n48" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2791" source="n451" target="n511" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2792" source="n451" target="n512" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2793" source="n451" target="n6" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e2794" source="n451" target="n1054" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2795" source="n451" target="n941" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2796" source="n451" target="n93" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2797" source="n451" target="n1055" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2798" source="n451" target="n1056" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2799" source="n451" target="n1057" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2800" source="n451" target="n1058" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2801" source="n451" target="n1059" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2802" source="n451" target="n49" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2803" source="n451" target="n1060" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2804" source="n451" target="n1061" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2805" source="n451" target="n1062" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2806" source="n451" target="n385" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2807" source="n451" target="n397" label="HAS"><data key="label">HAS</data><data key="score">12.941176470588237</data></edge>
<edge id="e2808" source="n451" target="n1063" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2809" source="n451" target="n1064" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2810" source="n451" target="n47" label="HAS"><data key="label">HAS</data><data key="score">23.52941176470588</data></edge>
<edge id="e2811" source="n451" target="n1065" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2812" source="n451" target="n1066" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2813" source="n451" target="n224" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2814" source="n451" target="n188" label="HAS"><data key="label">HAS</data><data key="score">16.470588235294116</data></edge>
<edge id="e2815" source="n451" target="n814" label="HAS"><data key="label">HAS</data><data key="score">14.117647058823529</data></edge>
<edge id="e2816" source="n451" target="n338" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2817" source="n451" target="n664" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2818" source="n451" target="n1067" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e2819" source="n451" target="n649" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2820" source="n451" target="n1068" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2821" source="n451" target="n412" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2822" source="n451" target="n577" label="HAS"><data key="label">HAS</data><data key="score">10.588235294117647</data></edge>
<edge id="e2823" source="n451" target="n1069" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2824" source="n451" target="n352" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e2825" source="n451" target="n349" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2826" source="n451" target="n947" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2827" source="n451" target="n298" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2828" source="n451" target="n411" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2829" source="n451" target="n379" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2830" source="n451" target="n75" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e2831" source="n451" target="n365" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2832" source="n451" target="n1070" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2833" source="n451" target="n407" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2834" source="n451" target="n57" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2835" source="n451" target="n1071" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2836" source="n451" target="n1072" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2837" source="n451" target="n803" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2838" source="n451" target="n1073" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2839" source="n451" target="n287" label="HAS"><data key="label">HAS</data><data key="score">10.588235294117647</data></edge>
<edge id="e2840" source="n451" target="n807" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2841" source="n451" target="n51" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2842" source="n451" target="n1074" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2843" source="n451" target="n404" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2844" source="n451" target="n401" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2845" source="n451" target="n1075" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2846" source="n451" target="n334" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e2847" source="n451" target="n11" label="HAS"><data key="label">HAS</data><data key="score">10.588235294117647</data></edge>
<edge id="e2848" source="n451" target="n1076" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2849" source="n451" target="n1077" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2850" source="n451" target="n1078" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2851" source="n451" target="n861" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2852" source="n451" target="n248" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2853" source="n451" target="n418" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2854" source="n451" target="n177" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2855" source="n451" target="n1079" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2856" source="n451" target="n1080" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2857" source="n451" target="n1081" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2858" source="n451" target="n1082" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2859" source="n451" target="n1083" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2860" source="n451" target="n1084" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2861" source="n451" target="n348" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2862" source="n451" target="n1085" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2863" source="n451" target="n1086" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2864" source="n451" target="n1087" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2865" source="n451" target="n303" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2866" source="n451" target="n40" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2867" source="n451" target="n576" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2868" source="n451" target="n45" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2869" source="n451" target="n8" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e2870" source="n451" target="n1088" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2871" source="n451" target="n1089" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2872" source="n451" target="n375" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2873" source="n451" target="n1090" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2874" source="n451" target="n535" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2875" source="n451" target="n345" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2876" source="n451" target="n65" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2877" source="n451" target="n25" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2878" source="n451" target="n178" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2879" source="n451" target="n1091" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2880" source="n451" target="n1092" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2881" source="n451" target="n1093" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2882" source="n451" target="n1094" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2883" source="n451" target="n1095" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2884" source="n451" target="n44" label="HAS"><data key="label">HAS</data><data key="score">14.117647058823529</data></edge>
<edge id="e2885" source="n451" target="n416" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2886" source="n451" target="n1096" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2887" source="n451" target="n668" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2888" source="n451" target="n360" label="HAS"><data key="label">HAS</data><data key="score">11.76470588235294</data></edge>
<edge id="e2889" source="n451" target="n589" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2890" source="n451" target="n406" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e2891" source="n451" target="n1097" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2892" source="n451" target="n1098" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2893" source="n451" target="n1099" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2894" source="n451" target="n1100" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2895" source="n451" target="n1101" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2896" source="n451" target="n645" label="HAS"><data key="label">HAS</data><data key="score">11.76470588235294</data></edge>
<edge id="e2897" source="n451" target="n1102" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2898" source="n451" target="n1103" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2899" source="n451" target="n26" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2900" source="n451" target="n527" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e2901" source="n451" target="n1104" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e2902" source="n451" target="n1105" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2903" source="n451" target="n1106" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e2904" source="n451" target="n578" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e2905" source="n451" target="n1107" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2906" source="n451" target="n64" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e2907" source="n451" target="n1108" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2908" source="n451" target="n1109" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2909" source="n451" target="n1110" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2910" source="n451" target="n1111" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e2911" source="n1112" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2912" source="n1112" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2913" source="n1112" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2914" source="n1112" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2915" source="n1112" target="n415" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2916" source="n1112" target="n27" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2917" source="n1112" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2918" source="n1112" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2919" source="n1112" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2920" source="n1112" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2921" source="n1112" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2922" source="n1112" target="n88" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2923" source="n1112" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2924" source="n1112" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2925" source="n1112" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2926" source="n1112" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2927" source="n1112" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2928" source="n1112" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2929" source="n1112" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2930" source="n1112" target="n127" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2931" source="n1112" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2932" source="n1112" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2933" source="n1112" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2934" source="n1112" target="n183" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2935" source="n1112" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2936" source="n1112" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2937" source="n1112" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2938" source="n1112" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2939" source="n1112" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2940" source="n1112" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2941" source="n1112" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2942" source="n1112" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2943" source="n1112" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2944" source="n1112" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2945" source="n1112" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2946" source="n1113" target="n631" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2947" source="n1113" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2948" source="n1113" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2949" source="n1113" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2950" source="n1113" target="n678" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2951" source="n1113" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2952" source="n1113" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2953" source="n1113" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2954" source="n1114" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2955" source="n1114" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2956" source="n1114" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2957" source="n1114" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2958" source="n1114" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2959" source="n1114" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2960" source="n1114" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2961" source="n1114" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2962" source="n1114" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2963" source="n1114" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2964" source="n1114" target="n623" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2965" source="n1114" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2966" source="n1114" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2967" source="n1114" target="n652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2968" source="n1114" target="n653" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2969" source="n1114" target="n656" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2970" source="n1114" target="n535" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2971" source="n1114" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2972" source="n1114" target="n547" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2973" source="n1114" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2974" source="n1114" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2975" source="n1114" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2976" source="n1114" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2977" source="n1114" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2978" source="n1114" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2979" source="n1114" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2980" source="n1114" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2981" source="n1114" target="n153" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2982" source="n1115" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2983" source="n1115" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2984" source="n1115" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2985" source="n1115" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2986" source="n1115" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2987" source="n1115" target="n832" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2988" source="n1115" target="n187" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2989" source="n1115" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2990" source="n1115" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2991" source="n1116" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2992" source="n1116" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2993" source="n1116" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2994" source="n1116" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2995" source="n1116" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2996" source="n1116" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2997" source="n1117" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2998" source="n1117" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e2999" source="n1117" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3000" source="n1117" target="n98" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3001" source="n1117" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3002" source="n1117" target="n302" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3003" source="n1118" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3004" source="n1118" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3005" source="n1118" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3006" source="n1118" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3007" source="n1118" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3008" source="n1118" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3009" source="n1118" target="n344" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3010" source="n1118" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3011" source="n1118" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3012" source="n1118" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3013" source="n1118" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3014" source="n1118" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3015" source="n1118" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3016" source="n1118" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3017" source="n1118" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3018" source="n1118" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3019" source="n1119" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3020" source="n1119" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3021" source="n1119" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3022" source="n1119" target="n99" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3023" source="n1119" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3024" source="n1119" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3025" source="n1119" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3026" source="n1119" target="n162" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3027" source="n1119" target="n811" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3028" source="n1119" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3029" source="n1120" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3030" source="n1120" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3031" source="n1120" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3032" source="n1120" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3033" source="n1120" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3034" source="n1120" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3035" source="n1120" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3036" source="n1120" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3037" source="n1120" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3038" source="n1120" target="n553" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3039" source="n1120" target="n552" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3040" source="n1120" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3041" source="n1120" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3042" source="n1120" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3043" source="n1120" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3044" source="n1120" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3045" source="n1120" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3046" source="n1120" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3047" source="n1121" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3048" source="n1121" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3049" source="n1121" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3050" source="n1121" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3051" source="n1121" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3052" source="n1121" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3053" source="n1121" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3054" source="n1121" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3055" source="n1121" target="n945" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3056" source="n1121" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3057" source="n1121" target="n346" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3058" source="n1121" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3059" source="n1121" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3060" source="n1121" target="n666" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3061" source="n1121" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3062" source="n1121" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3063" source="n1121" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3064" source="n1121" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3065" source="n1121" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3066" source="n1121" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3067" source="n1122" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3068" source="n1122" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3069" source="n1122" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3070" source="n1123" target="n306" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3071" source="n1123" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3072" source="n1123" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3073" source="n1123" target="n652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3074" source="n1123" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3075" source="n1123" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3076" source="n1123" target="n824" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3077" source="n1123" target="n512" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3078" source="n1123" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3079" source="n1123" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3080" source="n1123" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3081" source="n1123" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3082" source="n1123" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3083" source="n1123" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3084" source="n1123" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3085" source="n1123" target="n407" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3086" source="n1123" target="n429" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3087" source="n1123" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3088" source="n1123" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3089" source="n1123" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3090" source="n1123" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3091" source="n1123" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3092" source="n1123" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3093" source="n1123" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3094" source="n1123" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3095" source="n1123" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3096" source="n1123" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3097" source="n1124" target="n638" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3098" source="n1124" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3099" source="n1124" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3100" source="n1124" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3101" source="n1124" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3102" source="n1124" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3103" source="n1124" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3104" source="n1124" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3105" source="n1124" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3106" source="n1124" target="n1028" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3107" source="n1124" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3108" source="n1124" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3109" source="n1124" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3110" source="n1124" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3111" source="n1124" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3112" source="n1124" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3113" source="n1124" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3114" source="n1124" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3115" source="n1124" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3116" source="n1124" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3117" source="n1124" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3118" source="n1124" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3119" source="n1124" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3120" source="n1124" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3121" source="n1124" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3122" source="n1124" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3123" source="n1125" target="n592" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3124" source="n1125" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3125" source="n1125" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3126" source="n1125" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3127" source="n1125" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3128" source="n1125" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3129" source="n1125" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3130" source="n1126" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3131" source="n1126" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3132" source="n1126" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3133" source="n1126" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3134" source="n1126" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3135" source="n1126" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3136" source="n1126" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3137" source="n1126" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3138" source="n1126" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3139" source="n1126" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3140" source="n1126" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3141" source="n1126" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3142" source="n1127" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3143" source="n1127" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3144" source="n1127" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3145" source="n1127" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3146" source="n1127" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3147" source="n1127" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3148" source="n1127" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3149" source="n1127" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3150" source="n1128" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3151" source="n1128" target="n12" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3152" source="n1128" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3153" source="n1128" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3154" source="n1128" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3155" source="n1128" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3156" source="n1128" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3157" source="n1128" target="n114" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3158" source="n1129" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3159" source="n1129" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3160" source="n1129" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3161" source="n1129" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3162" source="n1129" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3163" source="n1129" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3164" source="n1129" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3165" source="n1129" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3166" source="n1129" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3167" source="n1129" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3168" source="n1129" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3169" source="n1129" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3170" source="n1129" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3171" source="n1129" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3172" source="n1129" target="n702" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3173" source="n1129" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3174" source="n1129" target="n203" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3175" source="n1129" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3176" source="n1129" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3177" source="n1129" target="n642" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3178" source="n1129" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3179" source="n1129" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3180" source="n1130" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3181" source="n1130" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3182" source="n1130" target="n900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3183" source="n1130" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3184" source="n1130" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3185" source="n1130" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3186" source="n1130" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3187" source="n1130" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3188" source="n1130" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3189" source="n1130" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3190" source="n1130" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3191" source="n1130" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3192" source="n1130" target="n185" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3193" source="n1130" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3194" source="n1130" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3195" source="n1130" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3196" source="n1130" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3197" source="n1130" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3198" source="n1131" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3199" source="n1131" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3200" source="n1131" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3201" source="n1131" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3202" source="n1131" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3203" source="n1131" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3204" source="n1131" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3205" source="n1131" target="n425" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3206" source="n1131" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3207" source="n1131" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3208" source="n1131" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3209" source="n1131" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3210" source="n1131" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3211" source="n1131" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3212" source="n1131" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3213" source="n1131" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3214" source="n1131" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3215" source="n1131" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3216" source="n1131" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3217" source="n1131" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3218" source="n1131" target="n652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3219" source="n1131" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3220" source="n1131" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3221" source="n1131" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3222" source="n1131" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3223" source="n1131" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3224" source="n1131" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3225" source="n1131" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3226" source="n1131" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3227" source="n1131" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3228" source="n1131" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3229" source="n1131" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3230" source="n1132" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3231" source="n1132" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3232" source="n1132" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3233" source="n1132" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3234" source="n1132" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3235" source="n1133" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3236" source="n1133" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3237" source="n1133" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3238" source="n1133" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3239" source="n1133" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3240" source="n1133" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3241" source="n1133" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3242" source="n1133" target="n1081" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3243" source="n1133" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3244" source="n1133" target="n535" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3245" source="n1133" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3246" source="n1133" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3247" source="n1133" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3248" source="n1133" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3249" source="n1133" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3250" source="n1133" target="n0" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3251" source="n1133" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3252" source="n1133" target="n51" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3253" source="n1133" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3254" source="n1134" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3255" source="n1134" target="n631" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3256" source="n1134" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3257" source="n1134" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3258" source="n1134" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3259" source="n1134" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3260" source="n1134" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3261" source="n1135" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3262" source="n1135" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3263" source="n1135" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3264" source="n1135" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3265" source="n1135" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3266" source="n1135" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3267" source="n1135" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3268" source="n1135" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3269" source="n1135" target="n623" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3270" source="n1135" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3271" source="n1135" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3272" source="n1135" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3273" source="n1135" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3274" source="n1135" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3275" source="n1135" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3276" source="n1135" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3277" source="n1135" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3278" source="n1135" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3279" source="n1135" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3280" source="n1135" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3281" source="n1135" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3282" source="n1135" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3283" source="n1135" target="n618" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3284" source="n1135" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3285" source="n1135" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3286" source="n1135" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3287" source="n1135" target="n552" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3288" source="n1135" target="n550" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3289" source="n1135" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3290" source="n1136" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3291" source="n1136" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3292" source="n1136" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3293" source="n1136" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3294" source="n1136" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3295" source="n1136" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3296" source="n1136" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3297" source="n1136" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3298" source="n1136" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3299" source="n1136" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3300" source="n1136" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3301" source="n1136" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3302" source="n1136" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3303" source="n1136" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3304" source="n1136" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3305" source="n1136" target="n187" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3306" source="n1137" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3307" source="n1137" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3308" source="n1137" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3309" source="n1137" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3310" source="n1137" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3311" source="n1137" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3312" source="n1137" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3313" source="n1137" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3314" source="n1137" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3315" source="n1137" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3316" source="n1137" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3317" source="n1137" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3318" source="n1137" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3319" source="n1137" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3320" source="n1137" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3321" source="n1137" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3322" source="n1137" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3323" source="n1137" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3324" source="n1137" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3325" source="n1137" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3326" source="n1137" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3327" source="n1137" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3328" source="n1137" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3329" source="n1137" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3330" source="n1137" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3331" source="n1137" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3332" source="n1138" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3333" source="n1138" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3334" source="n1138" target="n623" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3335" source="n1138" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3336" source="n1138" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3337" source="n1138" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3338" source="n1138" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3339" source="n1138" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3340" source="n1138" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3341" source="n1138" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3342" source="n1138" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3343" source="n1138" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3344" source="n1138" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3345" source="n1138" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3346" source="n1139" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3347" source="n1139" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3348" source="n1139" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3349" source="n1139" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3350" source="n1139" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3351" source="n1139" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3352" source="n1139" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3353" source="n1139" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3354" source="n1139" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3355" source="n1139" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3356" source="n1139" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3357" source="n1139" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3358" source="n1139" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3359" source="n1139" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3360" source="n1139" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3361" source="n1139" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3362" source="n1140" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3363" source="n1140" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3364" source="n1140" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3365" source="n1140" target="n367" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3366" source="n1140" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3367" source="n1140" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3368" source="n1140" target="n656" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3369" source="n1140" target="n802" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3370" source="n1140" target="n810" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3371" source="n1140" target="n900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3372" source="n1140" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3373" source="n1140" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3374" source="n1140" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3375" source="n1140" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3376" source="n1140" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3377" source="n1140" target="n318" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3378" source="n1140" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3379" source="n1140" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3380" source="n1140" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3381" source="n1140" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3382" source="n1140" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3383" source="n1140" target="n939" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3384" source="n1140" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3385" source="n1140" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3386" source="n1140" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3387" source="n1140" target="n500" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3388" source="n1140" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3389" source="n1140" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3390" source="n1140" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3391" source="n1140" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3392" source="n1140" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3393" source="n1140" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3394" source="n1140" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3395" source="n1140" target="n631" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3396" source="n1140" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3397" source="n1140" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3398" source="n1140" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3399" source="n1140" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3400" source="n1140" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3401" source="n1140" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3402" source="n1140" target="n140" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3403" source="n1140" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3404" source="n1140" target="n31" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3405" source="n1140" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3406" source="n1140" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3407" source="n1140" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3408" source="n1141" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3409" source="n1141" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3410" source="n1141" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3411" source="n1141" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3412" source="n1141" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3413" source="n1141" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3414" source="n1141" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3415" source="n1141" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3416" source="n1141" target="n642" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3417" source="n1141" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3418" source="n1141" target="n697" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3419" source="n1141" target="n653" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3420" source="n1141" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3421" source="n1141" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3422" source="n1141" target="n832" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3423" source="n1141" target="n233" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3424" source="n1141" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3425" source="n1141" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3426" source="n1141" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3427" source="n1141" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3428" source="n1141" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3429" source="n1141" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3430" source="n1141" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3431" source="n1141" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3432" source="n1141" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3433" source="n1141" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3434" source="n1141" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3435" source="n1141" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3436" source="n1141" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3437" source="n1141" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3438" source="n1141" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3439" source="n1141" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3440" source="n1141" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3441" source="n1141" target="n295" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3442" source="n1141" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3443" source="n1112" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3444" source="n1113" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3445" source="n1114" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3446" source="n1115" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3447" source="n1116" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3448" source="n1117" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3449" source="n1118" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3450" source="n1119" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3451" source="n1120" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3452" source="n1121" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3453" source="n1122" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3454" source="n1123" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3455" source="n1124" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3456" source="n1125" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3457" source="n1126" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3458" source="n1127" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3459" source="n1128" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3460" source="n1129" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3461" source="n1130" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3462" source="n1131" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3463" source="n1132" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3464" source="n1133" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3465" source="n1134" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3466" source="n1135" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3467" source="n1136" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3468" source="n1137" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3469" source="n1138" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3470" source="n1139" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3471" source="n1140" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3472" source="n1141" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e3473" source="n452" target="n401" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3474" source="n452" target="n415" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3475" source="n452" target="n27" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3476" source="n452" target="n221" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3477" source="n452" target="n37" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3478" source="n452" target="n397" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3479" source="n452" target="n88" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3480" source="n452" target="n360" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3481" source="n452" target="n127" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3482" source="n452" target="n46" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3483" source="n452" target="n60" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e3484" source="n452" target="n123" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3485" source="n452" target="n93" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3486" source="n452" target="n744" label="HAS"><data key="label">HAS</data><data key="score">11.711711711711711</data></edge>
<edge id="e3487" source="n452" target="n1090" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3488" source="n452" target="n971" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e3489" source="n452" target="n245" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3490" source="n452" target="n2" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3491" source="n452" target="n153" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e3492" source="n452" target="n138" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e3493" source="n452" target="n832" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3494" source="n452" target="n187" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3495" source="n452" target="n98" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3496" source="n452" target="n302" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3497" source="n452" target="n99" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3498" source="n452" target="n162" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3499" source="n452" target="n811" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3500" source="n452" target="n293" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e3501" source="n452" target="n65" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e3502" source="n452" target="n945" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3503" source="n452" target="n757" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e3504" source="n452" target="n33" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3505" source="n452" target="n306" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3506" source="n452" target="n157" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3507" source="n452" target="n824" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3508" source="n452" target="n407" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3509" source="n452" target="n429" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3510" source="n452" target="n396" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3511" source="n452" target="n363" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3512" source="n452" target="n1028" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3513" source="n452" target="n376" label="HAS"><data key="label">HAS</data><data key="score">8.108108108108109</data></edge>
<edge id="e3514" source="n452" target="n161" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e3515" source="n452" target="n925" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e3516" source="n452" target="n12" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3517" source="n452" target="n339" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e3518" source="n452" target="n82" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e3519" source="n452" target="n203" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3520" source="n452" target="n900" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3521" source="n452" target="n294" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3522" source="n452" target="n185" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3523" source="n452" target="n25" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e3524" source="n452" target="n425" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3525" source="n452" target="n379" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e3526" source="n452" target="n380" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3527" source="n452" target="n1069" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e3528" source="n452" target="n1081" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3529" source="n452" target="n0" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3530" source="n452" target="n51" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3531" source="n452" target="n367" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3532" source="n452" target="n802" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3533" source="n452" target="n810" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3534" source="n452" target="n318" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3535" source="n452" target="n939" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3536" source="n452" target="n303" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e3537" source="n452" target="n140" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3538" source="n452" target="n31" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e3539" source="n452" target="n233" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3540" source="n452" target="n384" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e3541" source="n452" target="n295" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e3542" source="n1292" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3543" source="n1292" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3544" source="n1292" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3545" source="n1292" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3546" source="n1292" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3547" source="n1292" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3548" source="n1292" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3549" source="n1292" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3550" source="n1292" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3551" source="n1292" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3552" source="n1292" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3553" source="n1292" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3554" source="n1292" target="n214" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3555" source="n1292" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3556" source="n1292" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3557" source="n1292" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3558" source="n1292" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3559" source="n1292" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3560" source="n1292" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3561" source="n1292" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3562" source="n1292" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3563" source="n1292" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3564" source="n1292" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3565" source="n1292" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3566" source="n1292" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3567" source="n1292" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3568" source="n1292" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3569" source="n1292" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3570" source="n1293" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3571" source="n1293" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3572" source="n1293" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3573" source="n1293" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3574" source="n1293" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3575" source="n1293" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3576" source="n1293" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3577" source="n1293" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3578" source="n1293" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3579" source="n1293" target="n343" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3580" source="n1293" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3581" source="n1293" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3582" source="n1293" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3583" source="n1293" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3584" source="n1293" target="n295" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3585" source="n1293" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3586" source="n1293" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3587" source="n1293" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3588" source="n1293" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3589" source="n1293" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3590" source="n1293" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3591" source="n1293" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3592" source="n1293" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3593" source="n1293" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3594" source="n1293" target="n434" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3595" source="n1293" target="n205" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3596" source="n1293" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3597" source="n1294" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3598" source="n1294" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3599" source="n1294" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3600" source="n1294" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3601" source="n1294" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3602" source="n1294" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3603" source="n1294" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3604" source="n1294" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3605" source="n1294" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3606" source="n1294" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3607" source="n1294" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3608" source="n1294" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3609" source="n1294" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3610" source="n1294" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3611" source="n1294" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3612" source="n1294" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3613" source="n1294" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3614" source="n1294" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3615" source="n1294" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3616" source="n1294" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3617" source="n1294" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3618" source="n1294" target="n366" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3619" source="n1294" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3620" source="n1294" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3621" source="n1294" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3622" source="n1294" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3623" source="n1294" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3624" source="n1294" target="n206" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3625" source="n1294" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3626" source="n1294" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3627" source="n1294" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3628" source="n1294" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3629" source="n1294" target="n116" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3630" source="n1294" target="n214" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3631" source="n1294" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3632" source="n1294" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3633" source="n1294" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3634" source="n1294" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3635" source="n1294" target="n410" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3636" source="n1294" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3637" source="n1294" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3638" source="n1294" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3639" source="n1294" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3640" source="n1294" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3641" source="n1294" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3642" source="n1294" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3643" source="n1294" target="n382" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3644" source="n1294" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3645" source="n1294" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3646" source="n1294" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3647" source="n1294" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3648" source="n1294" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3649" source="n1294" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3650" source="n1294" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3651" source="n1294" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3652" source="n1294" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3653" source="n1294" target="n407" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3654" source="n1294" target="n430" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3655" source="n1294" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3656" source="n1294" target="n429" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3657" source="n1294" target="n406" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3658" source="n1294" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3659" source="n1294" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3660" source="n1294" target="n26" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3661" source="n1294" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3662" source="n1294" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3663" source="n1294" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3664" source="n1294" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3665" source="n1295" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3666" source="n1295" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3667" source="n1295" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3668" source="n1295" target="n17" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3669" source="n1295" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3670" source="n1295" target="n954" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3671" source="n1295" target="n295" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3672" source="n1295" target="n292" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3673" source="n1295" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3674" source="n1295" target="n387" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3675" source="n1295" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3676" source="n1295" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3677" source="n1295" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3678" source="n1295" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3679" source="n1295" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3680" source="n1295" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3681" source="n1295" target="n210" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3682" source="n1295" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3683" source="n1295" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3684" source="n1296" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3685" source="n1296" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3686" source="n1296" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3687" source="n1296" target="n71" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3688" source="n1296" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3689" source="n1296" target="n272" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3690" source="n1296" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3691" source="n1296" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3692" source="n1296" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3693" source="n1296" target="n367" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3694" source="n1296" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3695" source="n1296" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3696" source="n1296" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3697" source="n1296" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3698" source="n1296" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3699" source="n1296" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3700" source="n1296" target="n249" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3701" source="n1296" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3702" source="n1296" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3703" source="n1296" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3704" source="n1296" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3705" source="n1296" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3706" source="n1296" target="n0" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3707" source="n1296" target="n1" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3708" source="n1296" target="n130" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3709" source="n1296" target="n800" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3710" source="n1296" target="n946" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3711" source="n1296" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3712" source="n1296" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3713" source="n1296" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3714" source="n1296" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3715" source="n1296" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3716" source="n1296" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3717" source="n1296" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3718" source="n1296" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3719" source="n1296" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3720" source="n1296" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3721" source="n1296" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3722" source="n1296" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3723" source="n1296" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3724" source="n1296" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3725" source="n1296" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3726" source="n1296" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3727" source="n1296" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3728" source="n1296" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3729" source="n1296" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3730" source="n1296" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3731" source="n1296" target="n884" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3732" source="n1296" target="n116" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3733" source="n1296" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3734" source="n1296" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3735" source="n1296" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3736" source="n1296" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3737" source="n1296" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3738" source="n1296" target="n382" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3739" source="n1296" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3740" source="n1296" target="n883" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3741" source="n1296" target="n343" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3742" source="n1296" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3743" source="n1296" target="n644" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3744" source="n1296" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3745" source="n1296" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3746" source="n1296" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3747" source="n1296" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3748" source="n1296" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3749" source="n1296" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3750" source="n1296" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3751" source="n1296" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3752" source="n1296" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3753" source="n1296" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3754" source="n1296" target="n368" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3755" source="n1296" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3756" source="n1296" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3757" source="n1296" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3758" source="n1296" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3759" source="n1296" target="n386" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3760" source="n1296" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3761" source="n1296" target="n318" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3762" source="n1296" target="n180" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3763" source="n1296" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3764" source="n1296" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3765" source="n1296" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3766" source="n1296" target="n344" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3767" source="n1296" target="n271" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3768" source="n1297" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3769" source="n1297" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3770" source="n1297" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3771" source="n1297" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3772" source="n1297" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3773" source="n1297" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3774" source="n1297" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3775" source="n1297" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3776" source="n1297" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3777" source="n1297" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3778" source="n1297" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3779" source="n1297" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3780" source="n1297" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3781" source="n1297" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3782" source="n1297" target="n296" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3783" source="n1297" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3784" source="n1297" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3785" source="n1297" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3786" source="n1297" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3787" source="n1297" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3788" source="n1297" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3789" source="n1297" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3790" source="n1297" target="n367" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3791" source="n1297" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3792" source="n1297" target="n180" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3793" source="n1297" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3794" source="n1297" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3795" source="n1297" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3796" source="n1297" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3797" source="n1297" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3798" source="n1297" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3799" source="n1297" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3800" source="n1297" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3801" source="n1297" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3802" source="n1297" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3803" source="n1297" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3804" source="n1297" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3805" source="n1297" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3806" source="n1297" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3807" source="n1297" target="n99" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3808" source="n1297" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3809" source="n1297" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3810" source="n1297" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3811" source="n1297" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3812" source="n1298" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3813" source="n1298" target="n1031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3814" source="n1298" target="n1048" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3815" source="n1298" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3816" source="n1298" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3817" source="n1298" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3818" source="n1298" target="n668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3819" source="n1298" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3820" source="n1298" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3821" source="n1298" target="n417" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3822" source="n1298" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3823" source="n1298" target="n344" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3824" source="n1298" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3825" source="n1298" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3826" source="n1298" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3827" source="n1298" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3828" source="n1298" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3829" source="n1298" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3830" source="n1298" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3831" source="n1298" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3832" source="n1298" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3833" source="n1298" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3834" source="n1298" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3835" source="n1298" target="n407" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3836" source="n1298" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3837" source="n1298" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3838" source="n1298" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3839" source="n1298" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3840" source="n1298" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3841" source="n1298" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3842" source="n1299" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3843" source="n1299" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3844" source="n1299" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3845" source="n1299" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3846" source="n1299" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3847" source="n1299" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3848" source="n1299" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3849" source="n1299" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3850" source="n1299" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3851" source="n1299" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3852" source="n1299" target="n644" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3853" source="n1299" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3854" source="n1299" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3855" source="n1299" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3856" source="n1299" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3857" source="n1299" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3858" source="n1299" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3859" source="n1299" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3860" source="n1299" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3861" source="n1299" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3862" source="n1299" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3863" source="n1299" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3864" source="n1299" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3865" source="n1299" target="n961" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3866" source="n1300" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3867" source="n1300" target="n166" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3868" source="n1300" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3869" source="n1300" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3870" source="n1300" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3871" source="n1300" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3872" source="n1300" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3873" source="n1300" target="n88" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3874" source="n1300" target="n19" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3875" source="n1300" target="n296" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3876" source="n1300" target="n233" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3877" source="n1300" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3878" source="n1300" target="n832" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3879" source="n1300" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3880" source="n1300" target="n550" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3881" source="n1300" target="n240" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3882" source="n1300" target="n153" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3883" source="n1300" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3884" source="n1301" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3885" source="n1301" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3886" source="n1301" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3887" source="n1301" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3888" source="n1301" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3889" source="n1301" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3890" source="n1301" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3891" source="n1301" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3892" source="n1301" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3893" source="n1301" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3894" source="n1301" target="n773" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3895" source="n1301" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3896" source="n1301" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3897" source="n1301" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3898" source="n1301" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3899" source="n1301" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3900" source="n1301" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3901" source="n1301" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3902" source="n1301" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3903" source="n1301" target="n292" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3904" source="n1301" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3905" source="n1301" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3906" source="n1301" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3907" source="n1301" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3908" source="n1301" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3909" source="n1301" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3910" source="n1301" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3911" source="n1301" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3912" source="n1301" target="n432" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3913" source="n1301" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3914" source="n1301" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3915" source="n1301" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3916" source="n1301" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3917" source="n1301" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3918" source="n1301" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3919" source="n1301" target="n101" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3920" source="n1301" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3921" source="n1301" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3922" source="n1301" target="n235" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3923" source="n1301" target="n216" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3924" source="n1301" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3925" source="n1301" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3926" source="n1301" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3927" source="n1301" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3928" source="n1301" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3929" source="n1301" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3930" source="n1301" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3931" source="n1301" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3932" source="n1301" target="n430" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3933" source="n1301" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3934" source="n1301" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3935" source="n1301" target="n99" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3936" source="n1301" target="n139" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3937" source="n1301" target="n180" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3938" source="n1301" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3939" source="n1301" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3940" source="n1301" target="n204" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3941" source="n1301" target="n187" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3942" source="n1301" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3943" source="n1301" target="n417" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3944" source="n1301" target="n668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3945" source="n1301" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3946" source="n1301" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3947" source="n1301" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3948" source="n1301" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3949" source="n1301" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3950" source="n1301" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3951" source="n1301" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3952" source="n1301" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3953" source="n1301" target="n29" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3954" source="n1301" target="n345" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3955" source="n1301" target="n115" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3956" source="n1301" target="n364" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3957" source="n1301" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3958" source="n1301" target="n816" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3959" source="n1301" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3960" source="n1301" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3961" source="n1301" target="n328" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3962" source="n1301" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3963" source="n1301" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3964" source="n1301" target="n206" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3965" source="n1301" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3966" source="n1301" target="n698" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3967" source="n1301" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3968" source="n1301" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3969" source="n1301" target="n214" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3970" source="n1301" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3971" source="n1301" target="n185" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3972" source="n1302" target="n89" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3973" source="n1302" target="n154" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3974" source="n1302" target="n109" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3975" source="n1302" target="n113" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3976" source="n1302" target="n832" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3977" source="n1302" target="n233" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3978" source="n1302" target="n145" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3979" source="n1302" target="n173" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3980" source="n1302" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3981" source="n1302" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3982" source="n1302" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3983" source="n1302" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3984" source="n1302" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3985" source="n1302" target="n87" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3986" source="n1302" target="n131" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3987" source="n1302" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3988" source="n1302" target="n166" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3989" source="n1302" target="n86" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3990" source="n1303" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3991" source="n1303" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3992" source="n1303" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3993" source="n1303" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3994" source="n1303" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3995" source="n1303" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3996" source="n1303" target="n29" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3997" source="n1303" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3998" source="n1303" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e3999" source="n1303" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4000" source="n1304" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4001" source="n1304" target="n6" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4002" source="n1304" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4003" source="n1304" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4004" source="n1304" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4005" source="n1304" target="n295" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4006" source="n1304" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4007" source="n1304" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4008" source="n1304" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4009" source="n1304" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4010" source="n1304" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4011" source="n1304" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4012" source="n1304" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4013" source="n1304" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4014" source="n1304" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4015" source="n1304" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4016" source="n1304" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4017" source="n1304" target="n160" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4018" source="n1304" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4019" source="n1304" target="n301" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4020" source="n1304" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4021" source="n1304" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4022" source="n1304" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4023" source="n1304" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4024" source="n1304" target="n204" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4025" source="n1304" target="n140" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4026" source="n1305" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4027" source="n1305" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4028" source="n1305" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4029" source="n1305" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4030" source="n1305" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4031" source="n1305" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4032" source="n1305" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4033" source="n1305" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4034" source="n1305" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4035" source="n1306" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4036" source="n1306" target="n1" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4037" source="n1306" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4038" source="n1306" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4039" source="n1306" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4040" source="n1306" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4041" source="n1306" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4042" source="n1306" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4043" source="n1306" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4044" source="n1306" target="n140" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4045" source="n1306" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4046" source="n1306" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4047" source="n1306" target="n296" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4048" source="n1306" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4049" source="n1306" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4050" source="n1306" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4051" source="n1306" target="n136" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4052" source="n1306" target="n976" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4053" source="n1306" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4054" source="n1306" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4055" source="n1306" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4056" source="n1306" target="n604" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4057" source="n1306" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4058" source="n1306" target="n127" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4059" source="n1306" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4060" source="n1306" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4061" source="n1306" target="n434" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4062" source="n1306" target="n102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4063" source="n1306" target="n19" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4064" source="n1306" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4065" source="n1306" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4066" source="n1306" target="n242" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4067" source="n1306" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4068" source="n1306" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4069" source="n1306" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4070" source="n1306" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4071" source="n1306" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4072" source="n1306" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4073" source="n1306" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4074" source="n1306" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4075" source="n1306" target="n203" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4076" source="n1306" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4077" source="n1306" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4078" source="n1306" target="n13" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4079" source="n1306" target="n656" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4080" source="n1306" target="n79" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4081" source="n1307" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4082" source="n1307" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4083" source="n1307" target="n877" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4084" source="n1307" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4085" source="n1307" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4086" source="n1307" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4087" source="n1307" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4088" source="n1307" target="n1" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4089" source="n1307" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4090" source="n1307" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4091" source="n1307" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4092" source="n1307" target="n214" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4093" source="n1307" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4094" source="n1307" target="n126" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4095" source="n1307" target="n116" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4096" source="n1307" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4097" source="n1307" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4098" source="n1307" target="n698" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4099" source="n1307" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4100" source="n1307" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4101" source="n1307" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4102" source="n1307" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4103" source="n1307" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4104" source="n1307" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4105" source="n1307" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4106" source="n1307" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4107" source="n1307" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4108" source="n1307" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4109" source="n1307" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4110" source="n1307" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4111" source="n1307" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4112" source="n1307" target="n328" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4113" source="n1307" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4114" source="n1307" target="n797" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4115" source="n1307" target="n904" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4116" source="n1307" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4117" source="n1307" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4118" source="n1307" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4119" source="n1307" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4120" source="n1307" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4121" source="n1307" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4122" source="n1307" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4123" source="n1307" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4124" source="n1307" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4125" source="n1307" target="n644" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4126" source="n1307" target="n840" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4127" source="n1307" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4128" source="n1307" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4129" source="n1307" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4130" source="n1307" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4131" source="n1307" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4132" source="n1307" target="n926" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4133" source="n1307" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4134" source="n1307" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4135" source="n1307" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4136" source="n1307" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4137" source="n1307" target="n944" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4138" source="n1307" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4139" source="n1307" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4140" source="n1307" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4141" source="n1307" target="n773" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4142" source="n1307" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4143" source="n1307" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4144" source="n1307" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4145" source="n1307" target="n1061" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4146" source="n1307" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4147" source="n1307" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4148" source="n1307" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4149" source="n1307" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4150" source="n1307" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4151" source="n1307" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4152" source="n1307" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4153" source="n1307" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4154" source="n1307" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4155" source="n1307" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4156" source="n1307" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4157" source="n1307" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4158" source="n1307" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4159" source="n1307" target="n270" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4160" source="n1307" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4161" source="n1307" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4162" source="n1308" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4163" source="n1308" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4164" source="n1308" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4165" source="n1308" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4166" source="n1308" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4167" source="n1308" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4168" source="n1308" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4169" source="n1308" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4170" source="n1308" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4171" source="n1308" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4172" source="n1308" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4173" source="n1308" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4174" source="n1308" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4175" source="n1308" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4176" source="n1308" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4177" source="n1308" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4178" source="n1308" target="n292" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4179" source="n1308" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4180" source="n1308" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4181" source="n1308" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4182" source="n1308" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4183" source="n1308" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4184" source="n1308" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4185" source="n1308" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4186" source="n1308" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4187" source="n1308" target="n432" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4188" source="n1308" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4189" source="n1308" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4190" source="n1308" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4191" source="n1308" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4192" source="n1308" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4193" source="n1308" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4194" source="n1308" target="n101" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4195" source="n1308" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4196" source="n1308" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4197" source="n1308" target="n235" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4198" source="n1308" target="n216" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4199" source="n1308" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4200" source="n1308" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4201" source="n1308" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4202" source="n1308" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4203" source="n1308" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4204" source="n1308" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4205" source="n1308" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4206" source="n1308" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4207" source="n1308" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4208" source="n1308" target="n430" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4209" source="n1308" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4210" source="n1308" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4211" source="n1308" target="n99" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4212" source="n1308" target="n139" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4213" source="n1308" target="n180" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4214" source="n1308" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4215" source="n1308" target="n204" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4216" source="n1308" target="n187" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4217" source="n1308" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4218" source="n1308" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4219" source="n1308" target="n417" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4220" source="n1308" target="n668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4221" source="n1308" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4222" source="n1308" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4223" source="n1308" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4224" source="n1308" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4225" source="n1308" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4226" source="n1308" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4227" source="n1308" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4228" source="n1308" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4229" source="n1308" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4230" source="n1308" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4231" source="n1308" target="n29" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4232" source="n1308" target="n345" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4233" source="n1308" target="n115" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4234" source="n1308" target="n364" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4235" source="n1308" target="n688" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4236" source="n1308" target="n816" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4237" source="n1308" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4238" source="n1308" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4239" source="n1308" target="n328" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4240" source="n1308" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4241" source="n1308" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4242" source="n1308" target="n206" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4243" source="n1308" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4244" source="n1308" target="n840" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4245" source="n1308" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4246" source="n1308" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4247" source="n1308" target="n214" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4248" source="n1308" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4249" source="n1308" target="n185" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4250" source="n1309" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4251" source="n1309" target="n261" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4252" source="n1309" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4253" source="n1309" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4254" source="n1309" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4255" source="n1309" target="n160" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4256" source="n1309" target="n64" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4257" source="n1309" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4258" source="n1309" target="n388" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4259" source="n1309" target="n240" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4260" source="n1309" target="n126" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4261" source="n1309" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4262" source="n1309" target="n153" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4263" source="n1309" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4264" source="n1309" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4265" source="n1309" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4266" source="n1310" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4267" source="n1310" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4268" source="n1310" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4269" source="n1310" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4270" source="n1310" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4271" source="n1310" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4272" source="n1310" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4273" source="n1310" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4274" source="n1310" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4275" source="n1310" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4276" source="n1310" target="n364" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4277" source="n1310" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4278" source="n1310" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4279" source="n1310" target="n101" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4280" source="n1310" target="n191" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4281" source="n1310" target="n239" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4282" source="n1310" target="n150" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4283" source="n1310" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4284" source="n1311" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4285" source="n1311" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4286" source="n1311" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4287" source="n1311" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4288" source="n1311" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4289" source="n1311" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4290" source="n1311" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4291" source="n1311" target="n249" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4292" source="n1311" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4293" source="n1311" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4294" source="n1311" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4295" source="n1311" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4296" source="n1311" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4297" source="n1311" target="n550" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4298" source="n1311" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4299" source="n1311" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4300" source="n1311" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4301" source="n1311" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4302" source="n1311" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4303" source="n1311" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4304" source="n1311" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4305" source="n1311" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4306" source="n1311" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4307" source="n1311" target="n939" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4308" source="n1311" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4309" source="n1311" target="n642" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4310" source="n1312" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4311" source="n1312" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4312" source="n1312" target="n153" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4313" source="n1312" target="n240" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4314" source="n1312" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4315" source="n1312" target="n388" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4316" source="n1312" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4317" source="n1312" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4318" source="n1312" target="n233" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4319" source="n1312" target="n832" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4320" source="n1312" target="n173" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4321" source="n1312" target="n105" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4322" source="n1312" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4323" source="n1312" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4324" source="n1312" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4325" source="n1312" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4326" source="n1313" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4327" source="n1313" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4328" source="n1313" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4329" source="n1313" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4330" source="n1313" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4331" source="n1313" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4332" source="n1313" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4333" source="n1313" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4334" source="n1313" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4335" source="n1313" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4336" source="n1313" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4337" source="n1313" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4338" source="n1313" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4339" source="n1313" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4340" source="n1313" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4341" source="n1313" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4342" source="n1313" target="n364" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4343" source="n1313" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4344" source="n1313" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4345" source="n1313" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4346" source="n1313" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4347" source="n1313" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4348" source="n1313" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4349" source="n1313" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4350" source="n1313" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4351" source="n1313" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4352" source="n1313" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4353" source="n1313" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4354" source="n1313" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4355" source="n1313" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4356" source="n1313" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4357" source="n1313" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4358" source="n1314" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4359" source="n1314" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4360" source="n1314" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4361" source="n1314" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4362" source="n1314" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4363" source="n1314" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4364" source="n1314" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4365" source="n1314" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4366" source="n1314" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4367" source="n1314" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4368" source="n1314" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4369" source="n1314" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4370" source="n1314" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4371" source="n1314" target="n292" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4372" source="n1314" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4373" source="n1314" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4374" source="n1314" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4375" source="n1314" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4376" source="n1314" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4377" source="n1314" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4378" source="n1314" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4379" source="n1314" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4380" source="n1314" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4381" source="n1314" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4382" source="n1314" target="n773" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4383" source="n1314" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4384" source="n1314" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4385" source="n1314" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4386" source="n1314" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4387" source="n1314" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4388" source="n1314" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4389" source="n1314" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4390" source="n1314" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4391" source="n1314" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4392" source="n1314" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4393" source="n1314" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4394" source="n1314" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4395" source="n1314" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4396" source="n1314" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4397" source="n1314" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4398" source="n1314" target="n430" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4399" source="n1314" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4400" source="n1314" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4401" source="n1314" target="n99" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4402" source="n1314" target="n139" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4403" source="n1314" target="n180" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4404" source="n1314" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4405" source="n1314" target="n204" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4406" source="n1314" target="n187" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4407" source="n1314" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4408" source="n1314" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4409" source="n1314" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4410" source="n1314" target="n417" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4411" source="n1314" target="n668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4412" source="n1314" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4413" source="n1314" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4414" source="n1314" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4415" source="n1314" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4416" source="n1314" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4417" source="n1314" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4418" source="n1314" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4419" source="n1314" target="n364" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4420" source="n1314" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4421" source="n1314" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4422" source="n1314" target="n328" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4423" source="n1314" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4424" source="n1314" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4425" source="n1314" target="n206" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4426" source="n1314" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4427" source="n1314" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4428" source="n1314" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4429" source="n1314" target="n101" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4430" source="n1314" target="n214" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4431" source="n1314" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4432" source="n1314" target="n185" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4433" source="n1315" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4434" source="n1315" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4435" source="n1315" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4436" source="n1315" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4437" source="n1315" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4438" source="n1315" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4439" source="n1315" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4440" source="n1315" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4441" source="n1315" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4442" source="n1315" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4443" source="n1315" target="n348" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4444" source="n1315" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4445" source="n1315" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4446" source="n1315" target="n140" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4447" source="n1315" target="n642" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4448" source="n1315" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4449" source="n1315" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4450" source="n1315" target="n296" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4451" source="n1315" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4452" source="n1315" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4453" source="n1315" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4454" source="n1315" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4455" source="n1315" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4456" source="n1315" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4457" source="n1315" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4458" source="n1315" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4459" source="n1315" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4460" source="n1299" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4461" source="n1299" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4462" source="n1301" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4463" source="n1301" target="n586" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4464" source="n1301" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4465" source="n1298" target="n970" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4466" source="n1292" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4467" source="n1293" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4468" source="n1294" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4469" source="n1295" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4470" source="n1296" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4471" source="n1297" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4472" source="n1298" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4473" source="n1299" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4474" source="n1300" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4475" source="n1301" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4476" source="n1302" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4477" source="n1303" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4478" source="n1304" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4479" source="n1305" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4480" source="n1306" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4481" source="n1307" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4482" source="n1308" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4483" source="n1309" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4484" source="n1310" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4485" source="n1311" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4486" source="n1312" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4487" source="n1313" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4488" source="n1314" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4489" source="n1315" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e4490" source="n450" target="n375" label="HAS"><data key="label">HAS</data><data key="score">9.615384615384617</data></edge>
<edge id="e4491" source="n450" target="n214" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e4492" source="n450" target="n295" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4493" source="n450" target="n63" label="HAS"><data key="label">HAS</data><data key="score">15.384615384615385</data></edge>
<edge id="e4494" source="n450" target="n434" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4495" source="n450" target="n205" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4496" source="n450" target="n374" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e4497" source="n450" target="n363" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e4498" source="n450" target="n44" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4499" source="n450" target="n116" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4500" source="n450" target="n359" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e4501" source="n450" target="n175" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e4502" source="n450" target="n407" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4503" source="n450" target="n430" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4504" source="n450" target="n401" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4505" source="n450" target="n429" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4506" source="n450" target="n406" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4507" source="n450" target="n404" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4508" source="n450" target="n402" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4509" source="n450" target="n387" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4510" source="n450" target="n245" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e4511" source="n450" target="n71" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4512" source="n450" target="n157" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4513" source="n450" target="n272" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4514" source="n450" target="n367" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4515" source="n450" target="n249" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4516" source="n450" target="n2" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e4517" source="n450" target="n0" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4518" source="n450" target="n1" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4519" source="n450" target="n130" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4520" source="n450" target="n383" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e4521" source="n450" target="n368" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4522" source="n450" target="n241" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4523" source="n450" target="n342" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e4524" source="n450" target="n318" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4525" source="n450" target="n180" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4526" source="n450" target="n354" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4527" source="n450" target="n271" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4528" source="n450" target="n622" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4529" source="n450" target="n99" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4530" source="n450" target="n1031" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4531" source="n450" target="n1048" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4532" source="n450" target="n668" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4533" source="n450" target="n1075" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4534" source="n450" target="n417" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4535" source="n450" target="n166" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4536" source="n450" target="n138" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4537" source="n450" target="n88" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4538" source="n450" target="n19" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4539" source="n450" target="n233" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4540" source="n450" target="n550" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4541" source="n450" target="n240" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4542" source="n450" target="n153" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4543" source="n450" target="n432" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4544" source="n450" target="n235" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4545" source="n450" target="n216" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4546" source="n450" target="n139" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4547" source="n450" target="n204" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4548" source="n450" target="n347" label="HAS"><data key="label">HAS</data><data key="score">11.538461538461538</data></edge>
<edge id="e4549" source="n450" target="n29" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4550" source="n450" target="n345" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4551" source="n450" target="n328" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4552" source="n450" target="n185" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4553" source="n450" target="n89" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4554" source="n450" target="n145" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4555" source="n450" target="n173" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4556" source="n450" target="n536" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e4557" source="n450" target="n87" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4558" source="n450" target="n352" label="HAS"><data key="label">HAS</data><data key="score">8.653846153846153</data></edge>
<edge id="e4559" source="n450" target="n6" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4560" source="n450" target="n326" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e4561" source="n450" target="n160" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4562" source="n450" target="n301" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4563" source="n450" target="n140" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e4564" source="n450" target="n256" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e4565" source="n450" target="n136" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4566" source="n450" target="n604" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4567" source="n450" target="n127" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4568" source="n450" target="n294" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4569" source="n450" target="n102" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4570" source="n450" target="n242" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4571" source="n450" target="n203" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4572" source="n450" target="n13" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4573" source="n450" target="n656" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4574" source="n450" target="n126" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4575" source="n450" target="n1061" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4576" source="n450" target="n270" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4577" source="n450" target="n191" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4578" source="n450" target="n150" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4579" source="n450" target="n625" label="HAS"><data key="label">HAS</data><data key="score">5.769230769230769</data></edge>
<edge id="e4580" source="n450" target="n303" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e4581" source="n450" target="n642" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4582" source="n450" target="n105" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e4583" source="n450" target="n337" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e4584" source="n450" target="n355" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4585" source="n450" target="n624" label="HAS"><data key="label">HAS</data><data key="score">4.807692307692308</data></edge>
<edge id="e4586" source="n450" target="n586" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e4587" source="n1434" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4588" source="n1434" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4589" source="n1434" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4590" source="n1434" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4591" source="n1434" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4592" source="n1434" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4593" source="n1434" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4594" source="n1434" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4595" source="n1434" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4596" source="n1434" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4597" source="n1434" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4598" source="n1434" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4599" source="n1434" target="n407" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4600" source="n1434" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4601" source="n1434" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4602" source="n1434" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4603" source="n1434" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4604" source="n1434" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4605" source="n1434" target="n1048" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4606" source="n1434" target="n1031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4607" source="n1434" target="n1043" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4608" source="n1434" target="n512" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4609" source="n1434" target="n1102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4610" source="n1434" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4611" source="n1434" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4612" source="n1434" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4613" source="n1434" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4614" source="n1434" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4615" source="n1434" target="n1095" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4616" source="n1434" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4617" source="n1434" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4618" source="n1434" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4619" source="n1434" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4620" source="n1434" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4621" source="n1434" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4622" source="n1434" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4623" source="n1434" target="n1104" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4624" source="n1434" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4625" source="n1434" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4626" source="n1434" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4627" source="n1434" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4628" source="n1434" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4629" source="n1434" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4630" source="n1434" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4631" source="n1434" target="n180" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4632" source="n1434" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4633" source="n1434" target="n31" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4634" source="n1434" target="n913" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4635" source="n1434" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4636" source="n1434" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4637" source="n1434" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4638" source="n1434" target="n189" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4639" source="n1434" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4640" source="n1434" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4641" source="n1434" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4642" source="n1434" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4643" source="n1434" target="n406" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4644" source="n1434" target="n1068" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4645" source="n1434" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4646" source="n1434" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4647" source="n1434" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4648" source="n1434" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4649" source="n1434" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4650" source="n1434" target="n1032" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4651" source="n1434" target="n1060" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4652" source="n1434" target="n1085" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4653" source="n1434" target="n302" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4654" source="n1434" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4655" source="n1434" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4656" source="n1434" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4657" source="n1434" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4658" source="n1434" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4659" source="n1434" target="n257" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4660" source="n1434" target="n105" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4661" source="n1434" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4662" source="n1434" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4663" source="n1434" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4664" source="n1434" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4665" source="n1434" target="n79" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4666" source="n1434" target="n547" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4667" source="n1434" target="n292" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4668" source="n1434" target="n255" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4669" source="n1434" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4670" source="n1434" target="n100" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4671" source="n1434" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4672" source="n1435" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4673" source="n1435" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4674" source="n1435" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4675" source="n1435" target="n805" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4676" source="n1435" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4677" source="n1435" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4678" source="n1435" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4679" source="n1435" target="n406" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4680" source="n1435" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4681" source="n1435" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4682" source="n1435" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4683" source="n1435" target="n415" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4684" source="n1435" target="n423" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4685" source="n1435" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4686" source="n1435" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4687" source="n1435" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4688" source="n1435" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4689" source="n1435" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4690" source="n1435" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4691" source="n1435" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4692" source="n1435" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4693" source="n1435" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4694" source="n1435" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4695" source="n1435" target="n1041" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4696" source="n1435" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4697" source="n1435" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4698" source="n1435" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4699" source="n1435" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4700" source="n1435" target="n1095" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4701" source="n1435" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4702" source="n1435" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4703" source="n1435" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4704" source="n1435" target="n382" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4705" source="n1435" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4706" source="n1436" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4707" source="n1436" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4708" source="n1436" target="n805" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4709" source="n1436" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4710" source="n1436" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4711" source="n1436" target="n972" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4712" source="n1436" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4713" source="n1436" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4714" source="n1436" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4715" source="n1436" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4716" source="n1436" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4717" source="n1436" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4718" source="n1436" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4719" source="n1436" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4720" source="n1436" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4721" source="n1436" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4722" source="n1436" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4723" source="n1436" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4724" source="n1436" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4725" source="n1436" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4726" source="n1436" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4727" source="n1436" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4728" source="n1436" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4729" source="n1436" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4730" source="n1436" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4731" source="n1436" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4732" source="n1436" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4733" source="n1436" target="n31" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4734" source="n1436" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4735" source="n1436" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4736" source="n1436" target="n668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4737" source="n1436" target="n413" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4738" source="n1436" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4739" source="n1436" target="n1104" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4740" source="n1436" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4741" source="n1436" target="n1030" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4742" source="n1436" target="n1031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4743" source="n1436" target="n1042" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4744" source="n1436" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4745" source="n1436" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4746" source="n1436" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4747" source="n1436" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4748" source="n1437" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4749" source="n1437" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4750" source="n1437" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4751" source="n1437" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4752" source="n1437" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4753" source="n1437" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4754" source="n1437" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4755" source="n1437" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4756" source="n1437" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4757" source="n1437" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4758" source="n1437" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4759" source="n1437" target="n1048" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4760" source="n1437" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4761" source="n1437" target="n1031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4762" source="n1437" target="n1029" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4763" source="n1437" target="n1095" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4764" source="n1437" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4765" source="n1437" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4766" source="n1437" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4767" source="n1437" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4768" source="n1437" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4769" source="n1437" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4770" source="n1437" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4771" source="n1437" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4772" source="n1437" target="n668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4773" source="n1437" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4774" source="n1437" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4775" source="n1437" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4776" source="n1437" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4777" source="n1437" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4778" source="n1437" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4779" source="n1437" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4780" source="n1437" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4781" source="n1437" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4782" source="n1437" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4783" source="n1437" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4784" source="n1438" target="n346" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4785" source="n1438" target="n500" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4786" source="n1438" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4787" source="n1438" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4788" source="n1438" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4789" source="n1438" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4790" source="n1438" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4791" source="n1438" target="n189" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4792" source="n1438" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4793" source="n1438" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4794" source="n1438" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4795" source="n1438" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4796" source="n1438" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4797" source="n1438" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4798" source="n1438" target="n428" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4799" source="n1438" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4800" source="n1438" target="n79" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4801" source="n1438" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4802" source="n1438" target="n58" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4803" source="n1438" target="n166" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4804" source="n1438" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4805" source="n1438" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4806" source="n1438" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4807" source="n1438" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4808" source="n1438" target="n127" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4809" source="n1438" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4810" source="n1438" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4811" source="n1438" target="n434" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4812" source="n1438" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4813" source="n1438" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4814" source="n1438" target="n408" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4815" source="n1438" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4816" source="n1438" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4817" source="n1438" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4818" source="n1438" target="n156" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4819" source="n1438" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4820" source="n1438" target="n92" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4821" source="n1438" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4822" source="n1438" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4823" source="n1439" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4824" source="n1439" target="n1031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4825" source="n1439" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4826" source="n1439" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4827" source="n1439" target="n1104" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4828" source="n1439" target="n1087" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4829" source="n1439" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4830" source="n1439" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4831" source="n1439" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4832" source="n1439" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4833" source="n1439" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4834" source="n1439" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4835" source="n1439" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4836" source="n1439" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4837" source="n1439" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4838" source="n1439" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4839" source="n1439" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4840" source="n1439" target="n824" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4841" source="n1439" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4842" source="n1439" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4843" source="n1439" target="n421" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4844" source="n1439" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4845" source="n1439" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4846" source="n1439" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4847" source="n1439" target="n1048" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4848" source="n1440" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4849" source="n1440" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4850" source="n1440" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4851" source="n1440" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4852" source="n1440" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4853" source="n1440" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4854" source="n1440" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4855" source="n1440" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4856" source="n1440" target="n1043" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4857" source="n1440" target="n1027" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4858" source="n1440" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4859" source="n1440" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4860" source="n1440" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4861" source="n1440" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4862" source="n1440" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4863" source="n1440" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4864" source="n1440" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4865" source="n1440" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4866" source="n1440" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4867" source="n1440" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4868" source="n1440" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4869" source="n1440" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4870" source="n1440" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4871" source="n1440" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4872" source="n1440" target="n51" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4873" source="n1440" target="n803" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4874" source="n1440" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4875" source="n1441" target="n36" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4876" source="n1441" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4877" source="n1441" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4878" source="n1441" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4879" source="n1441" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4880" source="n1441" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4881" source="n1441" target="n430" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4882" source="n1441" target="n410" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4883" source="n1441" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4884" source="n1441" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4885" source="n1441" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4886" source="n1441" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4887" source="n1441" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4888" source="n1441" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4889" source="n1441" target="n292" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4890" source="n1441" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4891" source="n1441" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4892" source="n1441" target="n436" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4893" source="n1441" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4894" source="n1441" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4895" source="n1441" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4896" source="n1441" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4897" source="n1441" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4898" source="n1441" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4899" source="n1442" target="n79" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4900" source="n1442" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4901" source="n1442" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4902" source="n1442" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4903" source="n1442" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4904" source="n1442" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4905" source="n1442" target="n189" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4906" source="n1442" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4907" source="n1442" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4908" source="n1442" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4909" source="n1442" target="n102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4910" source="n1442" target="n434" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4911" source="n1442" target="n954" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4912" source="n1442" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4913" source="n1442" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4914" source="n1442" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4915" source="n1442" target="n421" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4916" source="n1442" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4917" source="n1442" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4918" source="n1442" target="n81" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4919" source="n1442" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4920" source="n1442" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4921" source="n1442" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4922" source="n1442" target="n27" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4923" source="n1442" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4924" source="n1442" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4925" source="n1442" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4926" source="n1442" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4927" source="n1442" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4928" source="n1442" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4929" source="n1442" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4930" source="n1442" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4931" source="n1442" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4932" source="n1442" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4933" source="n1442" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4934" source="n1442" target="n127" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4935" source="n1442" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4936" source="n1442" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4937" source="n1442" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4938" source="n1442" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4939" source="n1442" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4940" source="n1442" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4941" source="n1443" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4942" source="n1443" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4943" source="n1443" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4944" source="n1443" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4945" source="n1443" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4946" source="n1443" target="n189" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4947" source="n1443" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4948" source="n1443" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4949" source="n1443" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4950" source="n1443" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4951" source="n1443" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4952" source="n1443" target="n401" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4953" source="n1443" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4954" source="n1443" target="n406" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4955" source="n1443" target="n1068" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4956" source="n1443" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4957" source="n1443" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4958" source="n1443" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4959" source="n1443" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4960" source="n1443" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4961" source="n1443" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4962" source="n1443" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4963" source="n1443" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4964" source="n1443" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4965" source="n1443" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4966" source="n1443" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4967" source="n1443" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4968" source="n1443" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4969" source="n1443" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4970" source="n1443" target="n1060" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4971" source="n1443" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4972" source="n1443" target="n295" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4973" source="n1443" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4974" source="n1443" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4975" source="n1443" target="n950" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4976" source="n1443" target="n1032" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4977" source="n1443" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4978" source="n1443" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4979" source="n1443" target="n302" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4980" source="n1443" target="n88" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4981" source="n1443" target="n428" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4982" source="n1443" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4983" source="n1443" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4984" source="n1443" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4985" source="n1443" target="n100" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4986" source="n1443" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4987" source="n1443" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4988" source="n1443" target="n79" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4989" source="n1444" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4990" source="n1444" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4991" source="n1444" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4992" source="n1444" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4993" source="n1444" target="n81" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4994" source="n1444" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4995" source="n1444" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4996" source="n1444" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4997" source="n1444" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4998" source="n1444" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e4999" source="n1444" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5000" source="n1444" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5001" source="n1444" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5002" source="n1444" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5003" source="n1444" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5004" source="n1444" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5005" source="n1444" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5006" source="n1444" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5007" source="n1444" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5008" source="n1445" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5009" source="n1445" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5010" source="n1445" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5011" source="n1445" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5012" source="n1445" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5013" source="n1445" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5014" source="n1445" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5015" source="n1445" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5016" source="n1445" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5017" source="n1445" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5018" source="n1445" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5019" source="n1445" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5020" source="n1445" target="n1042" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5021" source="n1445" target="n964" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5022" source="n1445" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5023" source="n1445" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5024" source="n1445" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5025" source="n1445" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5026" source="n1445" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5027" source="n1446" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5028" source="n1446" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5029" source="n1446" target="n421" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5030" source="n1446" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5031" source="n1446" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5032" source="n1446" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5033" source="n1446" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5034" source="n1446" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5035" source="n1446" target="n180" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5036" source="n1446" target="n434" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5037" source="n1446" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5038" source="n1446" target="n102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5039" source="n1446" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5040" source="n1446" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5041" source="n1446" target="n296" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5042" source="n1446" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5043" source="n1446" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5044" source="n1446" target="n373" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5045" source="n1446" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5046" source="n1446" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5047" source="n1446" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5048" source="n1446" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5049" source="n1446" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5050" source="n1446" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5051" source="n1446" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5052" source="n1446" target="n259" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5053" source="n1446" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5054" source="n1446" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5055" source="n1446" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5056" source="n1446" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5057" source="n1446" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5058" source="n1446" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5059" source="n1446" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5060" source="n1446" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5061" source="n1446" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5062" source="n1446" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5063" source="n1446" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5064" source="n1446" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5065" source="n1447" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5066" source="n1447" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5067" source="n1447" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5068" source="n1447" target="n406" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5069" source="n1447" target="n1023" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5070" source="n1447" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5071" source="n1447" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5072" source="n1447" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5073" source="n1447" target="n614" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5074" source="n1447" target="n863" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5075" source="n1447" target="n1087" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5076" source="n1447" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5077" source="n1447" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5078" source="n1447" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5079" source="n1447" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5080" source="n1447" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5081" source="n1447" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5082" source="n1447" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5083" source="n1447" target="n1030" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5084" source="n1447" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5085" source="n1447" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5086" source="n1448" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5087" source="n1448" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5088" source="n1448" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5089" source="n1448" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5090" source="n1448" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5091" source="n1449" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5092" source="n1449" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5093" source="n1434" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5094" source="n1435" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5095" source="n1436" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5096" source="n1437" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5097" source="n1438" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5098" source="n1439" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5099" source="n1440" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5100" source="n1441" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5101" source="n1442" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5102" source="n1443" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5103" source="n1444" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5104" source="n1445" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5105" source="n1446" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5106" source="n1447" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5107" source="n1448" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5108" source="n1449" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5109" source="n451" target="n33" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5110" source="n451" target="n376" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e5111" source="n451" target="n876" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e5112" source="n451" target="n359" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5113" source="n451" target="n949" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5114" source="n451" target="n915" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5115" source="n451" target="n380" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5116" source="n451" target="n76" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e5117" source="n451" target="n180" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5118" source="n451" target="n31" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5119" source="n451" target="n913" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5120" source="n451" target="n256" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5121" source="n451" target="n189" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5122" source="n451" target="n622" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e5123" source="n451" target="n323" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5124" source="n451" target="n361" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5125" source="n451" target="n302" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5126" source="n451" target="n896" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5127" source="n451" target="n63" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e5128" source="n451" target="n69" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e5129" source="n451" target="n257" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5130" source="n451" target="n105" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5131" source="n451" target="n339" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5132" source="n451" target="n79" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5133" source="n451" target="n547" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5134" source="n451" target="n292" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5135" source="n451" target="n255" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5136" source="n451" target="n175" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e5137" source="n451" target="n100" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5138" source="n451" target="n415" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5139" source="n451" target="n423" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5140" source="n451" target="n193" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e5141" source="n451" target="n962" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5142" source="n451" target="n381" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e5143" source="n451" target="n286" label="HAS"><data key="label">HAS</data><data key="score">12.941176470588237</data></edge>
<edge id="e5144" source="n451" target="n382" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5145" source="n451" target="n972" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5146" source="n451" target="n363" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5147" source="n451" target="n374" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5148" source="n451" target="n413" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5149" source="n451" target="n647" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5150" source="n451" target="n533" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5151" source="n451" target="n383" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5152" source="n451" target="n337" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5153" source="n451" target="n355" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5154" source="n451" target="n346" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5155" source="n451" target="n500" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5156" source="n451" target="n651" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5157" source="n451" target="n530" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5158" source="n451" target="n247" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5159" source="n451" target="n428" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5160" source="n451" target="n58" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5161" source="n451" target="n166" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5162" source="n451" target="n127" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5163" source="n451" target="n434" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5164" source="n451" target="n123" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5165" source="n451" target="n293" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e5166" source="n451" target="n408" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5167" source="n451" target="n265" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5168" source="n451" target="n156" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5169" source="n451" target="n72" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5170" source="n451" target="n92" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5171" source="n451" target="n161" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5172" source="n451" target="n824" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5173" source="n451" target="n421" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5174" source="n451" target="n241" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5175" source="n451" target="n36" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5176" source="n451" target="n430" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5177" source="n451" target="n410" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5178" source="n451" target="n436" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5179" source="n451" target="n290" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5180" source="n451" target="n102" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5181" source="n451" target="n954" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5182" source="n451" target="n81" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5183" source="n451" target="n84" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5184" source="n451" target="n27" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5185" source="n451" target="n294" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5186" source="n451" target="n341" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5187" source="n451" target="n625" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5188" source="n451" target="n295" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5189" source="n451" target="n950" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5190" source="n451" target="n326" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5191" source="n451" target="n964" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5192" source="n451" target="n296" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5193" source="n451" target="n342" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5194" source="n451" target="n373" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5195" source="n451" target="n259" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5196" source="n451" target="n3" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e5197" source="n451" target="n863" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5198" source="n1587" target="n1034" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5199" source="n1587" target="n278" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5200" source="n1587" target="n1613" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5201" source="n1587" target="n1614" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5202" source="n1587" target="n148" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5203" source="n1587" target="n1615" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5204" source="n1587" target="n814" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5205" source="n1587" target="n1616" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5206" source="n1587" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5207" source="n1588" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5208" source="n1588" target="n278" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5209" source="n1588" target="n39" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5210" source="n1588" target="n1617" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5211" source="n1588" target="n50" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5212" source="n1588" target="n876" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5213" source="n1588" target="n1618" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5214" source="n1588" target="n221" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5215" source="n1588" target="n1029" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5216" source="n1588" target="n1619" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5217" source="n1588" target="n1620" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5218" source="n1588" target="n1621" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5219" source="n1588" target="n1622" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5220" source="n1588" target="n1623" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5221" source="n1588" target="n1624" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5222" source="n1588" target="n1625" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5223" source="n1588" target="n1626" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5224" source="n1588" target="n1627" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5225" source="n1588" target="n1628" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5226" source="n1588" target="n1629" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5227" source="n1588" target="n1630" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5228" source="n1588" target="n1631" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5229" source="n1588" target="n1632" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5230" source="n1588" target="n1633" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5231" source="n1588" target="n1634" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5232" source="n1588" target="n44" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5233" source="n1588" target="n53" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5234" source="n1588" target="n55" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5235" source="n1589" target="n1635" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5236" source="n1589" target="n1015" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5237" source="n1589" target="n1636" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5238" source="n1589" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5239" source="n1589" target="n1041" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5240" source="n1589" target="n863" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5241" source="n1589" target="n775" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5242" source="n1589" target="n37" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5243" source="n1589" target="n1637" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5244" source="n1589" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5245" source="n1589" target="n1638" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5246" source="n1589" target="n1639" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5247" source="n1590" target="n1640" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5248" source="n1590" target="n1641" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5249" source="n1590" target="n53" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5250" source="n1590" target="n1642" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5251" source="n1590" target="n1643" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5252" source="n1590" target="n1644" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5253" source="n1590" target="n1645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5254" source="n1590" target="n1646" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5255" source="n1590" target="n1647" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5256" source="n1590" target="n411" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5257" source="n1590" target="n1648" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5258" source="n1591" target="n509" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5259" source="n1591" target="n34" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5260" source="n1591" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5261" source="n1592" target="n647" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5262" source="n1592" target="n522" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5263" source="n1592" target="n1649" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5264" source="n1592" target="n1650" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5265" source="n1592" target="n1651" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5266" source="n1592" target="n525" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5267" source="n1592" target="n1652" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5268" source="n1592" target="n826" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5269" source="n1593" target="n34" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5270" source="n1593" target="n1653" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5271" source="n1593" target="n1654" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5272" source="n1593" target="n1655" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5273" source="n1594" target="n1656" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5274" source="n1595" target="n169" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5275" source="n1595" target="n17" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5276" source="n1595" target="n1657" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5277" source="n1595" target="n1658" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5278" source="n1595" target="n1659" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5279" source="n1595" target="n1660" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5280" source="n1595" target="n108" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5281" source="n1595" target="n1661" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5282" source="n1595" target="n565" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5283" source="n1595" target="n1662" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5284" source="n1595" target="n58" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5285" source="n1596" target="n1638" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5286" source="n1597" target="n1663" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5287" source="n1597" target="n1664" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5288" source="n1597" target="n1665" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5289" source="n1597" target="n1666" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5290" source="n1597" target="n1667" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5291" source="n1598" target="n1668" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5292" source="n1598" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5293" source="n1598" target="n1669" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5294" source="n1598" target="n1670" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5295" source="n1598" target="n1044" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5296" source="n1598" target="n876" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5297" source="n1598" target="n776" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5298" source="n1599" target="n34" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5299" source="n1599" target="n655" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5300" source="n1599" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5301" source="n1599" target="n1671" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5302" source="n1599" target="n1672" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5303" source="n1599" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5304" source="n1599" target="n46" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5305" source="n1599" target="n1663" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5306" source="n1599" target="n1050" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5307" source="n1600" target="n1663" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5308" source="n1601" target="n1673" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5309" source="n1601" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5310" source="n1602" target="n1674" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5311" source="n1603" target="n34" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5312" source="n1603" target="n1638" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5313" source="n1604" target="n1675" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5314" source="n1604" target="n1676" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5315" source="n1604" target="n1677" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5316" source="n1604" target="n1067" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5317" source="n1604" target="n1678" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5318" source="n1604" target="n1679" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5319" source="n1604" target="n1680" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5320" source="n1604" target="n1681" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5321" source="n1604" target="n1682" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5322" source="n1604" target="n279" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5323" source="n1604" target="n1651" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5324" source="n1604" target="n1683" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5325" source="n1604" target="n1684" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5326" source="n1604" target="n1685" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5327" source="n1604" target="n1686" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5328" source="n1605" target="n1687" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5329" source="n1605" target="n44" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5330" source="n1605" target="n1688" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5331" source="n1605" target="n1689" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5332" source="n1605" target="n1690" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5333" source="n1605" target="n1691" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5334" source="n1605" target="n1692" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5335" source="n1606" target="n1693" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5336" source="n1606" target="n1694" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5337" source="n1606" target="n1695" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5338" source="n1607" target="n108" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5339" source="n1607" target="n80" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5340" source="n1607" target="n565" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5341" source="n1607" target="n17" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5342" source="n1607" target="n1696" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5343" source="n1607" target="n1697" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5344" source="n1607" target="n1660" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5345" source="n1607" target="n1698" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5346" source="n1607" target="n1699" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5347" source="n1607" target="n1700" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5348" source="n1607" target="n1701" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5349" source="n1607" target="n1702" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5350" source="n1608" target="n1703" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5351" source="n1608" target="n34" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5352" source="n1608" target="n161" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5353" source="n1608" target="n1704" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5354" source="n1608" target="n1705" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5355" source="n1609" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5356" source="n1609" target="n1706" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5357" source="n1610" target="n52" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5358" source="n1610" target="n1707" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5359" source="n1611" target="n1708" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5360" source="n1611" target="n1709" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5361" source="n1612" target="n17" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5362" source="n1612" target="n565" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5363" source="n1612" target="n108" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5364" source="n1612" target="n58" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5365" source="n1612" target="n5" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e5366" source="n1710" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5367" source="n1710" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5368" source="n1710" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5369" source="n1710" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5370" source="n1710" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5371" source="n1710" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5372" source="n1710" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5373" source="n1710" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5374" source="n1710" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5375" source="n1710" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5376" source="n1710" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5377" source="n1710" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5378" source="n1710" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5379" source="n1710" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5380" source="n1710" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5381" source="n1710" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5382" source="n1710" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5383" source="n1710" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5384" source="n1710" target="n1735" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5385" source="n1710" target="n64" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5386" source="n1710" target="n1736" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5387" source="n1710" target="n1737" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5388" source="n1710" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5389" source="n1710" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5390" source="n1710" target="n1083" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5391" source="n1710" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5392" source="n1710" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5393" source="n1710" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5394" source="n1710" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5395" source="n1710" target="n430" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5396" source="n1710" target="n499" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5397" source="n1710" target="n1738" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5398" source="n1710" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5399" source="n1710" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5400" source="n1710" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5401" source="n1710" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5402" source="n1710" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5403" source="n1710" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5404" source="n1710" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5405" source="n1710" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5406" source="n1710" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5407" source="n1710" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5408" source="n1710" target="n1709" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5409" source="n1711" target="n1739" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5410" source="n1711" target="n1740" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5411" source="n1711" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5412" source="n1711" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5413" source="n1711" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5414" source="n1711" target="n1741" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5415" source="n1711" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5416" source="n1711" target="n1742" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5417" source="n1711" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5418" source="n1711" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5419" source="n1711" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5420" source="n1712" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5421" source="n1712" target="n1743" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5422" source="n1712" target="n910" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5423" source="n1712" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5424" source="n1712" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5425" source="n1712" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5426" source="n1712" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5427" source="n1712" target="n853" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5428" source="n1712" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5429" source="n1712" target="n911" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5430" source="n1712" target="n912" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5431" source="n1712" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5432" source="n1712" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5433" source="n1712" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5434" source="n1712" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5435" source="n1712" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5436" source="n1712" target="n51" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5437" source="n1712" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5438" source="n1712" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5439" source="n1712" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5440" source="n1712" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5441" source="n1712" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5442" source="n1712" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5443" source="n1713" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5444" source="n1713" target="n31" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5445" source="n1713" target="n1071" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5446" source="n1713" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5447" source="n1713" target="n159" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5448" source="n1713" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5449" source="n1713" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5450" source="n1713" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5451" source="n1713" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5452" source="n1713" target="n951" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5453" source="n1713" target="n1744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5454" source="n1713" target="n1745" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5455" source="n1713" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5456" source="n1713" target="n1023" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5457" source="n1713" target="n1746" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5458" source="n1713" target="n964" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5459" source="n1713" target="n614" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5460" source="n1713" target="n1747" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5461" source="n1713" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5462" source="n1713" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5463" source="n1713" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5464" source="n1713" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5465" source="n1713" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5466" source="n1713" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5467" source="n1713" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5468" source="n1713" target="n412" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5469" source="n1713" target="n1748" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5470" source="n1713" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5471" source="n1713" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5472" source="n1713" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5473" source="n1713" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5474" source="n1713" target="n1749" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5475" source="n1713" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5476" source="n1714" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5477" source="n1714" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5478" source="n1714" target="n1630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5479" source="n1714" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5480" source="n1714" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5481" source="n1714" target="n635" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5482" source="n1714" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5483" source="n1714" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5484" source="n1714" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5485" source="n1714" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5486" source="n1714" target="n803" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5487" source="n1714" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5488" source="n1714" target="n630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5489" source="n1714" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5490" source="n1714" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5491" source="n1715" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5492" source="n1715" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5493" source="n1715" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5494" source="n1715" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5495" source="n1715" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5496" source="n1715" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5497" source="n1715" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5498" source="n1715" target="n1750" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5499" source="n1715" target="n1617" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5500" source="n1715" target="n1048" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5501" source="n1715" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5502" source="n1715" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5503" source="n1715" target="n1031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5504" source="n1715" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5505" source="n1715" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5506" source="n1715" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5507" source="n1715" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5508" source="n1715" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5509" source="n1715" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5510" source="n1715" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5511" source="n1715" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5512" source="n1715" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5513" source="n1715" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5514" source="n1715" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5515" source="n1715" target="n1751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5516" source="n1715" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5517" source="n1716" target="n30" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5518" source="n1716" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5519" source="n1716" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5520" source="n1716" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5521" source="n1716" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5522" source="n1716" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5523" source="n1716" target="n1085" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5524" source="n1716" target="n1752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5525" source="n1716" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5526" source="n1716" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5527" source="n1716" target="n1753" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5528" source="n1716" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5529" source="n1716" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5530" source="n1716" target="n825" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5531" source="n1716" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5532" source="n1716" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5533" source="n1716" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5534" source="n1717" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5535" source="n1717" target="n1704" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5536" source="n1717" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5537" source="n1717" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5538" source="n1717" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5539" source="n1717" target="n1709" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5540" source="n1717" target="n1754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5541" source="n1717" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5542" source="n1717" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5543" source="n1717" target="n1755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5544" source="n1717" target="n746" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5545" source="n1717" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5546" source="n1717" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5547" source="n1717" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5548" source="n1717" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5549" source="n1717" target="n1077" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5550" source="n1717" target="n585" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5551" source="n1717" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5552" source="n1717" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5553" source="n1717" target="n618" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5554" source="n1717" target="n1106" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5555" source="n1717" target="n214" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5556" source="n1717" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5557" source="n1717" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5558" source="n1717" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5559" source="n1717" target="n1756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5560" source="n1718" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5561" source="n1718" target="n185" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5562" source="n1718" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5563" source="n1718" target="n1757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5564" source="n1718" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5565" source="n1718" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5566" source="n1718" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5567" source="n1718" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5568" source="n1718" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5569" source="n1718" target="n1758" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5570" source="n1718" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5571" source="n1718" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5572" source="n1718" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5573" source="n1718" target="n1759" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5574" source="n1718" target="n846" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5575" source="n1718" target="n1760" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5576" source="n1719" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5577" source="n1719" target="n1020" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5578" source="n1719" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5579" source="n1719" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5580" source="n1719" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5581" source="n1719" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5582" source="n1719" target="n348" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5583" source="n1719" target="n697" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5584" source="n1719" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5585" source="n1719" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5586" source="n1719" target="n1761" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5587" source="n1719" target="n620" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5588" source="n1720" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5589" source="n1720" target="n1762" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5590" source="n1720" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5591" source="n1720" target="n1763" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5592" source="n1720" target="n1764" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5593" source="n1720" target="n1765" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5594" source="n1720" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5595" source="n1720" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5596" source="n1720" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5597" source="n1720" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5598" source="n1720" target="n1766" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5599" source="n1720" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5600" source="n1720" target="n1767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5601" source="n1720" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5602" source="n1720" target="n30" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5603" source="n1720" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5604" source="n1720" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5605" source="n1720" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5606" source="n1720" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5607" source="n1720" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5608" source="n1720" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5609" source="n1721" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5610" source="n1721" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5611" source="n1721" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5612" source="n1721" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5613" source="n1721" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5614" source="n1721" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5615" source="n1721" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5616" source="n1721" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5617" source="n1721" target="n553" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5618" source="n1721" target="n1768" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5619" source="n1721" target="n1769" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5620" source="n1721" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5621" source="n1721" target="n693" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5622" source="n1721" target="n1770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5623" source="n1721" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5624" source="n1721" target="n1771" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5625" source="n1721" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5626" source="n1721" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5627" source="n1721" target="n900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5628" source="n1721" target="n1772" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5629" source="n1721" target="n627" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5630" source="n1721" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5631" source="n1721" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5632" source="n1721" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5633" source="n1721" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5634" source="n1721" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5635" source="n1721" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5636" source="n1721" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5637" source="n1721" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5638" source="n1722" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5639" source="n1722" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5640" source="n1722" target="n1067" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5641" source="n1722" target="n614" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5642" source="n1722" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5643" source="n1722" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5644" source="n1722" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5645" source="n1722" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5646" source="n1722" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5647" source="n1722" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5648" source="n1722" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5649" source="n1722" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5650" source="n1722" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5651" source="n1722" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5652" source="n1722" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5653" source="n1722" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5654" source="n1722" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5655" source="n1722" target="n529" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5656" source="n1722" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5657" source="n1722" target="n532" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5658" source="n1722" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5659" source="n1723" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5660" source="n1723" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5661" source="n1723" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5662" source="n1723" target="n868" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5663" source="n1723" target="n551" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5664" source="n1723" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5665" source="n1723" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5666" source="n1723" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5667" source="n1723" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5668" source="n1723" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5669" source="n1723" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5670" source="n1723" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5671" source="n1723" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5672" source="n1723" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5673" source="n1723" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5674" source="n1723" target="n1081" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5675" source="n1723" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5676" source="n1723" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5677" source="n1723" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5678" source="n1723" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5679" source="n1723" target="n1773" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5680" source="n1723" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5681" source="n1723" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5682" source="n1724" target="n1774" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5683" source="n1724" target="n1775" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5684" source="n1724" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5685" source="n1724" target="n1037" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5686" source="n1724" target="n1034" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5687" source="n1724" target="n1776" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5688" source="n1724" target="n1777" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5689" source="n1724" target="n1778" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5690" source="n1724" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5691" source="n1724" target="n1779" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5692" source="n1724" target="n1780" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5693" source="n1724" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5694" source="n1724" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5695" source="n1724" target="n1781" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5696" source="n1724" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5697" source="n1724" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5698" source="n1724" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5699" source="n1724" target="n1782" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5700" source="n1724" target="n520" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5701" source="n1724" target="n846" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5702" source="n1724" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5703" source="n1724" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5704" source="n1724" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5705" source="n1724" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5706" source="n1724" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5707" source="n1725" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5708" source="n1725" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5709" source="n1725" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5710" source="n1725" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5711" source="n1725" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5712" source="n1725" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5713" source="n1725" target="n862" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5714" source="n1725" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5715" source="n1725" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5716" source="n1725" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5717" source="n1725" target="n500" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5718" source="n1725" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5719" source="n1725" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5720" source="n1725" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5721" source="n1725" target="n499" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5722" source="n1725" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5723" source="n1725" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5724" source="n1725" target="n140" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5725" source="n1725" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5726" source="n1725" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5727" source="n1725" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5728" source="n1726" target="n1783" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5729" source="n1726" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5730" source="n1726" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5731" source="n1726" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5732" source="n1726" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5733" source="n1726" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5734" source="n1726" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5735" source="n1726" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5736" source="n1726" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5737" source="n1726" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5738" source="n1726" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5739" source="n1726" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5740" source="n1727" target="n1784" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5741" source="n1727" target="n623" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5742" source="n1727" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5743" source="n1727" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5744" source="n1727" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5745" source="n1727" target="n1785" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5746" source="n1727" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5747" source="n1727" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5748" source="n1727" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5749" source="n1727" target="n951" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5750" source="n1727" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5751" source="n1727" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5752" source="n1727" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5753" source="n1727" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5754" source="n1727" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5755" source="n1727" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5756" source="n1727" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5757" source="n1728" target="n387" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5758" source="n1728" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5759" source="n1728" target="n1077" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5760" source="n1728" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5761" source="n1728" target="n6" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5762" source="n1728" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5763" source="n1728" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5764" source="n1728" target="n1786" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5765" source="n1728" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5766" source="n1728" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5767" source="n1728" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5768" source="n1728" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5769" source="n1728" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5770" source="n1728" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5771" source="n1728" target="n406" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5772" source="n1728" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5773" source="n1728" target="n1787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5774" source="n1728" target="n1034" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5775" source="n1728" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5776" source="n1729" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5777" source="n1729" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5778" source="n1729" target="n614" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5779" source="n1729" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5780" source="n1729" target="n1788" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5781" source="n1729" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5782" source="n1729" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5783" source="n1729" target="n1784" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5784" source="n1729" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5785" source="n1729" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5786" source="n1729" target="n44" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5787" source="n1729" target="n1067" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5788" source="n1729" target="n1043" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5789" source="n1729" target="n1102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5790" source="n1729" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5791" source="n1729" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5792" source="n1729" target="n45" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5793" source="n1729" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5794" source="n1729" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5795" source="n1729" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5796" source="n1729" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5797" source="n1729" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5798" source="n1729" target="n402" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5799" source="n1729" target="n403" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5800" source="n1729" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5801" source="n1730" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5802" source="n1730" target="n1082" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5803" source="n1730" target="n1789" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5804" source="n1730" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5805" source="n1730" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5806" source="n1730" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5807" source="n1730" target="n650" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5808" source="n1730" target="n1790" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5809" source="n1730" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5810" source="n1730" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5811" source="n1730" target="n1791" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5812" source="n1730" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5813" source="n1731" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5814" source="n1731" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5815" source="n1731" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5816" source="n1731" target="n959" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5817" source="n1731" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5818" source="n1731" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5819" source="n1731" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5820" source="n1731" target="n702" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5821" source="n1731" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5822" source="n1731" target="n882" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5823" source="n1731" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5824" source="n1731" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5825" source="n1731" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5826" source="n1731" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5827" source="n1731" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5828" source="n1731" target="n877" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5829" source="n1731" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5830" source="n1731" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5831" source="n1731" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5832" source="n1731" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5833" source="n1731" target="n944" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5834" source="n1731" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5835" source="n1731" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5836" source="n1731" target="n1792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5837" source="n1731" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5838" source="n1731" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5839" source="n1731" target="n1106" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5840" source="n1731" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5841" source="n1731" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5842" source="n1731" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5843" source="n1731" target="n640" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5844" source="n1731" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5845" source="n1732" target="n938" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5846" source="n1732" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5847" source="n1732" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5848" source="n1732" target="n1709" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5849" source="n1732" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5850" source="n1732" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5851" source="n1732" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5852" source="n1732" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5853" source="n1732" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5854" source="n1732" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5855" source="n1732" target="n1792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5856" source="n1732" target="n1793" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5857" source="n1732" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5858" source="n1732" target="n1794" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5859" source="n1732" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5860" source="n1732" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5861" source="n1732" target="n808" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5862" source="n1732" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5863" source="n1733" target="n1617" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5864" source="n1733" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5865" source="n1733" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5866" source="n1733" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5867" source="n1733" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5868" source="n1733" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5869" source="n1733" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5870" source="n1733" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5871" source="n1733" target="n1075" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5872" source="n1733" target="n404" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5873" source="n1733" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5874" source="n1733" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5875" source="n1733" target="n221" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5876" source="n1733" target="n1048" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5877" source="n1733" target="n1031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5878" source="n1733" target="n1795" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5879" source="n1733" target="n1796" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5880" source="n1733" target="n1797" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5881" source="n1733" target="n1622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5882" source="n1733" target="n1630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5883" source="n1733" target="n1798" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5884" source="n1733" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5885" source="n1733" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5886" source="n1733" target="n1043" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5887" source="n1733" target="n1799" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5888" source="n1733" target="n1800" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5889" source="n1733" target="n1801" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5890" source="n1733" target="n1802" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5891" source="n1733" target="n964" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5892" source="n1733" target="n814" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5893" source="n1733" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5894" source="n1733" target="n412" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5895" source="n1733" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5896" source="n1733" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5897" source="n1733" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5898" source="n1734" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5899" source="n1734" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5900" source="n1734" target="n618" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5901" source="n1734" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5902" source="n1734" target="n777" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5903" source="n1734" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5904" source="n1734" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5905" source="n1734" target="n702" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5906" source="n1734" target="n803" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5907" source="n1734" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5908" source="n1734" target="n1057" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5909" source="n1734" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5910" source="n1734" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5911" source="n1734" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5912" source="n1734" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5913" source="n1734" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5914" source="n1734" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5915" source="n1734" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5916" source="n1734" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5917" source="n1734" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5918" source="n1734" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5919" source="n1734" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5920" source="n1734" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e5921" source="n1710" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5922" source="n1711" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5923" source="n1712" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5924" source="n1713" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5925" source="n1714" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5926" source="n1715" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5927" source="n1716" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5928" source="n1717" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5929" source="n1718" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5930" source="n1719" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5931" source="n1720" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5932" source="n1721" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5933" source="n1722" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5934" source="n1723" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5935" source="n1724" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5936" source="n1725" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5937" source="n1726" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5938" source="n1727" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5939" source="n1728" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5940" source="n1729" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5941" source="n1730" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5942" source="n1731" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5943" source="n1732" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5944" source="n1733" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5945" source="n1734" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e5946" source="n451" target="n648" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e5947" source="n451" target="n932" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e5948" source="n451" target="n916" label="HAS"><data key="label">HAS</data><data key="score">22.35294117647059</data></edge>
<edge id="e5949" source="n451" target="n513" label="HAS"><data key="label">HAS</data><data key="score">14.117647058823529</data></edge>
<edge id="e5950" source="n451" target="n931" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5951" source="n451" target="n516" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5952" source="n451" target="n572" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e5953" source="n451" target="n1735" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5954" source="n451" target="n1736" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5955" source="n451" target="n1737" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5956" source="n451" target="n865" label="HAS"><data key="label">HAS</data><data key="score">11.76470588235294</data></edge>
<edge id="e5957" source="n451" target="n1652" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5958" source="n451" target="n400" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5959" source="n451" target="n499" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5960" source="n451" target="n1738" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5961" source="n451" target="n537" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5962" source="n451" target="n536" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5963" source="n451" target="n544" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5964" source="n451" target="n1709" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5965" source="n451" target="n1739" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5966" source="n451" target="n1740" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5967" source="n451" target="n670" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5968" source="n451" target="n752" label="HAS"><data key="label">HAS</data><data key="score">10.588235294117647</data></edge>
<edge id="e5969" source="n451" target="n1741" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5970" source="n451" target="n655" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e5971" source="n451" target="n1742" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5972" source="n451" target="n1743" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5973" source="n451" target="n910" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5974" source="n451" target="n598" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e5975" source="n451" target="n751" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5976" source="n451" target="n853" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5977" source="n451" target="n639" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5978" source="n451" target="n911" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5979" source="n451" target="n912" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5980" source="n451" target="n628" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e5981" source="n451" target="n866" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e5982" source="n451" target="n159" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5983" source="n451" target="n522" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5984" source="n451" target="n951" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5985" source="n451" target="n1744" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5986" source="n451" target="n1745" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5987" source="n451" target="n1746" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5988" source="n451" target="n1747" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5989" source="n451" target="n854" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5990" source="n451" target="n1748" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5991" source="n451" target="n1749" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5992" source="n451" target="n919" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5993" source="n451" target="n1630" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5994" source="n451" target="n635" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5995" source="n451" target="n354" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e5996" source="n451" target="n574" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e5997" source="n451" target="n843" label="HAS"><data key="label">HAS</data><data key="score">7.0588235294117645</data></edge>
<edge id="e5998" source="n451" target="n1750" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e5999" source="n451" target="n1617" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e6000" source="n451" target="n756" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6001" source="n451" target="n953" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6002" source="n451" target="n1751" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6003" source="n451" target="n754" label="HAS"><data key="label">HAS</data><data key="score">8.235294117647058</data></edge>
<edge id="e6004" source="n451" target="n30" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6005" source="n451" target="n1752" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6006" source="n451" target="n1753" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6007" source="n451" target="n825" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6008" source="n451" target="n1704" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6009" source="n451" target="n864" label="HAS"><data key="label">HAS</data><data key="score">20.0</data></edge>
<edge id="e6010" source="n451" target="n1754" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6011" source="n451" target="n858" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e6012" source="n451" target="n1755" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6013" source="n451" target="n746" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6014" source="n451" target="n384" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6015" source="n451" target="n585" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6016" source="n451" target="n618" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6017" source="n451" target="n214" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6018" source="n451" target="n1756" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6019" source="n451" target="n185" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6020" source="n451" target="n1757" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6021" source="n451" target="n1758" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6022" source="n451" target="n880" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6023" source="n451" target="n245" label="HAS"><data key="label">HAS</data><data key="score">4.705882352941177</data></edge>
<edge id="e6024" source="n451" target="n1759" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6025" source="n451" target="n846" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6026" source="n451" target="n1760" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6027" source="n451" target="n697" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6028" source="n451" target="n1761" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6029" source="n451" target="n620" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6030" source="n451" target="n1762" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6031" source="n451" target="n1763" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6032" source="n451" target="n1764" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6033" source="n451" target="n1765" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6034" source="n451" target="n792" label="HAS"><data key="label">HAS</data><data key="score">5.88235294117647</data></edge>
<edge id="e6035" source="n451" target="n1766" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6036" source="n451" target="n879" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e6037" source="n451" target="n1767" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6038" source="n451" target="n616" label="HAS"><data key="label">HAS</data><data key="score">9.411764705882353</data></edge>
<edge id="e6039" source="n451" target="n553" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6040" source="n451" target="n1768" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6041" source="n451" target="n1769" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6042" source="n451" target="n693" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6043" source="n451" target="n1770" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6044" source="n451" target="n9" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6045" source="n451" target="n1771" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6046" source="n451" target="n900" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6047" source="n451" target="n1772" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6048" source="n451" target="n627" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6049" source="n451" target="n632" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6050" source="n451" target="n588" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6051" source="n451" target="n529" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6052" source="n451" target="n532" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6053" source="n451" target="n868" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6054" source="n451" target="n551" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6055" source="n451" target="n757" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6056" source="n451" target="n1773" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6057" source="n451" target="n1774" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6058" source="n451" target="n1775" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6059" source="n451" target="n1776" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6060" source="n451" target="n1777" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6061" source="n451" target="n1778" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6062" source="n451" target="n1779" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6063" source="n451" target="n1780" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6064" source="n451" target="n1781" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6065" source="n451" target="n1782" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6066" source="n451" target="n520" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6067" source="n451" target="n862" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6068" source="n451" target="n60" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6069" source="n451" target="n140" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6070" source="n451" target="n1783" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6071" source="n451" target="n1784" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6072" source="n451" target="n623" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6073" source="n451" target="n1785" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6074" source="n451" target="n387" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6075" source="n451" target="n1786" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6076" source="n451" target="n1787" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6077" source="n451" target="n1788" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6078" source="n451" target="n1789" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6079" source="n451" target="n650" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6080" source="n451" target="n1790" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6081" source="n451" target="n624" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6082" source="n451" target="n1791" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6083" source="n451" target="n959" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6084" source="n451" target="n702" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e6085" source="n451" target="n882" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6086" source="n451" target="n877" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6087" source="n451" target="n813" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6088" source="n451" target="n686" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6089" source="n451" target="n944" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6090" source="n451" target="n1792" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6091" source="n451" target="n640" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6092" source="n451" target="n770" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6093" source="n451" target="n938" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6094" source="n451" target="n1793" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6095" source="n451" target="n1794" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6096" source="n451" target="n808" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e6097" source="n451" target="n1795" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6098" source="n451" target="n1796" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6099" source="n451" target="n1797" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6100" source="n451" target="n1622" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6101" source="n451" target="n1798" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6102" source="n451" target="n1799" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6103" source="n451" target="n1800" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6104" source="n451" target="n1801" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6105" source="n451" target="n1802" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6106" source="n451" target="n777" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e6107" source="n1803" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6108" source="n1803" target="n1827" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6109" source="n1803" target="n1828" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6110" source="n1803" target="n1829" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6111" source="n1803" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6112" source="n1803" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6113" source="n1803" target="n1830" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6114" source="n1803" target="n657" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6115" source="n1803" target="n1831" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6116" source="n1803" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6117" source="n1803" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6118" source="n1803" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6119" source="n1803" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6120" source="n1803" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6121" source="n1804" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6122" source="n1804" target="n1832" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6123" source="n1804" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6124" source="n1804" target="n1833" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6125" source="n1804" target="n1834" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6126" source="n1804" target="n633" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6127" source="n1804" target="n1835" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6128" source="n1804" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6129" source="n1804" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6130" source="n1804" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6131" source="n1804" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6132" source="n1804" target="n1024" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6133" source="n1804" target="n1071" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6134" source="n1804" target="n1617" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6135" source="n1804" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6136" source="n1804" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6137" source="n1804" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6138" source="n1804" target="n1836" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6139" source="n1804" target="n1837" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6140" source="n1804" target="n1838" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6141" source="n1805" target="n610" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6142" source="n1805" target="n1839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6143" source="n1805" target="n1840" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6144" source="n1805" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6145" source="n1805" target="n541" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6146" source="n1805" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6147" source="n1805" target="n1841" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6148" source="n1805" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6149" source="n1805" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6150" source="n1805" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6151" source="n1805" target="n1842" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6152" source="n1805" target="n1843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6153" source="n1805" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6154" source="n1805" target="n1844" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6155" source="n1805" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6156" source="n1805" target="n635" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6157" source="n1805" target="n1845" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6158" source="n1805" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6159" source="n1805" target="n1846" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6160" source="n1805" target="n679" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6161" source="n1805" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6162" source="n1806" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6163" source="n1806" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6164" source="n1806" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6165" source="n1806" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6166" source="n1806" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6167" source="n1806" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6168" source="n1806" target="n569" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6169" source="n1806" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6170" source="n1806" target="n679" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6171" source="n1806" target="n974" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6172" source="n1806" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6173" source="n1806" target="n1847" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6174" source="n1807" target="n636" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6175" source="n1807" target="n1848" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6176" source="n1807" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6177" source="n1807" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6178" source="n1807" target="n1849" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6179" source="n1807" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6180" source="n1807" target="n1850" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6181" source="n1807" target="n627" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6182" source="n1807" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6183" source="n1807" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6184" source="n1807" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6185" source="n1807" target="n529" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6186" source="n1807" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6187" source="n1807" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6188" source="n1807" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6189" source="n1807" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6190" source="n1807" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6191" source="n1807" target="n123" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6192" source="n1807" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6193" source="n1807" target="n1851" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6194" source="n1807" target="n1852" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6195" source="n1807" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6196" source="n1807" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6197" source="n1807" target="n1853" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6198" source="n1807" target="n897" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6199" source="n1807" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6200" source="n1807" target="n1854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6201" source="n1807" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6202" source="n1807" target="n532" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6203" source="n1807" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6204" source="n1807" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6205" source="n1807" target="n811" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6206" source="n1808" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6207" source="n1808" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6208" source="n1808" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6209" source="n1808" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6210" source="n1808" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6211" source="n1808" target="n702" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6212" source="n1808" target="n520" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6213" source="n1808" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6214" source="n1808" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6215" source="n1808" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6216" source="n1808" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6217" source="n1809" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6218" source="n1809" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6219" source="n1809" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6220" source="n1809" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6221" source="n1809" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6222" source="n1809" target="n702" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6223" source="n1809" target="n520" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6224" source="n1809" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6225" source="n1809" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6226" source="n1809" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6227" source="n1809" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6228" source="n1809" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6229" source="n1810" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6230" source="n1810" target="n1855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6231" source="n1810" target="n541" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6232" source="n1810" target="n1856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6233" source="n1810" target="n1857" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6234" source="n1810" target="n1858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6235" source="n1810" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6236" source="n1810" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6237" source="n1810" target="n811" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6238" source="n1811" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6239" source="n1811" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6240" source="n1811" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6241" source="n1811" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6242" source="n1811" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6243" source="n1811" target="n1859" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6244" source="n1811" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6245" source="n1811" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6246" source="n1811" target="n1751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6247" source="n1811" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6248" source="n1811" target="n340" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6249" source="n1811" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6250" source="n1812" target="n1709" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6251" source="n1812" target="n1860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6252" source="n1812" target="n676" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6253" source="n1812" target="n1861" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6254" source="n1812" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6255" source="n1812" target="n1862" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6256" source="n1812" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6257" source="n1812" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6258" source="n1812" target="n1863" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6259" source="n1812" target="n1864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6260" source="n1812" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6261" source="n1812" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6262" source="n1812" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6263" source="n1812" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6264" source="n1812" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6265" source="n1812" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6266" source="n1812" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6267" source="n1812" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6268" source="n1812" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6269" source="n1812" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6270" source="n1812" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6271" source="n1812" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6272" source="n1812" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6273" source="n1812" target="n1865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6274" source="n1812" target="n1866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6275" source="n1812" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6276" source="n1812" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6277" source="n1812" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6278" source="n1812" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6279" source="n1812" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6280" source="n1812" target="n1867" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6281" source="n1812" target="n1868" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6282" source="n1812" target="n1869" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6283" source="n1813" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6284" source="n1813" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6285" source="n1813" target="n557" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6286" source="n1813" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6287" source="n1813" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6288" source="n1813" target="n524" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6289" source="n1813" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6290" source="n1813" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6291" source="n1813" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6292" source="n1813" target="n1870" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6293" source="n1813" target="n1871" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6294" source="n1813" target="n1872" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6295" source="n1813" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6296" source="n1814" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6297" source="n1814" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6298" source="n1814" target="n1083" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6299" source="n1814" target="n855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6300" source="n1814" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6301" source="n1814" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6302" source="n1814" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6303" source="n1814" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6304" source="n1814" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6305" source="n1814" target="n1873" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6306" source="n1814" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6307" source="n1814" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6308" source="n1814" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6309" source="n1814" target="n1874" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6310" source="n1814" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6311" source="n1814" target="n856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6312" source="n1814" target="n690" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6313" source="n1814" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6314" source="n1814" target="n662" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6315" source="n1815" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6316" source="n1815" target="n1875" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6317" source="n1815" target="n1876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6318" source="n1815" target="n1877" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6319" source="n1815" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6320" source="n1815" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6321" source="n1815" target="n636" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6322" source="n1815" target="n562" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6323" source="n1815" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6324" source="n1815" target="n1878" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6325" source="n1815" target="n1879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6326" source="n1815" target="n408" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6327" source="n1815" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6328" source="n1815" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6329" source="n1815" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6330" source="n1815" target="n1880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6331" source="n1815" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6332" source="n1815" target="n1881" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6333" source="n1815" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6334" source="n1815" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6335" source="n1815" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6336" source="n1815" target="n679" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6337" source="n1815" target="n1882" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6338" source="n1815" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6339" source="n1815" target="n1883" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6340" source="n1816" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6341" source="n1816" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6342" source="n1816" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6343" source="n1816" target="n1884" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6344" source="n1816" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6345" source="n1816" target="n1885" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6346" source="n1816" target="n1886" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6347" source="n1816" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6348" source="n1816" target="n794" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6349" source="n1816" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6350" source="n1816" target="n1887" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6351" source="n1816" target="n1871" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6352" source="n1816" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6353" source="n1816" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6354" source="n1816" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6355" source="n1816" target="n882" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6356" source="n1817" target="n1751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6357" source="n1818" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6358" source="n1818" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6359" source="n1818" target="n1888" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6360" source="n1818" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6361" source="n1818" target="n1889" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6362" source="n1818" target="n1890" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6363" source="n1818" target="n1891" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6364" source="n1818" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6365" source="n1818" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6366" source="n1818" target="n1892" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6367" source="n1818" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6368" source="n1818" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6369" source="n1818" target="n1763" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6370" source="n1818" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6371" source="n1818" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6372" source="n1818" target="n13" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6373" source="n1818" target="n1893" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6374" source="n1818" target="n1894" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6375" source="n1819" target="n1895" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6376" source="n1819" target="n1896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6377" source="n1819" target="n1897" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6378" source="n1819" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6379" source="n1820" target="n1898" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6380" source="n1820" target="n1899" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6381" source="n1820" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6382" source="n1820" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6383" source="n1820" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6384" source="n1820" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6385" source="n1820" target="n1831" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6386" source="n1820" target="n1900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6387" source="n1820" target="n1901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6388" source="n1820" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6389" source="n1820" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6390" source="n1820" target="n1902" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6391" source="n1820" target="n1903" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6392" source="n1821" target="n1709" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6393" source="n1821" target="n1904" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6394" source="n1821" target="n1905" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6395" source="n1821" target="n1906" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6396" source="n1821" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6397" source="n1821" target="n584" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6398" source="n1821" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6399" source="n1821" target="n630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6400" source="n1821" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6401" source="n1821" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6402" source="n1821" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6403" source="n1821" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6404" source="n1822" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6405" source="n1822" target="n520" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6406" source="n1822" target="n1907" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6407" source="n1822" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6408" source="n1822" target="n1755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6409" source="n1822" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6410" source="n1822" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6411" source="n1822" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6412" source="n1822" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6413" source="n1822" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6414" source="n1822" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6415" source="n1822" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6416" source="n1822" target="n1908" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6417" source="n1822" target="n1751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6418" source="n1822" target="n1909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6419" source="n1822" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6420" source="n1822" target="n1910" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6421" source="n1822" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6422" source="n1822" target="n1911" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6423" source="n1822" target="n1912" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6424" source="n1822" target="n1913" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6425" source="n1823" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6426" source="n1823" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6427" source="n1823" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6428" source="n1823" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6429" source="n1823" target="n1755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6430" source="n1823" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6431" source="n1823" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6432" source="n1823" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6433" source="n1823" target="n1914" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6434" source="n1823" target="n1915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6435" source="n1823" target="n1916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6436" source="n1823" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6437" source="n1824" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6438" source="n1824" target="n562" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6439" source="n1824" target="n784" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6440" source="n1824" target="n592" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6441" source="n1824" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6442" source="n1824" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6443" source="n1824" target="n567" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6444" source="n1824" target="n590" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6445" source="n1824" target="n898" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6446" source="n1824" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6447" source="n1824" target="n553" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6448" source="n1824" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6449" source="n1824" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6450" source="n1824" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6451" source="n1825" target="n1917" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6452" source="n1825" target="n1918" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6453" source="n1825" target="n1919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6454" source="n1825" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6455" source="n1825" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6456" source="n1826" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6457" source="n1826" target="n898" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6458" source="n1826" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6459" source="n1826" target="n579" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6460" source="n1826" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6461" source="n1826" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6462" source="n1826" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6463" source="n1826" target="n1920" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6464" source="n1826" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6465" source="n1826" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6466" source="n1826" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6467" source="n1826" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6468" source="n1826" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6469" source="n1826" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6470" source="n1826" target="n1921" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6471" source="n1826" target="n562" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6472" source="n1826" target="n545" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6473" source="n1803" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6474" source="n1804" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6475" source="n1805" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6476" source="n1806" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6477" source="n1807" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6478" source="n1808" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6479" source="n1809" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6480" source="n1810" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6481" source="n1811" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6482" source="n1812" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6483" source="n1813" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6484" source="n1814" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6485" source="n1815" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6486" source="n1816" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6487" source="n1817" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6488" source="n1818" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6489" source="n1819" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6490" source="n1820" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6491" source="n1821" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6492" source="n1822" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6493" source="n1823" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6494" source="n1824" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6495" source="n1825" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6496" source="n1826" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e6497" source="n452" target="n1827" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6498" source="n452" target="n1828" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6499" source="n452" target="n1829" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6500" source="n452" target="n1830" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6501" source="n452" target="n1831" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6502" source="n452" target="n1832" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6503" source="n452" target="n1833" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6504" source="n452" target="n1834" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6505" source="n452" target="n1835" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6506" source="n452" target="n916" label="HAS"><data key="label">HAS</data><data key="score">13.513513513513514</data></edge>
<edge id="e6507" source="n452" target="n866" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e6508" source="n452" target="n1024" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6509" source="n452" target="n1071" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6510" source="n452" target="n1617" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6511" source="n452" target="n1836" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6512" source="n452" target="n1837" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6513" source="n452" target="n1838" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6514" source="n452" target="n1839" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6515" source="n452" target="n1840" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6516" source="n452" target="n1841" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6517" source="n452" target="n1842" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6518" source="n452" target="n1843" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6519" source="n452" target="n1844" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6520" source="n452" target="n1845" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6521" source="n452" target="n1846" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6522" source="n452" target="n974" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6523" source="n452" target="n1847" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6524" source="n452" target="n1848" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6525" source="n452" target="n864" label="HAS"><data key="label">HAS</data><data key="score">10.81081081081081</data></edge>
<edge id="e6526" source="n452" target="n1849" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6527" source="n452" target="n1850" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6528" source="n452" target="n1851" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6529" source="n452" target="n1852" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6530" source="n452" target="n1853" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6531" source="n452" target="n897" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6532" source="n452" target="n953" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e6533" source="n452" target="n1854" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6534" source="n452" target="n1855" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6535" source="n452" target="n1856" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6536" source="n452" target="n1857" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6537" source="n452" target="n1858" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6538" source="n452" target="n919" label="HAS"><data key="label">HAS</data><data key="score">5.405405405405405</data></edge>
<edge id="e6539" source="n452" target="n1859" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6540" source="n452" target="n1751" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e6541" source="n452" target="n340" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6542" source="n452" target="n1709" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6543" source="n452" target="n1860" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6544" source="n452" target="n1861" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6545" source="n452" target="n1862" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6546" source="n452" target="n879" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e6547" source="n452" target="n1863" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6548" source="n452" target="n1864" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6549" source="n452" target="n1865" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6550" source="n452" target="n1866" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6551" source="n452" target="n111" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6552" source="n452" target="n1867" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6553" source="n452" target="n1868" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6554" source="n452" target="n1869" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6555" source="n452" target="n1870" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6556" source="n452" target="n1871" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6557" source="n452" target="n1872" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6558" source="n452" target="n1083" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6559" source="n452" target="n855" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e6560" source="n452" target="n865" label="HAS"><data key="label">HAS</data><data key="score">6.306306306306306</data></edge>
<edge id="e6561" source="n452" target="n1873" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6562" source="n452" target="n1652" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e6563" source="n452" target="n854" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e6564" source="n452" target="n1874" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6565" source="n452" target="n856" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6566" source="n452" target="n1875" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6567" source="n452" target="n1876" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6568" source="n452" target="n1877" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6569" source="n452" target="n1878" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6570" source="n452" target="n1879" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6571" source="n452" target="n408" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6572" source="n452" target="n1880" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6573" source="n452" target="n1881" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6574" source="n452" target="n1882" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6575" source="n452" target="n787" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e6576" source="n452" target="n1883" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6577" source="n452" target="n1884" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6578" source="n452" target="n1885" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6579" source="n452" target="n1886" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6580" source="n452" target="n794" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6581" source="n452" target="n1887" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6582" source="n452" target="n770" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6583" source="n452" target="n882" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6584" source="n452" target="n1888" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6585" source="n452" target="n1889" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6586" source="n452" target="n1890" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6587" source="n452" target="n1891" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6588" source="n452" target="n843" label="HAS"><data key="label">HAS</data><data key="score">7.207207207207207</data></edge>
<edge id="e6589" source="n452" target="n1892" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6590" source="n452" target="n880" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6591" source="n452" target="n1763" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6592" source="n452" target="n13" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6593" source="n452" target="n1893" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6594" source="n452" target="n1894" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6595" source="n452" target="n1895" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6596" source="n452" target="n1896" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6597" source="n452" target="n1897" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6598" source="n452" target="n1898" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6599" source="n452" target="n1899" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6600" source="n452" target="n1900" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6601" source="n452" target="n1901" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6602" source="n452" target="n1902" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6603" source="n452" target="n1903" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6604" source="n452" target="n1904" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6605" source="n452" target="n1905" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6606" source="n452" target="n1906" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6607" source="n452" target="n949" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6608" source="n452" target="n1907" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6609" source="n452" target="n1755" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6610" source="n452" target="n1908" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6611" source="n452" target="n1909" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6612" source="n452" target="n1910" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6613" source="n452" target="n1911" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6614" source="n452" target="n1912" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6615" source="n452" target="n1913" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6616" source="n452" target="n1914" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6617" source="n452" target="n1915" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6618" source="n452" target="n1916" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6619" source="n452" target="n784" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6620" source="n452" target="n898" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e6621" source="n452" target="n1917" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6622" source="n452" target="n1918" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6623" source="n452" target="n1919" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6624" source="n452" target="n1920" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6625" source="n452" target="n1921" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e6626" source="n1922" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6627" source="n1922" target="n1947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6628" source="n1922" target="n1948" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6629" source="n1922" target="n1949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6630" source="n1922" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6631" source="n1923" target="n842" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6632" source="n1923" target="n1950" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6633" source="n1923" target="n1951" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6634" source="n1923" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6635" source="n1923" target="n388" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6636" source="n1923" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6637" source="n1923" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6638" source="n1924" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6639" source="n1924" target="n1952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6640" source="n1924" target="n1953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6641" source="n1924" target="n620" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6642" source="n1924" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6643" source="n1924" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6644" source="n1924" target="n694" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6645" source="n1924" target="n1954" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6646" source="n1924" target="n1955" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6647" source="n1924" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6648" source="n1924" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6649" source="n1924" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6650" source="n1924" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6651" source="n1925" target="n638" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6652" source="n1925" target="n1956" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6653" source="n1925" target="n1957" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6654" source="n1925" target="n1958" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6655" source="n1925" target="n1959" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6656" source="n1925" target="n1960" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6657" source="n1925" target="n1961" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6658" source="n1926" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6659" source="n1926" target="n1962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6660" source="n1926" target="n902" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6661" source="n1926" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6662" source="n1926" target="n1105" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6663" source="n1926" target="n944" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6664" source="n1926" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6665" source="n1926" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6666" source="n1926" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6667" source="n1926" target="n1963" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6668" source="n1926" target="n1964" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6669" source="n1926" target="n1641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6670" source="n1926" target="n1047" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6671" source="n1926" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6672" source="n1926" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6673" source="n1926" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6674" source="n1926" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6675" source="n1926" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6676" source="n1926" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6677" source="n1926" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6678" source="n1926" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6679" source="n1926" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6680" source="n1926" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6681" source="n1926" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6682" source="n1927" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6683" source="n1927" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6684" source="n1927" target="n921" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6685" source="n1927" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6686" source="n1927" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6687" source="n1927" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6688" source="n1927" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6689" source="n1927" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6690" source="n1927" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6691" source="n1927" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6692" source="n1927" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6693" source="n1927" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6694" source="n1927" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6695" source="n1927" target="n581" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6696" source="n1927" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6697" source="n1927" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6698" source="n1927" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6699" source="n1927" target="n750" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6700" source="n1927" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6701" source="n1927" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6702" source="n1927" target="n1077" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6703" source="n1927" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6704" source="n1927" target="n1965" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6705" source="n1927" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6706" source="n1927" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6707" source="n1927" target="n374" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6708" source="n1927" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6709" source="n1927" target="n846" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6710" source="n1927" target="n747" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6711" source="n1927" target="n888" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6712" source="n1927" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6713" source="n1927" target="n1070" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6714" source="n1927" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6715" source="n1927" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6716" source="n1927" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6717" source="n1927" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6718" source="n1927" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6719" source="n1927" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6720" source="n1927" target="n610" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6721" source="n1927" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6722" source="n1928" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6723" source="n1928" target="n755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6724" source="n1928" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6725" source="n1928" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6726" source="n1928" target="n1966" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6727" source="n1928" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6728" source="n1928" target="n1709" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6729" source="n1928" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6730" source="n1928" target="n1083" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6731" source="n1928" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6732" source="n1928" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6733" source="n1928" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6734" source="n1928" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6735" source="n1928" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6736" source="n1928" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6737" source="n1928" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6738" source="n1928" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6739" source="n1928" target="n1106" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6740" source="n1928" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6741" source="n1928" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6742" source="n1928" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6743" source="n1928" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6744" source="n1928" target="n1885" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6745" source="n1928" target="n52" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6746" source="n1928" target="n804" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6747" source="n1928" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6748" source="n1928" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6749" source="n1928" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6750" source="n1928" target="n239" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6751" source="n1928" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6752" source="n1928" target="n1967" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6753" source="n1928" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6754" source="n1928" target="n104" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6755" source="n1929" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6756" source="n1929" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6757" source="n1929" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6758" source="n1929" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6759" source="n1929" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6760" source="n1929" target="n1020" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6761" source="n1929" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6762" source="n1929" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6763" source="n1929" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6764" source="n1929" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6765" source="n1929" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6766" source="n1929" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6767" source="n1929" target="n941" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6768" source="n1929" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6769" source="n1929" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6770" source="n1929" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6771" source="n1929" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6772" source="n1929" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6773" source="n1929" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6774" source="n1929" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6775" source="n1929" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6776" source="n1929" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6777" source="n1929" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6778" source="n1929" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6779" source="n1929" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6780" source="n1929" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6781" source="n1929" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6782" source="n1929" target="n1105" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6783" source="n1930" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6784" source="n1930" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6785" source="n1930" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6786" source="n1930" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6787" source="n1930" target="n639" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6788" source="n1930" target="n855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6789" source="n1930" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6790" source="n1930" target="n690" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6791" source="n1930" target="n943" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6792" source="n1930" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6793" source="n1930" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6794" source="n1930" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6795" source="n1930" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6796" source="n1930" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6797" source="n1930" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6798" source="n1930" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6799" source="n1930" target="n856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6800" source="n1930" target="n857" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6801" source="n1930" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6802" source="n1930" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6803" source="n1930" target="n1968" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6804" source="n1930" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6805" source="n1930" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6806" source="n1930" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6807" source="n1930" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6808" source="n1930" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6809" source="n1930" target="n1969" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6810" source="n1930" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6811" source="n1930" target="n618" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6812" source="n1930" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6813" source="n1930" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6814" source="n1930" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6815" source="n1930" target="n638" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6816" source="n1930" target="n1970" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6817" source="n1930" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6818" source="n1930" target="n904" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6819" source="n1930" target="n883" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6820" source="n1930" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6821" source="n1930" target="n644" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6822" source="n1930" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6823" source="n1930" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6824" source="n1930" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6825" source="n1930" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6826" source="n1930" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6827" source="n1930" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6828" source="n1930" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6829" source="n1931" target="n1971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6830" source="n1931" target="n1972" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6831" source="n1931" target="n1973" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6832" source="n1931" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6833" source="n1931" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6834" source="n1931" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6835" source="n1931" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6836" source="n1931" target="n1751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6837" source="n1931" target="n1974" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6838" source="n1931" target="n1891" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6839" source="n1931" target="n1975" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6840" source="n1931" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6841" source="n1931" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6842" source="n1932" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6843" source="n1932" target="n1976" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6844" source="n1932" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6845" source="n1932" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6846" source="n1932" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6847" source="n1932" target="n765" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6848" source="n1932" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6849" source="n1932" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6850" source="n1932" target="n1630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6851" source="n1932" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6852" source="n1932" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6853" source="n1932" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6854" source="n1932" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6855" source="n1932" target="n849" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6856" source="n1933" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6857" source="n1933" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6858" source="n1933" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6859" source="n1933" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6860" source="n1933" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6861" source="n1933" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6862" source="n1933" target="n856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6863" source="n1933" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6864" source="n1933" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6865" source="n1933" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6866" source="n1933" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6867" source="n1933" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6868" source="n1933" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6869" source="n1933" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6870" source="n1933" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6871" source="n1933" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6872" source="n1933" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6873" source="n1933" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6874" source="n1933" target="n750" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6875" source="n1933" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6876" source="n1933" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6877" source="n1933" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6878" source="n1933" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6879" source="n1933" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6880" source="n1933" target="n1671" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6881" source="n1933" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6882" source="n1933" target="n796" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6883" source="n1933" target="n599" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6884" source="n1933" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6885" source="n1933" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6886" source="n1933" target="n1977" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6887" source="n1933" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6888" source="n1933" target="n1978" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6889" source="n1933" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6890" source="n1933" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6891" source="n1933" target="n532" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6892" source="n1933" target="n1077" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6893" source="n1933" target="n918" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6894" source="n1933" target="n644" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6895" source="n1933" target="n1979" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6896" source="n1933" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6897" source="n1933" target="n1980" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6898" source="n1933" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6899" source="n1933" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6900" source="n1933" target="n943" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6901" source="n1933" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6902" source="n1933" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6903" source="n1933" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6904" source="n1933" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6905" source="n1933" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6906" source="n1933" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6907" source="n1934" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6908" source="n1934" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6909" source="n1934" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6910" source="n1934" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6911" source="n1934" target="n1981" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6912" source="n1934" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6913" source="n1934" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6914" source="n1934" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6915" source="n1934" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6916" source="n1934" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6917" source="n1934" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6918" source="n1934" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6919" source="n1934" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6920" source="n1934" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6921" source="n1934" target="n1982" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6922" source="n1934" target="n640" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6923" source="n1934" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6924" source="n1934" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6925" source="n1934" target="n768" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6926" source="n1934" target="n688" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6927" source="n1934" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6928" source="n1934" target="n1983" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6929" source="n1934" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6930" source="n1934" target="n335" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6931" source="n1934" target="n32" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6932" source="n1934" target="n867" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6933" source="n1934" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6934" source="n1934" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6935" source="n1934" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6936" source="n1934" target="n1984" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6937" source="n1934" target="n1709" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6938" source="n1934" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6939" source="n1935" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6940" source="n1935" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6941" source="n1935" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6942" source="n1935" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6943" source="n1935" target="n1985" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6944" source="n1935" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6945" source="n1935" target="n897" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6946" source="n1935" target="n750" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6947" source="n1935" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6948" source="n1935" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6949" source="n1935" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6950" source="n1935" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6951" source="n1935" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6952" source="n1935" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6953" source="n1935" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6954" source="n1935" target="n1671" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6955" source="n1935" target="n1986" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6956" source="n1935" target="n1854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6957" source="n1935" target="n950" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6958" source="n1935" target="n1987" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6959" source="n1935" target="n755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6960" source="n1935" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6961" source="n1935" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6962" source="n1935" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6963" source="n1935" target="n551" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6964" source="n1935" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6965" source="n1935" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6966" source="n1935" target="n1630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6967" source="n1935" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6968" source="n1935" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6969" source="n1935" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6970" source="n1935" target="n1988" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6971" source="n1935" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6972" source="n1935" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6973" source="n1935" target="n46" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6974" source="n1935" target="n1989" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6975" source="n1935" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6976" source="n1935" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6977" source="n1935" target="n804" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6978" source="n1935" target="n765" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6979" source="n1935" target="n383" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6980" source="n1935" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6981" source="n1935" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6982" source="n1935" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6983" source="n1935" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6984" source="n1935" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6985" source="n1935" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6986" source="n1935" target="n1990" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6987" source="n1935" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6988" source="n1935" target="n1991" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6989" source="n1935" target="n1992" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6990" source="n1935" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6991" source="n1935" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6992" source="n1935" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6993" source="n1935" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6994" source="n1935" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6995" source="n1935" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6996" source="n1935" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6997" source="n1935" target="n955" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6998" source="n1935" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e6999" source="n1936" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7000" source="n1936" target="n509" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7001" source="n1936" target="n1993" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7002" source="n1936" target="n1077" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7003" source="n1936" target="n1947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7004" source="n1936" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7005" source="n1936" target="n649" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7006" source="n1936" target="n1994" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7007" source="n1936" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7008" source="n1936" target="n886" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7009" source="n1936" target="n565" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7010" source="n1937" target="n746" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7011" source="n1937" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7012" source="n1937" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7013" source="n1937" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7014" source="n1937" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7015" source="n1937" target="n1995" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7016" source="n1937" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7017" source="n1937" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7018" source="n1937" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7019" source="n1937" target="n1015" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7020" source="n1937" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7021" source="n1937" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7022" source="n1937" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7023" source="n1937" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7024" source="n1937" target="n1996" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7025" source="n1937" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7026" source="n1937" target="n111" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7027" source="n1937" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7028" source="n1937" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7029" source="n1937" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7030" source="n1937" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7031" source="n1937" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7032" source="n1937" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7033" source="n1937" target="n955" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7034" source="n1937" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7035" source="n1937" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7036" source="n1937" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7037" source="n1937" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7038" source="n1937" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7039" source="n1937" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7040" source="n1937" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7041" source="n1937" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7042" source="n1938" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7043" source="n1938" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7044" source="n1938" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7045" source="n1938" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7046" source="n1938" target="n750" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7047" source="n1938" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7048" source="n1938" target="n679" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7049" source="n1938" target="n1997" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7050" source="n1938" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7051" source="n1938" target="n779" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7052" source="n1938" target="n683" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7053" source="n1938" target="n1998" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7054" source="n1938" target="n1999" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7055" source="n1938" target="n2000" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7056" source="n1938" target="n2001" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7057" source="n1938" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7058" source="n1938" target="n810" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7059" source="n1938" target="n2002" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7060" source="n1938" target="n2003" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7061" source="n1938" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7062" source="n1938" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7063" source="n1938" target="n2004" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7064" source="n1938" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7065" source="n1938" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7066" source="n1938" target="n1037" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7067" source="n1938" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7068" source="n1938" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7069" source="n1938" target="n693" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7070" source="n1938" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7071" source="n1938" target="n1056" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7072" source="n1938" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7073" source="n1938" target="n765" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7074" source="n1938" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7075" source="n1938" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7076" source="n1939" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7077" source="n1939" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7078" source="n1939" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7079" source="n1939" target="n645" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7080" source="n1939" target="n2005" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7081" source="n1939" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7082" source="n1939" target="n561" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7083" source="n1939" target="n2006" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7084" source="n1939" target="n574" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7085" source="n1939" target="n1033" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7086" source="n1939" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7087" source="n1939" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7088" source="n1939" target="n274" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7089" source="n1939" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7090" source="n1939" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7091" source="n1939" target="n178" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7092" source="n1939" target="n2007" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7093" source="n1939" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7094" source="n1939" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7095" source="n1939" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7096" source="n1939" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7097" source="n1939" target="n2008" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7098" source="n1939" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7099" source="n1939" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7100" source="n1939" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7101" source="n1939" target="n363" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7102" source="n1939" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7103" source="n1939" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7104" source="n1939" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7105" source="n1939" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7106" source="n1939" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7107" source="n1939" target="n1036" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7108" source="n1939" target="n2009" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7109" source="n1939" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7110" source="n1939" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7111" source="n1939" target="n2010" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7112" source="n1939" target="n116" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7113" source="n1939" target="n214" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7114" source="n1939" target="n418" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7115" source="n1939" target="n250" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7116" source="n1939" target="n2011" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7117" source="n1939" target="n428" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7118" source="n1939" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7119" source="n1939" target="n185" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7120" source="n1939" target="n1076" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7121" source="n1939" target="n560" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7122" source="n1939" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7123" source="n1939" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7124" source="n1940" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7125" source="n1940" target="n2012" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7126" source="n1940" target="n2013" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7127" source="n1940" target="n561" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7128" source="n1940" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7129" source="n1940" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7130" source="n1940" target="n855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7131" source="n1940" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7132" source="n1940" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7133" source="n1940" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7134" source="n1940" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7135" source="n1940" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7136" source="n1940" target="n400" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7137" source="n1940" target="n661" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7138" source="n1940" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7139" source="n1940" target="n1105" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7140" source="n1940" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7141" source="n1940" target="n800" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7142" source="n1940" target="n2014" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7143" source="n1940" target="n902" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7144" source="n1940" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7145" source="n1940" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7146" source="n1940" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7147" source="n1940" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7148" source="n1940" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7149" source="n1940" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7150" source="n1940" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7151" source="n1940" target="n769" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7152" source="n1940" target="n2015" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7153" source="n1941" target="n1020" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7154" source="n1941" target="n1047" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7155" source="n1941" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7156" source="n1941" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7157" source="n1941" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7158" source="n1941" target="n772" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7159" source="n1941" target="n755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7160" source="n1941" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7161" source="n1941" target="n765" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7162" source="n1941" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7163" source="n1941" target="n380" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7164" source="n1941" target="n972" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7165" source="n1941" target="n600" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7166" source="n1941" target="n2016" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7167" source="n1941" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7168" source="n1941" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7169" source="n1941" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7170" source="n1941" target="n292" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7171" source="n1941" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7172" source="n1941" target="n1106" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7173" source="n1941" target="n239" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7174" source="n1941" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7175" source="n1941" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7176" source="n1941" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7177" source="n1941" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7178" source="n1941" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7179" source="n1941" target="n1037" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7180" source="n1941" target="n585" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7181" source="n1941" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7182" source="n1941" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7183" source="n1941" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7184" source="n1941" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7185" source="n1941" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7186" source="n1941" target="n930" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7187" source="n1941" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7188" source="n1941" target="n888" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7189" source="n1941" target="n747" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7190" source="n1941" target="n385" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7191" source="n1941" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7192" source="n1941" target="n365" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7193" source="n1941" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7194" source="n1941" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7195" source="n1941" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7196" source="n1942" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7197" source="n1942" target="n959" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7198" source="n1942" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7199" source="n1942" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7200" source="n1942" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7201" source="n1942" target="n2017" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7202" source="n1942" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7203" source="n1942" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7204" source="n1942" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7205" source="n1942" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7206" source="n1942" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7207" source="n1942" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7208" source="n1942" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7209" source="n1942" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7210" source="n1942" target="n2018" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7211" source="n1943" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7212" source="n1943" target="n561" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7213" source="n1943" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7214" source="n1943" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7215" source="n1943" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7216" source="n1943" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7217" source="n1943" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7218" source="n1943" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7219" source="n1943" target="n856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7220" source="n1943" target="n857" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7221" source="n1943" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7222" source="n1943" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7223" source="n1943" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7224" source="n1943" target="n2019" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7225" source="n1943" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7226" source="n1943" target="n581" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7227" source="n1943" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7228" source="n1943" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7229" source="n1943" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7230" source="n1943" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7231" source="n1943" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7232" source="n1943" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7233" source="n1943" target="n688" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7234" source="n1943" target="n1991" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7235" source="n1943" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7236" source="n1943" target="n769" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7237" source="n1943" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7238" source="n1943" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7239" source="n1944" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7240" source="n1944" target="n532" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7241" source="n1944" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7242" source="n1944" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7243" source="n1944" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7244" source="n1944" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7245" source="n1944" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7246" source="n1944" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7247" source="n1944" target="n581" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7248" source="n1944" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7249" source="n1944" target="n683" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7250" source="n1944" target="n56" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7251" source="n1944" target="n2020" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7252" source="n1944" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7253" source="n1944" target="n951" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7254" source="n1944" target="n2021" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7255" source="n1944" target="n2022" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7256" source="n1944" target="n2023" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7257" source="n1944" target="n766" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7258" source="n1944" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7259" source="n1944" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7260" source="n1944" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7261" source="n1944" target="n2024" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7262" source="n1944" target="n2025" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7263" source="n1944" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7264" source="n1944" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7265" source="n1944" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7266" source="n1944" target="n758" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7267" source="n1944" target="n759" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7268" source="n1944" target="n760" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7269" source="n1944" target="n2026" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7270" source="n1944" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7271" source="n1944" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7272" source="n1944" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7273" source="n1944" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7274" source="n1944" target="n835" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7275" source="n1945" target="n876" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7276" source="n1945" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7277" source="n1945" target="n1709" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7278" source="n1945" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7279" source="n1945" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7280" source="n1945" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7281" source="n1945" target="n897" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7282" source="n1945" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7283" source="n1945" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7284" source="n1945" target="n880" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7285" source="n1945" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7286" source="n1945" target="n185" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7287" source="n1945" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7288" source="n1945" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7289" source="n1945" target="n805" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7290" source="n1945" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7291" source="n1945" target="n933" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7292" source="n1945" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7293" source="n1945" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7294" source="n1945" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7295" source="n1945" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7296" source="n1945" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7297" source="n1945" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7298" source="n1945" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7299" source="n1945" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7300" source="n1945" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7301" source="n1945" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7302" source="n1945" target="n33" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7303" source="n1945" target="n618" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7304" source="n1945" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7305" source="n1945" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7306" source="n1945" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7307" source="n1945" target="n868" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7308" source="n1945" target="n1981" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7309" source="n1945" target="n2027" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7310" source="n1945" target="n210" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7311" source="n1945" target="n323" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7312" source="n1945" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7313" source="n1945" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7314" source="n1945" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7315" source="n1945" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7316" source="n1945" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7317" source="n1945" target="n519" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7318" source="n1945" target="n955" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7319" source="n1945" target="n1038" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7320" source="n1945" target="n398" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7321" source="n1945" target="n399" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7322" source="n1945" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7323" source="n1945" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7324" source="n1945" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7325" source="n1945" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7326" source="n1945" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7327" source="n1945" target="n877" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7328" source="n1945" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7329" source="n1945" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7330" source="n1945" target="n973" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7331" source="n1945" target="n2028" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7332" source="n1945" target="n2029" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7333" source="n1945" target="n2030" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7334" source="n1945" target="n2031" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7335" source="n1945" target="n1957" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7336" source="n1945" target="n101" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7337" source="n1945" target="n93" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7338" source="n1945" target="n2032" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7339" source="n1945" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7340" source="n1945" target="n804" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7341" source="n1945" target="n813" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7342" source="n1945" target="n686" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7343" source="n1946" target="n2033" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7344" source="n1946" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7345" source="n1946" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7346" source="n1946" target="n663" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7347" source="n1946" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7348" source="n1946" target="n2034" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7349" source="n1946" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7350" source="n1946" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7351" source="n1946" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7352" source="n1946" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7353" source="n1946" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7354" source="n1946" target="n857" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7355" source="n1946" target="n629" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7356" source="n1946" target="n1630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7357" source="n1946" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7358" source="n1946" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7359" source="n1946" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7360" source="n1946" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7361" source="n1946" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7362" source="n1946" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7363" source="n1946" target="n1020" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7364" source="n1946" target="n807" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7365" source="n1946" target="n524" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7366" source="n1946" target="n581" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7367" source="n1946" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7368" source="n1946" target="n835" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7369" source="n1946" target="n751" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7370" source="n1946" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7371" source="n1946" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7372" source="n1946" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7373" source="n1946" target="n532" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7374" source="n1946" target="n362" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7375" source="n1946" target="n360" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7376" source="n1946" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7377" source="n1946" target="n386" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7378" source="n1946" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7379" source="n1946" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7380" source="n1946" target="n381" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7381" source="n1946" target="n860" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7382" source="n1946" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7383" source="n1946" target="n769" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7384" source="n1946" target="n770" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7385" source="n1946" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7386" source="n1946" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7387" source="n1946" target="n285" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7388" source="n1946" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7389" source="n1946" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7390" source="n1946" target="n396" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7391" source="n1946" target="n926" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7392" source="n1946" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7393" source="n1946" target="n343" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7394" source="n1946" target="n338" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7395" source="n1946" target="n1081" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7396" source="n1946" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7397" source="n1946" target="n569" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7398" source="n1946" target="n43" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7399" source="n1946" target="n2035" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7400" source="n1946" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7401" source="n1922" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7402" source="n1923" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7403" source="n1924" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7404" source="n1925" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7405" source="n1926" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7406" source="n1927" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7407" source="n1928" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7408" source="n1929" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7409" source="n1930" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7410" source="n1931" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7411" source="n1932" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7412" source="n1933" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7413" source="n1934" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7414" source="n1935" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7415" source="n1936" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7416" source="n1937" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7417" source="n1938" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7418" source="n1939" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7419" source="n1940" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7420" source="n1941" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7421" source="n1942" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7422" source="n1943" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7423" source="n1944" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7424" source="n1945" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7425" source="n1946" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e7426" source="n450" target="n1947" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7427" source="n450" target="n1948" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7428" source="n450" target="n1949" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7429" source="n450" target="n1950" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7430" source="n450" target="n1951" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7431" source="n450" target="n1952" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7432" source="n450" target="n1953" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7433" source="n450" target="n694" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7434" source="n450" target="n1954" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7435" source="n450" target="n1955" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7436" source="n450" target="n638" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7437" source="n450" target="n1956" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7438" source="n450" target="n1957" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7439" source="n450" target="n1958" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7440" source="n450" target="n1959" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7441" source="n450" target="n1960" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7442" source="n450" target="n1961" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7443" source="n450" target="n1962" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7444" source="n450" target="n1105" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7445" source="n450" target="n1963" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7446" source="n450" target="n1964" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7447" source="n450" target="n1641" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7448" source="n450" target="n1047" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7449" source="n450" target="n572" label="HAS"><data key="label">HAS</data><data key="score">6.730769230769231</data></edge>
<edge id="e7450" source="n450" target="n516" label="HAS"><data key="label">HAS</data><data key="score">7.6923076923076925</data></edge>
<edge id="e7451" source="n450" target="n581" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e7452" source="n450" target="n1077" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7453" source="n450" target="n1965" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7454" source="n450" target="n1070" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7455" source="n450" target="n610" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7456" source="n450" target="n1966" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7457" source="n450" target="n1709" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7458" source="n450" target="n1083" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7459" source="n450" target="n522" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7460" source="n450" target="n1050" label="HAS"><data key="label">HAS</data><data key="score">10.576923076923077</data></edge>
<edge id="e7461" source="n450" target="n1106" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7462" source="n450" target="n1885" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7463" source="n450" target="n1967" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7464" source="n450" target="n104" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7465" source="n450" target="n655" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7466" source="n450" target="n1020" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7467" source="n450" target="n1968" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7468" source="n450" target="n1969" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7469" source="n450" target="n1970" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7470" source="n450" target="n1090" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7471" source="n450" target="n1971" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7472" source="n450" target="n1972" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7473" source="n450" target="n1973" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7474" source="n450" target="n1751" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7475" source="n450" target="n1974" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7476" source="n450" target="n1891" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7477" source="n450" target="n1975" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7478" source="n450" target="n1976" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7479" source="n450" target="n513" label="HAS"><data key="label">HAS</data><data key="score">13.461538461538462</data></edge>
<edge id="e7480" source="n450" target="n1630" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e7481" source="n450" target="n1671" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7482" source="n450" target="n1977" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7483" source="n450" target="n1978" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7484" source="n450" target="n532" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7485" source="n450" target="n1979" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7486" source="n450" target="n1980" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7487" source="n450" target="n1981" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7488" source="n450" target="n1982" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7489" source="n450" target="n640" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7490" source="n450" target="n1983" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7491" source="n450" target="n1984" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7492" source="n450" target="n1985" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7493" source="n450" target="n1986" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7494" source="n450" target="n1854" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7495" source="n450" target="n1987" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7496" source="n450" target="n551" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7497" source="n450" target="n558" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7498" source="n450" target="n1988" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7499" source="n450" target="n46" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7500" source="n450" target="n1989" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7501" source="n450" target="n1990" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7502" source="n450" target="n1991" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7503" source="n450" target="n1992" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7504" source="n450" target="n1993" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7505" source="n450" target="n1994" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7506" source="n450" target="n1995" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7507" source="n450" target="n1015" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7508" source="n450" target="n1996" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7509" source="n450" target="n1997" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7510" source="n450" target="n1998" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7511" source="n450" target="n1999" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7512" source="n450" target="n2000" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7513" source="n450" target="n2001" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7514" source="n450" target="n2002" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7515" source="n450" target="n2003" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7516" source="n450" target="n2004" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7517" source="n450" target="n1037" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7518" source="n450" target="n693" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7519" source="n450" target="n1056" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7520" source="n450" target="n2005" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7521" source="n450" target="n561" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7522" source="n450" target="n2006" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7523" source="n450" target="n574" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7524" source="n450" target="n1033" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7525" source="n450" target="n274" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7526" source="n450" target="n2007" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7527" source="n450" target="n598" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7528" source="n450" target="n2008" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7529" source="n450" target="n1036" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7530" source="n450" target="n2009" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7531" source="n450" target="n2010" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7532" source="n450" target="n418" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7533" source="n450" target="n250" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7534" source="n450" target="n2011" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7535" source="n450" target="n428" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7536" source="n450" target="n1076" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7537" source="n450" target="n560" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7538" source="n450" target="n576" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7539" source="n450" target="n2012" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7540" source="n450" target="n2013" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7541" source="n450" target="n661" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7542" source="n450" target="n2014" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7543" source="n450" target="n2015" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7544" source="n450" target="n600" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7545" source="n450" target="n2016" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7546" source="n450" target="n585" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e7547" source="n450" target="n2017" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7548" source="n450" target="n632" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e7549" source="n450" target="n2018" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e7550" source="n450" target="n2019" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7551" source="n450" target="n2020" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7552" source="n450" target="n2021" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7553" source="n450" target="n2022" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7554" source="n450" target="n2023" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7555" source="n450" target="n2024" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7556" source="n450" target="n2025" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7557" source="n450" target="n2026" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7558" source="n450" target="n2027" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7559" source="n450" target="n1038" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7560" source="n450" target="n2028" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7561" source="n450" target="n2029" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7562" source="n450" target="n2030" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7563" source="n450" target="n2031" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7564" source="n450" target="n2032" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7565" source="n450" target="n2033" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7566" source="n450" target="n2034" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7567" source="n450" target="n524" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7568" source="n450" target="n1081" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7569" source="n450" target="n2035" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e7570" source="n2036" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7571" source="n2036" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7572" source="n2036" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7573" source="n2036" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7574" source="n2036" target="n1978" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7575" source="n2036" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7576" source="n2036" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7577" source="n2036" target="n166" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7578" source="n2036" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7579" source="n2036" target="n604" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7580" source="n2036" target="n36" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7581" source="n2036" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7582" source="n2036" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7583" source="n2036" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7584" source="n2036" target="n836" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7585" source="n2036" target="n939" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7586" source="n2036" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7587" source="n2036" target="n1092" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7588" source="n2036" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7589" source="n2036" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7590" source="n2036" target="n163" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7591" source="n2036" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7592" source="n2036" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7593" source="n2036" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7594" source="n2036" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7595" source="n2037" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7596" source="n2037" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7597" source="n2037" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7598" source="n2037" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7599" source="n2037" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7600" source="n2037" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7601" source="n2037" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7602" source="n2037" target="n671" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7603" source="n2037" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7604" source="n2037" target="n1092" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7605" source="n2037" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7606" source="n2037" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7607" source="n2038" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7608" source="n2038" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7609" source="n2038" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7610" source="n2038" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7611" source="n2038" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7612" source="n2038" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7613" source="n2038" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7614" source="n2038" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7615" source="n2038" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7616" source="n2038" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7617" source="n2038" target="n1668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7618" source="n2038" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7619" source="n2038" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7620" source="n2038" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7621" source="n2038" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7622" source="n2038" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7623" source="n2039" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7624" source="n2039" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7625" source="n2039" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7626" source="n2039" target="n551" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7627" source="n2039" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7628" source="n2039" target="n6" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7629" source="n2039" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7630" source="n2039" target="n16" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7631" source="n2039" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7632" source="n2039" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7633" source="n2039" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7634" source="n2039" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7635" source="n2039" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7636" source="n2039" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7637" source="n2039" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7638" source="n2039" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7639" source="n2039" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7640" source="n2039" target="n88" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7641" source="n2039" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7642" source="n2039" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7643" source="n2039" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7644" source="n2039" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7645" source="n2039" target="n390" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7646" source="n2039" target="n290" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7647" source="n2039" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7648" source="n2039" target="n295" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7649" source="n2039" target="n208" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7650" source="n2039" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7651" source="n2039" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7652" source="n2039" target="n939" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7653" source="n2039" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7654" source="n2039" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7655" source="n2039" target="n2018" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7656" source="n2039" target="n350" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7657" source="n2039" target="n173" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7658" source="n2039" target="n334" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7659" source="n2039" target="n75" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7660" source="n2039" target="n237" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7661" source="n2039" target="n367" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7662" source="n2039" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7663" source="n2039" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7664" source="n2039" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7665" source="n2039" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7666" source="n2039" target="n183" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7667" source="n2039" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7668" source="n2039" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7669" source="n2039" target="n920" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7670" source="n2039" target="n623" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7671" source="n2039" target="n590" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7672" source="n2039" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7673" source="n2039" target="n523" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7674" source="n2039" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7675" source="n2039" target="n959" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7676" source="n2039" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7677" source="n2039" target="n562" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7678" source="n2039" target="n1858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7679" source="n2039" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7680" source="n2039" target="n166" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7681" source="n2039" target="n1664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7682" source="n2039" target="n248" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7683" source="n2039" target="n559" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7684" source="n2039" target="n1836" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7685" source="n2039" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7686" source="n2039" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7687" source="n2039" target="n1082" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7688" source="n2039" target="n1978" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7689" source="n2040" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7690" source="n2040" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7691" source="n2040" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7692" source="n2040" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7693" source="n2040" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7694" source="n2040" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7695" source="n2040" target="n12" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7696" source="n2040" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7697" source="n2040" target="n590" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7698" source="n2040" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7699" source="n2040" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7700" source="n2040" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7701" source="n2040" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7702" source="n2040" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7703" source="n2040" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7704" source="n2040" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7705" source="n2040" target="n1858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7706" source="n2040" target="n1789" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7707" source="n2041" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7708" source="n2041" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7709" source="n2041" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7710" source="n2041" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7711" source="n2041" target="n671" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7712" source="n2041" target="n964" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7713" source="n2041" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7714" source="n2041" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7715" source="n2041" target="n504" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7716" source="n2041" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7717" source="n2041" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7718" source="n2041" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7719" source="n2041" target="n1793" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7720" source="n2042" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7721" source="n2042" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7722" source="n2042" target="n808" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7723" source="n2042" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7724" source="n2042" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7725" source="n2042" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7726" source="n2042" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7727" source="n2042" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7728" source="n2043" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7729" source="n2043" target="n657" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7730" source="n2043" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7731" source="n2043" target="n897" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7732" source="n2043" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7733" source="n2043" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7734" source="n2043" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7735" source="n2043" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7736" source="n2043" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7737" source="n2043" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7738" source="n2043" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7739" source="n2043" target="n1668" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7740" source="n2043" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7741" source="n2043" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7742" source="n2043" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7743" source="n2043" target="n5" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7744" source="n2043" target="n567" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7745" source="n2043" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7746" source="n2043" target="n162" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7747" source="n2043" target="n689" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7748" source="n2043" target="n598" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7749" source="n2044" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7750" source="n2044" target="n12" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7751" source="n2044" target="n839" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7752" source="n2044" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7753" source="n2044" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7754" source="n2044" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7755" source="n2044" target="n557" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7756" source="n2044" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7757" source="n2044" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7758" source="n2044" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7759" source="n2044" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7760" source="n2044" target="n581" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7761" source="n2044" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7762" source="n2044" target="n1767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7763" source="n2044" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7764" source="n2044" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7765" source="n2044" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7766" source="n2044" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7767" source="n2044" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7768" source="n2044" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7769" source="n2044" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7770" source="n2044" target="n702" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7771" source="n2044" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7772" source="n2044" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7773" source="n2044" target="n909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7774" source="n2044" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7775" source="n2044" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7776" source="n2044" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7777" source="n2044" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7778" source="n2044" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7779" source="n2044" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7780" source="n2044" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7781" source="n2045" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7782" source="n2045" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7783" source="n2045" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7784" source="n2045" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7785" source="n2045" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7786" source="n2045" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7787" source="n2045" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7788" source="n2045" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7789" source="n2045" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7790" source="n2045" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7791" source="n2046" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7792" source="n2046" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7793" source="n2046" target="n114" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7794" source="n2046" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7795" source="n2046" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7796" source="n2046" target="n301" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7797" source="n2046" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7798" source="n2046" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7799" source="n2046" target="n162" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7800" source="n2046" target="n20" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7801" source="n2046" target="n6" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7802" source="n2046" target="n1082" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7803" source="n2046" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7804" source="n2047" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7805" source="n2047" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7806" source="n2047" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7807" source="n2047" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7808" source="n2047" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7809" source="n2047" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7810" source="n2047" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7811" source="n2047" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7812" source="n2047" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7813" source="n2047" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7814" source="n2047" target="n259" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7815" source="n2047" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7816" source="n2047" target="n71" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7817" source="n2047" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7818" source="n2047" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7819" source="n2047" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7820" source="n2047" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7821" source="n2047" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7822" source="n2047" target="n1617" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7823" source="n2047" target="n346" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7824" source="n2047" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7825" source="n2047" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7826" source="n2047" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7827" source="n2047" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7828" source="n2047" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7829" source="n2047" target="n939" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7830" source="n2047" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7831" source="n2047" target="n205" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7832" source="n2047" target="n523" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7833" source="n2047" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7834" source="n2047" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7835" source="n2047" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7836" source="n2047" target="n824" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7837" source="n2047" target="n970" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7838" source="n2047" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7839" source="n2047" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7840" source="n2047" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7841" source="n2047" target="n560" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7842" source="n2047" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7843" source="n2047" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7844" source="n2047" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7845" source="n2047" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7846" source="n2047" target="n318" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7847" source="n2047" target="n542" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7848" source="n2036" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7849" source="n2037" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7850" source="n2038" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7851" source="n2039" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7852" source="n2040" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7853" source="n2041" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7854" source="n2042" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7855" source="n2043" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7856" source="n2044" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7857" source="n2045" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7858" source="n2046" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7859" source="n2047" target="n451" label="IS"><data key="label">IS</data></edge>
<edge id="e7860" source="n451" target="n1978" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7861" source="n451" target="n604" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7862" source="n451" target="n836" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7863" source="n451" target="n939" label="HAS"><data key="label">HAS</data><data key="score">3.5294117647058822</data></edge>
<edge id="e7864" source="n451" target="n163" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7865" source="n451" target="n157" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7866" source="n451" target="n671" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7867" source="n451" target="n1668" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7868" source="n451" target="n16" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7869" source="n451" target="n390" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7870" source="n451" target="n208" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7871" source="n451" target="n2018" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7872" source="n451" target="n173" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7873" source="n451" target="n237" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7874" source="n451" target="n367" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7875" source="n451" target="n183" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7876" source="n451" target="n920" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7877" source="n451" target="n590" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7878" source="n451" target="n523" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7879" source="n451" target="n562" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7880" source="n451" target="n1858" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7881" source="n451" target="n1664" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7882" source="n451" target="n559" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7883" source="n451" target="n1836" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7884" source="n451" target="n12" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7885" source="n451" target="n952" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7886" source="n451" target="n504" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7887" source="n451" target="n744" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7888" source="n451" target="n657" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7889" source="n451" target="n897" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7890" source="n451" target="n538" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7891" source="n451" target="n567" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7892" source="n451" target="n162" label="HAS"><data key="label">HAS</data><data key="score">2.3529411764705883</data></edge>
<edge id="e7893" source="n451" target="n689" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7894" source="n451" target="n557" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7895" source="n451" target="n581" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7896" source="n451" target="n909" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7897" source="n451" target="n114" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7898" source="n451" target="n301" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7899" source="n451" target="n20" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7900" source="n451" target="n71" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7901" source="n451" target="n351" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7902" source="n451" target="n205" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7903" source="n451" target="n901" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7904" source="n451" target="n970" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7905" source="n451" target="n641" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7906" source="n451" target="n560" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7907" source="n451" target="n318" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7908" source="n451" target="n542" label="HAS"><data key="label">HAS</data><data key="score">1.1764705882352942</data></edge>
<edge id="e7909" source="n2075" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7910" source="n2075" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7911" source="n2075" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7912" source="n2075" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7913" source="n2075" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7914" source="n2075" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7915" source="n2075" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7916" source="n2075" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7917" source="n2075" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7918" source="n2075" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7919" source="n2075" target="n855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7920" source="n2075" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7921" source="n2075" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7922" source="n2075" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7923" source="n2075" target="n747" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7924" source="n2075" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7925" source="n2075" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7926" source="n2075" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7927" source="n2075" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7928" source="n2075" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7929" source="n2075" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7930" source="n2075" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7931" source="n2075" target="n972" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7932" source="n2075" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7933" source="n2075" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7934" source="n2075" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7935" source="n2075" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7936" source="n2075" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7937" source="n2075" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7938" source="n2075" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7939" source="n2075" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7940" source="n2075" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7941" source="n2075" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7942" source="n2075" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7943" source="n2075" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7944" source="n2075" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7945" source="n2075" target="n878" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7946" source="n2075" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7947" source="n2075" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7948" source="n2075" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7949" source="n2075" target="n900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7950" source="n2075" target="n627" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7951" source="n2075" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7952" source="n2075" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7953" source="n2075" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7954" source="n2075" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7955" source="n2075" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7956" source="n2075" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7957" source="n2075" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7958" source="n2075" target="n6" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7959" source="n2075" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7960" source="n2075" target="n4" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7961" source="n2075" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7962" source="n2075" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7963" source="n2075" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7964" source="n2075" target="n1058" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7965" source="n2075" target="n662" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7966" source="n2075" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7967" source="n2075" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7968" source="n2075" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7969" source="n2075" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7970" source="n2075" target="n951" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7971" source="n2075" target="n1998" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7972" source="n2075" target="n868" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7973" source="n2075" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7974" source="n2075" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7975" source="n2076" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7976" source="n2076" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7977" source="n2076" target="n150" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7978" source="n2076" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7979" source="n2076" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7980" source="n2076" target="n58" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7981" source="n2076" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7982" source="n2076" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7983" source="n2076" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7984" source="n2076" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7985" source="n2076" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7986" source="n2076" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7987" source="n2076" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7988" source="n2076" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7989" source="n2076" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7990" source="n2076" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7991" source="n2076" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7992" source="n2076" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7993" source="n2076" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7994" source="n2076" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7995" source="n2077" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7996" source="n2077" target="n856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7997" source="n2077" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7998" source="n2077" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e7999" source="n2077" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8000" source="n2077" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8001" source="n2077" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8002" source="n2077" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8003" source="n2077" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8004" source="n2077" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8005" source="n2077" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8006" source="n2077" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8007" source="n2077" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8008" source="n2077" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8009" source="n2077" target="n582" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8010" source="n2077" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8011" source="n2077" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8012" source="n2077" target="n31" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8013" source="n2077" target="n696" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8014" source="n2077" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8015" source="n2077" target="n1058" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8016" source="n2077" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8017" source="n2077" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8018" source="n2077" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8019" source="n2077" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8020" source="n2077" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8021" source="n2077" target="n909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8022" source="n2077" target="n920" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8023" source="n2077" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8024" source="n2077" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8025" source="n2077" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8026" source="n2077" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8027" source="n2077" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8028" source="n2078" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8029" source="n2078" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8030" source="n2078" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8031" source="n2078" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8032" source="n2078" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8033" source="n2078" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8034" source="n2078" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8035" source="n2078" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8036" source="n2078" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8037" source="n2078" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8038" source="n2078" target="n1634" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8039" source="n2078" target="n1761" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8040" source="n2078" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8041" source="n2078" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8042" source="n2078" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8043" source="n2078" target="n1624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8044" source="n2078" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8045" source="n2078" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8046" source="n2078" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8047" source="n2078" target="n261" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8048" source="n2078" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8049" source="n2078" target="n788" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8050" source="n2078" target="n340" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8051" source="n2078" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8052" source="n2078" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8053" source="n2078" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8054" source="n2078" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8055" source="n2078" target="n88" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8056" source="n2078" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8057" source="n2078" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8058" source="n2078" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8059" source="n2078" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8060" source="n2079" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8061" source="n2079" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8062" source="n2079" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8063" source="n2079" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8064" source="n2079" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8065" source="n2079" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8066" source="n2079" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8067" source="n2079" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8068" source="n2079" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8069" source="n2079" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8070" source="n2079" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8071" source="n2079" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8072" source="n2079" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8073" source="n2079" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8074" source="n2079" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8075" source="n2079" target="n909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8076" source="n2079" target="n584" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8077" source="n2079" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8078" source="n2079" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8079" source="n2079" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8080" source="n2079" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8081" source="n2079" target="n20" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8082" source="n2079" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8083" source="n2079" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8084" source="n2079" target="n809" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8085" source="n2079" target="n1793" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8086" source="n2079" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8087" source="n2079" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8088" source="n2080" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8089" source="n2080" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8090" source="n2080" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8091" source="n2080" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8092" source="n2080" target="n1082" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8093" source="n2080" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8094" source="n2080" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8095" source="n2080" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8096" source="n2080" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8097" source="n2080" target="n1915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8098" source="n2080" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8099" source="n2080" target="n1741" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8100" source="n2080" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8101" source="n2080" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8102" source="n2080" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8103" source="n2080" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8104" source="n2080" target="n1791" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8105" source="n2080" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8106" source="n2080" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8107" source="n2080" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8108" source="n2080" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8109" source="n2080" target="n1978" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8110" source="n2080" target="n650" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8111" source="n2080" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8112" source="n2080" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8113" source="n2080" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8114" source="n2080" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8115" source="n2080" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8116" source="n2080" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8117" source="n2080" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8118" source="n2080" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8119" source="n2080" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8120" source="n2080" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8121" source="n2080" target="n441" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8122" source="n2080" target="n765" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8123" source="n2080" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8124" source="n2080" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8125" source="n2080" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8126" source="n2080" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8127" source="n2080" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8128" source="n2080" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8129" source="n2080" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8130" source="n2080" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8131" source="n2080" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8132" source="n2081" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8133" source="n2081" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8134" source="n2081" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8135" source="n2081" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8136" source="n2081" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8137" source="n2081" target="n810" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8138" source="n2081" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8139" source="n2081" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8140" source="n2081" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8141" source="n2081" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8142" source="n2081" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8143" source="n2081" target="n2018" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8144" source="n2081" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8145" source="n2081" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8146" source="n2081" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8147" source="n2081" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8148" source="n2081" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8149" source="n2081" target="n1037" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8150" source="n2081" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8151" source="n2081" target="n29" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8152" source="n2081" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8153" source="n2081" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8154" source="n2081" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8155" source="n2081" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8156" source="n2081" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8157" source="n2081" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8158" source="n2081" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8159" source="n2081" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8160" source="n2081" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8161" source="n2081" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8162" source="n2081" target="n367" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8163" source="n2081" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8164" source="n2081" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8165" source="n2081" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8166" source="n2081" target="n800" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8167" source="n2081" target="n1969" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8168" source="n2081" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8169" source="n2081" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8170" source="n2081" target="n950" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8171" source="n2081" target="n584" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8172" source="n2081" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8173" source="n2081" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8174" source="n2081" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8175" source="n2081" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8176" source="n2081" target="n1791" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8177" source="n2081" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8178" source="n2081" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8179" source="n2081" target="n801" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8180" source="n2081" target="n261" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8181" source="n2081" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8182" source="n2081" target="n1028" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8183" source="n2081" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8184" source="n2081" target="n1615" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8185" source="n2081" target="n388" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8186" source="n2081" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8187" source="n2081" target="n98" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8188" source="n2081" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8189" source="n2081" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8190" source="n2081" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8191" source="n2081" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8192" source="n2081" target="n205" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8193" source="n2081" target="n434" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8194" source="n2081" target="n102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8195" source="n2081" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8196" source="n2081" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8197" source="n2081" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8198" source="n2081" target="n1977" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8199" source="n2081" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8200" source="n2081" target="n604" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8201" source="n2081" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8202" source="n2081" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8203" source="n2081" target="n140" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8204" source="n2081" target="n153" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8205" source="n2081" target="n939" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8206" source="n2081" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8207" source="n2081" target="n345" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8208" source="n2081" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8209" source="n2081" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8210" source="n2081" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8211" source="n2081" target="n585" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8212" source="n2081" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8213" source="n2081" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8214" source="n2081" target="n1630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8215" source="n2081" target="n656" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8216" source="n2081" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8217" source="n2081" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8218" source="n2081" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8219" source="n2081" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8220" source="n2081" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8221" source="n2081" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8222" source="n2081" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8223" source="n2081" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8224" source="n2081" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8225" source="n2081" target="n127" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8226" source="n2081" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8227" source="n2081" target="n237" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8228" source="n2081" target="n540" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8229" source="n2081" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8230" source="n2081" target="n330" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8231" source="n2081" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8232" source="n2081" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8233" source="n2081" target="n61" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8234" source="n2081" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8235" source="n2081" target="n855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8236" source="n2081" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8237" source="n2081" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8238" source="n2081" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8239" source="n2081" target="n846" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8240" source="n2081" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8241" source="n2081" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8242" source="n2081" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8243" source="n2081" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8244" source="n2081" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8245" source="n2081" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8246" source="n2081" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8247" source="n2081" target="n627" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8248" source="n2081" target="n900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8249" source="n2081" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8250" source="n2081" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8251" source="n2081" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8252" source="n2081" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8253" source="n2082" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8254" source="n2082" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8255" source="n2082" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8256" source="n2082" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8257" source="n2082" target="n836" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8258" source="n2082" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8259" source="n2082" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8260" source="n2082" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8261" source="n2082" target="n2018" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8262" source="n2082" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8263" source="n2082" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8264" source="n2082" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8265" source="n2082" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8266" source="n2082" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8267" source="n2082" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8268" source="n2082" target="n697" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8269" source="n2082" target="n115" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8270" source="n2082" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8271" source="n2082" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8272" source="n2082" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8273" source="n2082" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8274" source="n2082" target="n261" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8275" source="n2082" target="n105" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8276" source="n2082" target="n255" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8277" source="n2082" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8278" source="n2082" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8279" source="n2082" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8280" source="n2082" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8281" source="n2082" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8282" source="n2082" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8283" source="n2082" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8284" source="n2082" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8285" source="n2082" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8286" source="n2082" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8287" source="n2082" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8288" source="n2082" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8289" source="n2082" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8290" source="n2082" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8291" source="n2082" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8292" source="n2082" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8293" source="n2082" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8294" source="n2082" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8295" source="n2082" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8296" source="n2082" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8297" source="n2082" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8298" source="n2082" target="n781" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8299" source="n2082" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8300" source="n2082" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8301" source="n2082" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8302" source="n2082" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8303" source="n2082" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8304" source="n2082" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8305" source="n2082" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8306" source="n2082" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8307" source="n2082" target="n974" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8308" source="n2082" target="n961" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8309" source="n2082" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8310" source="n2082" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8311" source="n2082" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8312" source="n2082" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8313" source="n2082" target="n547" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8314" source="n2082" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8315" source="n2082" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8316" source="n2083" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8317" source="n2083" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8318" source="n2083" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8319" source="n2083" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8320" source="n2083" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8321" source="n2083" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8322" source="n2083" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8323" source="n2083" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8324" source="n2083" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8325" source="n2083" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8326" source="n2083" target="n748" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8327" source="n2083" target="n653" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8328" source="n2083" target="n652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8329" source="n2083" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8330" source="n2083" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8331" source="n2083" target="n552" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8332" source="n2083" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8333" source="n2083" target="n301" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8334" source="n2083" target="n64" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8335" source="n2083" target="n687" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8336" source="n2083" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8337" source="n2083" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8338" source="n2083" target="n520" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8339" source="n2083" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8340" source="n2083" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8341" source="n2083" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8342" source="n2083" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8343" source="n2083" target="n604" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8344" source="n2084" target="n634" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8345" source="n2084" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8346" source="n2084" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8347" source="n2084" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8348" source="n2084" target="n500" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8349" source="n2084" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8350" source="n2084" target="n1085" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8351" source="n2084" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8352" source="n2084" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8353" source="n2084" target="n667" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8354" source="n2084" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8355" source="n2085" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8356" source="n2085" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8357" source="n2085" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8358" source="n2085" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8359" source="n2085" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8360" source="n2085" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8361" source="n2085" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8362" source="n2085" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8363" source="n2085" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8364" source="n2085" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8365" source="n2085" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8366" source="n2085" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8367" source="n2085" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8368" source="n2086" target="n902" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8369" source="n2086" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8370" source="n2086" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8371" source="n2086" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8372" source="n2086" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8373" source="n2086" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8374" source="n2086" target="n542" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8375" source="n2086" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8376" source="n2086" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8377" source="n2086" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8378" source="n2086" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8379" source="n2086" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8380" source="n2086" target="n600" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8381" source="n2086" target="n909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8382" source="n2086" target="n153" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8383" source="n2086" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8384" source="n2086" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8385" source="n2087" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8386" source="n2087" target="n353" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8387" source="n2087" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8388" source="n2087" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8389" source="n2087" target="n590" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8390" source="n2087" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8391" source="n2087" target="n610" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8392" source="n2087" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8393" source="n2087" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8394" source="n2088" target="n592" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8395" source="n2088" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8396" source="n2088" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8397" source="n2088" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8398" source="n2088" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8399" source="n2088" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8400" source="n2088" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8401" source="n2088" target="n1654" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8402" source="n2088" target="n631" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8403" source="n2088" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8404" source="n2088" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8405" source="n2088" target="n209" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8406" source="n2088" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8407" source="n2088" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8408" source="n2088" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8409" source="n2088" target="n1749" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8410" source="n2088" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8411" source="n2088" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8412" source="n2088" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8413" source="n2088" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8414" source="n2089" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8415" source="n2089" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8416" source="n2089" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8417" source="n2089" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8418" source="n2089" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8419" source="n2089" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8420" source="n2089" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8421" source="n2089" target="n1749" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8422" source="n2089" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8423" source="n2089" target="n755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8424" source="n2089" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8425" source="n2089" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8426" source="n2089" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8427" source="n2089" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8428" source="n2089" target="n81" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8429" source="n2089" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8430" source="n2089" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8431" source="n2089" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8432" source="n2089" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8433" source="n2089" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8434" source="n2090" target="n576" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8435" source="n2090" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8436" source="n2090" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8437" source="n2090" target="n1704" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8438" source="n2090" target="n631" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8439" source="n2090" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8440" source="n2090" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8441" source="n2090" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8442" source="n2090" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8443" source="n2090" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8444" source="n2090" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8445" source="n2090" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8446" source="n2090" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8447" source="n2090" target="n857" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8448" source="n2090" target="n603" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8449" source="n2090" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8450" source="n2090" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8451" source="n2091" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8452" source="n2091" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8453" source="n2091" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8454" source="n2091" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8455" source="n2091" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8456" source="n2091" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8457" source="n2091" target="n500" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8458" source="n2091" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8459" source="n2091" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8460" source="n2091" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8461" source="n2091" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8462" source="n2091" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8463" source="n2091" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8464" source="n2091" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8465" source="n2091" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8466" source="n2091" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8467" source="n2091" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8468" source="n2091" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8469" source="n2091" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8470" source="n2091" target="n1082" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8471" source="n2091" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8472" source="n2091" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8473" source="n2092" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8474" source="n2092" target="n1630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8475" source="n2092" target="n805" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8476" source="n2092" target="n346" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8477" source="n2092" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8478" source="n2092" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8479" source="n2092" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8480" source="n2092" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8481" source="n2092" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8482" source="n2092" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8483" source="n2092" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8484" source="n2092" target="n499" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8485" source="n2092" target="n40" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8486" source="n2092" target="n811" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8487" source="n2092" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8488" source="n2092" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8489" source="n2092" target="n614" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8490" source="n2092" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8491" source="n2092" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8492" source="n2092" target="n558" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8493" source="n2092" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8494" source="n2092" target="n1671" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8495" source="n2092" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8496" source="n2092" target="n31" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8497" source="n2092" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8498" source="n2092" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8499" source="n2092" target="n1073" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8500" source="n2092" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8501" source="n2092" target="n803" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8502" source="n2092" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8503" source="n2092" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8504" source="n2092" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8505" source="n2092" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8506" source="n2075" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8507" source="n2076" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8508" source="n2077" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8509" source="n2078" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8510" source="n2079" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8511" source="n2080" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8512" source="n2081" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8513" source="n2082" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8514" source="n2083" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8515" source="n2084" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8516" source="n2085" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8517" source="n2086" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8518" source="n2087" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8519" source="n2088" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8520" source="n2089" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8521" source="n2090" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8522" source="n2091" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8523" source="n2092" target="n452" label="IS"><data key="label">IS</data></edge>
<edge id="e8524" source="n452" target="n792" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e8525" source="n452" target="n747" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8526" source="n452" target="n361" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8527" source="n452" target="n972" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8528" source="n452" target="n878" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8529" source="n452" target="n756" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8530" source="n452" target="n752" label="HAS"><data key="label">HAS</data><data key="score">4.504504504504505</data></edge>
<edge id="e8531" source="n452" target="n378" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8532" source="n452" target="n6" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8533" source="n452" target="n1058" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8534" source="n452" target="n901" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8535" source="n452" target="n951" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8536" source="n452" target="n1998" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8537" source="n452" target="n868" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8538" source="n452" target="n952" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e8539" source="n452" target="n58" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8540" source="n452" target="n754" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8541" source="n452" target="n915" label="HAS"><data key="label">HAS</data><data key="score">3.6036036036036037</data></edge>
<edge id="e8542" source="n452" target="n858" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8543" source="n452" target="n909" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e8544" source="n452" target="n920" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8545" source="n452" target="n1634" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8546" source="n452" target="n1761" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8547" source="n452" target="n1624" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8548" source="n452" target="n261" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e8549" source="n452" target="n788" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8550" source="n452" target="n809" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8551" source="n452" target="n1793" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8552" source="n452" target="n1082" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8553" source="n452" target="n1741" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8554" source="n452" target="n1791" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8555" source="n452" target="n1978" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8556" source="n452" target="n441" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8557" source="n452" target="n765" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8558" source="n452" target="n1050" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8559" source="n452" target="n896" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8560" source="n452" target="n2018" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8561" source="n452" target="n1037" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8562" source="n452" target="n29" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8563" source="n452" target="n962" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8564" source="n452" target="n375" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8565" source="n452" target="n800" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8566" source="n452" target="n1969" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8567" source="n452" target="n950" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8568" source="n452" target="n801" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8569" source="n452" target="n932" label="HAS"><data key="label">HAS</data><data key="score">2.7027027027027026</data></edge>
<edge id="e8570" source="n452" target="n287" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8571" source="n452" target="n1615" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8572" source="n452" target="n388" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8573" source="n452" target="n205" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8574" source="n452" target="n102" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8575" source="n452" target="n1977" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8576" source="n452" target="n345" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8577" source="n452" target="n1630" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8578" source="n452" target="n237" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8579" source="n452" target="n330" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8580" source="n452" target="n61" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8581" source="n452" target="n846" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8582" source="n452" target="n767" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8583" source="n452" target="n836" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8584" source="n452" target="n115" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8585" source="n452" target="n72" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8586" source="n452" target="n105" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8587" source="n452" target="n255" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8588" source="n452" target="n947" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8589" source="n452" target="n377" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8590" source="n452" target="n1044" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8591" source="n452" target="n781" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8592" source="n452" target="n961" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8593" source="n452" target="n748" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8594" source="n452" target="n301" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8595" source="n452" target="n1085" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8596" source="n452" target="n902" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8597" source="n452" target="n353" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8598" source="n452" target="n1654" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8599" source="n452" target="n209" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8600" source="n452" target="n1749" label="HAS"><data key="label">HAS</data><data key="score">1.8018018018018018</data></edge>
<edge id="e8601" source="n452" target="n755" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8602" source="n452" target="n81" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8603" source="n452" target="n1704" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8604" source="n452" target="n931" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8605" source="n452" target="n857" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8606" source="n452" target="n805" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8607" source="n452" target="n1671" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8608" source="n452" target="n1073" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8609" source="n452" target="n803" label="HAS"><data key="label">HAS</data><data key="score">0.9009009009009009</data></edge>
<edge id="e8610" source="n2129" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8611" source="n2129" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8612" source="n2129" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8613" source="n2129" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8614" source="n2129" target="n54" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8615" source="n2129" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8616" source="n2129" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8617" source="n2129" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8618" source="n2129" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8619" source="n2129" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8620" source="n2129" target="n855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8621" source="n2129" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8622" source="n2129" target="n664" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8623" source="n2129" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8624" source="n2129" target="n747" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8625" source="n2129" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8626" source="n2129" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8627" source="n2129" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8628" source="n2129" target="n361" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8629" source="n2129" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8630" source="n2129" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8631" source="n2129" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8632" source="n2129" target="n972" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8633" source="n2129" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8634" source="n2129" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8635" source="n2129" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8636" source="n2129" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8637" source="n2129" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8638" source="n2129" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8639" source="n2129" target="n241" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8640" source="n2129" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8641" source="n2129" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8642" source="n2129" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8643" source="n2129" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8644" source="n2129" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8645" source="n2129" target="n878" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8646" source="n2129" target="n756" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8647" source="n2129" target="n24" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8648" source="n2129" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8649" source="n2129" target="n900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8650" source="n2129" target="n627" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8651" source="n2129" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8652" source="n2129" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8653" source="n2129" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8654" source="n2129" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8655" source="n2129" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8656" source="n2129" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8657" source="n2129" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8658" source="n2129" target="n6" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8659" source="n2129" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8660" source="n2129" target="n4" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8661" source="n2129" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8662" source="n2129" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8663" source="n2129" target="n1058" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8664" source="n2129" target="n662" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8665" source="n2129" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8666" source="n2129" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8667" source="n2129" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8668" source="n2129" target="n901" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8669" source="n2129" target="n951" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8670" source="n2129" target="n868" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8671" source="n2129" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8672" source="n2129" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8673" source="n2130" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8674" source="n2130" target="n150" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8675" source="n2130" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8676" source="n2130" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8677" source="n2130" target="n58" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8678" source="n2130" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8679" source="n2130" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8680" source="n2130" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8681" source="n2130" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8682" source="n2130" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8683" source="n2130" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8684" source="n2130" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8685" source="n2130" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8686" source="n2130" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8687" source="n2130" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8688" source="n2130" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8689" source="n2130" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8690" source="n2130" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8691" source="n2130" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8692" source="n2130" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8693" source="n2131" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8694" source="n2131" target="n856" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8695" source="n2131" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8696" source="n2131" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8697" source="n2131" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8698" source="n2131" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8699" source="n2131" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8700" source="n2131" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8701" source="n2131" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8702" source="n2131" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8703" source="n2131" target="n11" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8704" source="n2131" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8705" source="n2131" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8706" source="n2131" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8707" source="n2131" target="n582" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8708" source="n2131" target="n754" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8709" source="n2131" target="n31" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8710" source="n2131" target="n696" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8711" source="n2131" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8712" source="n2131" target="n1058" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8713" source="n2131" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8714" source="n2131" target="n858" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8715" source="n2131" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8716" source="n2131" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8717" source="n2131" target="n909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8718" source="n2131" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8719" source="n2131" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8720" source="n2131" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8721" source="n2131" target="n920" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8722" source="n2132" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8723" source="n2132" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8724" source="n2132" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8725" source="n2132" target="n1994" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8726" source="n2132" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8727" source="n2132" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8728" source="n2132" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8729" source="n2132" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8730" source="n2132" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8731" source="n2132" target="n1634" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8732" source="n2132" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8733" source="n2132" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8734" source="n2132" target="n1624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8735" source="n2132" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8736" source="n2132" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8737" source="n2132" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8738" source="n2132" target="n261" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8739" source="n2132" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8740" source="n2132" target="n788" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8741" source="n2132" target="n340" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8742" source="n2132" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8743" source="n2132" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8744" source="n2132" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8745" source="n2132" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8746" source="n2132" target="n88" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8747" source="n2132" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8748" source="n2132" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8749" source="n2132" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8750" source="n2132" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8751" source="n2133" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8752" source="n2133" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8753" source="n2133" target="n9" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8754" source="n2133" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8755" source="n2133" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8756" source="n2133" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8757" source="n2133" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8758" source="n2133" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8759" source="n2133" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8760" source="n2133" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8761" source="n2133" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8762" source="n2133" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8763" source="n2133" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8764" source="n2133" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8765" source="n2133" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8766" source="n2133" target="n909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8767" source="n2133" target="n584" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8768" source="n2133" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8769" source="n2133" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8770" source="n2133" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8771" source="n2133" target="n20" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8772" source="n2133" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8773" source="n2133" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8774" source="n2133" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8775" source="n2133" target="n809" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8776" source="n2133" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8777" source="n2133" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8778" source="n2134" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8779" source="n2134" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8780" source="n2134" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8781" source="n2134" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8782" source="n2134" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8783" source="n2134" target="n1082" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8784" source="n2134" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8785" source="n2134" target="n149" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8786" source="n2134" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8787" source="n2134" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8788" source="n2134" target="n3" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8789" source="n2134" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8790" source="n2134" target="n1069" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8791" source="n2134" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8792" source="n2134" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8793" source="n2134" target="n1791" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8794" source="n2134" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8795" source="n2134" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8796" source="n2134" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8797" source="n2134" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8798" source="n2134" target="n650" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8799" source="n2134" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8800" source="n2134" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8801" source="n2134" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8802" source="n2134" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8803" source="n2134" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8804" source="n2134" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8805" source="n2134" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8806" source="n2134" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8807" source="n2134" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8808" source="n2134" target="n441" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8809" source="n2134" target="n765" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8810" source="n2134" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8811" source="n2134" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8812" source="n2134" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8813" source="n2134" target="n98" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8814" source="n2134" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8815" source="n2134" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8816" source="n2134" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8817" source="n2134" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8818" source="n2134" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8819" source="n2134" target="n1978" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8820" source="n2135" target="n336" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8821" source="n2135" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8822" source="n2135" target="n518" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8823" source="n2135" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8824" source="n2135" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8825" source="n2135" target="n810" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8826" source="n2135" target="n757" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8827" source="n2135" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8828" source="n2135" target="n63" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8829" source="n2135" target="n138" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8830" source="n2135" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8831" source="n2135" target="n2018" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8832" source="n2135" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8833" source="n2135" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8834" source="n2135" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8835" source="n2135" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8836" source="n2135" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8837" source="n2135" target="n1037" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8838" source="n2135" target="n29" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8839" source="n2135" target="n1652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8840" source="n2135" target="n648" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8841" source="n2135" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8842" source="n2135" target="n655" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8843" source="n2135" target="n333" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8844" source="n2135" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8845" source="n2135" target="n245" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8846" source="n2135" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8847" source="n2135" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8848" source="n2135" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8849" source="n2135" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8850" source="n2135" target="n1854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8851" source="n2135" target="n367" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8852" source="n2135" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8853" source="n2135" target="n375" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8854" source="n2135" target="n589" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8855" source="n2135" target="n800" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8856" source="n2135" target="n1969" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8857" source="n2135" target="n379" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8858" source="n2135" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8859" source="n2135" target="n55" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8860" source="n2135" target="n950" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8861" source="n2135" target="n584" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8862" source="n2135" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8863" source="n2135" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8864" source="n2135" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8865" source="n2135" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8866" source="n2135" target="n1791" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8867" source="n2135" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8868" source="n2135" target="n293" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8869" source="n2135" target="n801" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8870" source="n2135" target="n261" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8871" source="n2135" target="n932" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8872" source="n2135" target="n1028" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8873" source="n2135" target="n287" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8874" source="n2135" target="n1615" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8875" source="n2135" target="n388" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8876" source="n2135" target="n69" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8877" source="n2135" target="n98" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8878" source="n2135" target="n157" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8879" source="n2135" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8880" source="n2135" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8881" source="n2135" target="n282" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8882" source="n2135" target="n205" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8883" source="n2135" target="n434" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8884" source="n2135" target="n102" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8885" source="n2135" target="n342" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8886" source="n2135" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8887" source="n2135" target="n971" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8888" source="n2135" target="n351" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8889" source="n2135" target="n604" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8890" source="n2135" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8891" source="n2135" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8892" source="n2135" target="n140" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8893" source="n2135" target="n153" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8894" source="n2135" target="n939" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8895" source="n2135" target="n345" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8896" source="n2135" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8897" source="n2135" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8898" source="n2135" target="n522" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8899" source="n2135" target="n585" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8900" source="n2135" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8901" source="n2135" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8902" source="n2135" target="n1630" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8903" source="n2135" target="n656" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8904" source="n2135" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8905" source="n2135" target="n39" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8906" source="n2135" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8907" source="n2135" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8908" source="n2135" target="n854" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8909" source="n2135" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8910" source="n2135" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8911" source="n2135" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8912" source="n2135" target="n294" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8913" source="n2135" target="n127" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8914" source="n2135" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8915" source="n2135" target="n237" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8916" source="n2135" target="n540" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8917" source="n2135" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8918" source="n2135" target="n330" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8919" source="n2135" target="n588" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8920" source="n2135" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8921" source="n2135" target="n61" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8922" source="n2135" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8923" source="n2135" target="n855" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8924" source="n2135" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8925" source="n2135" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8926" source="n2135" target="n247" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8927" source="n2135" target="n949" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8928" source="n2135" target="n846" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8929" source="n2135" target="n533" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8930" source="n2135" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8931" source="n2135" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8932" source="n2135" target="n624" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8933" source="n2135" target="n2" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8934" source="n2135" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8935" source="n2135" target="n359" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8936" source="n2135" target="n627" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8937" source="n2135" target="n900" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8938" source="n2135" target="n25" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8939" source="n2135" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8940" source="n2135" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8941" source="n2136" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8942" source="n2136" target="n355" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8943" source="n2136" target="n378" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8944" source="n2136" target="n767" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8945" source="n2136" target="n836" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8946" source="n2136" target="n397" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8947" source="n2136" target="n384" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8948" source="n2136" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8949" source="n2136" target="n2018" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8950" source="n2136" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8951" source="n2136" target="n278" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8952" source="n2136" target="n280" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8953" source="n2136" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8954" source="n2136" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8955" source="n2136" target="n303" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8956" source="n2136" target="n697" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8957" source="n2136" target="n115" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8958" source="n2136" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8959" source="n2136" target="n72" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8960" source="n2136" target="n84" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8961" source="n2136" target="n82" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8962" source="n2136" target="n261" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8963" source="n2136" target="n105" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8964" source="n2136" target="n255" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8965" source="n2136" target="n947" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8966" source="n2136" target="n265" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8967" source="n2136" target="n65" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8968" source="n2136" target="n684" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8969" source="n2136" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8970" source="n2136" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8971" source="n2136" target="n896" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8972" source="n2136" target="n377" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8973" source="n2136" target="n516" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8974" source="n2136" target="n175" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8975" source="n2136" target="n962" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8976" source="n2136" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8977" source="n2136" target="n531" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8978" source="n2136" target="n1044" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8979" source="n2136" target="n1090" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8980" source="n2136" target="n107" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8981" source="n2136" target="n256" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8982" source="n2136" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8983" source="n2136" target="n781" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8984" source="n2136" target="n8" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8985" source="n2136" target="n865" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8986" source="n2136" target="n952" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8987" source="n2136" target="n628" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8988" source="n2136" target="n572" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8989" source="n2136" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8990" source="n2136" target="n47" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8991" source="n2136" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8992" source="n2136" target="n974" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8993" source="n2136" target="n961" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8994" source="n2136" target="n121" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8995" source="n2136" target="n34" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8996" source="n2136" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8997" source="n2136" target="n76" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8998" source="n2136" target="n547" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e8999" source="n2136" target="n284" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9000" source="n2136" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9001" source="n2136" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9002" source="n2137" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9003" source="n2137" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9004" source="n2137" target="n347" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9005" source="n2137" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9006" source="n2137" target="n161" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9007" source="n2137" target="n787" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9008" source="n2137" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9009" source="n2137" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9010" source="n2137" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9011" source="n2137" target="n326" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9012" source="n2137" target="n748" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9013" source="n2137" target="n653" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9014" source="n2137" target="n652" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9015" source="n2137" target="n193" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9016" source="n2137" target="n670" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9017" source="n2137" target="n552" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9018" source="n2137" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9019" source="n2137" target="n301" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9020" source="n2137" target="n64" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9021" source="n2137" target="n687" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9022" source="n2137" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9023" source="n2137" target="n625" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9024" source="n2137" target="n520" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9025" source="n2137" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9026" source="n2137" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9027" source="n2137" target="n953" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9028" source="n2137" target="n864" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9029" source="n2137" target="n604" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9030" source="n2138" target="n634" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9031" source="n2138" target="n148" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9032" source="n2138" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9033" source="n2138" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9034" source="n2138" target="n500" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9035" source="n2138" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9036" source="n2138" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9037" source="n2138" target="n931" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9038" source="n2138" target="n1085" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9039" source="n2138" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9040" source="n2138" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9041" source="n2138" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9042" source="n2138" target="n667" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9043" source="n2138" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9044" source="n2138" target="n792" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9045" source="n2139" target="n544" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9046" source="n2139" target="n538" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9047" source="n2139" target="n341" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9048" source="n2139" target="n915" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9049" source="n2139" target="n320" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9050" source="n2139" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9051" source="n2139" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9052" source="n2139" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9053" source="n2139" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9054" source="n2139" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9055" source="n2139" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9056" source="n2139" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9057" source="n2139" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9058" source="n2139" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9059" source="n2140" target="n902" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9060" source="n2140" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9061" source="n2140" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9062" source="n2140" target="n1050" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9063" source="n2140" target="n60" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9064" source="n2140" target="n542" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9065" source="n2140" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9066" source="n2140" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9067" source="n2140" target="n513" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9068" source="n2140" target="n647" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9069" source="n2140" target="n616" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9070" source="n2140" target="n37" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9071" source="n2140" target="n600" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9072" source="n2140" target="n909" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9073" source="n2140" target="n153" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9074" source="n2140" target="n744" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9075" source="n2141" target="n525" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9076" source="n2141" target="n353" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9077" source="n2141" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9078" source="n2141" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9079" source="n2141" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9080" source="n2141" target="n590" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9081" source="n2141" target="n610" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9082" source="n2141" target="n376" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9083" source="n2141" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9084" source="n2142" target="n592" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9085" source="n2142" target="n843" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9086" source="n2142" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9087" source="n2142" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9088" source="n2142" target="n752" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9089" source="n2142" target="n641" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9090" source="n2142" target="n879" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9091" source="n2142" target="n349" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9092" source="n2142" target="n1654" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9093" source="n2142" target="n866" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9094" source="n2142" target="n631" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9095" source="n2142" target="n622" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9096" source="n2142" target="n632" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9097" source="n2142" target="n209" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9098" source="n2142" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9099" source="n2142" target="n536" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9100" source="n2142" target="n1749" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9101" source="n2142" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9102" source="n2142" target="n188" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9103" source="n2142" target="n352" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9104" source="n2143" target="n53" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9105" source="n2143" target="n41" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9106" source="n2143" target="n279" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9107" source="n2143" target="n286" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9108" source="n2143" target="n577" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9109" source="n2143" target="n537" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9110" source="n2143" target="n527" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9111" source="n2143" target="n1749" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9112" source="n2143" target="n354" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9113" source="n2143" target="n755" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9114" source="n2143" target="n337" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9115" source="n2143" target="n925" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9116" source="n2143" target="n919" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9117" source="n2143" target="n651" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9118" source="n2143" target="n916" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9119" source="n2143" target="n21" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9120" source="n2143" target="n81" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9121" source="n2143" target="n50" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9122" source="n2143" target="n339" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9123" source="n2143" target="n530" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9124" source="n2143" target="n578" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9125" source="n2143" target="n277" label="CONTAINS"><data key="label">CONTAINS</data></edge>
<edge id="e9126" source="n2129" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9127" source="n2130" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9128" source="n2131" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9129" source="n2132" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9130" source="n2133" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9131" source="n2134" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9132" source="n2135" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9133" source="n2136" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9134" source="n2137" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9135" source="n2138" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9136" source="n2139" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9137" source="n2140" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9138" source="n2141" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9139" source="n2142" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9140" source="n2143" target="n450" label="IS"><data key="label">IS</data></edge>
<edge id="e9141" source="n450" target="n588" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9142" source="n450" target="n339" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e9143" source="n450" target="n627" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9144" source="n450" target="n1652" label="HAS"><data key="label">HAS</data><data key="score">2.8846153846153846</data></edge>
<edge id="e9145" source="n450" target="n4" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9146" source="n450" target="n1058" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9147" source="n450" target="n662" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9148" source="n450" target="n58" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9149" source="n450" target="n538" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e9150" source="n450" target="n582" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9151" source="n450" target="n31" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9152" source="n450" target="n696" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9153" source="n450" target="n651" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9154" source="n450" target="n1634" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9155" source="n450" target="n1624" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9156" source="n450" target="n340" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9157" source="n450" target="n1069" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9158" source="n450" target="n320" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e9159" source="n450" target="n584" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9160" source="n450" target="n20" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9161" source="n450" target="n1082" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9162" source="n450" target="n1791" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9163" source="n450" target="n441" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9164" source="n450" target="n98" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9165" source="n450" target="n616" label="HAS"><data key="label">HAS</data><data key="score">3.8461538461538463</data></edge>
<edge id="e9166" source="n450" target="n1028" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9167" source="n450" target="n1615" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9168" source="n450" target="n237" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9169" source="n450" target="n540" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9170" source="n450" target="n330" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9171" source="n450" target="n61" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9172" source="n450" target="n697" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9173" source="n450" target="n255" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9174" source="n450" target="n1044" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9175" source="n450" target="n547" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9176" source="n450" target="n653" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9177" source="n450" target="n652" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9178" source="n450" target="n552" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9179" source="n450" target="n687" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9180" source="n450" target="n520" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9181" source="n450" target="n634" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9182" source="n450" target="n500" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9183" source="n450" target="n1085" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9184" source="n450" target="n667" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9185" source="n450" target="n542" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9186" source="n450" target="n353" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9187" source="n450" target="n590" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9188" source="n450" target="n592" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9189" source="n450" target="n1654" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9190" source="n450" target="n631" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9191" source="n450" target="n209" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9192" source="n450" target="n1749" label="HAS"><data key="label">HAS</data><data key="score">1.9230769230769231</data></edge>
<edge id="e9193" source="n450" target="n81" label="HAS"><data key="label">HAS</data><data key="score">0.9615384615384616</data></edge>
<edge id="e9194" source="n2166" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9195" source="n2166" target="n916" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9196" source="n2167" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9197" source="n2167" target="n121" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9198" source="n2167" target="n2212" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9199" source="n2167" target="n2213" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9200" source="n2167" target="n411" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9201" source="n2167" target="n2214" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9202" source="n2167" target="n2215" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9203" source="n2167" target="n598" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9204" source="n2168" target="n2216" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9205" source="n2168" target="n1674" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9206" source="n2168" target="n2217" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9207" source="n2168" target="n2218" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9208" source="n2168" target="n2219" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9209" source="n2169" target="n1034" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9210" source="n2169" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9211" source="n2169" target="n1033" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9212" source="n2169" target="n917" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9213" source="n2169" target="n902" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9214" source="n2170" target="n1709" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9215" source="n2170" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9216" source="n2171" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9217" source="n2171" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9218" source="n2171" target="n2220" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9219" source="n2172" target="n2221" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9220" source="n2172" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9221" source="n2172" target="n917" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9222" source="n2172" target="n2222" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9223" source="n2172" target="n865" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9224" source="n2172" target="n411" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9225" source="n2172" target="n2223" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9226" source="n2172" target="n973" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9227" source="n2172" target="n24" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9228" source="n2172" target="n2224" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9229" source="n2172" target="n2225" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9230" source="n2172" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9231" source="n2172" target="n2226" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9232" source="n2172" target="n1082" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9233" source="n2173" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9234" source="n2173" target="n865" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9235" source="n2173" target="n2227" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9236" source="n2173" target="n1751" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9237" source="n2174" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9238" source="n2174" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9239" source="n2175" target="n1617" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9240" source="n2175" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9241" source="n2175" target="n2228" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9242" source="n2175" target="n1033" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9243" source="n2175" target="n121" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9244" source="n2175" target="n47" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9245" source="n2175" target="n868" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9246" source="n2175" target="n664" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9247" source="n2175" target="n858" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9248" source="n2175" target="n1709" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9249" source="n2175" target="n5" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9250" source="n2176" target="n44" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9251" source="n2176" target="n2229" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9252" source="n2176" target="n1108" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9253" source="n2176" target="n2230" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9254" source="n2176" target="n2231" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9255" source="n2176" target="n2232" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9256" source="n2176" target="n2233" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9257" source="n2176" target="n2234" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9258" source="n2176" target="n2235" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9259" source="n2176" target="n2236" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9260" source="n2176" target="n2237" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9261" source="n2176" target="n2238" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9262" source="n2176" target="n1043" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9263" source="n2176" target="n512" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9264" source="n2176" target="n2239" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9265" source="n2176" target="n2240" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9266" source="n2176" target="n2241" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9267" source="n2176" target="n2242" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9268" source="n2176" target="n2243" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9269" source="n2176" target="n1689" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9270" source="n2176" target="n2244" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9271" source="n2176" target="n2245" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9272" source="n2176" target="n36" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9273" source="n2177" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9274" source="n2177" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9275" source="n2177" target="n44" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9276" source="n2177" target="n2246" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9277" source="n2177" target="n2247" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9278" source="n2177" target="n2248" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9279" source="n2178" target="n2249" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9280" source="n2178" target="n1050" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9281" source="n2178" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9282" source="n2178" target="n1033" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9283" source="n2178" target="n286" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9284" source="n2178" target="n1688" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9285" source="n2178" target="n655" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9286" source="n2179" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9287" source="n2179" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9288" source="n2180" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9289" source="n2180" target="n628" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9290" source="n2180" target="n2250" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9291" source="n2180" target="n5" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9292" source="n2181" target="n1795" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9293" source="n2181" target="n2251" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9294" source="n2181" target="n1050" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9295" source="n2181" target="n786" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9296" source="n2181" target="n1044" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9297" source="n2181" target="n824" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9298" source="n2181" target="n2252" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9299" source="n2181" target="n1634" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9300" source="n2181" target="n1092" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9301" source="n2181" target="n53" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9302" source="n2181" target="n1027" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9303" source="n2181" target="n279" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9304" source="n2182" target="n2225" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9305" source="n2182" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9306" source="n2182" target="n2253" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9307" source="n2182" target="n2254" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9308" source="n2183" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9309" source="n2183" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9310" source="n2183" target="n533" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9311" source="n2184" target="n17" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9312" source="n2184" target="n902" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9313" source="n2184" target="n411" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9314" source="n2184" target="n187" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9315" source="n2184" target="n1662" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9316" source="n2184" target="n1697" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9317" source="n2184" target="n2255" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9318" source="n2184" target="n1698" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9319" source="n2184" target="n1699" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9320" source="n2184" target="n1661" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9321" source="n2184" target="n565" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9322" source="n2184" target="n1701" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9323" source="n2184" target="n1702" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9324" source="n2185" target="n1697" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9325" source="n2185" target="n752" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9326" source="n2185" target="n754" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9327" source="n2185" target="n1659" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9328" source="n2185" target="n17" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9329" source="n2185" target="n2255" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9330" source="n2185" target="n1698" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9331" source="n2185" target="n1699" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9332" source="n2185" target="n1661" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9333" source="n2185" target="n565" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9334" source="n2185" target="n1701" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9335" source="n2185" target="n1702" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9336" source="n2185" target="n1077" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9337" source="n2185" target="n5" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9338" source="n2186" target="n1033" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9339" source="n2186" target="n1034" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9340" source="n2187" target="n910" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9341" source="n2187" target="n531" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9342" source="n2187" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9343" source="n2187" target="n2256" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9344" source="n2187" target="n1077" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9345" source="n2187" target="n335" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9346" source="n2187" target="n752" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9347" source="n2187" target="n574" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9348" source="n2187" target="n915" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9349" source="n2188" target="n1778" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9350" source="n2188" target="n1035" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9351" source="n2188" target="n55" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9352" source="n2188" target="n951" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9353" source="n2188" target="n121" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9354" source="n2188" target="n643" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9355" source="n2188" target="n648" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9356" source="n2188" target="n756" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9357" source="n2188" target="n2257" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9358" source="n2188" target="n2258" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9359" source="n2188" target="n2259" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9360" source="n2188" target="n2260" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9361" source="n2188" target="n919" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9362" source="n2188" target="n932" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9363" source="n2189" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9364" source="n2189" target="n759" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9365" source="n2189" target="n2261" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9366" source="n2189" target="n2262" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9367" source="n2189" target="n47" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9368" source="n2189" target="n121" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9369" source="n2190" target="n2263" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9370" source="n2190" target="n2264" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9371" source="n2190" target="n1695" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9372" source="n2190" target="n50" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9373" source="n2190" target="n2265" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9374" source="n2190" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9375" source="n2190" target="n45" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9376" source="n2190" target="n2266" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9377" source="n2190" target="n2267" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9378" source="n2190" target="n2268" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9379" source="n2191" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9380" source="n2191" target="n2269" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9381" source="n2191" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9382" source="n2191" target="n628" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9383" source="n2191" target="n1688" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9384" source="n2191" target="n1689" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9385" source="n2192" target="n1663" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9386" source="n2193" target="n1636" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9387" source="n2193" target="n921" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9388" source="n2193" target="n2270" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9389" source="n2193" target="n2271" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9390" source="n2193" target="n1655" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9391" source="n2193" target="n282" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9392" source="n2193" target="n954" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9393" source="n2193" target="n2272" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9394" source="n2193" target="n2273" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9395" source="n2193" target="n2274" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9396" source="n2193" target="n932" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9397" source="n2193" target="n1641" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9398" source="n2193" target="n2259" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9399" source="n2193" target="n901" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9400" source="n2194" target="n915" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9401" source="n2194" target="n1641" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9402" source="n2194" target="n1043" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9403" source="n2194" target="n2275" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9404" source="n2194" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9405" source="n2194" target="n2276" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9406" source="n2194" target="n2277" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9407" source="n2194" target="n2278" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9408" source="n2194" target="n973" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9409" source="n2194" target="n411" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9410" source="n2194" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9411" source="n2195" target="n53" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9412" source="n2195" target="n916" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9413" source="n2195" target="n843" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9414" source="n2195" target="n667" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9415" source="n2195" target="n2279" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9416" source="n2195" target="n2280" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9417" source="n2195" target="n2281" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9418" source="n2195" target="n1678" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9419" source="n2195" target="n41" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9420" source="n2196" target="n2282" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9421" source="n2196" target="n2283" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9422" source="n2196" target="n21" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9423" source="n2196" target="n2284" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9424" source="n2196" target="n2285" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9425" source="n2196" target="n2286" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9426" source="n2196" target="n2287" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9427" source="n2197" target="n2288" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9428" source="n2197" target="n628" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9429" source="n2197" target="n648" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9430" source="n2197" target="n916" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9431" source="n2197" target="n411" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9432" source="n2197" target="n2289" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9433" source="n2197" target="n53" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9434" source="n2197" target="n670" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9435" source="n2197" target="n47" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9436" source="n2197" target="n2290" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9437" source="n2198" target="n49" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9438" source="n2198" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9439" source="n2198" target="n1109" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9440" source="n2198" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9441" source="n2198" target="n1108" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9442" source="n2198" target="n2291" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9443" source="n2198" target="n1043" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9444" source="n2198" target="n1092" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9445" source="n2198" target="n2292" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9446" source="n2198" target="n2293" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9447" source="n2198" target="n2294" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9448" source="n2198" target="n2295" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9449" source="n2198" target="n2296" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9450" source="n2198" target="n2297" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9451" source="n2199" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9452" source="n2199" target="n614" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9453" source="n2199" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9454" source="n2199" target="n51" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9455" source="n2199" target="n1689" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9456" source="n2199" target="n1709" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9457" source="n2199" target="n34" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9458" source="n2199" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9459" source="n2200" target="n1092" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9460" source="n2200" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9461" source="n2200" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9462" source="n2201" target="n511" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9463" source="n2201" target="n53" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9464" source="n2201" target="n916" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9465" source="n2201" target="n864" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9466" source="n2201" target="n279" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9467" source="n2201" target="n667" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9468" source="n2202" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9469" source="n2202" target="n2298" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9470" source="n2202" target="n2299" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9471" source="n2203" target="n2300" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9472" source="n2203" target="n50" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9473" source="n2203" target="n628" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9474" source="n2203" target="n916" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9475" source="n2203" target="n121" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9476" source="n2203" target="n279" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9477" source="n2204" target="n1994" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9478" source="n2204" target="n39" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9479" source="n2204" target="n1773" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9480" source="n2204" target="n2301" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9481" source="n2204" target="n2302" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9482" source="n2204" target="n2303" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9483" source="n2205" target="n279" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9484" source="n2205" target="n420" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9485" source="n2205" target="n1630" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9486" source="n2205" target="n864" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9487" source="n2205" target="n1015" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9488" source="n2205" target="n53" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9489" source="n2205" target="n868" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9490" source="n2205" target="n50" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9491" source="n2205" target="n417" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9492" source="n2205" target="n2304" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9493" source="n2205" target="n2305" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9494" source="n2205" target="n2306" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9495" source="n2205" target="n853" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9496" source="n2205" target="n2307" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9497" source="n2205" target="n920" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9498" source="n2205" target="n148" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9499" source="n2205" target="n932" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9500" source="n2205" target="n917" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9501" source="n2205" target="n965" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9502" source="n2205" target="n34" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9503" source="n2205" target="n533" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9504" source="n2206" target="n1055" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9505" source="n2206" target="n628" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9506" source="n2206" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9507" source="n2207" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9508" source="n2207" target="n2308" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9509" source="n2207" target="n408" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9510" source="n2207" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9511" source="n2207" target="n3" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9512" source="n2208" target="n278" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9513" source="n2208" target="n1630" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9514" source="n2208" target="n916" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9515" source="n2208" target="n148" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9516" source="n2208" target="n628" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9517" source="n2208" target="n1081" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9518" source="n2208" target="n404" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9519" source="n2208" target="n668" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9520" source="n2208" target="n1015" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9521" source="n2208" target="n41" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9522" source="n2209" target="n2309" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9523" source="n2209" target="n2310" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9524" source="n2209" target="n2311" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9525" source="n2209" target="n2312" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9526" source="n2209" target="n2313" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9527" source="n2209" target="n2314" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9528" source="n2209" target="n2315" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9529" source="n2209" target="n2316" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9530" source="n2210" target="n858" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9531" source="n2210" target="n47" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9532" source="n2210" target="n864" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9533" source="n2210" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9534" source="n2210" target="n2317" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9535" source="n2210" target="n916" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9536" source="n2210" target="n37" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9537" source="n2210" target="n1043" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9538" source="n2210" target="n781" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9539" source="n2210" target="n931" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9540" source="n2210" target="n148" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9541" source="n2210" target="n1663" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9542" source="n2210" target="n915" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9543" source="n2211" target="n111" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9544" source="n2211" target="n43" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9545" source="n2211" target="n44" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9546" source="n2211" target="n45" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9547" source="n2211" target="n814" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9548" source="n2211" target="n628" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9549" source="n2211" target="n645" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9550" source="n2211" target="n1668" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9551" source="n2211" target="n1102" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9552" source="n2211" target="n931" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9553" source="n2211" target="n5" label="OFFERS"><data key="label">OFFERS</data></edge>
<edge id="e9554" source="n2211" target="n1092" label="OFFERS"><data key="label">OFFERS</data></edge>
</graph>
</graphml>