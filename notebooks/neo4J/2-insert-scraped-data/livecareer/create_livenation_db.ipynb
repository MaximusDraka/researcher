{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "import warnings\n",
    "# Settings the warnings to be ignored \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'C:/Users/tom/projects/skill-skeleton/utils/')\n",
    "sys.path.insert(2, 'C:/Users/tom/projects/skill-skeleton/utils/neo4j/')\n",
    "from connection import Neo4jConnection\n",
    "import query as query\n",
    "import manage as manage\n",
    "import kb_util\n",
    "\n",
    "\n",
    "\n",
    "def add_skills(rows, batch_size=10000):\n",
    "    # Adds skill nodes to the Neo4j graph as a batch job.  \n",
    "\n",
    "    query = '''UNWIND $rows AS row   \n",
    "    MERGE (Skill {name: row.skills})\n",
    "    RETURN count(*) as total\n",
    "    '''     \n",
    "    \n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "def fix_new_skills(rows, batch_size=10000):\n",
    "    \n",
    "    parameters = {'cat': \"BD-ML-AI\"}   \n",
    "    \n",
    "    query = '''UNWIND $rows AS row    \n",
    "    MATCH (s:Skill {name: row.skills})\n",
    "    WHERE s.category IS NULL\n",
    "    SET s.category = $cat   \n",
    "    RETURN count(*) as total\n",
    "    '''      \n",
    "    \n",
    "    return insert_data(query, rows, batch_size, parameters)\n",
    "\n",
    "\n",
    "def add_cvs(rows, batch_size=10000):\n",
    "    # Adds cvs nodes to the Neo4j db as a batch job.\n",
    "\n",
    "    query = '''UNWIND $rows AS row\n",
    "    MERGE (cv:CV {id: row.id, url: row.url, resume: row.data})\n",
    "    RETURN count(*) as total\n",
    "    '''\n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "\n",
    "def add_skill_links(rows, batch_size=5000):\n",
    "    # Adds profile nodes and (:CV)--(:Skill)\n",
    "    # relationships to the Neo4j graph as a batch job.  (Note the smaller batch\n",
    "    # size due to the fact that this function is adding much more data than the\n",
    "    # add_skills() function.)\n",
    "\n",
    "    query = '''\n",
    "    UNWIND $rows as row   \n",
    "    WITH row\n",
    "    MATCH (s:Skill {name: row.skills})   \n",
    "    MATCH (cv:CV {id: row.id})\n",
    "    MERGE (cv)-[:CONTAINS]->(s)\n",
    "    RETURN count(distinct s) as total\n",
    "    '''\n",
    "\n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "\n",
    "def add_profile_link(rows, batch_size=5000):\n",
    "    # Adds profile nodes and (:CV)--(:Profile)\n",
    "    # relationships to the Neo4j graph as a batch job.  (Note the smaller batch\n",
    "    # size due to the fact that this function is adding much more data than the\n",
    "    # add_skills() function.)\n",
    "\n",
    "    query = '''   \n",
    "    UNWIND $rows as row   \n",
    "    WITH row\n",
    "    MATCH (p:Profile {name: row.profile})   \n",
    "    MATCH (cv:CV {id: row.id})\n",
    "    MERGE (cv)-[:IS]->(p)\n",
    "    RETURN count(distinct cv) as total\n",
    "    '''\n",
    "\n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "def add_profile_skill_links(rows, batch_size=5000):\n",
    "    # Adds profile nodes and (:Profile)--(:Skill)\n",
    "    # relationships to the Neo4j graph as a batch job.  (Note the smaller batch\n",
    "    # size due to the fact that this function is adding much more data than the\n",
    "    # add_skills() function.)\n",
    "\n",
    "    query = '''\n",
    "    UNWIND $rows as row   \n",
    "    WITH row\n",
    "    MATCH (p:Profile {name: row.profile})   \n",
    "    MATCH (s:Skill {name: row.skills})\n",
    "    MERGE (p)-[:HAS]->(s)\n",
    "    RETURN count(distinct s) as total\n",
    "    '''\n",
    "\n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "\n",
    "def insert_data(query, rows, batch_size = 10000, parameters=None):\n",
    "    # Function to handle the updating the Neo4j database in batch mode.\n",
    "\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    start = time.time()\n",
    "    result = None\n",
    "\n",
    "    while batch * batch_size < len(rows):\n",
    "\n",
    "        if parameters is None:\n",
    "            param = {'rows': rows[batch*batch_size:(batch+1)*batch_size].to_dict('records')}        \n",
    "        else:\n",
    "            param = parameters | {'rows': rows[batch*batch_size:(batch+1)*batch_size].to_dict('records')}\n",
    "\n",
    "        print(param)\n",
    "        \n",
    "        res = conn.query(query, parameters=param)\n",
    "        total += res[0]['total']\n",
    "        batch += 1\n",
    "        result = {\"total\":total, \"batches\":batch, \"time\":time.time()-start}\n",
    "        print(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_skills_by_cv(df):    \n",
    "    return df[['id','skills']]\n",
    "\n",
    "\n",
    "def get_skills_by_profile(df):\n",
    "    return df[['profile','skills']]\n",
    "     \n",
    "\n",
    "def get_cvs(df):       \n",
    "    return df.drop_duplicates()\n",
    "    \n",
    "\n",
    "def populate_db(df):    \n",
    "     \n",
    "    df = df[df['skills']!=\"[]\"]   \n",
    "    cvs = get_cvs(df)    \n",
    "       \n",
    "    \n",
    "    cv_skills = get_skills_by_cv(df)    \n",
    "    cv_skills['skills'] = cv_skills['skills'].apply(kb_util.fix)\n",
    "    cv_skills['skills'] = cv_skills['skills'].apply(literal_eval)    \n",
    "    exploded_cv_skills = cv_skills.explode('skills').drop_duplicates().dropna()    \n",
    "    \n",
    "    profile_skills = get_skills_by_profile(df)\n",
    "    profile_skills['skills'] = profile_skills['skills'].apply(kb_util.fix)    \n",
    "    profile_skills['skills'] = profile_skills['skills'].apply(literal_eval)      \n",
    "    exploded_profile_skills = profile_skills.explode('skills').drop_duplicates().dropna()\n",
    "    \n",
    "    \n",
    "    add_cvs(cvs)        \n",
    "    add_skills(exploded_cv_skills)\n",
    "    add_skill_links(exploded_cv_skills)\n",
    "    \n",
    "    add_profile_link(cvs)\n",
    "    add_profile_skill_links(exploded_profile_skills)\n",
    "    fix_new_skills(exploded_cv_skills)\n",
    "        \n",
    "    \n",
    "def configure_db():\n",
    "    conn.query('CREATE CONSTRAINT cvs IF NOT EXISTS FOR (cv:CV) REQUIRE cv.id IS UNIQUE')\n",
    "    conn.query('CREATE TEXT INDEX cv_text_index_id IF NOT EXISTS FOR (cv:CV) ON (cv.id)')\n",
    "    conn.query('CREATE TEXT INDEX skill_text_index_name IF NOT EXISTS FOR (s:Skill) ON (s.name)')\n",
    "\n",
    "\n",
    "def setup_db(file):\n",
    "    df = pd.read_csv(file,delimiter='|',dtype={'profile':str, 'url':str, 'id':str, 'data':str, 'skills':str})\n",
    "    configure_db()\n",
    "    populate_db(df)\n",
    "\n",
    "\n",
    "def delete_cv_skill_link():\n",
    "    conn.query('MATCH (cv:CV)-[e:CONTAINS]->(s:Skill) delete e')\n",
    "\n",
    "\n",
    "def delete_cv_profile_link():\n",
    "    conn.query('MATCH (cv:CV)-[e:IS]->(p:Profile) delete e')\n",
    "\n",
    "\n",
    "def delete_cvs():\n",
    "    conn.query('MATCH (cv:CV) DETACH DELETE cv')\n",
    "    \n",
    "    \n",
    "def delete_skills(category):    \n",
    "    parameters = {'cat': category}    \n",
    "    conn.query('MATCH (s:Skill) WHERE s.category = $cat DETACH DELETE s', parameters=parameters)\n",
    "\n",
    "\n",
    "def refresh_matcher_and_scores(conn):\n",
    "    manage.refresh_all_profile_skill_scores(conn)\n",
    "    \n",
    "    skill_without_duplicates = query.get_all_skills(conn)\n",
    "    kb_util.create_matcher_from_db('C:/Users/tom/projects/skill-skeleton/models/NER/finalized_matcher.sav',skill_without_duplicates, save=True)\n",
    "\n",
    "    \n",
    "   \n",
    "conn = Neo4jConnection(uri=\"bolt://localhost:7687\", \n",
    "                       user=\"neo4j\",              \n",
    "                       pwd=\"neo4jneo4j\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rows': [{'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-engineer-iii-e867612a74144501b4d70b51d5b2f7e9', 'id': '5533667039977703598659155174781970519', 'data': 'Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Seasoned Senior Data Engineer possessing indepth knowledge of RDBSM environments ETL techniques architecture data modeling and integration between systems Offering 10 years of background managing various aspects of development design and delivery of database solutions Analytical passionate and aspiring leader bringing proven communication and organizational abilities Seeking a fulltime remote position        Skills            Expertise in Microsoft SQL Server Management Studio 20052019     Microsoft Certified Professional Designation  Microsoft Database Fundamentals Designation  Temporary Tables DLL statements Loops Schema creations Common Table Expressions View Dynamic SQL scripts Merge statements Indices Performance Tuning Triggers Functions Store Procedures Joins Parametrization Cross Apply Outer Apply Automation Job Creation Scheduling Environment Variable Deployment Variables Store Procedures Pivots Performance Tuning     Expertise in BIDSVisual Studio     Extraction from various sources eg flat files DBs XML etc Dynamic SQL Automation Deployment Parameterization Looping ProgramProcess Executions Custom coding using Script Tasking Flat FileXML File Creation UTF8 conversion MergeSCD implementation     Adept Excel User     Pivots use of Macros Formulas Charts Tabular data from SSAS cubes     Data Architecture and Modeling     Use of Star and Snowflake Schema utilizing tools like Visio to create Flow Charts using data modeling tools like SQLDB to create denormalized structures creating business specific entities to create balance between efficiency and use case       Oracle     Familiarity and use of PLSQL corroborated financial reports using Oracle Transaction Business Intelligence OTBI and the UCM as well     Familiarity of Presentation Layer Tools     Salesforce Tableau Qlikview SSRS OTBI     Subject Matter Expert     Deep IT derived knowledge in business operations including Finance Accounting Marketing Underwriting Compliance and some Specialty Risk Programs     Other Pertinent Skills     Agile HybridKANANWaterfalls methodology Familiarity with AzureDevOps GitHub TFS Skilled Design Documentation     Personal Skills     Diligent Adaptive Organized Methodical Analytical HonestBlunt                       Experience       Data Engineer III       072016      Current     Crown Castle Usa Inc    –    Salt Lake City     UT            Development    Utilized various IDEs including Visual Studio SQL Server Management Studio Astera Centerprise to build ETL solutions  Created dynamic SQL to droprecreate objects  Utilized Activity Monitortype tools such as FogLight and Solarwinds DPA tool for bottlenecks  Created technical and operational audits within process to better catch issues  Performance tuned process cutting down an 8 hour process to 5 minutes  Created and ingested delimited files  Created pivot structures within SQL taking data and transposing it in the other direction eg horizontal data to vertical  Utilized VB and C languages to create custom Script Task components to use in ETL solutions    Integration    Integrated claims personal and commercial lines products  Extensively developed piloted and implemented integration solution between OLTP data and Salesforce created API pipeline delivering quote details to business thereby allowing them to make key decisions with agents  Created data consumption and delivery solutions for claims including though not limited to sending flat files to vendors containing company data receiving decompressing unpacking ingesting data creating a wrapper shell on a program that was called by SSIS solution dynamically recreating dll with parameterized inputs    Analytics    Researched financial disparities between Oracle and TSQL restructured format  Sequenced data dictionary from Oracle to extrapolate and originate data sources to better deliver solutions  Created pivoted OLAPTabulated SSAS structures to help display intersection of trends and data    Reporting    Created canned subscription and ad hoc reports for business stakeholders sectors including finance claims actuary legal statistical statutory marketing and underwriting  Substantiated differences between expected and actual results in control reports allowing business to see disparities    Architecture    Architected a SCD Type 1  2 and 3 denormalized and normalized structures  Created conceptual logical and physical data models by utilizing tools such as Visio SysDiagram and SQLDBM    Leadership    Led and organized scrumstatus meetings Provided guidance and focused delivery tasks to colleagues whilst assessing for gaps  Provided instructional and informational background on projects to new comers detailing them thoroughly on the historical and current SDLC processes lessons learned etc  Piloted groups to create best practices and code vettingreview within organization    Organizational    Incorporated various methodology including though not limited to Waterfall Agile KANBAN Hybrid  Created several detailed design documents some serving as a standard and template for other developers  Coordinated project management updates to determine project scopes and limitations  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Part of SWAT delivery service a team designed to tackle high velocity and profile requests in company Managed solution implementation for several highprofile projects from start to finish           Application Developer       122014      072016     Honeywell    –    Deer Park     TX            Development    Created ETL solutions using Visual Studio and SQL Studio Management Studio  Parsed and massaged data into Guidewire specific formats following tight edits  Promoted and automated ETL processes  Performance tuned existing queries to cut down on time reducing overall process from 2 hours to 8 minutes    Integration    Integrated commercial and personal lines Farm and Ranch insurance products into Guidewire Claims Center  Worked closely with business stakeholders to understand data whilst also pushing for systemoriginated corrections where fixes could not be done via ETL    Leadership    Mentored colleagues and fellow testers in TSQL scripting    Organizational    Utilized Agile Methodology and TFS to check inout code from repository  Created design documents detailing process and flow  Worked closely with business stakeholders to understand requirements and provide deliverables per their specifications    Educational    Actively sought out areas to increase insurance domain of knowledge  Enrolled and successfully completed AINS 24 offered by The Institutes  Informed and trained with Guidewire and SAP HANA software  Became Farm and Ranch SME for data           ETL Developer       032013      102014     American Homes 4 Rent    –    Cincinnati     OH            Development    Created complex integration solution to ingest and parse rowdelimited flat files using the FiServ Data model  Ingested outputted files from FNMA and FHLMC  Further refined this data using ETL based on businessend user requirements  Participated in code review sessions with colleague to assess bottlenecks strategize resource consumption kill deadlocks and tailor code to become more IO efficient  Created an SDLC lifecycle for process flow including environments for development testing and production  Installed and Upgraded SQL Server Studio    Integration    Delivered outputs using webservice calls fetching unique systemoriginated keys from OLTP system to use within TSQL    Leadership    Trained and mentored users on how to utilize TSQL and create simple SSIS packages  Crosstrained colleagues on ETL solution    Organizational    Created several design documents detailing pipeline of ETL solution caveatslimitations and future development requirements          Education and Training       Associate of Arts              Expected in   122011                Collin College      Frisco TX          GPA        Status           Summa Cum Laude Honors graduate  Phi Theta Kappa International Honor Society member  Sigma Kappa Delta National English Honor Society member  Student Leadership Academy graduate', 'resume_html': 'none', 'skills': \"['BI', 'Design', 'Business Intelligence', 'Tableau', 'Tableau', 'Tableau', 'Data Visualization', 'Dashboards', 'Database Design', 'Designing', 'Business Intelligence', 'ETL', 'Transform', 'Load', 'Extract', 'Transform', 'Load', 'data warehouse', 'trends', 'business requirements', 'business intelligence', 'evaluating trends', 'realtime data', 'Tableau', 'MS Excel', 'SSRS', 'Snowflake', 'Cassandra', 'Oracle', 'MS SQL Server', 'MS Access', 'Postgres', 'Amazon S3', 'SQL', 'Python', 'TSQL', 'HTML', 'CSS', 'Java', 'Salesforce', 'MS Word', 'MS Excel', 'Outlook', 'FrontPage', 'PowerPoint', 'BI', 'Reporting', 'dashboards', 'Tableau', 'Data Quality', 'Python', 'Cassandra', 'Snowflake', 'MEMSQL', 'warehouses', 'Tableau', 'SQL', 'Amazon S3', 'pipelines', 'Amazon S3', 'Snowflake', 'Extract', 'Load', 'Transform', 'Data Governance', 'Data Analysis', 'Data Mapping', 'Business Requirement', 'Data Validation', 'Tableau', 'Tableau', 'Snowflake', 'Cassandra', 'MemSQL', 'Oracle', 'AWS', 'Postgres', 'Excel', 'Salesforce', 'MS Access', 'BI', 'Tableau', 'Tableau', 'Dashboards', 'dashboards', 'visualizations', 'adhoc analysis', 'reporting', 'customized analysis', 'dashboards', 'Designing', 'developing', 'data warehouse', 'Redshift', 'BI', 'optimization', 'Designing', 'developing', 'prototyping', 'dashboards', 'dashboards', 'designing', 'Dashboard', 'reporting', 'dashboarding', 'Waterfall', 'Agile', 'statistical techniques', 'validate data', 'Develop', 'data collection', 'dashboards', 'Tableau', 'design', 'visualizations', 'Tableau', 'SAS', 'Data Integration', 'extracting', 'transforming', 'loading data', 'Data warehouse', 'Data Sets', 'business analysis', 'SAS', 'testing']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/senior-data-engineer-c2991ccc65f9419cbf914b7149f33d26', 'id': '274097087237322696861922203277862748921', 'data': 'Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Professional Summary       Having 9 years of Industry experience in ETL Tools such as DataStage  Informatica and SSI Packages  Having experience in Data Bricks Hive SQL Azure CICD pipeline Delta Lake Data Lake Hadoop File system Snowflake  Having experience in Building ETL pipe lines using Apache Spark Python  Excellent Experience in Designing Developing Documenting Testing of ETL jobs and mappings in Server and Parallel jobs using DataStageInformatica to populate tables in Data Warehouse DataMart ODS and Large Data sets  Experience in working on streaming data using IBM MQ and Kafka  Experience in working using AgileSCRUM and Waterfall Development Methodology  Experience in logging tickets in Service now Version control tools PVCS Azure DevOps CICD pipeline  Experience in Azure work environment  Work experience in IBM Master Data ManagementMDM Architecture  Working Experience on Azure Cloud  Experience in working with multiple Data Bases like Oracle SQL Server DB2 Netezza NOSQL Mongo DB Salesforce Snowflake DB  Experience in working on data migration from oracle 9i to 10g and DB2 to Netezza  Expertise in using DataStageInformatica to integrate with different Sources and Targets like Azure SQL database Oracle Mainframe systems Netezza Salesforce SOAP and REST services XML SQL Server and MongoDB  Experience in UNIX AIX and Linux server resource monitoring and load balancing  Ensured that user requirements are effectively and accurately communicated to the other members of the development team and Facilitate communications between business users developers and testing teams  Conducting internal and external reviews as well as formal walkthrough among various teams and documenting the proceedings  Excellent problemsolving and troubleshooting capabilities Quick learner highly motivated result oriented and an enthusiastic team player             Education       Sri Krishnadevaraya University    Kurnool AndhraPradesh           Expected in   052011     –      –       Bachelor of Engineering        Computer Science  Information Technology          GPA                   Skills         ETL Tools  DataStage Informatica Power Centre and SSIS Packages  Big Data Technologies  HiveSpark  HDFSKafka Sqoop  Database  Oracle SQL ServerDB2 Netezza  Mongo Snowflake  Programming Languages  UNIX Python PLSQL  Working experience in Agile Waterfall model and tracking in JIRRA and Microsoft Devops  Configuration Tools  PVCS  Microsoft TFS Azure CICD pipeline  Cloud Experience Azure      Job Scheduling Tools CA7 Control M  Operating System  Win XP 7 10 and UNIX  Adaptability  Data management  Organization and Time management  Teamwork                     Certifications      IBM Data Stage  Oracle  Informatica  Netezza          Work History       Factset Research Systems Inc      Senior Data Engineer   San Francisco     CA                   022021      Current     Bank Operational Data Distribution Hub is highly availability distribution center for operational data The servers have been set up to provide failover capabilities in the event of any issues which could cause the hardware to shut down The design of this system focuses on four main vendors We load data to  HDFS storage as well and built HIVE on top of this to analyze   Developed implemented supported and maintained data analytics protocols standards and documentation  Analyzed complex data and identified anomalies trends and risks to provide useful insights to improve internal controls  Contributed to internal activities for overall process improvements efficiencies and innovation  Communicated new or updated data requirements to global team  Explained data results clearly and discussed how it can be utilized to support project objectives  Planned and implemented security measures to safeguard vital business data  Created and implemented database designs and data models  Monitored incoming data analytics requests executed analytics and efficiently distributed results to support strategies  Built databases and table structures following OLAPOLTP architecture methodology for web applications           Cox Communications Inc      Lead Data Engineer   Dayton     OH                   012018      012021    Master Data ManagementMDM EQH is primary vehicle for customer selfservice for Life and Annuity products Displays current policy values statements confirmation notices and prospectuses Supports profile maintenance including address phone and email address changes financial profile and investment strategies Selfservice tools include performance financial transactions ACH payments and loans   Responsibilities    Leads programproject application engineering teams consisting of cross functional global and virtual groups directly supervises staff assigns responsibility to members monitors progress of daily activities  Monitor and manage programproject application engineering baseline to ensure activities are occurring as planned  scope budget and schedule and managing variances  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Proactively identify risks issues and problems on programsprojects  leading engineering and projectprogram team to develop risk management and issue management plans  I have saved 1 person monitoring work by performing optimization   Ability to clearly articulate problems and proposed options and solutions and apply judgment in implementing application engineering methodologies processes and practices to ensure security resilience maintainability and quality of MDM solutions  Analyses and defines detailed MDM processes tasks data flows and dependencies  Develop custom mapping functions  Participates in system and integration testing  Produces database code SQL stored procedures and any other database specific code solutions meeting technical specifications and business requirements according to established designs  Proactively resolved issues within team  Validated warehouse data structure and accuracy  Cooperated fully with product owners and enterprise architects to understand requirements  Collaborated with multifunctional roles to communicate and align development efforts  Mapped data between source systems and warehouses  Performed systems and data analysis using variety of computer languages and procedures  Documented data warehouse architecture to guarantee capacity met current and forecasted needs  Developed and modified programs to meet customer requirements  Quickly learned new skills and applied them to daily tasks improving efficiency and productivity  Provided global thought leadership in analytics solutions to benefit customers           Epam Systems Inc      Senior ETL Developer   Washington     DC                   082017      122017    Netezza Rewrite This project is about migrating around 70 Data Marts runs on Oracle to Netezza in 10 phases This includes redesigning DataStage jobs integrates with oracle to change it to Netezza and Informatica to DataStage migration   Role  Responsibilities     Requirement Analysis Creating mappings Unit Testing Defect Fixing Documentation and Status Reporting  Identifying Entities cardinality and developing Logical and Physical Data Model  Mentored newly hired employees offering insight into job duties and company policies for easier transition to job position  Prioritized and organized tasks to efficiently accomplish service goals  Analyzing existing process scripts and preparing design document with performance optimized approach  Performed impact analysis on every source and target tables  Responsible for estimation of Design Development and Unit testing  Analyzing dependent objects and data involved and updating efficient unit testing approach  Responsible for scheduling changes which includes changing node to new 115 server change in run time parameters change in predecessor or successor requirements and removing jobs  Have published Play book or Implementation plan for every release  Responsible for driving implementation and doing post implementation data checks  Resolved complex DataStage performance issues and other environment issues  Designed and Developed reusable components which can parse dsx and provide input and output SQLs used in DataStage code  Reviewed Netezza Deliverables and DataStage deliverables in every phase of project  Resolved Netezza SQL issues to Business users  Coordinated with downstream systems and worked on impacted system sign off review and production preparation           Epam Systems Inc      Senior ETL Developer   PA     State                   012016      072017    Financial Move Forward inforce Data Equitable Enterprise Data Warehouse EDW manages collection of components in both Mainframe Information DatabaseIDB and Distributed environments Open system Data WarehouseOSDW that include batch processes operational data stores business intelligence BI data marts and general data services to all IT lines of business   FMFInforce Data project consists of two phases  First phase involves migration of DB2 data to Netezza DB with help of integrated ETL tool Data Stage 87  Second phase contains Data Modelling and DataMart design of migrated tables in Netezza  First phase basically involves initial data load IDL of 400 Db2 tables to Netezza DB  Took care of installing Netezza client on UNIX box where Data stage client exists and ensured connectivity is good for designing data stage jobs Role  Responsibilities  Understanding requirements and coming up with high level design  Created Lowlevel design of mapping document  Created mappings and transformations as per business requirements  Writing reusable mapplets and Oracle PLSQL stored procedures  Unit test jobs according to test plans  Monitored debugged and scheduled mappings according to requirements  Improving performance of mapping execution thus reducing CPU and execution cost and time  Providing System Testing and User Testing Support IQA of Mappings  Capable of assisting team of developers both onshore and offshore to provide strategic plan for execution of this project  Ability to work with key team members to ensure solution meets business requirements  Provided Proof of Concept POC for technical approach regarding design of data stage jobs  Understanding requirements and coming up with technical design strategies with project team and business users  Contributed to detailed estimation of development work  Involved in Estimation of DBA Effort for this project  Designed Field level mapping template design based on business rules transformations and validations  Performed problem assessment resolution and documentation for new and existing database objects  Prepared Knowledge Transition documents which were appreciated by business IT people  Communicated with data architects programmers and engineers to keep projects on track', 'resume_html': 'none', 'skills': \"['business analysis', 'quality assurance', 'functional testing', 'critical thinking', 'project management', 'cryptography', 'threat analysis', 'Cloud', 'Microsoft Office', 'reporting', 'Cryptography', 'JAVA', 'Programming', 'Windows', 'business analysis', 'data integrity', 'optimization', 'testing', 'technical documentation', 'reporting', 'quality assurance', 'reporting', 'business analysis', 'Networking', 'Cryptography', 'data warehouse', 'JAVA', 'Programming', 'Linux', 'Access', 'Microsoft Office', 'Windows', 'optimization', 'project management', 'quality assurance', 'reporting', 'technical documentation', 'UNIX']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/aws-data-engineer-63b337f6d6bf36b19ae995691c672ca0', 'id': '270987204323396713165325793790364446993', 'data': 'Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases             Skills         ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub      ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                     Education and Training       Purdue University    West Lafayette     IN      Expected in   022022     –      –       Post Graduate         Data Engineering           GPA                    University of Texas At Austin    Austin     TX      Expected in   092021     –      –       Post Graduate         Data Science And Business Analytics           GPA                    Califonia State University     Fullerton CA           Expected in   122009     –      –       Bachelor of Arts        Business Administration And Management          GPA                     Experience       Deloitte      AWS Data Engineer   Rosslyn     NV                   012022      022022     Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process           Bank Of America Corporation      Data Engineer   Arcadia     CA                   012016      112019     Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark           Cumming Llc      Big Data Engineer  Hadoop Developer   Aliso Viejo     CA                   102013      122015   AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Fiserv      Python Developer    City     STATE                   092012      102013     AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes', 'resume_html': 'none', 'skills': \"['business requirements', 'data accuracy', 'project management', 'Excel', 'SQL', 'Data Mining', 'MS Office', 'Analyzing trends', 'VBA', 'Data Analysis', 'Optimization', 'Quality Assurance', 'Project Management', 'programming', 'data integrity', 'SQL', 'Excel', 'VBA', 'data quality', 'process redesign', 'maintaining', 'Excel', 'SQL', 'accurate data', 'data cleaning', 'data preparation', 'processing', 'statistical techniques', 'SQL', 'research data', 'Excel', 'data sources', 'data cleaning', 'reliability', 'excel', 'excel', 'spreadsheets', 'access', 'excel', 'processing', 'HIPAA', 'processing', 'Processing', 'Problem solving', 'Problem Solving', 'Excel', 'SQL', 'Data Mining', 'MS Office', 'Analyzing trends', 'VBA', 'Data Analysis', 'Optimization', 'Quality Assurance', 'Project Management', 'programming', 'data integrity', 'SQL', 'Excel', 'VBA', 'data quality', 'process redesign', 'maintaining', 'Excel', 'SQL', 'accurate data', 'implement', 'data cleaning', 'data preparation', 'processing', 'statistical techniques', 'SQL', 'research', 'research', 'Excel', 'data sources', 'data cleaning', 'reliability', 'excel', 'excel', 'spreadsheets', 'access', 'excel', 'processing', 'HIPAA', 'processing', 'Processing', 'consistent', 'Problem solving']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-analyst-data-engineer-5d0c7b4ff12f4323b51fb55c6284662a', 'id': '76199997604772823340207052419224154975', 'data': 'Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary       Logical Data Analyst skilled in requirement analysis software development and database management Selfdirected and proactive professional with 35 years of vast experience collecting cleaning and interpreting data sets Natural problemsolver possessing strong crossfunctional understanding of information technology and business processes  Strong knowledge in AWS cloud services like  ECS   EC2  infrastructure  S3  for storage Elastic MapReduce EMR   Athena  as query manager and  CloudWatch   Very well experienced with various visualization tools like  Tableau  by extracting data from various data sources  MasteringLeading in development of applicationstools using  Python  for 3 years  Worked on performance tuning and optimization to improve the efficiency in script executions  Good working experience loading Data Files in  AWS  Environment and Performed SQL Testing on AWS redshift databases  Exceptional ability to research analyze and convey complex technical information to diverse endusers at all levels               Skills         Data Validation  UNIX System  SQL  Python3  BI ToolsTableau Looker Datapoint  Data BasesSqlServer Postgres MYSQl PythonOracle      Amazon Web Services AWS  Servicenow  Jenkins  Pagerduty  Splunk  GIT                     Education and Training       University of Mary HardinBaylor    Belton     TX      Expected in   122017     –      –               Management Information Systems          GPA                    Auroras Technological And Research Institute    UppalIndia           Expected in   082015     –      –       Bachelor of Science        Electrical Electronics And Communications Engineering          GPA                   Certifications       Licensed AWS Solution Architect  2019           Experience       Management Decisions Inc      Data AnalystData Engineer   Reston     VA                   092021      Current     Worked in Banking industry under Risk Management sector to maintain various applications tools data pipelines which have both upstream and downstream applications  Saved at least 7 hoursweek of team effort by automating manual business tasks using python pandas within first 3 months of joining the team  Strong experience in implementing various tables and schemas in Amazon Redshift DB Snowflake DB also worked on migrating various tables from Redshift to Snowflake DB  Organized several empathy sessions with business users and established brand new high impact Tableau dashboards along with improving existing dashboards as per new user requirement which received immaculate user response  Working knowledge of Amazon’s Elastic Cloud Compute EC2 infrastructure for computational tasks Simple Storage Service S3 as storage mechanism  Managed timely flow of business intelligence information to users  Collected tracked and evaluated current business and market trend data  Proven ability to manage all stages of project development Strong Problem solving skills and Analytical skills and abilities to make balanced and independent decisions           Capital One      Data Analyst   City     STATE                   042019      082021     Involved in analysis design and documenting business reports such as Executive summaries Scorecards and drilldown reports  Worked on performance tuning and Query Optimization for increasing the efficiency of scripts  Developed monitored the workflows and responsible for performance tuning of staging and 3NF workflows  Analyzing and profiling data returned for data integrity and business decisions  Responsible for migrating legacy reports to a new platform by rebuilding them to meet current business requirements  Audited internal data and processes to identify and manage initiatives improving business performance  Assisted integration of internal and external data tools and products maintaining stability and performance across systems  Provided technical support for existing reports dashboards or other tools  Maintained or updated business intelligence tools databases or dashboards  Disseminated information regarding tools reports or metadata enhancements  Communicated with customers competitors and suppliers to stay abreast of industry or business trends           Dollar Shave Club      Data AnalystProduction Support Analyst   City     STATE                   052018      032019     Used JIRA Agile methodology extensively to track day to day scrum activities  Writing SQL scripts for selecting the data from servers and modifying data as need with python pandas and stored back to different data base servers Organized and facilitated sprint planning daily standup meetings Scrum of Scrum Sprint review Sprint retrospectives and other Scrumrelated meetings  Writing SQL scripts for selecting the data from servers and modifying data as need with python pandas and stored back to different data base servers Organized and facilitated sprint planning daily standup meetings Scrum of Scrum Sprint review Sprint retrospectives and other Scrumrelated meetings  Manipulating cleansing and processing data using python code and SQL  Troubleshooted Various production failures related to data loads  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Initiated daily status email to track the data loads in production environment', 'resume_html': 'none', 'skills': \"['Excel', 'pivot tables', 'SAS', 'data storage', 'office', 'data analysis', 'dashboard', 'SAS', 'Problem Solving', 'MS Project', 'explanation', 'Pivot tables', 'Waterfall', 'dashboard', 'SAS', 'Excel', 'MS Access', 'SAS', 'Excel', 'MS Access', 'MS Access', 'Regularization', 'MS Access', 'Regularization', 'MS Excel', 'Pivot tables', 'SAS', 'MS Excel', 'Pivot tables', 'SAS', 'SAS', 'Excel', 'SAS', 'Excel', 'Business Objects', 'SQL', 'Business Objects', 'SQL', 'SQL', 'Query Builder', 'SQL', 'Query Builder', 'MS Visio', 'SharePoint', 'OneNote', 'Office', 'MS Visio', 'MS SharePoint', 'MS OneNote', 'Apple Sheets', 'Apple Pages', 'Open Office', 'MS PowerPoint', 'MS PowerPoint', 'MS Word', 'MS Word', 'MS Excel', 'MS Excel', 'Programming', 'SQL', 'SAS', 'Java', 'VBNet', 'Programming', 'SQL', 'SAS', 'Java', 'VBNet', 'Windows', 'Android', 'MS Project', 'Access', 'Access', 'reporting', 'Java', 'VBnet', 'performance analysis', 'VBNet', 'Business Objects', 'Java', 'access', 'Access', 'MS Access', 'MS Excel', 'Excel', 'Office', 'MS PowerPoint', 'MS Project', 'SharePoint', 'Windows', 'MS Word', 'analysis', 'Pivot tables', 'reporting', 'SAS', 'SQL', 'Visio']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/sr-data-engineer-architect-898ff76e69d44c7c964261bd55776363', 'id': '305667889169402374245195369321522535673', 'data': 'Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary      Seasoned Data Architect adept at understanding mandates developing plans and implementing enterprisewide solutions Complex problemsolver with an innovative approach Ready to bring 1Oplus years of progressive experience and take on a challenging new role with growth potential        Skills           Data Management  Organizational Skills  Critical Thinking  Team Management  Problem Resolution  Customer Service      Relationship Building  Team Building  Supervision  Leadership  Planning  Organizing  Friendly Positive Attitude                       Experience       Sr Data Engineer   Architect        082016   to   Current     Honeywell    –    Baltimore     MD             Used statistical software to analyze and process large data sets  Followed industry innovations and emerging trends through scientific articles conference papers or selfdirected research  Distilled data to devise solutions related to budgeting staffing and marketing decisions  Recommended data analysis tools to address business issues  Provided global thought leadership in analytics solutions to benefit customers  Captured and shared bestpractice knowledge amongst developers community  Cleaned and manipulated raw data  Created graphs and charts detailing data analysis results  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Developed new functions and applications to conduct analyses  Assisted solution providers with definition and implementation of technical and business strategies  Tested validated and reformulated models to foster accurate prediction of outcomes  Contributed to maintaining AzureSQL SQL Sever and DB2 databases in conjunction with data development and software engineering teams  Adept in troubleshooting and identifying current issues and providing effective solutions  Promoted customer success in building and migrating applications software and services on Azure platform  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Sr ETL Developer        012016   to   082016     Millennium Health    –    City     STATE             Assessed code during testing stage to determine potential glitches and bugs  Conferred with project managers and other stakeholders to fully understand software design specifications and plan optimal development approaches  Employed integrated development environments IDEs  Devised automation backup and recovery protocols to preserve and safeguard data  Analyzed user needs and software requirements to determine design feasibility  Collaborated with support team to assist client stakeholders with emergent technical issues and develop effective solutions  Applied security measures into systems development supporting final products resistance to intrusion and exploitation  Designed customized technical solutions to meet functional specifications outlined by Millennium Health database customers  Worked closely with systems analysts engineers and programmers to understand limitations develop capabilities and resolve software problems  Collaborated with Architects to define data extraction methodologies and data source tracking protocols  Utilized established design patterns to expedite novel software creation and support consistent performance results  Integrated objectoriented design and development techniques into projects to support usability goals  Analyzed code and corrected errors to optimize output  Identified opportunities for process improvements to decrease in support calls  Defined and documented SSIS ETL data mapping plans using Windows PowerShell scripting and custom solution from vendors ie Pragmatic Works  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes  Programmed applications and tools using objectoriented languages with goals of code abstraction stability and reuse  Assisted in User Acceptance Testing for Millennium customers verifying ETL jobs complied with assigned parameters achieving success during execution phases  Combined rootlevel authentication and authorization technologies with ongoing system design to harden finished solutions  Applied prescribed policies to programming syntax in compliance with internal language policies  Modified existing software to correct errors adapt to newly implemented hardware or upgrade interfaces  Leveraged Agile methodologies to move development lifecycle rapidly through initial prototyping to enterprisequality testing and final implementation  Developed software for embedded systems coding solutions for both new installations and insitu hardware  Performed troubleshooting of postrelease software faults to support live service and installed software patch design  Developed requirements for system modifications and new system installations  Recommended improvements to facilitate team and project workflow  Coordinated testing and validation procedures through software development lifecycle  Managed endtoend operations of ETL data pipelines maintaining uptime of 96  Improved and corrected existing software and system applications  Applied Conceptual Logical and Physical  DimensionalRelational model designs to ETL tasks           Sr Database App   BI Developer       052013   to   102015     TransCanada Corporation    –    City     STATE             Analyzed and developed technical and functional specifications for databases  Developed and updated all documentation related to database technologies for department  Built integrations from multiple data sources including Salesforce and Pardot  Identified databases not reaching peak performance and determined ways to solve concerns  Applied various skills to evaluate design implement and optimize databases and database applications  Managed financial management systems customer and production databases and inventory production equipment and editing systems  Provided support to clients in understanding and manipulating data to obtain value through SQL and ETL technical processes and visual analytics tools  Managed all levels of internal analytics practice including ETL database administration report development and integration  Produced complex database project with zero issues due to effective troubleshooting  Developed designed and optimized data structures for analysis  Partnered with project management teams on development of scope and timelines  Assisted clients in understanding and manipulating data to gain value through SQL and ETL technical processes and visual analytics tools  Developed data models and database designs to plan projects  Developed and implemented security initiatives to protect important company data  Constructed database and warehouse streamlined disparate data sources and unverified queries into main source  Wrote scripts and processes for data integration and bug fixes  Planned designed and streamlined data structures for analysis  Supervised all levels of internal analytics practice including ETL database administration report creation and integration  Built database and warehouse including consolidating disparate data sources and unverified queries into central source  Created integrations from various data sources including Salesforce and ERD Systems           Database Application Developer        022011   to   042013     Barclays Plc    –    City     STATE             Modified existing software to correct errors adapt to newly implemented hardware or upgrade interfaces  Utilized established design patterns to expedite novel software creation and support consistent performance results  Employed integrated development environments IDEs  Developed logic flowcharts and diagrams to use in program coding and workflow planning  Developed software for embedded systems coding solutions for both new installations and insitu hardware  Identified opportunities for process improvements to decrease in support calls  Analyzed code and corrected errors to optimize output  Liaised with clients to clarify business challenges and objectives to optimize performance of existing systems  Trained and coached new hires and junior developers and shared insight into ways to meet tight deadlines and improve overall efficiency  Coordinated testing and validation procedures through software development lifecycle  Combined rootlevel authentication and authorization technologies with ongoing system design to harden finished solutions  Identified debugged and fixed system bottlenecks and problems  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes  Devised automation backup and recovery protocols to preserve and safeguard data  Applied innovative approaches to application design through creative inception and planning  Contributed to requirements gathering and design development meetings  Improved and corrected existing software and system applications  Leveraged Agile methodologies to move development lifecycle rapidly through initial prototyping to enterprisequality testing and final implementation  Assessed code during testing stage to determine potential glitches and bugs  Worked closely with systems analysts engineers and programmers to understand limitations develop capabilities and resolve software problems  Conferred with project managers and other stakeholders to fully understand software design specifications and plan optimal development approaches  Applied prescribed policies to programming syntax in compliance with internal language policies  Recommended strategies to maximize performance and lifespan of equipment involved in software installations  Developed requirements for system modifications and new system installations  Performed testing on user defined functions and triggers  Utilized best practices to identify and remedy bugs in applications within specific timeframe  Recommended improvements to facilitate team and project workflow  Optimized application process flow to improve performance  Programmed applications and tools using objectoriented languages with goals of code abstraction stability and reuse  Updated software upon release of vendor patches to mitigate vulnerabilities  Monitored equipment function to verify conformance with specifications  Analyzed user needs and software requirements to determine design feasibility  Applied application product support to contractors located internationally  Worked closely with brand and marketing teams across organizations to promote specific applications  Increased efficiency through task automation  Applied security measures into systems development supporting final products resistance to intrusion and exploitation  Assessed project scope to identify necessary requirements  Established clear system performance standards and wrote specifications  Collaborated with support team to assist client stakeholders with emergent technical issues and develop effective solutions  Integrated objectoriented design and development techniques into projects to support usability goals  Reviewed project requirements to identify customer expectations and resources needed to meet goals  Performed troubleshooting of postrelease software faults to support live service and installed software patch design          Education and Training       Bachelor of Science     Electrical Electronics Engineering     Expected in   072006     University Of Leicester      Leicestershire England           GPA               Accomplishments       Led team to achieve overhaul and enhancement of our Operational Data Store as well as upgrade to our Cognos Application earning recognition from upper management and financial reward  Negotiated with vendors saving company over US1M annually  Improved delivery of Data for data driven decisions by modernizing our applicationsystems realizing overall increase in customer satisfaction and cost efficiency         Activities and Honors       Member Alumni Association         Certifications       Certified Microsoft ProfessionalSQL Server 2008R2  2010  Certified Microsoft Azure Data Engineer  2020  Certified Microsoft Azure Architect  2020', 'resume_html': 'none', 'skills': \"['data analysis', 'reporting', 'user acceptance testing', 'Microsoft Access', 'Microsoft Excel', 'SAS', 'SQL Server', 'Cognos', 'Crystal', 'Business Objects', 'SQL', 'Tableau', 'Project Management', 'Data Analysis', 'Data Quality', 'Monitor', 'HIPAA', 'process analysis', 'SAS', 'SQL', 'MS Office', 'excel', 'access', 'claims analysis', 'access', 'project planning', 'testing', 'business requirements', 'reporting', 'analysis', 'reconciliation', 'Microsoft Access', 'reporting', 'Access', 'processing', 'Microsoft Access', 'Project Management', 'Project Management', 'Business Objects', 'Cognos', 'Crystal', 'Data Analysis', 'Microsoft Access', 'Microsoft Excel', 'Microsoft Office', 'Process Analysis', 'Project Management', 'Project Planning', 'User Acceptance Testing', 'Reporting', 'SAS', 'Reporting', 'SQL', 'Tableau', 'Tableau']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/aws-data-engineer-5cc9299938587076e54cae645ae21ec1', 'id': '83985006356839563643115453489150250520', 'data': 'Jessica    Claire                                 609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor   Home   555 4321000        Cell           resumesampleexamplecom                  Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases         Skills          ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                      Experience      012022   to   022022     AWS Data Engineer      Deloitte    –    Rosslyn     MA             Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Used Spark SQL for Scala  amp Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process  Configured computing networking and security systems within cloud environment  Implemented cloud policies managed technology requests and maintained service availability          012016   to   112019     Data Engineer      Verizon Communications    –    Irving     TX             Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts          102013   to   122015     Big Data Engineer  Hadoop Developer      Cumming Llc    –    Boston     MA           AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes          092012   to   102013     Python Developer       Fiserv    –    City     STATE             AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes          Education and Training      Expected in   022022     Post Graduate      Data Engineering      Purdue University      West Lafayette     IN     GPA               Expected in   092021     Post Graduate      Data Science And Business Analytics      University of Texas At Austin      Austin     TX     GPA               Expected in   122009     Bachelor of Arts     Business Administration And Management     Califonia State University       Fullerton CA          GPA', 'resume_html': 'none', 'skills': \"['Designing', 'Testing', 'monitoring', 'Designing', 'testing', 'Analysis', 'Reporting', 'Risk Assessment', 'Statistical Analysis', 'Problem Solving', 'Modeling', 'Project Management', 'Rapid Application Development', 'Adhoc testing', 'Backend testing', 'Reporting', 'Critical Thinking', 'Decision Making', 'Microsoft Access', 'Microsoft Excel', 'MS Office', 'Microsoft Project', 'PowerPoint', 'SharePoint', 'SQL', 'Tableau', 'Spotfire', 'Visio', 'CRM', 'SQL Server', 'AWS', 'Agile', 'data analysis', 'business requirements', 'design', 'developing', 'SQL', 'Designing', 'Drupal', 'Data Warehouses', 'research', 'cloud', 'MS project', 'analysis', 'Cisco', 'design', 'testing', 'technical documentation', 'Microsoft Project', 'designing', 'developing', 'Big Data', 'HDFS', 'access', 'access', 'project', 'MS project', 'research', 'Agile', 'Big Data', 'business analysis', 'CRM', 'data analysis', 'Data Warehouses', 'Designing', 'Drupal', 'access', 'Microsoft Access', 'Microsoft Excel', 'MS Office', 'PowerPoint', 'MS project', 'Microsoft Project', 'SharePoint', 'SQL', 'SQL Server', 'Tableau', 'technical documentation', 'Visio', 'Visual Studio']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/manager-student-and-resident-data-3e141e5ce83341f0b9b1ba584e2a5912', 'id': '278624286922027916746373616168944850452', 'data': 'Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary      Management professional seeking a director position  which allows me to utilize my strong administrative and technical experience managerial and interpersonal skills and quality assurance initiatives to maximize efficiency data procurement and customer satisfaction Resultsoriented handson professional with a highly successful record of accomplishments in quality process improvement survey management communications and career development             Key Skills         Continuous Quality Improvement Measures DMAIC  Survey Management  Data Analysis  Database Management      Customer Service  Outcomes Projections  Performance Tracking and Evaluation  Team Building                     Education       Indiana University               Expected in        –      –       Bachelor of Arts        Theater and Drama          GPA            Minor in English  Member Phi Beta Sigma           Accomplishments      Personally facilitated the creation of a new Graduate Medical Education GME Census online data application This included serving as a liaison between the American Medical Association AMA and the Association of American Medical Colleges AAMC creating and testing online technical design and functionality establishing new procedural standards for quality assurance and implementing new marketing constructs  Created all publishing promotional and help manuals for the Census with both an electronic delivery and hardcopy format  Since established in 2002 Census response rates have consistently grown by 20 to their present mark of 98    Spearheaded and supervised the collection monitoring and quality control of GME data from over 8900 US residency programs and 160000 residency training segments This includes personally maintaining multiple Access databases and coordinating and documenting all process flows between multiple departments and organizations Coordinated and participated  in all annual design and development changes of GME Track with the AMA GME Department and any AAMC staff Improved and streamlined processes which coincided with staff reductions of 70 due to reorganizations and departmental reconstructions  Despite losing personnel improved productivity and results by over 60    Directly took over and managed all student data procurement and quality without  increasing staff or resources despite these tasks being formerly held by a separate unit  Led multiple process reviews and implemented new standards which reduced the amount of manual processing by 70    Asked to be on multiple crossdepartmental and creative teams formed to address a variety of issues including departmental skills matrix development contract process improvement organizational strategic planning and various departmental improvement initiatives   Created a departmental electronic forum to capture common questions and encourage process review This forum led to several policy and process adjustments by the data and business divisions        Experience       American Medical Association      Manager Student and Resident Data   City     STATE                   112000      Current     Hired and developed staff on the use of the AMA Graduate Medical Education GME Database AMA physician Masterfile GME Track Online Census and departmental procedures  Provided professional and highly regarded GME Census customer service for all content and technical questions from both external and internal customers  Became the leader for all GMErelated data questions within the Division used consistently by divisional executives  Spearheaded and supervised the collection monitoring and quality control of all GME data  Managed all student data procurement and quality  Served as PC Coordinator providing PC technical and application support participating with rollouts of critical updates and testing new software and services as they applied to the department  Encouraged the AMA to become members of the American Society for Quality ASQ Became the direct contact between the AMA and ASQ facilitating organizational and departmental process improvement and creating crossdivisional teams to address organizational deficiencies and issues  Conducted cost schedule contract performance variance and risk analysis  Offered feedback to executivelevel management on the effectiveness of strategies selling programs and initiatives  Coached and mentored staff members by offering constructive feedback and taking interest in their longterm career growth           American Medical Association      Survey Management Specialist   City     STATE                   091998      112000     Collected and monitored large quantities of GME data from over 8000 residency programs and 100000 residents                                Provided ongoing quality control of all data received through direct use upkeep and modification of Access programming   Created and helped to implement powerful survey applications that serve to track update and collect data as well as insure a high response rate and data accuracy   Participated directly in training all new employees and temporary staff on the GME Database AMA physician Masterfile and departmental procedures   Provided constant customer service through phone support for all survey questions as well as questions regarding GME                   Skills      10 year advanced experience with Customer service Database development and integration Team leadership and Professional Development Survey marketing and creation and MS Access  Concentration on Process Improvement Quality quality assurance quality control Six Sigma and strategic planning', 'resume_html': 'none', 'skills': \"['Tableau', 'developing', 'Tableau', 'Tableau', 'dashboards', 'Tableau', 'Tableau', 'Tableau', 'Tableau', 'Tableau', 'design', 'monitoring', 'data profiling', 'BI', 'teams', 'Tableau', 'Windows', 'Active Directory', 'SQL', 'PLSQL', 'XML', 'Python', 'programming', 'Data modeling', 'Tableau', 'SharePoint', 'Performance tuning', 'SQL', 'PLSQL', 'Performance tuning', 'Tableau', 'Tableau', 'Tableau', 'data blending', 'dashboard', 'data analytics', 'Tableau', 'Waterfall', 'Decision making', 'ETL', 'agile', 'Software Development Life Cycle', 'SDLC', 'testing', 'Data Analysis', 'Data Validation', 'Data Cleansing', 'Data Verification', 'data mismatch', 'DB2', 'Oracle', 'Teradata', 'SQL Server', 'data modeling', 'Erwin', 'Dimensional Modeling', 'Snowflake', 'Data warehouse', 'ETL', 'Snowflake', 'data modeling', 'data modeling', 'Erwin', 'Modeling', 'Snowflake', 'Normalization', 'Process Analysis', 'Data modeling', 'data modeling', 'BI', 'Tableau', 'ETL', 'SQL Server', 'Microsoft Office', 'Word', 'PowerPoint', 'Excel', 'Microsoft Project', 'Microsoft Office', 'Oracle', 'SQL Server', 'DB2', 'Teradata', 'Programming', 'C', 'C', 'HTML', 'SQL', 'PLSQL', 'XML', 'Python', 'C', 'Data modeling', 'ERwin', 'ER Studio', 'MS Visio', 'SQL', 'Tableau', 'datasets', 'data modeling', 'time series analysis', 'modeling', 'MATLAB', 'programming', 'Collecting data', 'text mining', 'python', 'ETL', 'Oracle', 'Windows', 'VMware', 'Active Directory', 'Linux', 'Cisco', 'Exchange', 'MS Office', 'MS Visio', 'HTML', 'MS Access', 'SQL', 'JavaScript', 'AJAX', 'jQuery', 'Bootstrap', 'AngularJS', 'SQL Server', 'SSIS', 'SSRS', 'DB2', 'TSQL', 'Oracle', 'AspNet', 'C', 'JQuery', 'MATLAB', 'Cognos', 'MS Office', 'MS Visio', 'HTML', 'UML', 'C', 'Java', 'Visual Basic', 'VB', 'Windows XP', 'MS Access', 'SQL', 'NET', 'Agile', 'Tableau', 'Alteryx', 'Develop', 'Tableau', 'dashboards', 'Tableau', 'BI', 'project', 'design', 'develop', 'visualizations', 'POC', 'dashboards', 'dashboards', 'PLSQL', 'Design', 'Tableau', 'reporting', 'Business Intelligence', 'data analytics', 'visualization', 'reporting', 'OLAP', 'Processing', 'data mining', 'SQL', 'UNIX', 'Linux', 'Java', 'Tableau', 'Active Directory', 'Tableau', 'Design', 'develop', 'dashboards', 'Tableau', 'Access', 'transform', 'datasets', 'Develop', 'Tableau', 'data sources', 'Data Blending', 'design', 'develop', 'BI', 'design', 'data warehouse', 'visualization', 'big data', 'Design', 'visualizations', 'Dashboard', 'Cognos', 'Cognos', 'Tableau', 'business requirements', 'design', 'Reporting', 'reporting', 'Cognos', 'SQL', 'Data modeling', 'Design', 'CSS', 'HTML', 'JavaScript', 'HTML5', 'CSS', 'JavaScript', 'JQuery', 'Angularjs', 'Tableau', 'Tableau', 'HTML5', 'CSS', 'JavaScript', 'JQuery', 'Angularjs', 'Java', 'PLSQL', 'Windows', 'Oracle', 'Crystal', 'Excel', 'JAVA', 'Tomcat', 'MS Visio', 'SQL', 'Teradata', 'Cognos', 'BI', 'Tableau', 'Tableau', 'dashboard development', 'Tableau', 'data analysis', 'Statistics', 'Dashboard', 'JavaScript', 'HTML5', 'jQuery', 'SASS', 'AngularJS', 'database design', 'normalization', 'OLAP', 'analysis', 'Cognos', 'dashboards', 'Tableau', 'Cognos', 'reporting', 'Tableau', 'JavaScript', 'DML', 'SQL', 'visualizations', 'Tableau', 'data quality', 'access', 'SSIS', 'C', 'SSIS', 'PLSQL', 'SSIS', 'load', 'SSIS', 'data validation', 'loading', 'Data warehouse', 'SQL', 'Teradata', 'BI', 'tableau', 'reporting', 'loading', 'Business Intelligence', 'Tableau', 'Tableau', 'dashboards', 'R', 'data blending', 'dashboard', 'design', 'data modeling', 'Tableau', 'Tableau', 'Tableau', 'REST', 'Sharepoint', 'Exchange', 'transformation', 'code reviews', 'ETL', 'design', 'data processing', 'performance tuning', 'Tableau', 'SQL server', 'Oracle', 'SAP', 'design', 'SAP', 'designing', 'Business Objects', 'Crystal', 'Business Objects', 'reporting', 'Business Objects', 'SQL Server', 'SSIS', 'ETL', 'load', 'SQL Server', 'data quality', 'SQL Server Reporting Services', 'SSRS', 'Reporting', 'Business Intelligence', 'design', 'develop', 'Windows', 'Active Directory', 'SAP', 'SAP', 'Business Objects', 'PLSQL', 'SQL server', 'Office', 'Oracle', 'Windows', 'Excel', 'MS SQL Server', 'Active Directory', 'Oracle', 'Windows', 'VMware', 'Linux', 'Cisco', 'VMware', 'Exchange', 'MS Office', 'MS Visio', 'HTML', 'MS Access', 'JavaScript', 'AJAX', 'jQuery', 'Bootstrap', 'SASS', 'AngularJS', 'project', 'Develop', 'Project', 'Project', 'Business Requirement', 'Data Analysis', 'Data Mapping', 'design', 'designing', 'Data Validation', 'Data Modeling', 'Data Analysis', 'MS Visio', 'Modeling', 'Data Warehousing', 'monitoring', 'reporting', 'Tableau', 'Tableau', 'NET', 'AspNet', 'Active Directory', 'Agile', 'AJAX', 'reporting', 'big data', 'BI', 'Business Intelligence', 'Business Objects', 'C', 'C', 'Cisco', 'Cognos', 'Cognos', 'Crystal', 'CSS', 'Data Analysis', 'data processing', 'data mining', 'Data Modeling', 'Data Validation', 'data warehouse', 'Data Warehousing', 'designing', 'DML', 'ERwin', 'ETL', 'XML', 'modeling', 'HTML', 'HTML5', 'DB2', 'Java', 'JavaScript', 'JAVA', 'JQuery', 'JQuery', 'Linux', 'MATLAB', 'Access', 'MS Access', 'C', 'MS Excel', 'Excel', 'Exchange', 'Exchange', 'Microsoft Office', 'MS Office', 'Office', 'PowerPoint', 'Microsoft Project', 'Sharepoint', 'Windows', 'Windows XP', 'Word', 'Modeling', 'OLAP', 'Oracle', 'Oracle', 'PLSQL', 'PLSQL', 'Programming', 'Python', 'database design', 'reporting', 'SAP', 'SAP', 'SAP', 'Sql', 'MS SQL Server', 'SQL', 'SQL Server', 'SQL Server', 'Statistics', 'Tableau', 'Teradata', 'Tomcat', 'TSQL', 'UML', 'UNIX', 'VB', 'Visio', 'Visual Basic', 'Windows']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-specialist-3e8b2e5124574d3081485139c3bd36f3', 'id': '6361630242691360309919431676680490692', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market       Skills           Performance driven  10years combined experience in customer service patient services entry processing and scheduling  Microsoft Office  Excel intermediate to advanced Word intermediate Outlook advanced  PowerPoint intermediate  Typing at a speed of 100 wmp 10 key                         Experience       Data Entry Specialist       032016      Present     Elara Caring    –    Harker Heights     TX            Compile sort and verify the accuracy of data before it is entered  Compare data with source documents or reenter data in verification format to detect errors  Store completed documents in appropriate locations  Locate and correct data entry errors or report them to supervisors  Ensures that all invoices are completely entered in a timely and accurate manner  On assigned days all mail is broken down sorted in an accurate efficient and timely manner  Ensures that on assigned days mail is scanned in an accurate efficient and timely manner           Administrative Assistant       112015      022016     Qualtek    –    Pittsburgh     PA            Answer telephones and give information to callers take messages or transfer calls to appropriate individuals  Operate office equipment such as fax machines copiers and phone systems and use computers for spreadsheet word processing database management and other applications  Greet visitors or callers and handle their inquiries or direct them to the appropriate persons according to their needs  Set up and maintain paper and electronic filing systems for records correspondence and other material  Open read route and distribute incoming mail or other materials and answer routine letters  Complete forms in accordance with company procedures  Make copies of correspondence or other printed material  Compose type and distribute meeting notes routine correspondence and reports  Maintain scheduling and event calendars  Schedule and confirm appointments for clients customers or supervisors  Order and dispense supplies  Provide services to customers such as order placement or account information  Coordinate conferences and meetings  Operate electronic mail systems and coordinate the flow of information internally or with other organizations           SchedulerCustomer Service Representative       082015      022016     Eastern Metal Supply    –    Winston     FL            Obtain customers names addresses and billing information product numbers and specifications of items to be purchased and enter this information on order forms  Prepare invoices shipping documents and contracts  Inform customers by mail or telephone of order information such as unit prices shipping dates and any anticipated delays  Receive and respond to customer complaints  Check inventory records to determine availability of requested merchandise  Compute total charges for merchandise or services and shipping charges  Schedule appointments for customers needing repairs on their existing appliances           Receptionist       042015      082015     Laz Parking    –    Westminster     CO            Greet persons entering establishment determine nature and purpose of visit and direct or escort them to specific destinations  Transmit information or documents to customers using computer mail or facsimile machine  Hear and resolve complaints from customers or the public  Provide information about establishment such as location of departments or offices employees within the organization or services provided  Receive payment and record receipts for services  Schedule appointments and maintain and update appointment calendars  Keep a current record of staff members whereabouts and availability  Provide support for sales team in managing operation work flow  Demonstrate proficiency in telephone email fax and front desk reception within a high volume environment  Schedule appointments for appliances being delivered           Customer Service RepresentativeCashier       052014      042015     STOPNGO    –    City     STATE            Receive payment by cash check credit cards vouchers or automatic debits  Issue receipts refunds credits or change due to customers  Assist customers by providing information and resolving their complaints  Establish or identify prices of goods services or admission and tabulate bills using calculators cash registers or optical price scanners  Greet customers entering establishments  Answer customers questions and provide information on procedures or policies  Sell tickets and other items to customers  Process merchandise returns and exchanges  Maintain clean and orderly checkout areas and complete other general cleaning duties such as mopping floors and emptying trash cans  Stock shelves and mark prices on shelves and items  Request information or assistance using paging systems  Count money in cash drawers at the beginning of shifts to ensure that amounts are correct and that there is adequate change  Calculate total payments received during a time period and reconcile this with total sales  Monitor checkout stations to ensure that they have adequate cash available and that they are staffed appropriately  Supervise others and provide onthejob training  Keep periodic balance sheets of amounts and numbers of transactions           PullerProcessor       042013      122013     CTI PAPER    –    City     STATE            Read orders to ascertain catalog numbers sizes colors and quantities of merchandise  Update daily logs for tracking file movements  Obtain customers names addresses and billing information product numbers and specifications of items to be purchased and enter this information on order forms  Check inventory records to determine availability of requested merchandise  Review orders for completeness according to reporting procedures and forward incomplete orders for further processing  Confer with production sales shipping warehouse or common carrier personnel in order to expedite or trace shipments  File copies of orders received or post orders on records  Verify customer and order information for correctness checking it against previously obtained information as necessary  Prepare invoices shipping documents and contracts  Inform customers by mail or telephone of order information such as unit prices shipping dates and any anticipated delays  Compute total charges for merchandise or services and shipping charges           Patient Services AssistantTravel Coordinator       032005      012010     WILLIAM S MIDDLETON VETERANS MEMORIAL HOSPITAL    –    City     STATE            Coordinate communication between patients family members medical staff administrative staff or regulatory agencies  Interview patients or their representatives to identify problems relating to care  Maintain knowledge of community services and resources available to patients  Investigate and direct patient inquiries or complaints to appropriate medical staff members and follow up to ensure satisfactory resolution  Explain policies procedures or services to patients using medical or administrative knowledge  Answer applicants questions about benefits and claim procedures  Interview benefits recipients at specified intervals to certify their eligibility for continuing benefits  Compile record and evaluate personal and financial data in order to verify completeness and accuracy and to determine eligibility status  Utilize knowledge and skills of medical terminology for emergency department check ins admitting dictation records and eligibility  Coordinate admission processes and prepare agreement packets          Education and Training       High School Diploma              Expected in   Jun 1999                MADISON EAST HIGH SCHOOL      MADISON     WI     GPA        Status                  Bachelor of Arts       Business Administration       Expected in   Jun                ASHFORD UNIVERSITY FORBES SCHOOL OF BUSINESS      SAN DIEGO     CA     GPA        Status         Business Administration        Skills     10 key administrative Schedule appointments balance sheets benefits billing calculators cash registers catalog conferences contracts Make copies credit clients customer service data entry database management dictation electronic mail email facsimile machine fax machines fax filing financial forms inventory Prepare invoices Issue receipts letters notes managing mark materials medical terminology meetings Excel mail money Microsoft Office Outlook PowerPoint Word office equipment direct patient personnel phone systems copiers policies processes Read reception repairs reporting sales scanners scheduling shipping spreadsheet take messages telephone telephones Typing type word processing       Activities and Honors', 'resume_html': 'none', 'skills': \"['Oracle', 'MS Access', 'Redshift', 'S3', 'Software Products', 'Oracle', 'Oracle', 'Hadoop', 'Amazon Web services', 'Redshift', 'Programming', 'Oracle', 'PLSQL', 'SQL', 'Impala', 'SQL', 'Python', 'C', 'Visual Basic', 'MS Excel', 'VBA', 'Power Query', 'PLSQL Developer', 'Kibana', 'excel', 'Eclipse', 'MS Office', 'MS Project', 'MS Visio', 'Power Query', 'Excel', 'Putty', 'Tortoise SVN', 'GitHub', 'JIRA', 'Hadoop', 'Oracle', 'UNIX', 'Amazon Web Services', 'AWS', 'Redshift', 'business analysis', 'system analysis', 'design', 'Project management', 'Hadoop', 'HDFS', 'AWS', 'Oracle', 'Hive', 'Redshift', 'Hadoop', 'Oracle', 'Hadoop', 'data sources', 'XML', 'SDLC', 'analysis', 'design', 'testing', 'Regression', 'design', 'SQL', 'Stored Procedures', 'SQL', 'SQL', 'query optimization', 'Agile', 'Waterfall', 'Project management', 'Agile', 'Scrum', 'Oracle', 'UNIX', 'Analysis', 'Design', 'Estimation', 'Analysis', 'business requirements', 'develop', 'loading', 'UNIX', 'data science', 'data sets', 'reporting', 'Oracle', 'Analysis', 'Design', 'reporting', 'Oracle', 'UNIX', 'Analysis', 'Design', 'Estimation', 'Analysis', 'Requirement analysis', 'technical design', 'Technical design', 'optimization', 'monitoring', 'SQL', 'stored procedures', 'system testing', 'Integration testing', 'reporting', 'Agile', 'automation', 'Big Data', 'business analysis', 'C', 'Data Analysis', 'data modeling', 'Eclipse', 'XML', 'FTP', 'Java', 'MS Access', 'MS Excel', 'Excel', 'MS Office', 'MS Project', 'optimization', 'Oracle', 'Oracle', 'PLSQL', 'PLSQL', 'Programming', 'Project management', 'Python', 'reporting', 'Research', 'Scrum', 'SDLC', 'SQL', 'analysis', 'UNIX', 'Visio', 'Visual Basic', 'VBA']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/secretary-customer-service-sales-data-base-entry-46df311c156e477c8d7f5406e116de02', 'id': '147203991935502946470515237354155400269', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Career Overview      Dedicated personable and motivated secretarycustomer service representative My focus is to maintain customer satisfaction and contribute to company success I have extensive work experience in a variety of customer service settings such as hospitals businesses and retail         Core Strengths           Strong organizational skills  Active listening skills  Sharp problem solver  Courteous demeanor  Energetic work attitude  Telephone inquiries specialist  Customer service expert  Invoice processing  Adaptive team player  Openingclosing procedures  Quick learner   Have worked with many business specific computer programs       Data collection  Data entry  Documentation  Email  Internet research  Speaking  Telephone skills  Multitasking                        Accomplishments       Customer Service    Researched calmed and rapidly resolved client conflicts to prevent loss of key accounts    Customer Interface    Greeted customers upon entrance and handled all cash and credit transactions  Assisted customers over the phone regarding store operations product promotions and orders     Database Maintenance    Assisted in the managing of the company database and verified edited and modified customers’ information         Work Experience       SecretaryCustomer ServiceSalesData Base Entry       092012   to   062015     Taco Bell    –    Gravette     AR             Responsible for daily operations and overall finances of a small but busy satellite company which includes but is not limited to billing budgeting customer invoicing through QuickBooks payroll quarterly payroll and company taxes Knowledge in word and excel spreadsheets Created customer accounts revising as necessary  Developed highly empathetic client relationships  Computed accurate sales prices for purchase transactions  Resolved product issues and shared benefits of new technology  Expressed appreciation and invited customers to return  Managed quality communication and customer support for each client  Interacted with customers to followup on shipping statuses and expedited orders  Promptly responded to general inquiries from members staff and clients via mail email and fax  Guaranteed positive customer experiences and resolved customer complaints  Executed outbound calls to existing customer            Manager       2011   to   042012     Smart Cow    –    City     STATE             Managed team of 34 employees  Served as mentor to junior team members  Took necessary steps to meet customer needs and effectively resolve food or service issues  Communicated clearly and positively with employees  Resolved customer complaints in a postive manner  Assisted in important decisions on new products and new employee hire   Quickly and efficiently processed payments and made accurate change  Closely followed standard procedures for safe food preparation assembly and presentation to ensure customer satisfaction  Performed general maintenance duties including mopping floors washing dishes wiping counter tops and emptying traps           SecretaryCNA in MomBaby Unit       082009   to   112011     Exempla Lutheran Medical Center    –    City     STATE              CNA     Took vital signs of mothers and newborns for a floor of up to 20 patients per shift  Took and recorded patients temperature pulse and blood pressure  Performed 24 hr infant testing including PKU hearing and jaundice  Worked as part of team to ensure proper care and safety of myself  and  patient and newborns  Assisted physicians with the circumcision of newborns  Accurately identified patients with patient chart by verbalizing and checking patient bracelet  Cleaned and sterilized instruments and disposed of contaminated supplies     Secretary     Accurately documented all elements of inpatient information discharge instructions and followup care  Managed smooth and effective communication among physicians patients families and staff  Handled incoming and outgoing correspondence including mail email and faxes  Screened telephone calls and inquiries and directed them as appropriate  Devised and maintained office systems to efficiently deal with paper flow  Organized personal and professional calendars and supplied reminders of upcoming meetings and events when necessary  Flexible and trained to fillin as secretary in sister units such as Labor and Delivery  and  NICU  Actively maintained strict confidentiality and safeguarded all patientrelated information with HIPPA knowledge    Well trained in hospital specific computer programs such as Epic and CPN            Educational Background       Obtained Cosmetology License      Cosmetology     Expected in   2012     Empire Beauty School      Arvada     CO     GPA                Obtained CNA License     Nursing     Expected in   2009     Front Range Community College      Denver     CO     GPA                           Expected in   2008     Community College of Denver      Denver     CO     GPA        Completed necessary courses that contributed to nursing career          Obtained high school diploma     Basic     Expected in   2004     Arvada High School      Denver     CO     GPA                Skills       Patientfocused care  Excellent interpersonal skills  Compassionate and trustworthy caregiver  Detailoriented  Effectively interacts with patients and families  Medical terminology  Charting and record keeping', 'resume_html': 'none', 'skills': \"['Analysis', 'Reporting', 'SSRS', 'Crystal', 'CRM', 'technical documentation', 'project management', 'data quality', 'CRM', 'Excel', 'Excel', 'Visual Basic', 'technical documentation', 'project designs', 'SSRS', 'Crystal', 'Pivot table', 'VLOOKUP', 'Scrum', 'teams', 'Test Cases', 'SharePoint', 'assembly', 'technical assistance', 'assembly', 'gateway', 'analyzing data', 'business intelligence', 'business decisions', 'Pivot Table', 'Microsoft Excel', 'data integrity', 'storage', 'Technical Documentation', 'Design', 'UML', 'ERD']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-specialist-36e2b426632b41d08cf146119467bc1c', 'id': '97098914450695563702884660921271152925', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary     \\xa0Meticulous and detailoriented Executive AssistantOffice Administrator adept at implementing innovative practices and procedures to improve efficiency Organized and implemented an efficient work flow system that resulted in 30 increase in productivity Established good working relationships with students faculty staff and members of the public Selected to assume additional duties as Recording Secretary for committee meetings averaging 20 attendees \\xa0Executive Assistant with management experience and exceptional people skills Versed in  communication  and  organization  Desires a challenging role as a teamplayer        Core Qualifications         Microsoft Office Suite Google Docs Datatel Excels in area pf details\\xa0  Resultsoriented     Proficiency in supervision   \\xa0  Operations management   Clientfocused                       Experience       Data Entry Specialist       092015      102015     Elliot Davis    –    Nashville     TN            Prepared source data for computer entry by compiling and sorting information establishing entry priorities  Processed employee and account source documents by reviewing data for deficiencies resolving discrepancies by using standard procedures or returning incomplete documents to the team leader for resolution  Entered employee and account data by inputting alphabetic and numeric information on keyboard or optical scanner according to screen format  Maintained data entry requirements by following data program techniques and procedures           Executive Assistant       022015      072015     Xl Group    –    Miami     FL            Reviewed and provided comments on the adequacy of documents and took necessary steps to cure any deficiencies       Effectively controlled the release of proprietary and confidential information for general client lists  Represented Administrator through communication both verbal and electronic as well as in person  Extensive planning and preparation of meetings and events  Extensive creation and maintenance of records  Worked in liaison with College Deans offices organize faculty interviews  Organized coordinated and maintained Administrators calendar  Assisted prospective faculty with house hunting trips  Organized and coordinate prospective faculty moves  Arranged meetings with top level administrators  Coordinated travel arrangements for Administrator  Processed Sabbatical applications and various reports Processed faculty overloads and tuition reimbursements applications  Managed use and reconciled Administrators credit cards           Administrative AssistantOffice Administrator       2011      2015     Soriano Tax Services    –    City     STATE              Prepared correspondence accounting and financial documents for analysis  Provided onsite training      Provided tax preparation services represented clients before IRS resolved outstanding tax issues  Responded to inquiries maintain office database and files  Maintained and processed confidential information  Followed up on client inquires  Heavy telephone sales to secure clients           Administrative Secretary Vice President       082008      072011     California State University    –    City     STATE          Full time student at California State University Northridge majoring in Sociology         Office of Vice President       061997      032007     East Los Angeles College    –    City     STATE            Managed the daily operations of the Office of Academic Affairs  Prepared and distributed agendas of various committee meetings  Took transcribed and distributed minutes of various committee meetings  Maintained daily calendarschedule of administrator  Managed communication flow and work flow to administrators office  Established cross training system that improved efficiency of department by 50  Supervised trained clerical staff provided key input on performance evaluations Managed confidential personnel records  Served as official liaison between administrator and campus staff students and members of the public  Initiated coordinated and monitored hiring process for faculty and administrators  Composed correspondences reports presentations Proof read documents reports press releases for accuracy  Provided research support for reports and meetings  Communicated and implemented District policies  Monitored and tracked departments budget expenditures  Managed official travel system processed expense reports and reimbursements  Facilitated website updates on Office of Academic Affairs working in liaison with webmaster          Education       Bachelor of Arts       Sociology       Expected in   2011                California State\\t\\tCollege of Behavioral Sciences      Northridge     CA     GPA        Status           Coursework in  Sociology      NonProfit  Humanitarian Work   Assisted in the coordination of fund raising campaigns for Doctors without Borders St Josephs Indian School and Oxfam International         Certifications     EDD Certification 73 words per minute with 98 accuracy Reeswood Secretarial College Certificate in Shorthand 120 wpm Graduated with honors in the program       Interests     Volunteered at Lighthouse for Women and Children a homeless shelter in Oxnard California       Languages     French Basic Spanish Basic       Skills     Academic budget clerical credit client clients data entry database expense reports French fund raising hiring keyboard team leader meetings Microsoft Office Suite Office 98 personnel policies presentations press releases Profit read research sales scanner Secretarial Shorthand sorting Spanish \\xa0tax preparation telephone travel arrangements website       Additional Information       Volunteered at Lighthouse for Women and Children a homeless shelter in Oxnard California', 'resume_html': 'none', 'skills': \"['STATA', 'programming', 'R', 'programming', 'SAS', 'programming', 'GIS', 'NVIVO', 'Microsoft Office', 'analytical research', 'Project', 'Evaluation', 'statistical models', 'Research', 'data collection', 'data quality', 'statistical methods', 'analysis', 'Designing', 'data collection', 'data collection', 'Designing', 'GIS', 'Microsoft Office', 'programming', 'research', 'SAS', 'STATA']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-operator-3586d5320e6848b198a737749a7eba1b', 'id': '138536846257210096623495337605137733129', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary       Committed and motivated Administrative Assistant with exceptional customer service and decision making skills Strong work ethic professional demeanor and great initiative  Energetic and reliable Office Manager skilled with working with a diverse group of people  Executive Assistant who is skilled at multitasking and maintaining a strong attention to detail Employs professionalism and superior communication skills to meet client and company needs          Areas of Expertise           Word Excel Access Word Perfect  Operations management  Communication   Interpersonal       Time management  Flexible  Works well under pressure  Employee training and development                       Work Experience       Data Entry Operator       032014      082014     Iron Mountain Incorporated    –    Fort Myers     FL            Performed general data entry using SAP Microsoft Excel and Word    Performed a wide variety of secretarial tasks in support of the business   Answered phones and create notifications in the system   Contacted with internal and external customers  Collaborated with other administrative team members human resources and the finance department on special projects and events  Developed and managed thirdtier resolution process to resolve issues originating from the customer retention team           Secretary       2010      2013     Walt Disney Co    –    Auburn Hills     MI            Arranged appointments sales calendars trainings for the sales of department  Maintained the operations sales database  Customized sales reports and sales literature  Verified and logged in deadlines for responding to daily inquiries  Improved communication efficiency as primary liaison between departments clients and vendors           Assistant Manager       2008      2010     Sumitomo Electric Group    –    Mount Prospect     IL            Recruited hired scheduled and motivated a staff of up to 8 people  Adapted in communicating effectively with customers vendors and staff  Reached the monthly goals  Managed the daytoday tactical and longterm strategic activities within the business  Reduced and controlled expenses by improving resource allocation          Education       Associate       Arts       Expected in                   Community College of Philadelphia      Philadelphia     PA     GPA        Status          Arts          Certified with diploma              Expected in   1 2013                Notary Public                GPA        Status                 Professional Affiliations              Languages     English Spanish       Skills      Interpersonal data entry database English Languages Access Microsoft Excel Excel Word sales SAP secretarial Spanish phones Word Perfect', 'resume_html': 'none', 'skills': \"['business processes', 'data integrity', 'SAP', 'SAP', 'processing', 'budget analysis', 'resource analysis', 'Crystal', 'analyzing data', 'SAP', 'analysis', 'SAP', 'SAP', 'MicroSoft Office', 'Excel', 'Word', 'PowerPoint', 'Outlook', 'Visio', 'Access']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-customer-service-357f0974fe214cdfad27461bf1b36f96', 'id': '321556423433782918032074297826301460362', 'data': 'JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary       Energetic and reliable Retail Sales Associate skilled in highend merchandise environments  Personable and responsible Cashier with 5 years in retail and customer service Solid team player with upbeat positive attitude  Resultsdriven with proven ability to establish rapport with clients  Dedicated Customer Service Representative motivated to maintain customer satisfaction and contribute to company success         Highlights          Problem resolution  Selfstarter  Deadlineoriented  Microsoft Office  Employee training and development  Customer service expert  Openingclosing procedures  Telecommunication skills      Strong organizational skills  Active listening skills  Large cashcheck deposits expert  Energetic work attitude  Top sales performer  Invoice processing  Courteous demeanor  Sharp problem solver                       Experience        072013   to   Current   Data Entry Customer Service    A Duie Pyle Inc         Stoughton     MA           •Performed general clerical duties as needed such as completing forms and reports  •Confered with Assembly Technician Lead and Manager to determine progress of work and to provide order status  •Assembled various components within established assembly time standards while adhering to procedures and set specifications  •Used Method Instructions to assemble equipment  •Inspect parts to ensure that quality standards are maintained  •Provide responsibility for the quality of work  •May cross train to perform other duties on the production lines  •Performs simple calculations  •Clean and maintain work area  •Will be required to wear personal protective equipment relevant to work area            Other duties as needed and assigned           072010   to   062013   Data Entry Clerk    Ferguson         Red Bank     NJ           •Received and scans large stacks of documents with excellent attention to quality  •Scan to file of hard copy job following standard operating procedures  •Inspect finished work for accuracy  •Conscientious and consistent effort to quickly and accurately complete each task andor job is the companys productivity standard  •File records as needed  •Destroy documents as stated within the policy           2010   to   062012   Front Desk ClerkNight Manager    Western Inn         City     STATE           • Process guest registrations including collecting payment   • Complete shift reports   • Respond to guest needs special requests and complaints alert the appropriate manager to potential issues as needed   • Assist customer with making room keys  • Assist customers in various assignments including finding nearby attractions restaurants and etc   • Prepare coffee for guest  • Transmit and receive messages via telephone and fax machine   • Sort and rack incoming messages and mail   • Assist guests with requests and problems related to their stay at the property   •Resolve guest complaints ensuring guest satisfaction  •Maintain complete knowledge of or where to access to following information a all hotel featuresservices hours of operation b all room types numbers layout decor appointments and location c all room rates special packages and promotions d daily house count and expected arrivalsdepartures e room availability status for any given day f scheduled daily group activities  •Pick up count and maintain bank Secure bank at all times  •Read the log book daily and record all pertinent information in the log book  •Process currency exchange and payments to guest accounts  •Process adjustments rebates paid outs and credits as required  •Verified that all checks are closed and closes and logs any open check in the POS Point of Sale system  •Run Room  Tax verifying that all room rates posted  •Verify Cashiers Report to drop log and paperwork  •Record room statistics  •Close POS after all work was balanced  •Run end of day program and close day  •Check that interfaces are up and running  •Run daily Flash Reports and distribute accordingly  •Run morning reports and backup reports and distribute accordingly  •Print express check out folios and distribute  •Sign out and brief relief  •Review Night Audit checklist and verify that all work has been completed  •Restock all printers  •Fill out and deposit payment and corresponding checks  •Review status of assignments and any followup action with oncoming Supervisor  •Document maintenance needs on work orders and submit to ManagerSupervisor             102008   to   2010   Retail Sales Consultant    Sprint         City     STATE           •Provided a total sales solution to the customer regarding their wirelessmobility needs that includes selling the value for Sprints devices accessories and service plans maximizing customer connections saving the customer money personalizing the customer experience protecting their investment  •Delivered an outstanding store experience that improves customer loyalty and strengthens the Sprint Brand  •Met key performance objectives that include sales and customer satisfaction goals  •Made certain accurate customer account setup so they are ready to use when leaving the store  •Identified the right solutions to customer billing technical and or account issues  •Completed all courses in your curriculum path with the required time frame  •Complied with all operational policies and procedures including the Sprint Code of Conduct  •Promote innovation and friendly competition to deliver unparalleled customer experience           112006   to   112008   ExpeditorCashier    Macys         City     STATE           •Assist customers in all aspects of service fulfillment by demonstrating proficient use of proprietary devices and applications proactively create enhanced shopping experiences through the heightened use of tools technology and collaboration  •Determined customer needs based on personal features and other customer preference related factors  •Demonstrated knowledge of store products and services to build sales and minimize returns  •Maintained a professional attitude with sincerity and enthusiasm reflecting Macy’s commitment to our customer – the most important person in our stores  •  POS procedures  •Assisted customers in all aspects of service fulfillment by demonstrating proficient use of proprietary devices and applications proactively create enhanced shopping experiences through the heightened use of tools technology and collaboration  •Recovered shoe sales floor and scan inventory back into stock  •Maintained integrity of shoe inventory by ensuring accuracy of scanning and placement  •Receive and process new merchandise          Education        Expected in   2008   Associates       Network Support System    Everest Institute     Houston     TX      GPA                 Expected in   2006   High School Diploma           United Christian School     Houston     TX      GPA               Accomplishments       Volunteer at Mustang Center teaching English to adults and teens 2010  until        Skills       10Key Account Management Active Learning Calendaring Client Relations Computer Proficiency Coordination Creative Problem Solving Critical Thinking Customer Needs Assessment Customer Service Data Collection Data Entry Documentation Email Executive Management Support Filing Grammar Internet Research Report Transcription Research Scheduling Service Orientation Speaking Spreadsheets Telephone Skills Time Management Travel Arrangements Travel Booking Travel Planning Type 3040 WPM Typing Writing Letters and Memos Lotus Notes Microsoft Excel Microsoft Office Suite Microsoft Outlook Microsoft PowerPoint Microsoft Word Minute Taking MultiTask Management Organizational Skills Prioritization Proofreading Reading ComprehensionCash handlingProfessional and friendly Careful and active listener Multitasking Production Mechanical Assembler Packager Labeling Inventory', 'resume_html': 'none', 'skills': \"['Data privacy', 'Research', 'data analysis', 'cloud', 'Networking', 'user training', 'monitoring', 'statistical analysis', 'reporting', 'data collection', 'statistical analysis', 'project', 'algorithms', 'workflow diagrams', 'HIPAA', 'HIPAA', 'HIPAA', 'HIPAA', 'Windows XP', 'Office', 'Data privacy', 'Research', 'data analysis', 'cloud', 'Networking', 'user training', 'monitoring', 'statistical analysis', 'reporting', 'data collection', 'statistical analysis', 'project', 'algorithms', 'workflow diagrams', 'HIPAA', 'HIPAA', 'HIPAA', 'HIPAA', 'Windows XP', 'Office']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-operator-381caf3c16b649328748d7e6c673a7d8', 'id': '283692866702135157759709078394118382144', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Energetic worker with a broad range of customer service and team leader skills Data Entry specialist adept at developing and maintaining databases Highly skilled at creating effective organizational and filing systems Dynamic Data Analyst trained in an IT environment who goes above and beyond given job responsibilities to achieve superior results and maintain companywide integrity        Highlights           Certified in 10key  Time management  Meticulous attention to detail  Resultsoriented  Selfdirected  Excellent communication skills  Strong problem solver  Filing and data archiving  HIPAA compliance      Advanced MS Office Suite knowledge  Resourceful  Strong interpersonal skills  Pleasant demeanor  Understands grammar  Customer serviceoriented  Advanced clerical knowledge  Critical thinker                       Accomplishments       Data Entry    Preserved an accuracy of 98 during 5 years of employment    Training    Successfully trained staff in all office systems and databases policies and procedures while focusing on minimizing errors and generating superior results    Reporting    Maintained status reports to provide management with updated information for client projects    Administration    Performed administration tasks such as filing developing spreadsheets faxing reports photocopying collateral and scanning documents for externaldepartmental use    Multitasking    Demonstrated proficiencies in telephone email fax and frontdesk reception within highvolume environment   Implementation   Assisted in implementation of new tracking system that resulted in improved patient care    OSHA Compliance    Properly disposed of daily biohazard waste in compliance with federal and local regulations   Documentation   Drafted documents for internal meetings          Experience       Data Entry Operator       082009      Current     Iron Mountain Incorporated    –    Essex Junction     VT            8000 key strokes per hour  Trained staff to operate new environmental health technology  Verified that information in the computer system was uptodate and accurate  Processed confidential medical information  Identified and resolved system and account issues  Developed and created a more effective filing system to accelerate paperwork processing  Trained new employees and explained protocols clearly and efficiently  Provided base level IT support to company personnel  Troubleshot hardware issues and worked with service providers to facilitate repairs           Tutor       032012      062012     Arizona State University    –    Tempe     AZ            Tutored college level students in the fields of reading writing and computers  Routinely met with students regarding inclass issues and learning interruptions to discuss solutions  Performed student background reviews to develop tailored lessons based on student needs  Taught Creative writing to a diverse class of 20 students  Developed and implemented interesting and interactive learning mediums to increase student understanding of course materials           VolunteerMentor       082007      052008     Boys And Girls Club Of Northeast Florida    –    City     STATE            Coordinated after school tutoring hours to help students in need of extra attention  Received high remarks for the creativity of classroom lesson plans and instructional techniques from students parents and faculty  Created and enforced childbased handson curriculum to promote student interest and receptive learning  Designed lesson plans focused on age and levelappropriate material  Developed interesting course plans to meet academic intellectual and social needs of students  Consistently met schedules and deadlines for all illustration projects  Worked alongside the entire development team in an energetic and creative environment          Education       Associate of Arts       Business Administration       Expected in   2014                Florida State College at Jacksonville      Jacksonville     Florida     GPA        Status          360 GPA  Member of Phi Theta Kappa Honor Society  Recipient of 2014 Academic Achievement Award  Coursework in Marketing Public Relations and Business Management           Skills      10Key  Customer Service  Data Entry  Microsoft Office Suite', 'resume_html': 'none', 'skills': \"['Data Quality', 'Oracle Forms', 'Google Drive', 'Google Docs', 'Google Sheets', 'Microsoft Office', 'Data Quality', 'data analysis', 'developing', 'data quality', 'data patterns', 'data linking', 'test scripts', 'interviews', 'SWOT analysis', 'data quality', 'data analysis', 'Microsoft Office', 'Oracle', 'research']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-management-services-coordinator-368330306fc74431af7e0137d3706178', 'id': '189067289296683940487677712533012870883', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  9XXX5    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary     Administrator with over 15 years of professional experience Skilled in all aspects of office administration organization of filing systems use of electronic office equipment handling multiline phone systems reception data entry coordinating with staff scheduling appointments banking and accounts receivable and payable Communication skills demonstrated through verbal and writing abilities client relations marketing expertise customer service skills training new employees and the ability to produce indepth reports and correspondence       Highlights           Confidential Correspondence and Data  Microsoft Excel  Microsoft Word  Microsoft Outlook  Microsoft PowerPoint  Data Entry  Document Creation and Maintenance  Editing and Proofreading  Information Resource  Knowledge of Office Equipment CopierFax  10Key Calculator  Agenda and Event Coordination  Business Correspondence  Client Services  Call Screening  Mail Distribution  Stocking and Supplies  Typing  Data Entry  Billing Processes  Purchasing and Inventory  Payroll and Accounts Administration                         Experience       Data Management Services Coordinator       2009   to   Current     Penguin Random House    –    City     STATE             Performs highly accurate and detailed data entry for end of month invoicing  Performs data entry for business account orders in a timely manner  Responsible for several monthly reports submitted to management   Compiled statistical information for special reports     Organized billing and invoice data     Updated departmental standard operating procedures and database to accurately reflect the current practices       Identified and resolved system and account issues         Crosstrained and provided backup for other data management representatives when needed       Resolved spreadsheet issues and shared benefits of new technology         Interacted with customers to followup on shipping status and expedited orders           Promptly responded to general inquiries from members staff and clients via mail email and fax             Assisted customers in finding outofprint items             Kept abreast of rapidly evolving technology             Provided accurate and appropriate information in response to customer inquiries             Dispersed incoming mail to correct recipients throughout the office               Organized files developed spreadsheets faxed reports and scanned documents               Received and screened a high volume of internal and external communications including email and mail               Created and maintained spreadsheets using advanced Excel functions and calculations to develop reports and lists                  ReceptionistCashier Supervisor       2008   to   2009     Koons Of Westminster    –    City     STATE              Assessed customer needs and responded to questions       Organized register supplies      Worked with customer service to resolve issues        Provided professional and courteous service at all times       Worked overtime shifts during busy periods       Monitored a \\xa0 team of  78  of professionals    Trained and mentored new cashiers      Hired 34 team members     Managed cashier shifts and breaks       Built and maintained productive relationships with employees       Greeted customers promptly and responded to questions       Documented performance issues       Counted and balanced cashier drawers       Worked in competitive team environment to exceed revenue quotas             OfficeProgram Assistant       2004   to   2008     General Dynamics Information    Technology    –    City     STATE             Maximized productivity by maintaining multiple calendars scheduling meetings tracking expenses and prioritizing phone calls for Program ManagersMaintained office equipment and ordered supplies  Prepared weekly spreadsheets monitoring more than 15 ongoing projectsOversaw status of projects by continually gathering information and followingup with Program Managers  Updated dynamic organizational charts and headcount spreadsheets  Answered multiline telephone system maintained appointment calendar filed personnel records and assisted Program Manager  Performed timely and highly accurate data entry to ensure fastest turnaround possible for end of month invoicing  Developed planned organized and administered policies and procedures for organization to ensure administrative and operational objectives were met  Implemented corrective action plan to solve problems  Established and maintained comprehensive and current record keeping system of activities and operational procedures in business office  Prepared reviewed and submitted reports concerning activities expenses budget government statutes and rulings and other items affecting business and program services  Consulted with staff and others in government business and private organizations to discuss issued coordinate activities and resolve problems  Prepared budget and directed and monitored expenditures of department funds  Directed and conducted studies and research and prepared reports and other publications relating to operational trends and program objectives and accomplishments           Loan Editor       2001   to   2004     BancFirst    –    City     STATE             Verified and examined information and accuracy of loan application and closing documents  Recorded applications for loan and credit loan information and disbursement of funds using computer  Accepted payment on accounts   Filed and maintained loan records    Presented loan and repayment schedule to customer   Calculated reviewed and corrected errors on interest principal payment and closing costs using computer and calculator    Contacted credit bureaus employers and other sources to check applicant credit and personal references    Assembled and compiled documents for closing such as title abstract insurance form loan form and tax receipt   Prepared and typed loan applications closing documents legal documents letters forms government notices and checks using computer    Interviewed loan applicant to obtain personal and financial data and to assist in filling out application    Complied with federal state and company policies procedures and regulations    Debited and credits accounts   Processed negotiable instruments such as checks and vouchers   Evaluated records for accuracy of balanced postings calculations and other records pertaining to business and operating transactions and reconciled and notes discrepancies   Recorded financial transactions and other account information to update and maintain accounting records          Education       Associate     Accounting     Expected in   2007     Ashworth University      Norcross     GA     GPA   GPA 355    Accounting GPA 355        Skills     10Key Calculator accounting administrative Billing budget Business Correspondence calculator charts closing credit Client Data Entry Editing Event Coordination Fax filling Financial forms funds government insurance Inventory invoicing legal documents letters notes meetings Microsoft Excel Mail office Microsoft Outlook Microsoft PowerPoint Microsoft Word Office Equipment Office Management organizational Payroll personnel Copier policies Processes Proofreading publications Purchasing record keeping research scheduling spreadsheets tax telephone phone Typing', 'resume_html': 'none', 'skills': \"['functional requirements', 'monitoring', 'excel', 'access', 'developing', 'Data Warehouse', 'monitor', 'Sharepoint', 'Sharepoint', 'data sharing', 'implement', 'Microsoft Word', 'Microsoft PowerPoint', 'Microsoft Excel', 'developing', 'analyzing data', 'Excel', 'Sharepoint', 'Sharepoint', 'data sharing', 'Adobe Photoshop', 'Data Warehouse', 'Access', 'Microsoft Excel', 'Excel', 'Microsoft Office', 'Outlook', 'Microsoft PowerPoint', 'Power Point', 'Microsoft Publisher', 'Word', 'Microsoft Word', 'Project Management', 'VISIO']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-warehouse-developer-35a285e749a74031816e915414d2ae8b', 'id': '14259385793885776770164514431890372233', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary       Proactive performancedriven professional with 105  years of experience in IT industry in Investment Banking domain   Proficient in the Integration of various data sources with multiple relational databases like Oracle11g Oracle10g9i Sybase into the staging area Data Warehouse which includes developing PLSQL Procedures Packages Triggers Bulk collections Cursors Views Objects and Performance Tuning of Data Warehouse environment  Data Warehousing ETL experience of using Informatica PowerCenter Client tools  Mapping Designer Repository manager Workflow ManagerMonitor and Server tool Repository Server manager   Proficient in UNIX shell scripting in automation of various processes Hands on experience in server setup to host files to automate the database refresh with help of Perlcgi scripts Developing web page applications using HTML5 CSS3 and Javascript for form validations Also using Ajax and PHP scripting to get back data from Oracle DB     Experience in using Automation Scheduling tools like Autosys and ControlM     Knowledge and hands on in Informatica ETL Tool for loading huge data files     Handson experience across all stages of Software Development Life Cycle SDLC including business requirement analysis data mapping build unit testing systems integration and user acceptance testing     Experience working in OnShoreOffshore Development Models     Strong time management skills in leading multiple projects and deadlines under minimal supervision     Recognized and highly appreciated for consistent success knowledge and flexibility          Areas of Expertise           Windows NTXP2007 UNIX MSDOS  ETL Tools Informatica Power Center 918171 Designer Workflow Manager Workflow Monitor Repository manager and Informatica Server  Databases Oracle 11g10g9i8i Sybase  Languages SQL PLSQL UNIX Shell scripts Perl HTML5 Java Scripting PHP  Scheduling Tools Autosys ControlM                         Work Experience       Data Warehouse Developer       032015      Current     22Nd Century Technologies       Bothell     WA            Customer Warehouse Environment CWE is a group in Fidelity focusing on providing analytical data to various business groups  This data helps the business community in increasing the Fidelity business in Retail and Institutional areas  The data constitutes of various customers accumulated from various sources  Major responsibilities include impact analysis support production support activities install planning and help answer business adhoc requests  Role and Responsibilities Providing resolution to the issues raised by users  Monitoring and providing batch support for the daily and monthly batches  Conduct regular meetings to check for weekly and monthly install  Provide month end batch support and provide oncall support  Enhancement of the existing application to provide more functionality  Identify long running process and tune the same  Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  Used SQLDeveloperToad for creating all types of PLSQL objects like tables Indexes packages triggers sequences and stored procedures  Optimized the existing long running processes to run faster with usage of Bulk Loader Hints and limits on bulk loader Used Explain Plans to analyze long running queries and degree of Parallel to make queries run faster  Create new web applications from HTML5 and CSS  Javascript for form validation Host the web applications on windows server 2012 R2 machine Used PHP as the server side language to psftp the files from users machine  Call oracle client functions ODBC to connect to db and get back the results on HTML forms           ETLData Warehouse Developer       072012      032015     22Nd Century Technologies        Burkeville     VA            The Strategic Investment Product team works on providing investment plans for individuals investment plan and guidance tool for retailers retirement plans 401k for organizations  The tasks involved as part of this are loading and processing the stocks and mutual fund details from different sources categorize and customize such that retail and individual investors get the accurate details which would help in investments  Role and Responsibilities Understand and analyze requirements follow up with business analysts and subject matter expert team for any clarification if required  DesignReview the Test Cases for Integration testing System testing and User Acceptance testing Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  From staging area data is processed and modified as per the requirement and then loaded to integration area through stored procedures and ETL Informatica mappings  Use Toad for developing all types of PLSQL objects like tables Indexes packages triggers sequences and stored procedures  Developed and modified Oracle packages stored procedures functions Scripts synonyms tables and indexes to implement business requirements and rules  Applied optimizer hints to tune the queries for faster performance  Worked on Performance tuning by using Explain plans and various hints  Worked on Table Partitioning and deploying various methods of indexing like local Indexes and Global Indexes on partitioned tables  Organize and analyze the behavior of each Investment Instrument like Stocks Bonds etc for a certain period and provide the complete detailed analysis to investors which would help them in planning their portfolio and the investments  Evaluate the performance of different funds which would be ideal for long term investing which is the primary requirement of a retirement plan 401k aimed at institutions  Project Title PARA reporting for an Investment Bank           Data Warehouse Developer       2009      062012     22Nd Century Technologies        Columbia     MO            Location Bangalore Tokyo and NY Description The department division deals in reporting profit and loss to back to the central system  It also involves doing the daily adjustments and sending the feed to different streams which will do further processing at their end  The data involved capital markets data like Securities Bonds Funds Repos etc  Role and Responsibilities Handling all user support requests from across the regions APAC UK  USA  Automating frequent process with shell scripting and Autosys scheduler  Handling major UATs independently  representing client from offshore office  Responsible for all Production Release across regions  Generate reports required by Japanese and US Federal Gov  Developed and modified Packages Functions Synonyms tables and indexes to implement business requirements and rules  Bulk loading of data done using utilities like BCP and Sybase Central  Code tuning or Query Optimization done using Explain Plans Hints  Analyzing and monitoring system performances using DBCC Trace on commands query plan outputs system  Handling various enhancements to the system this entailed writing new codes and changing some existing codes  Wrote new complex stored procedures in Sybase and optimized the existing code written in Sybase  Debugging the scripts and jobs in Production environment written in Shell and PERL  Handling and creating various Autosys jobs in Production environment  Providing daily status reports to the clients  Provided production support and 247 support for resolving the critical production issues  Involved in the solving the tickets that are raised by the end users  Responsible for creating PLSQL Programs and UNIX Scripts for Data Validation and Data Conversion  Project Title Basel 2 Risk Platform UK based bank           Data Warehouse Developer       012006      112008     Royal Bank Of Scotland        City     STATE            Location Bangalore Description FMIT is a dedicated IT division within RBS and we provide a fully managed service for Finance IT Business As Usual BAU team  Designed the tables indexes triggers stored procedures functions and packages to implement the requirement  Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  Informatica Mappings are used in loading the target databases which is usually on different schema and the data is transformed in the process before loading to target  Analysis of Source data Oracle Source Objects and identifying the methods for loading data to target  Created and used External Tables for migrating flat file data into target  Responsible for the creation of Packages Functions and Procedures  Performed Testing of the Packages Procedures and Functions  Wrote scripts for creating tables Indexes Grants and Synonyms in different schemas and made modifications for the existing tables as per business logic  Preparation of test scripts for System and Integration Testing and Traceability Matrix for the assurance of complete coverage of system requirements  Develop bug fixes and enhancements  Production support of some of the applications of the bank which includes resolving issues of the overnight batch processes and daily users queries          Education       Bachelors       Telecommunication       Expected in   2005                VTU                GPA        Status         Telecommunication        Professional Affiliations              Skills     Bonds capital markets CSS client clients Data Conversion Data Validation Databases Debugging ETL Finance forms ftp Funds Grants HTML HTML5 PHP indexing Informatica investments Japanese Javascript Java Scripting logic meetings office Windows NT works MSDOS ODBC Operating Systems Optimization Oracle db Developer PLSQL PLSQL PERL processes profit and loss reporting requirement Retail Scheduling Securities Shell Scripts Shell scripts shell scripting SQL Strategic Sybase Tables user support Toad UNIX Unix scripts utilities validation web applications windows server Workflow written', 'resume_html': 'none', 'skills': \"['data analysis', 'statistical analysis', 'R', 'SAS', 'Microsoft Office', 'Word', 'Excel', 'Power Point', 'spreadsheets', 'data platform', 'data analysis', 'MS Excel', 'data validation', 'spreadsheets', 'accurate', 'data analysis', 'pivot tables', 'MS Excel', 'analysis', 'Research', 'analysis', 'Statistical analysis', 'R studio', 'Statistics', 'SAS', 'SAS', 'MS Word', 'PowerPoint', 'Excel', 'Access', 'R Studio', 'SAS', 'SQL']\"}]}\n",
      "{'total': 15, 'batches': 1, 'time': 0.03499007225036621}\n",
      "{'rows': [{'id': '5533667039977703598659155174781970519', 'skills': 'bi'}, {'id': '5533667039977703598659155174781970519', 'skills': 'design'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business intelligence'}, {'id': '5533667039977703598659155174781970519', 'skills': 'tableau'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data visualization'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboards'}, {'id': '5533667039977703598659155174781970519', 'skills': 'database design'}, {'id': '5533667039977703598659155174781970519', 'skills': 'designing'}, {'id': '5533667039977703598659155174781970519', 'skills': 'etl'}, {'id': '5533667039977703598659155174781970519', 'skills': 'transform'}, {'id': '5533667039977703598659155174781970519', 'skills': 'load'}, {'id': '5533667039977703598659155174781970519', 'skills': 'extract'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data warehouse'}, {'id': '5533667039977703598659155174781970519', 'skills': 'trends'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business requirements'}, {'id': '5533667039977703598659155174781970519', 'skills': 'evaluating trends'}, {'id': '5533667039977703598659155174781970519', 'skills': 'realtime data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms excel'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ssrs'}, {'id': '5533667039977703598659155174781970519', 'skills': 'snowflake'}, {'id': '5533667039977703598659155174781970519', 'skills': 'cassandra'}, {'id': '5533667039977703598659155174781970519', 'skills': 'oracle'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms sql server'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms access'}, {'id': '5533667039977703598659155174781970519', 'skills': 'postgres'}, {'id': '5533667039977703598659155174781970519', 'skills': 'amazon s3'}, {'id': '5533667039977703598659155174781970519', 'skills': 'sql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'python'}, {'id': '5533667039977703598659155174781970519', 'skills': 'tsql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'html'}, {'id': '5533667039977703598659155174781970519', 'skills': 'css'}, {'id': '5533667039977703598659155174781970519', 'skills': 'java'}, {'id': '5533667039977703598659155174781970519', 'skills': 'salesforce'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms word'}, {'id': '5533667039977703598659155174781970519', 'skills': 'outlook'}, {'id': '5533667039977703598659155174781970519', 'skills': 'frontpage'}, {'id': '5533667039977703598659155174781970519', 'skills': 'powerpoint'}, {'id': '5533667039977703598659155174781970519', 'skills': 'reporting'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data quality'}, {'id': '5533667039977703598659155174781970519', 'skills': 'memsql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'warehouses'}, {'id': '5533667039977703598659155174781970519', 'skills': 'pipelines'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data governance'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data mapping'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business requirement'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data validation'}, {'id': '5533667039977703598659155174781970519', 'skills': 'aws'}, {'id': '5533667039977703598659155174781970519', 'skills': 'excel'}, {'id': '5533667039977703598659155174781970519', 'skills': 'visualizations'}, {'id': '5533667039977703598659155174781970519', 'skills': 'adhoc analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'customized analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'developing'}, {'id': '5533667039977703598659155174781970519', 'skills': 'redshift'}, {'id': '5533667039977703598659155174781970519', 'skills': 'optimization'}, {'id': '5533667039977703598659155174781970519', 'skills': 'prototyping'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboard'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboarding'}, {'id': '5533667039977703598659155174781970519', 'skills': 'waterfall'}, {'id': '5533667039977703598659155174781970519', 'skills': 'agile'}, {'id': '5533667039977703598659155174781970519', 'skills': 'statistical techniques'}, {'id': '5533667039977703598659155174781970519', 'skills': 'validate data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'develop'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data collection'}, {'id': '5533667039977703598659155174781970519', 'skills': 'sas'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data integration'}, {'id': '5533667039977703598659155174781970519', 'skills': 'extracting'}, {'id': '5533667039977703598659155174781970519', 'skills': 'transforming'}, {'id': '5533667039977703598659155174781970519', 'skills': 'loading data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data sets'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'business analysis'}, {'id': '274097087237322696861922203277862748921', 'skills': 'quality assurance'}, {'id': '274097087237322696861922203277862748921', 'skills': 'functional testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'critical thinking'}, {'id': '274097087237322696861922203277862748921', 'skills': 'project management'}, {'id': '274097087237322696861922203277862748921', 'skills': 'cryptography'}, {'id': '274097087237322696861922203277862748921', 'skills': 'threat analysis'}, {'id': '274097087237322696861922203277862748921', 'skills': 'cloud'}, {'id': '274097087237322696861922203277862748921', 'skills': 'microsoft office'}, {'id': '274097087237322696861922203277862748921', 'skills': 'reporting'}, {'id': '274097087237322696861922203277862748921', 'skills': 'java'}, {'id': '274097087237322696861922203277862748921', 'skills': 'programming'}, {'id': '274097087237322696861922203277862748921', 'skills': 'windows'}, {'id': '274097087237322696861922203277862748921', 'skills': 'data integrity'}, {'id': '274097087237322696861922203277862748921', 'skills': 'optimization'}, {'id': '274097087237322696861922203277862748921', 'skills': 'testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'technical documentation'}, {'id': '274097087237322696861922203277862748921', 'skills': 'networking'}, {'id': '274097087237322696861922203277862748921', 'skills': 'data warehouse'}, {'id': '274097087237322696861922203277862748921', 'skills': 'linux'}, {'id': '274097087237322696861922203277862748921', 'skills': 'access'}, {'id': '274097087237322696861922203277862748921', 'skills': 'unix'}, {'id': '270987204323396713165325793790364446993', 'skills': 'business requirements'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data accuracy'}, {'id': '270987204323396713165325793790364446993', 'skills': 'project management'}, {'id': '270987204323396713165325793790364446993', 'skills': 'excel'}, {'id': '270987204323396713165325793790364446993', 'skills': 'sql'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data mining'}, {'id': '270987204323396713165325793790364446993', 'skills': 'ms office'}, {'id': '270987204323396713165325793790364446993', 'skills': 'analyzing trends'}, {'id': '270987204323396713165325793790364446993', 'skills': 'vba'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data analysis'}, {'id': '270987204323396713165325793790364446993', 'skills': 'optimization'}, {'id': '270987204323396713165325793790364446993', 'skills': 'quality assurance'}, {'id': '270987204323396713165325793790364446993', 'skills': 'programming'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data integrity'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data quality'}, {'id': '270987204323396713165325793790364446993', 'skills': 'process redesign'}, {'id': '270987204323396713165325793790364446993', 'skills': 'maintaining'}, {'id': '270987204323396713165325793790364446993', 'skills': 'accurate data'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data cleaning'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data preparation'}, {'id': '270987204323396713165325793790364446993', 'skills': 'processing'}, {'id': '270987204323396713165325793790364446993', 'skills': 'statistical techniques'}, {'id': '270987204323396713165325793790364446993', 'skills': 'research data'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data sources'}, {'id': '270987204323396713165325793790364446993', 'skills': 'reliability'}, {'id': '270987204323396713165325793790364446993', 'skills': 'spreadsheets'}, {'id': '270987204323396713165325793790364446993', 'skills': 'access'}, {'id': '270987204323396713165325793790364446993', 'skills': 'hipaa'}, {'id': '270987204323396713165325793790364446993', 'skills': 'problem solving'}, {'id': '270987204323396713165325793790364446993', 'skills': 'implement'}, {'id': '270987204323396713165325793790364446993', 'skills': 'research'}, {'id': '270987204323396713165325793790364446993', 'skills': 'consistent'}, {'id': '76199997604772823340207052419224154975', 'skills': 'excel'}, {'id': '76199997604772823340207052419224154975', 'skills': 'pivot tables'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sas'}, {'id': '76199997604772823340207052419224154975', 'skills': 'data storage'}, {'id': '76199997604772823340207052419224154975', 'skills': 'office'}, {'id': '76199997604772823340207052419224154975', 'skills': 'data analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'dashboard'}, {'id': '76199997604772823340207052419224154975', 'skills': 'problem solving'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms project'}, {'id': '76199997604772823340207052419224154975', 'skills': 'explanation'}, {'id': '76199997604772823340207052419224154975', 'skills': 'waterfall'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms access'}, {'id': '76199997604772823340207052419224154975', 'skills': 'regularization'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms excel'}, {'id': '76199997604772823340207052419224154975', 'skills': 'business objects'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sql'}, {'id': '76199997604772823340207052419224154975', 'skills': 'query builder'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms visio'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sharepoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'onenote'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms sharepoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms onenote'}, {'id': '76199997604772823340207052419224154975', 'skills': 'apple sheets'}, {'id': '76199997604772823340207052419224154975', 'skills': 'apple pages'}, {'id': '76199997604772823340207052419224154975', 'skills': 'open office'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms powerpoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms word'}, {'id': '76199997604772823340207052419224154975', 'skills': 'programming'}, {'id': '76199997604772823340207052419224154975', 'skills': 'java'}, {'id': '76199997604772823340207052419224154975', 'skills': 'vbnet'}, {'id': '76199997604772823340207052419224154975', 'skills': 'windows'}, {'id': '76199997604772823340207052419224154975', 'skills': 'android'}, {'id': '76199997604772823340207052419224154975', 'skills': 'access'}, {'id': '76199997604772823340207052419224154975', 'skills': 'reporting'}, {'id': '76199997604772823340207052419224154975', 'skills': 'performance analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'visio'}, {'id': '305667889169402374245195369321522535673', 'skills': 'data analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'reporting'}, {'id': '305667889169402374245195369321522535673', 'skills': 'user acceptance testing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft access'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft excel'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sas'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sql server'}, {'id': '305667889169402374245195369321522535673', 'skills': 'cognos'}, {'id': '305667889169402374245195369321522535673', 'skills': 'crystal'}, {'id': '305667889169402374245195369321522535673', 'skills': 'business objects'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sql'}, {'id': '305667889169402374245195369321522535673', 'skills': 'tableau'}, {'id': '305667889169402374245195369321522535673', 'skills': 'project management'}, {'id': '305667889169402374245195369321522535673', 'skills': 'data quality'}, {'id': '305667889169402374245195369321522535673', 'skills': 'monitor'}, {'id': '305667889169402374245195369321522535673', 'skills': 'hipaa'}, {'id': '305667889169402374245195369321522535673', 'skills': 'process analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'ms office'}, {'id': '305667889169402374245195369321522535673', 'skills': 'excel'}, {'id': '305667889169402374245195369321522535673', 'skills': 'access'}, {'id': '305667889169402374245195369321522535673', 'skills': 'claims analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'project planning'}, {'id': '305667889169402374245195369321522535673', 'skills': 'testing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'business requirements'}, {'id': '305667889169402374245195369321522535673', 'skills': 'analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'reconciliation'}, {'id': '305667889169402374245195369321522535673', 'skills': 'processing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft office'}, {'id': '83985006356839563643115453489150250520', 'skills': 'designing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'monitoring'}, {'id': '83985006356839563643115453489150250520', 'skills': 'analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'reporting'}, {'id': '83985006356839563643115453489150250520', 'skills': 'risk assessment'}, {'id': '83985006356839563643115453489150250520', 'skills': 'statistical analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'problem solving'}, {'id': '83985006356839563643115453489150250520', 'skills': 'modeling'}, {'id': '83985006356839563643115453489150250520', 'skills': 'project management'}, {'id': '83985006356839563643115453489150250520', 'skills': 'rapid application development'}, {'id': '83985006356839563643115453489150250520', 'skills': 'adhoc testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'backend testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'critical thinking'}, {'id': '83985006356839563643115453489150250520', 'skills': 'decision making'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft access'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft excel'}, {'id': '83985006356839563643115453489150250520', 'skills': 'ms office'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'powerpoint'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sharepoint'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sql'}, {'id': '83985006356839563643115453489150250520', 'skills': 'tableau'}, {'id': '83985006356839563643115453489150250520', 'skills': 'spotfire'}, {'id': '83985006356839563643115453489150250520', 'skills': 'visio'}, {'id': '83985006356839563643115453489150250520', 'skills': 'crm'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sql server'}, {'id': '83985006356839563643115453489150250520', 'skills': 'aws'}, {'id': '83985006356839563643115453489150250520', 'skills': 'agile'}, {'id': '83985006356839563643115453489150250520', 'skills': 'data analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'business requirements'}, {'id': '83985006356839563643115453489150250520', 'skills': 'design'}, {'id': '83985006356839563643115453489150250520', 'skills': 'developing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'drupal'}, {'id': '83985006356839563643115453489150250520', 'skills': 'data warehouses'}, {'id': '83985006356839563643115453489150250520', 'skills': 'research'}, {'id': '83985006356839563643115453489150250520', 'skills': 'cloud'}, {'id': '83985006356839563643115453489150250520', 'skills': 'ms project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'cisco'}, {'id': '83985006356839563643115453489150250520', 'skills': 'technical documentation'}, {'id': '83985006356839563643115453489150250520', 'skills': 'big data'}, {'id': '83985006356839563643115453489150250520', 'skills': 'hdfs'}, {'id': '83985006356839563643115453489150250520', 'skills': 'access'}, {'id': '83985006356839563643115453489150250520', 'skills': 'project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'business analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'visual studio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tableau'}, {'id': '278624286922027916746373616168944850452', 'skills': 'developing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboards'}, {'id': '278624286922027916746373616168944850452', 'skills': 'design'}, {'id': '278624286922027916746373616168944850452', 'skills': 'monitoring'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data profiling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'bi'}, {'id': '278624286922027916746373616168944850452', 'skills': 'teams'}, {'id': '278624286922027916746373616168944850452', 'skills': 'windows'}, {'id': '278624286922027916746373616168944850452', 'skills': 'active directory'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'plsql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'xml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'python'}, {'id': '278624286922027916746373616168944850452', 'skills': 'programming'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sharepoint'}, {'id': '278624286922027916746373616168944850452', 'skills': 'performance tuning'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data blending'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboard'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data analytics'}, {'id': '278624286922027916746373616168944850452', 'skills': 'waterfall'}, {'id': '278624286922027916746373616168944850452', 'skills': 'decision making'}, {'id': '278624286922027916746373616168944850452', 'skills': 'etl'}, {'id': '278624286922027916746373616168944850452', 'skills': 'agile'}, {'id': '278624286922027916746373616168944850452', 'skills': 'software development life cycle'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sdlc'}, {'id': '278624286922027916746373616168944850452', 'skills': 'testing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data validation'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data cleansing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data verification'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mismatch'}, {'id': '278624286922027916746373616168944850452', 'skills': 'db2'}, {'id': '278624286922027916746373616168944850452', 'skills': 'oracle'}, {'id': '278624286922027916746373616168944850452', 'skills': 'teradata'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql server'}, {'id': '278624286922027916746373616168944850452', 'skills': 'erwin'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dimensional modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'snowflake'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data warehouse'}, {'id': '278624286922027916746373616168944850452', 'skills': 'modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'normalization'}, {'id': '278624286922027916746373616168944850452', 'skills': 'process analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'microsoft office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'word'}, {'id': '278624286922027916746373616168944850452', 'skills': 'powerpoint'}, {'id': '278624286922027916746373616168944850452', 'skills': 'excel'}, {'id': '278624286922027916746373616168944850452', 'skills': 'microsoft project'}, {'id': '278624286922027916746373616168944850452', 'skills': 'c'}, {'id': '278624286922027916746373616168944850452', 'skills': 'html'}, {'id': '278624286922027916746373616168944850452', 'skills': 'er studio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms visio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'datasets'}, {'id': '278624286922027916746373616168944850452', 'skills': 'time series analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'matlab'}, {'id': '278624286922027916746373616168944850452', 'skills': 'collecting data'}, {'id': '278624286922027916746373616168944850452', 'skills': 'text mining'}, {'id': '278624286922027916746373616168944850452', 'skills': 'vmware'}, {'id': '278624286922027916746373616168944850452', 'skills': 'linux'}, {'id': '278624286922027916746373616168944850452', 'skills': 'cisco'}, {'id': '278624286922027916746373616168944850452', 'skills': 'exchange'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms access'}, {'id': '278624286922027916746373616168944850452', 'skills': 'javascript'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ajax'}, {'id': '278624286922027916746373616168944850452', 'skills': 'jquery'}, {'id': '278624286922027916746373616168944850452', 'skills': 'bootstrap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'angularjs'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ssis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ssrs'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tsql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'aspnet'}, {'id': '278624286922027916746373616168944850452', 'skills': 'cognos'}, {'id': '278624286922027916746373616168944850452', 'skills': 'uml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'java'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visual basic'}, {'id': '278624286922027916746373616168944850452', 'skills': 'vb'}, {'id': '278624286922027916746373616168944850452', 'skills': 'windows xp'}, {'id': '278624286922027916746373616168944850452', 'skills': 'net'}, {'id': '278624286922027916746373616168944850452', 'skills': 'alteryx'}, {'id': '278624286922027916746373616168944850452', 'skills': 'develop'}, {'id': '278624286922027916746373616168944850452', 'skills': 'project'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visualizations'}, {'id': '278624286922027916746373616168944850452', 'skills': 'poc'}, {'id': '278624286922027916746373616168944850452', 'skills': 'reporting'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business intelligence'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visualization'}, {'id': '278624286922027916746373616168944850452', 'skills': 'olap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'processing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mining'}, {'id': '278624286922027916746373616168944850452', 'skills': 'unix'}, {'id': '278624286922027916746373616168944850452', 'skills': 'access'}, {'id': '278624286922027916746373616168944850452', 'skills': 'transform'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data sources'}, {'id': '278624286922027916746373616168944850452', 'skills': 'big data'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business requirements'}, {'id': '278624286922027916746373616168944850452', 'skills': 'css'}, {'id': '278624286922027916746373616168944850452', 'skills': 'html5'}, {'id': '278624286922027916746373616168944850452', 'skills': 'crystal'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tomcat'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboard development'}, {'id': '278624286922027916746373616168944850452', 'skills': 'statistics'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sass'}, {'id': '278624286922027916746373616168944850452', 'skills': 'database design'}, {'id': '278624286922027916746373616168944850452', 'skills': 'analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data quality'}, {'id': '278624286922027916746373616168944850452', 'skills': 'load'}, {'id': '278624286922027916746373616168944850452', 'skills': 'loading'}, {'id': '278624286922027916746373616168944850452', 'skills': 'r'}, {'id': '278624286922027916746373616168944850452', 'skills': 'rest'}, {'id': '278624286922027916746373616168944850452', 'skills': 'transformation'}, {'id': '278624286922027916746373616168944850452', 'skills': 'code reviews'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data processing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'designing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business objects'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql server reporting services'}, {'id': '278624286922027916746373616168944850452', 'skills': 'office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms sql server'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business requirement'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mapping'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data warehousing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms excel'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visio'}, {'id': '6361630242691360309919431676680490692', 'skills': 'oracle'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms access'}, {'id': '6361630242691360309919431676680490692', 'skills': 'redshift'}, {'id': '6361630242691360309919431676680490692', 'skills': 's3'}, {'id': '6361630242691360309919431676680490692', 'skills': 'software products'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hadoop'}, {'id': '6361630242691360309919431676680490692', 'skills': 'amazon web services'}, {'id': '6361630242691360309919431676680490692', 'skills': 'programming'}, {'id': '6361630242691360309919431676680490692', 'skills': 'plsql'}, {'id': '6361630242691360309919431676680490692', 'skills': 'sql'}, {'id': '6361630242691360309919431676680490692', 'skills': 'impala'}, {'id': '6361630242691360309919431676680490692', 'skills': 'python'}, {'id': '6361630242691360309919431676680490692', 'skills': 'c'}, {'id': '6361630242691360309919431676680490692', 'skills': 'visual basic'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms excel'}, {'id': '6361630242691360309919431676680490692', 'skills': 'vba'}, {'id': '6361630242691360309919431676680490692', 'skills': 'power query'}, {'id': '6361630242691360309919431676680490692', 'skills': 'plsql developer'}, {'id': '6361630242691360309919431676680490692', 'skills': 'kibana'}, {'id': '6361630242691360309919431676680490692', 'skills': 'excel'}, {'id': '6361630242691360309919431676680490692', 'skills': 'eclipse'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms office'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms project'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms visio'}, {'id': '6361630242691360309919431676680490692', 'skills': 'putty'}, {'id': '6361630242691360309919431676680490692', 'skills': 'tortoise svn'}, {'id': '6361630242691360309919431676680490692', 'skills': 'github'}, {'id': '6361630242691360309919431676680490692', 'skills': 'jira'}, {'id': '6361630242691360309919431676680490692', 'skills': 'unix'}, {'id': '6361630242691360309919431676680490692', 'skills': 'aws'}, {'id': '6361630242691360309919431676680490692', 'skills': 'business analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'system analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'design'}, {'id': '6361630242691360309919431676680490692', 'skills': 'project management'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hdfs'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hive'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data sources'}, {'id': '6361630242691360309919431676680490692', 'skills': 'xml'}, {'id': '6361630242691360309919431676680490692', 'skills': 'sdlc'}, {'id': '6361630242691360309919431676680490692', 'skills': 'analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'regression'}, {'id': '6361630242691360309919431676680490692', 'skills': 'stored procedures'}, {'id': '6361630242691360309919431676680490692', 'skills': 'query optimization'}, {'id': '6361630242691360309919431676680490692', 'skills': 'agile'}, {'id': '6361630242691360309919431676680490692', 'skills': 'waterfall'}, {'id': '6361630242691360309919431676680490692', 'skills': 'scrum'}, {'id': '6361630242691360309919431676680490692', 'skills': 'estimation'}, {'id': '6361630242691360309919431676680490692', 'skills': 'business requirements'}, {'id': '6361630242691360309919431676680490692', 'skills': 'develop'}, {'id': '6361630242691360309919431676680490692', 'skills': 'loading'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data science'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data sets'}, {'id': '6361630242691360309919431676680490692', 'skills': 'reporting'}, {'id': '6361630242691360309919431676680490692', 'skills': 'requirement analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'technical design'}, {'id': '6361630242691360309919431676680490692', 'skills': 'optimization'}, {'id': '6361630242691360309919431676680490692', 'skills': 'monitoring'}, {'id': '6361630242691360309919431676680490692', 'skills': 'system testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'integration testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'automation'}, {'id': '6361630242691360309919431676680490692', 'skills': 'big data'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data modeling'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ftp'}, {'id': '6361630242691360309919431676680490692', 'skills': 'java'}, {'id': '6361630242691360309919431676680490692', 'skills': 'research'}, {'id': '6361630242691360309919431676680490692', 'skills': 'visio'}, {'id': '147203991935502946470515237354155400269', 'skills': 'analysis'}, {'id': '147203991935502946470515237354155400269', 'skills': 'reporting'}, {'id': '147203991935502946470515237354155400269', 'skills': 'ssrs'}, {'id': '147203991935502946470515237354155400269', 'skills': 'crystal'}, {'id': '147203991935502946470515237354155400269', 'skills': 'crm'}, {'id': '147203991935502946470515237354155400269', 'skills': 'technical documentation'}, {'id': '147203991935502946470515237354155400269', 'skills': 'project management'}, {'id': '147203991935502946470515237354155400269', 'skills': 'data quality'}, {'id': '147203991935502946470515237354155400269', 'skills': 'excel'}, {'id': '147203991935502946470515237354155400269', 'skills': 'visual basic'}, {'id': '147203991935502946470515237354155400269', 'skills': 'project designs'}, {'id': '147203991935502946470515237354155400269', 'skills': 'pivot table'}, {'id': '147203991935502946470515237354155400269', 'skills': 'vlookup'}, {'id': '147203991935502946470515237354155400269', 'skills': 'scrum'}, {'id': '147203991935502946470515237354155400269', 'skills': 'teams'}, {'id': '147203991935502946470515237354155400269', 'skills': 'test cases'}, {'id': '147203991935502946470515237354155400269', 'skills': 'sharepoint'}, {'id': '147203991935502946470515237354155400269', 'skills': 'assembly'}, {'id': '147203991935502946470515237354155400269', 'skills': 'technical assistance'}, {'id': '147203991935502946470515237354155400269', 'skills': 'gateway'}, {'id': '147203991935502946470515237354155400269', 'skills': 'analyzing data'}, {'id': '147203991935502946470515237354155400269', 'skills': 'business intelligence'}, {'id': '147203991935502946470515237354155400269', 'skills': 'business decisions'}, {'id': '147203991935502946470515237354155400269', 'skills': 'microsoft excel'}, {'id': '147203991935502946470515237354155400269', 'skills': 'data integrity'}, {'id': '147203991935502946470515237354155400269', 'skills': 'storage'}, {'id': '147203991935502946470515237354155400269', 'skills': 'design'}, {'id': '147203991935502946470515237354155400269', 'skills': 'uml'}, {'id': '147203991935502946470515237354155400269', 'skills': 'erd'}, {'id': '97098914450695563702884660921271152925', 'skills': 'stata'}, {'id': '97098914450695563702884660921271152925', 'skills': 'programming'}, {'id': '97098914450695563702884660921271152925', 'skills': 'r'}, {'id': '97098914450695563702884660921271152925', 'skills': 'sas'}, {'id': '97098914450695563702884660921271152925', 'skills': 'gis'}, {'id': '97098914450695563702884660921271152925', 'skills': 'nvivo'}, {'id': '97098914450695563702884660921271152925', 'skills': 'microsoft office'}, {'id': '97098914450695563702884660921271152925', 'skills': 'analytical research'}, {'id': '97098914450695563702884660921271152925', 'skills': 'project'}, {'id': '97098914450695563702884660921271152925', 'skills': 'evaluation'}, {'id': '97098914450695563702884660921271152925', 'skills': 'statistical models'}, {'id': '97098914450695563702884660921271152925', 'skills': 'research'}, {'id': '97098914450695563702884660921271152925', 'skills': 'data collection'}, {'id': '97098914450695563702884660921271152925', 'skills': 'data quality'}, {'id': '97098914450695563702884660921271152925', 'skills': 'statistical methods'}, {'id': '97098914450695563702884660921271152925', 'skills': 'analysis'}, {'id': '97098914450695563702884660921271152925', 'skills': 'designing'}, {'id': '138536846257210096623495337605137733129', 'skills': 'business processes'}, {'id': '138536846257210096623495337605137733129', 'skills': 'data integrity'}, {'id': '138536846257210096623495337605137733129', 'skills': 'sap'}, {'id': '138536846257210096623495337605137733129', 'skills': 'processing'}, {'id': '138536846257210096623495337605137733129', 'skills': 'budget analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'resource analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'crystal'}, {'id': '138536846257210096623495337605137733129', 'skills': 'analyzing data'}, {'id': '138536846257210096623495337605137733129', 'skills': 'analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'microsoft office'}, {'id': '138536846257210096623495337605137733129', 'skills': 'excel'}, {'id': '138536846257210096623495337605137733129', 'skills': 'word'}, {'id': '138536846257210096623495337605137733129', 'skills': 'powerpoint'}, {'id': '138536846257210096623495337605137733129', 'skills': 'outlook'}, {'id': '138536846257210096623495337605137733129', 'skills': 'visio'}, {'id': '138536846257210096623495337605137733129', 'skills': 'access'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data privacy'}, {'id': '321556423433782918032074297826301460362', 'skills': 'research'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data analysis'}, {'id': '321556423433782918032074297826301460362', 'skills': 'cloud'}, {'id': '321556423433782918032074297826301460362', 'skills': 'networking'}, {'id': '321556423433782918032074297826301460362', 'skills': 'user training'}, {'id': '321556423433782918032074297826301460362', 'skills': 'monitoring'}, {'id': '321556423433782918032074297826301460362', 'skills': 'statistical analysis'}, {'id': '321556423433782918032074297826301460362', 'skills': 'reporting'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data collection'}, {'id': '321556423433782918032074297826301460362', 'skills': 'project'}, {'id': '321556423433782918032074297826301460362', 'skills': 'algorithms'}, {'id': '321556423433782918032074297826301460362', 'skills': 'workflow diagrams'}, {'id': '321556423433782918032074297826301460362', 'skills': 'hipaa'}, {'id': '321556423433782918032074297826301460362', 'skills': 'windows xp'}, {'id': '321556423433782918032074297826301460362', 'skills': 'office'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data quality'}, {'id': '283692866702135157759709078394118382144', 'skills': 'oracle forms'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google drive'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google docs'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google sheets'}, {'id': '283692866702135157759709078394118382144', 'skills': 'microsoft office'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data analysis'}, {'id': '283692866702135157759709078394118382144', 'skills': 'developing'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data patterns'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data linking'}, {'id': '283692866702135157759709078394118382144', 'skills': 'test scripts'}, {'id': '283692866702135157759709078394118382144', 'skills': 'interviews'}, {'id': '283692866702135157759709078394118382144', 'skills': 'swot analysis'}, {'id': '283692866702135157759709078394118382144', 'skills': 'oracle'}, {'id': '283692866702135157759709078394118382144', 'skills': 'research'}, {'id': '189067289296683940487677712533012870883', 'skills': 'functional requirements'}, {'id': '189067289296683940487677712533012870883', 'skills': 'monitoring'}, {'id': '189067289296683940487677712533012870883', 'skills': 'excel'}, {'id': '189067289296683940487677712533012870883', 'skills': 'access'}, {'id': '189067289296683940487677712533012870883', 'skills': 'developing'}, {'id': '189067289296683940487677712533012870883', 'skills': 'data warehouse'}, {'id': '189067289296683940487677712533012870883', 'skills': 'monitor'}, {'id': '189067289296683940487677712533012870883', 'skills': 'sharepoint'}, {'id': '189067289296683940487677712533012870883', 'skills': 'data sharing'}, {'id': '189067289296683940487677712533012870883', 'skills': 'implement'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft word'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft powerpoint'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft excel'}, {'id': '189067289296683940487677712533012870883', 'skills': 'analyzing data'}, {'id': '189067289296683940487677712533012870883', 'skills': 'adobe photoshop'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft office'}, {'id': '189067289296683940487677712533012870883', 'skills': 'outlook'}, {'id': '189067289296683940487677712533012870883', 'skills': 'power point'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft publisher'}, {'id': '189067289296683940487677712533012870883', 'skills': 'word'}, {'id': '189067289296683940487677712533012870883', 'skills': 'project management'}, {'id': '189067289296683940487677712533012870883', 'skills': 'visio'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'statistical analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'r'}, {'id': '14259385793885776770164514431890372233', 'skills': 'sas'}, {'id': '14259385793885776770164514431890372233', 'skills': 'microsoft office'}, {'id': '14259385793885776770164514431890372233', 'skills': 'word'}, {'id': '14259385793885776770164514431890372233', 'skills': 'excel'}, {'id': '14259385793885776770164514431890372233', 'skills': 'power point'}, {'id': '14259385793885776770164514431890372233', 'skills': 'spreadsheets'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data platform'}, {'id': '14259385793885776770164514431890372233', 'skills': 'ms excel'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data validation'}, {'id': '14259385793885776770164514431890372233', 'skills': 'accurate'}, {'id': '14259385793885776770164514431890372233', 'skills': 'pivot tables'}, {'id': '14259385793885776770164514431890372233', 'skills': 'analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'research'}, {'id': '14259385793885776770164514431890372233', 'skills': 'r studio'}, {'id': '14259385793885776770164514431890372233', 'skills': 'statistics'}, {'id': '14259385793885776770164514431890372233', 'skills': 'ms word'}, {'id': '14259385793885776770164514431890372233', 'skills': 'powerpoint'}, {'id': '14259385793885776770164514431890372233', 'skills': 'access'}, {'id': '14259385793885776770164514431890372233', 'skills': 'sql'}]}\n",
      "{'total': 578, 'batches': 1, 'time': 0.5596323013305664}\n",
      "{'rows': [{'id': '5533667039977703598659155174781970519', 'skills': 'bi'}, {'id': '5533667039977703598659155174781970519', 'skills': 'design'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business intelligence'}, {'id': '5533667039977703598659155174781970519', 'skills': 'tableau'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data visualization'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboards'}, {'id': '5533667039977703598659155174781970519', 'skills': 'database design'}, {'id': '5533667039977703598659155174781970519', 'skills': 'designing'}, {'id': '5533667039977703598659155174781970519', 'skills': 'etl'}, {'id': '5533667039977703598659155174781970519', 'skills': 'transform'}, {'id': '5533667039977703598659155174781970519', 'skills': 'load'}, {'id': '5533667039977703598659155174781970519', 'skills': 'extract'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data warehouse'}, {'id': '5533667039977703598659155174781970519', 'skills': 'trends'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business requirements'}, {'id': '5533667039977703598659155174781970519', 'skills': 'evaluating trends'}, {'id': '5533667039977703598659155174781970519', 'skills': 'realtime data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms excel'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ssrs'}, {'id': '5533667039977703598659155174781970519', 'skills': 'snowflake'}, {'id': '5533667039977703598659155174781970519', 'skills': 'cassandra'}, {'id': '5533667039977703598659155174781970519', 'skills': 'oracle'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms sql server'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms access'}, {'id': '5533667039977703598659155174781970519', 'skills': 'postgres'}, {'id': '5533667039977703598659155174781970519', 'skills': 'amazon s3'}, {'id': '5533667039977703598659155174781970519', 'skills': 'sql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'python'}, {'id': '5533667039977703598659155174781970519', 'skills': 'tsql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'html'}, {'id': '5533667039977703598659155174781970519', 'skills': 'css'}, {'id': '5533667039977703598659155174781970519', 'skills': 'java'}, {'id': '5533667039977703598659155174781970519', 'skills': 'salesforce'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms word'}, {'id': '5533667039977703598659155174781970519', 'skills': 'outlook'}, {'id': '5533667039977703598659155174781970519', 'skills': 'frontpage'}, {'id': '5533667039977703598659155174781970519', 'skills': 'powerpoint'}, {'id': '5533667039977703598659155174781970519', 'skills': 'reporting'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data quality'}, {'id': '5533667039977703598659155174781970519', 'skills': 'memsql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'warehouses'}, {'id': '5533667039977703598659155174781970519', 'skills': 'pipelines'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data governance'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data mapping'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business requirement'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data validation'}, {'id': '5533667039977703598659155174781970519', 'skills': 'aws'}, {'id': '5533667039977703598659155174781970519', 'skills': 'excel'}, {'id': '5533667039977703598659155174781970519', 'skills': 'visualizations'}, {'id': '5533667039977703598659155174781970519', 'skills': 'adhoc analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'customized analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'developing'}, {'id': '5533667039977703598659155174781970519', 'skills': 'redshift'}, {'id': '5533667039977703598659155174781970519', 'skills': 'optimization'}, {'id': '5533667039977703598659155174781970519', 'skills': 'prototyping'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboard'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboarding'}, {'id': '5533667039977703598659155174781970519', 'skills': 'waterfall'}, {'id': '5533667039977703598659155174781970519', 'skills': 'agile'}, {'id': '5533667039977703598659155174781970519', 'skills': 'statistical techniques'}, {'id': '5533667039977703598659155174781970519', 'skills': 'validate data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'develop'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data collection'}, {'id': '5533667039977703598659155174781970519', 'skills': 'sas'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data integration'}, {'id': '5533667039977703598659155174781970519', 'skills': 'extracting'}, {'id': '5533667039977703598659155174781970519', 'skills': 'transforming'}, {'id': '5533667039977703598659155174781970519', 'skills': 'loading data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data sets'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'business analysis'}, {'id': '274097087237322696861922203277862748921', 'skills': 'quality assurance'}, {'id': '274097087237322696861922203277862748921', 'skills': 'functional testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'critical thinking'}, {'id': '274097087237322696861922203277862748921', 'skills': 'project management'}, {'id': '274097087237322696861922203277862748921', 'skills': 'cryptography'}, {'id': '274097087237322696861922203277862748921', 'skills': 'threat analysis'}, {'id': '274097087237322696861922203277862748921', 'skills': 'cloud'}, {'id': '274097087237322696861922203277862748921', 'skills': 'microsoft office'}, {'id': '274097087237322696861922203277862748921', 'skills': 'reporting'}, {'id': '274097087237322696861922203277862748921', 'skills': 'java'}, {'id': '274097087237322696861922203277862748921', 'skills': 'programming'}, {'id': '274097087237322696861922203277862748921', 'skills': 'windows'}, {'id': '274097087237322696861922203277862748921', 'skills': 'data integrity'}, {'id': '274097087237322696861922203277862748921', 'skills': 'optimization'}, {'id': '274097087237322696861922203277862748921', 'skills': 'testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'technical documentation'}, {'id': '274097087237322696861922203277862748921', 'skills': 'networking'}, {'id': '274097087237322696861922203277862748921', 'skills': 'data warehouse'}, {'id': '274097087237322696861922203277862748921', 'skills': 'linux'}, {'id': '274097087237322696861922203277862748921', 'skills': 'access'}, {'id': '274097087237322696861922203277862748921', 'skills': 'unix'}, {'id': '270987204323396713165325793790364446993', 'skills': 'business requirements'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data accuracy'}, {'id': '270987204323396713165325793790364446993', 'skills': 'project management'}, {'id': '270987204323396713165325793790364446993', 'skills': 'excel'}, {'id': '270987204323396713165325793790364446993', 'skills': 'sql'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data mining'}, {'id': '270987204323396713165325793790364446993', 'skills': 'ms office'}, {'id': '270987204323396713165325793790364446993', 'skills': 'analyzing trends'}, {'id': '270987204323396713165325793790364446993', 'skills': 'vba'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data analysis'}, {'id': '270987204323396713165325793790364446993', 'skills': 'optimization'}, {'id': '270987204323396713165325793790364446993', 'skills': 'quality assurance'}, {'id': '270987204323396713165325793790364446993', 'skills': 'programming'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data integrity'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data quality'}, {'id': '270987204323396713165325793790364446993', 'skills': 'process redesign'}, {'id': '270987204323396713165325793790364446993', 'skills': 'maintaining'}, {'id': '270987204323396713165325793790364446993', 'skills': 'accurate data'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data cleaning'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data preparation'}, {'id': '270987204323396713165325793790364446993', 'skills': 'processing'}, {'id': '270987204323396713165325793790364446993', 'skills': 'statistical techniques'}, {'id': '270987204323396713165325793790364446993', 'skills': 'research data'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data sources'}, {'id': '270987204323396713165325793790364446993', 'skills': 'reliability'}, {'id': '270987204323396713165325793790364446993', 'skills': 'spreadsheets'}, {'id': '270987204323396713165325793790364446993', 'skills': 'access'}, {'id': '270987204323396713165325793790364446993', 'skills': 'hipaa'}, {'id': '270987204323396713165325793790364446993', 'skills': 'problem solving'}, {'id': '270987204323396713165325793790364446993', 'skills': 'implement'}, {'id': '270987204323396713165325793790364446993', 'skills': 'research'}, {'id': '270987204323396713165325793790364446993', 'skills': 'consistent'}, {'id': '76199997604772823340207052419224154975', 'skills': 'excel'}, {'id': '76199997604772823340207052419224154975', 'skills': 'pivot tables'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sas'}, {'id': '76199997604772823340207052419224154975', 'skills': 'data storage'}, {'id': '76199997604772823340207052419224154975', 'skills': 'office'}, {'id': '76199997604772823340207052419224154975', 'skills': 'data analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'dashboard'}, {'id': '76199997604772823340207052419224154975', 'skills': 'problem solving'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms project'}, {'id': '76199997604772823340207052419224154975', 'skills': 'explanation'}, {'id': '76199997604772823340207052419224154975', 'skills': 'waterfall'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms access'}, {'id': '76199997604772823340207052419224154975', 'skills': 'regularization'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms excel'}, {'id': '76199997604772823340207052419224154975', 'skills': 'business objects'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sql'}, {'id': '76199997604772823340207052419224154975', 'skills': 'query builder'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms visio'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sharepoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'onenote'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms sharepoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms onenote'}, {'id': '76199997604772823340207052419224154975', 'skills': 'apple sheets'}, {'id': '76199997604772823340207052419224154975', 'skills': 'apple pages'}, {'id': '76199997604772823340207052419224154975', 'skills': 'open office'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms powerpoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms word'}, {'id': '76199997604772823340207052419224154975', 'skills': 'programming'}, {'id': '76199997604772823340207052419224154975', 'skills': 'java'}, {'id': '76199997604772823340207052419224154975', 'skills': 'vbnet'}, {'id': '76199997604772823340207052419224154975', 'skills': 'windows'}, {'id': '76199997604772823340207052419224154975', 'skills': 'android'}, {'id': '76199997604772823340207052419224154975', 'skills': 'access'}, {'id': '76199997604772823340207052419224154975', 'skills': 'reporting'}, {'id': '76199997604772823340207052419224154975', 'skills': 'performance analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'visio'}, {'id': '305667889169402374245195369321522535673', 'skills': 'data analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'reporting'}, {'id': '305667889169402374245195369321522535673', 'skills': 'user acceptance testing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft access'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft excel'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sas'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sql server'}, {'id': '305667889169402374245195369321522535673', 'skills': 'cognos'}, {'id': '305667889169402374245195369321522535673', 'skills': 'crystal'}, {'id': '305667889169402374245195369321522535673', 'skills': 'business objects'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sql'}, {'id': '305667889169402374245195369321522535673', 'skills': 'tableau'}, {'id': '305667889169402374245195369321522535673', 'skills': 'project management'}, {'id': '305667889169402374245195369321522535673', 'skills': 'data quality'}, {'id': '305667889169402374245195369321522535673', 'skills': 'monitor'}, {'id': '305667889169402374245195369321522535673', 'skills': 'hipaa'}, {'id': '305667889169402374245195369321522535673', 'skills': 'process analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'ms office'}, {'id': '305667889169402374245195369321522535673', 'skills': 'excel'}, {'id': '305667889169402374245195369321522535673', 'skills': 'access'}, {'id': '305667889169402374245195369321522535673', 'skills': 'claims analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'project planning'}, {'id': '305667889169402374245195369321522535673', 'skills': 'testing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'business requirements'}, {'id': '305667889169402374245195369321522535673', 'skills': 'analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'reconciliation'}, {'id': '305667889169402374245195369321522535673', 'skills': 'processing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft office'}, {'id': '83985006356839563643115453489150250520', 'skills': 'designing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'monitoring'}, {'id': '83985006356839563643115453489150250520', 'skills': 'analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'reporting'}, {'id': '83985006356839563643115453489150250520', 'skills': 'risk assessment'}, {'id': '83985006356839563643115453489150250520', 'skills': 'statistical analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'problem solving'}, {'id': '83985006356839563643115453489150250520', 'skills': 'modeling'}, {'id': '83985006356839563643115453489150250520', 'skills': 'project management'}, {'id': '83985006356839563643115453489150250520', 'skills': 'rapid application development'}, {'id': '83985006356839563643115453489150250520', 'skills': 'adhoc testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'backend testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'critical thinking'}, {'id': '83985006356839563643115453489150250520', 'skills': 'decision making'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft access'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft excel'}, {'id': '83985006356839563643115453489150250520', 'skills': 'ms office'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'powerpoint'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sharepoint'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sql'}, {'id': '83985006356839563643115453489150250520', 'skills': 'tableau'}, {'id': '83985006356839563643115453489150250520', 'skills': 'spotfire'}, {'id': '83985006356839563643115453489150250520', 'skills': 'visio'}, {'id': '83985006356839563643115453489150250520', 'skills': 'crm'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sql server'}, {'id': '83985006356839563643115453489150250520', 'skills': 'aws'}, {'id': '83985006356839563643115453489150250520', 'skills': 'agile'}, {'id': '83985006356839563643115453489150250520', 'skills': 'data analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'business requirements'}, {'id': '83985006356839563643115453489150250520', 'skills': 'design'}, {'id': '83985006356839563643115453489150250520', 'skills': 'developing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'drupal'}, {'id': '83985006356839563643115453489150250520', 'skills': 'data warehouses'}, {'id': '83985006356839563643115453489150250520', 'skills': 'research'}, {'id': '83985006356839563643115453489150250520', 'skills': 'cloud'}, {'id': '83985006356839563643115453489150250520', 'skills': 'ms project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'cisco'}, {'id': '83985006356839563643115453489150250520', 'skills': 'technical documentation'}, {'id': '83985006356839563643115453489150250520', 'skills': 'big data'}, {'id': '83985006356839563643115453489150250520', 'skills': 'hdfs'}, {'id': '83985006356839563643115453489150250520', 'skills': 'access'}, {'id': '83985006356839563643115453489150250520', 'skills': 'project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'business analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'visual studio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tableau'}, {'id': '278624286922027916746373616168944850452', 'skills': 'developing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboards'}, {'id': '278624286922027916746373616168944850452', 'skills': 'design'}, {'id': '278624286922027916746373616168944850452', 'skills': 'monitoring'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data profiling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'bi'}, {'id': '278624286922027916746373616168944850452', 'skills': 'teams'}, {'id': '278624286922027916746373616168944850452', 'skills': 'windows'}, {'id': '278624286922027916746373616168944850452', 'skills': 'active directory'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'plsql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'xml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'python'}, {'id': '278624286922027916746373616168944850452', 'skills': 'programming'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sharepoint'}, {'id': '278624286922027916746373616168944850452', 'skills': 'performance tuning'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data blending'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboard'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data analytics'}, {'id': '278624286922027916746373616168944850452', 'skills': 'waterfall'}, {'id': '278624286922027916746373616168944850452', 'skills': 'decision making'}, {'id': '278624286922027916746373616168944850452', 'skills': 'etl'}, {'id': '278624286922027916746373616168944850452', 'skills': 'agile'}, {'id': '278624286922027916746373616168944850452', 'skills': 'software development life cycle'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sdlc'}, {'id': '278624286922027916746373616168944850452', 'skills': 'testing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data validation'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data cleansing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data verification'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mismatch'}, {'id': '278624286922027916746373616168944850452', 'skills': 'db2'}, {'id': '278624286922027916746373616168944850452', 'skills': 'oracle'}, {'id': '278624286922027916746373616168944850452', 'skills': 'teradata'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql server'}, {'id': '278624286922027916746373616168944850452', 'skills': 'erwin'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dimensional modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'snowflake'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data warehouse'}, {'id': '278624286922027916746373616168944850452', 'skills': 'modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'normalization'}, {'id': '278624286922027916746373616168944850452', 'skills': 'process analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'microsoft office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'word'}, {'id': '278624286922027916746373616168944850452', 'skills': 'powerpoint'}, {'id': '278624286922027916746373616168944850452', 'skills': 'excel'}, {'id': '278624286922027916746373616168944850452', 'skills': 'microsoft project'}, {'id': '278624286922027916746373616168944850452', 'skills': 'c'}, {'id': '278624286922027916746373616168944850452', 'skills': 'html'}, {'id': '278624286922027916746373616168944850452', 'skills': 'er studio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms visio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'datasets'}, {'id': '278624286922027916746373616168944850452', 'skills': 'time series analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'matlab'}, {'id': '278624286922027916746373616168944850452', 'skills': 'collecting data'}, {'id': '278624286922027916746373616168944850452', 'skills': 'text mining'}, {'id': '278624286922027916746373616168944850452', 'skills': 'vmware'}, {'id': '278624286922027916746373616168944850452', 'skills': 'linux'}, {'id': '278624286922027916746373616168944850452', 'skills': 'cisco'}, {'id': '278624286922027916746373616168944850452', 'skills': 'exchange'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms access'}, {'id': '278624286922027916746373616168944850452', 'skills': 'javascript'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ajax'}, {'id': '278624286922027916746373616168944850452', 'skills': 'jquery'}, {'id': '278624286922027916746373616168944850452', 'skills': 'bootstrap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'angularjs'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ssis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ssrs'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tsql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'aspnet'}, {'id': '278624286922027916746373616168944850452', 'skills': 'cognos'}, {'id': '278624286922027916746373616168944850452', 'skills': 'uml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'java'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visual basic'}, {'id': '278624286922027916746373616168944850452', 'skills': 'vb'}, {'id': '278624286922027916746373616168944850452', 'skills': 'windows xp'}, {'id': '278624286922027916746373616168944850452', 'skills': 'net'}, {'id': '278624286922027916746373616168944850452', 'skills': 'alteryx'}, {'id': '278624286922027916746373616168944850452', 'skills': 'develop'}, {'id': '278624286922027916746373616168944850452', 'skills': 'project'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visualizations'}, {'id': '278624286922027916746373616168944850452', 'skills': 'poc'}, {'id': '278624286922027916746373616168944850452', 'skills': 'reporting'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business intelligence'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visualization'}, {'id': '278624286922027916746373616168944850452', 'skills': 'olap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'processing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mining'}, {'id': '278624286922027916746373616168944850452', 'skills': 'unix'}, {'id': '278624286922027916746373616168944850452', 'skills': 'access'}, {'id': '278624286922027916746373616168944850452', 'skills': 'transform'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data sources'}, {'id': '278624286922027916746373616168944850452', 'skills': 'big data'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business requirements'}, {'id': '278624286922027916746373616168944850452', 'skills': 'css'}, {'id': '278624286922027916746373616168944850452', 'skills': 'html5'}, {'id': '278624286922027916746373616168944850452', 'skills': 'crystal'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tomcat'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboard development'}, {'id': '278624286922027916746373616168944850452', 'skills': 'statistics'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sass'}, {'id': '278624286922027916746373616168944850452', 'skills': 'database design'}, {'id': '278624286922027916746373616168944850452', 'skills': 'analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data quality'}, {'id': '278624286922027916746373616168944850452', 'skills': 'load'}, {'id': '278624286922027916746373616168944850452', 'skills': 'loading'}, {'id': '278624286922027916746373616168944850452', 'skills': 'r'}, {'id': '278624286922027916746373616168944850452', 'skills': 'rest'}, {'id': '278624286922027916746373616168944850452', 'skills': 'transformation'}, {'id': '278624286922027916746373616168944850452', 'skills': 'code reviews'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data processing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'designing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business objects'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql server reporting services'}, {'id': '278624286922027916746373616168944850452', 'skills': 'office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms sql server'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business requirement'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mapping'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data warehousing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms excel'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visio'}, {'id': '6361630242691360309919431676680490692', 'skills': 'oracle'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms access'}, {'id': '6361630242691360309919431676680490692', 'skills': 'redshift'}, {'id': '6361630242691360309919431676680490692', 'skills': 's3'}, {'id': '6361630242691360309919431676680490692', 'skills': 'software products'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hadoop'}, {'id': '6361630242691360309919431676680490692', 'skills': 'amazon web services'}, {'id': '6361630242691360309919431676680490692', 'skills': 'programming'}, {'id': '6361630242691360309919431676680490692', 'skills': 'plsql'}, {'id': '6361630242691360309919431676680490692', 'skills': 'sql'}, {'id': '6361630242691360309919431676680490692', 'skills': 'impala'}, {'id': '6361630242691360309919431676680490692', 'skills': 'python'}, {'id': '6361630242691360309919431676680490692', 'skills': 'c'}, {'id': '6361630242691360309919431676680490692', 'skills': 'visual basic'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms excel'}, {'id': '6361630242691360309919431676680490692', 'skills': 'vba'}, {'id': '6361630242691360309919431676680490692', 'skills': 'power query'}, {'id': '6361630242691360309919431676680490692', 'skills': 'plsql developer'}, {'id': '6361630242691360309919431676680490692', 'skills': 'kibana'}, {'id': '6361630242691360309919431676680490692', 'skills': 'excel'}, {'id': '6361630242691360309919431676680490692', 'skills': 'eclipse'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms office'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms project'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms visio'}, {'id': '6361630242691360309919431676680490692', 'skills': 'putty'}, {'id': '6361630242691360309919431676680490692', 'skills': 'tortoise svn'}, {'id': '6361630242691360309919431676680490692', 'skills': 'github'}, {'id': '6361630242691360309919431676680490692', 'skills': 'jira'}, {'id': '6361630242691360309919431676680490692', 'skills': 'unix'}, {'id': '6361630242691360309919431676680490692', 'skills': 'aws'}, {'id': '6361630242691360309919431676680490692', 'skills': 'business analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'system analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'design'}, {'id': '6361630242691360309919431676680490692', 'skills': 'project management'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hdfs'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hive'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data sources'}, {'id': '6361630242691360309919431676680490692', 'skills': 'xml'}, {'id': '6361630242691360309919431676680490692', 'skills': 'sdlc'}, {'id': '6361630242691360309919431676680490692', 'skills': 'analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'regression'}, {'id': '6361630242691360309919431676680490692', 'skills': 'stored procedures'}, {'id': '6361630242691360309919431676680490692', 'skills': 'query optimization'}, {'id': '6361630242691360309919431676680490692', 'skills': 'agile'}, {'id': '6361630242691360309919431676680490692', 'skills': 'waterfall'}, {'id': '6361630242691360309919431676680490692', 'skills': 'scrum'}, {'id': '6361630242691360309919431676680490692', 'skills': 'estimation'}, {'id': '6361630242691360309919431676680490692', 'skills': 'business requirements'}, {'id': '6361630242691360309919431676680490692', 'skills': 'develop'}, {'id': '6361630242691360309919431676680490692', 'skills': 'loading'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data science'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data sets'}, {'id': '6361630242691360309919431676680490692', 'skills': 'reporting'}, {'id': '6361630242691360309919431676680490692', 'skills': 'requirement analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'technical design'}, {'id': '6361630242691360309919431676680490692', 'skills': 'optimization'}, {'id': '6361630242691360309919431676680490692', 'skills': 'monitoring'}, {'id': '6361630242691360309919431676680490692', 'skills': 'system testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'integration testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'automation'}, {'id': '6361630242691360309919431676680490692', 'skills': 'big data'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data modeling'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ftp'}, {'id': '6361630242691360309919431676680490692', 'skills': 'java'}, {'id': '6361630242691360309919431676680490692', 'skills': 'research'}, {'id': '6361630242691360309919431676680490692', 'skills': 'visio'}, {'id': '147203991935502946470515237354155400269', 'skills': 'analysis'}, {'id': '147203991935502946470515237354155400269', 'skills': 'reporting'}, {'id': '147203991935502946470515237354155400269', 'skills': 'ssrs'}, {'id': '147203991935502946470515237354155400269', 'skills': 'crystal'}, {'id': '147203991935502946470515237354155400269', 'skills': 'crm'}, {'id': '147203991935502946470515237354155400269', 'skills': 'technical documentation'}, {'id': '147203991935502946470515237354155400269', 'skills': 'project management'}, {'id': '147203991935502946470515237354155400269', 'skills': 'data quality'}, {'id': '147203991935502946470515237354155400269', 'skills': 'excel'}, {'id': '147203991935502946470515237354155400269', 'skills': 'visual basic'}, {'id': '147203991935502946470515237354155400269', 'skills': 'project designs'}, {'id': '147203991935502946470515237354155400269', 'skills': 'pivot table'}, {'id': '147203991935502946470515237354155400269', 'skills': 'vlookup'}, {'id': '147203991935502946470515237354155400269', 'skills': 'scrum'}, {'id': '147203991935502946470515237354155400269', 'skills': 'teams'}, {'id': '147203991935502946470515237354155400269', 'skills': 'test cases'}, {'id': '147203991935502946470515237354155400269', 'skills': 'sharepoint'}, {'id': '147203991935502946470515237354155400269', 'skills': 'assembly'}, {'id': '147203991935502946470515237354155400269', 'skills': 'technical assistance'}, {'id': '147203991935502946470515237354155400269', 'skills': 'gateway'}, {'id': '147203991935502946470515237354155400269', 'skills': 'analyzing data'}, {'id': '147203991935502946470515237354155400269', 'skills': 'business intelligence'}, {'id': '147203991935502946470515237354155400269', 'skills': 'business decisions'}, {'id': '147203991935502946470515237354155400269', 'skills': 'microsoft excel'}, {'id': '147203991935502946470515237354155400269', 'skills': 'data integrity'}, {'id': '147203991935502946470515237354155400269', 'skills': 'storage'}, {'id': '147203991935502946470515237354155400269', 'skills': 'design'}, {'id': '147203991935502946470515237354155400269', 'skills': 'uml'}, {'id': '147203991935502946470515237354155400269', 'skills': 'erd'}, {'id': '97098914450695563702884660921271152925', 'skills': 'stata'}, {'id': '97098914450695563702884660921271152925', 'skills': 'programming'}, {'id': '97098914450695563702884660921271152925', 'skills': 'r'}, {'id': '97098914450695563702884660921271152925', 'skills': 'sas'}, {'id': '97098914450695563702884660921271152925', 'skills': 'gis'}, {'id': '97098914450695563702884660921271152925', 'skills': 'nvivo'}, {'id': '97098914450695563702884660921271152925', 'skills': 'microsoft office'}, {'id': '97098914450695563702884660921271152925', 'skills': 'analytical research'}, {'id': '97098914450695563702884660921271152925', 'skills': 'project'}, {'id': '97098914450695563702884660921271152925', 'skills': 'evaluation'}, {'id': '97098914450695563702884660921271152925', 'skills': 'statistical models'}, {'id': '97098914450695563702884660921271152925', 'skills': 'research'}, {'id': '97098914450695563702884660921271152925', 'skills': 'data collection'}, {'id': '97098914450695563702884660921271152925', 'skills': 'data quality'}, {'id': '97098914450695563702884660921271152925', 'skills': 'statistical methods'}, {'id': '97098914450695563702884660921271152925', 'skills': 'analysis'}, {'id': '97098914450695563702884660921271152925', 'skills': 'designing'}, {'id': '138536846257210096623495337605137733129', 'skills': 'business processes'}, {'id': '138536846257210096623495337605137733129', 'skills': 'data integrity'}, {'id': '138536846257210096623495337605137733129', 'skills': 'sap'}, {'id': '138536846257210096623495337605137733129', 'skills': 'processing'}, {'id': '138536846257210096623495337605137733129', 'skills': 'budget analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'resource analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'crystal'}, {'id': '138536846257210096623495337605137733129', 'skills': 'analyzing data'}, {'id': '138536846257210096623495337605137733129', 'skills': 'analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'microsoft office'}, {'id': '138536846257210096623495337605137733129', 'skills': 'excel'}, {'id': '138536846257210096623495337605137733129', 'skills': 'word'}, {'id': '138536846257210096623495337605137733129', 'skills': 'powerpoint'}, {'id': '138536846257210096623495337605137733129', 'skills': 'outlook'}, {'id': '138536846257210096623495337605137733129', 'skills': 'visio'}, {'id': '138536846257210096623495337605137733129', 'skills': 'access'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data privacy'}, {'id': '321556423433782918032074297826301460362', 'skills': 'research'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data analysis'}, {'id': '321556423433782918032074297826301460362', 'skills': 'cloud'}, {'id': '321556423433782918032074297826301460362', 'skills': 'networking'}, {'id': '321556423433782918032074297826301460362', 'skills': 'user training'}, {'id': '321556423433782918032074297826301460362', 'skills': 'monitoring'}, {'id': '321556423433782918032074297826301460362', 'skills': 'statistical analysis'}, {'id': '321556423433782918032074297826301460362', 'skills': 'reporting'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data collection'}, {'id': '321556423433782918032074297826301460362', 'skills': 'project'}, {'id': '321556423433782918032074297826301460362', 'skills': 'algorithms'}, {'id': '321556423433782918032074297826301460362', 'skills': 'workflow diagrams'}, {'id': '321556423433782918032074297826301460362', 'skills': 'hipaa'}, {'id': '321556423433782918032074297826301460362', 'skills': 'windows xp'}, {'id': '321556423433782918032074297826301460362', 'skills': 'office'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data quality'}, {'id': '283692866702135157759709078394118382144', 'skills': 'oracle forms'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google drive'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google docs'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google sheets'}, {'id': '283692866702135157759709078394118382144', 'skills': 'microsoft office'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data analysis'}, {'id': '283692866702135157759709078394118382144', 'skills': 'developing'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data patterns'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data linking'}, {'id': '283692866702135157759709078394118382144', 'skills': 'test scripts'}, {'id': '283692866702135157759709078394118382144', 'skills': 'interviews'}, {'id': '283692866702135157759709078394118382144', 'skills': 'swot analysis'}, {'id': '283692866702135157759709078394118382144', 'skills': 'oracle'}, {'id': '283692866702135157759709078394118382144', 'skills': 'research'}, {'id': '189067289296683940487677712533012870883', 'skills': 'functional requirements'}, {'id': '189067289296683940487677712533012870883', 'skills': 'monitoring'}, {'id': '189067289296683940487677712533012870883', 'skills': 'excel'}, {'id': '189067289296683940487677712533012870883', 'skills': 'access'}, {'id': '189067289296683940487677712533012870883', 'skills': 'developing'}, {'id': '189067289296683940487677712533012870883', 'skills': 'data warehouse'}, {'id': '189067289296683940487677712533012870883', 'skills': 'monitor'}, {'id': '189067289296683940487677712533012870883', 'skills': 'sharepoint'}, {'id': '189067289296683940487677712533012870883', 'skills': 'data sharing'}, {'id': '189067289296683940487677712533012870883', 'skills': 'implement'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft word'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft powerpoint'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft excel'}, {'id': '189067289296683940487677712533012870883', 'skills': 'analyzing data'}, {'id': '189067289296683940487677712533012870883', 'skills': 'adobe photoshop'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft office'}, {'id': '189067289296683940487677712533012870883', 'skills': 'outlook'}, {'id': '189067289296683940487677712533012870883', 'skills': 'power point'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft publisher'}, {'id': '189067289296683940487677712533012870883', 'skills': 'word'}, {'id': '189067289296683940487677712533012870883', 'skills': 'project management'}, {'id': '189067289296683940487677712533012870883', 'skills': 'visio'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'statistical analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'r'}, {'id': '14259385793885776770164514431890372233', 'skills': 'sas'}, {'id': '14259385793885776770164514431890372233', 'skills': 'microsoft office'}, {'id': '14259385793885776770164514431890372233', 'skills': 'word'}, {'id': '14259385793885776770164514431890372233', 'skills': 'excel'}, {'id': '14259385793885776770164514431890372233', 'skills': 'power point'}, {'id': '14259385793885776770164514431890372233', 'skills': 'spreadsheets'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data platform'}, {'id': '14259385793885776770164514431890372233', 'skills': 'ms excel'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data validation'}, {'id': '14259385793885776770164514431890372233', 'skills': 'accurate'}, {'id': '14259385793885776770164514431890372233', 'skills': 'pivot tables'}, {'id': '14259385793885776770164514431890372233', 'skills': 'analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'research'}, {'id': '14259385793885776770164514431890372233', 'skills': 'r studio'}, {'id': '14259385793885776770164514431890372233', 'skills': 'statistics'}, {'id': '14259385793885776770164514431890372233', 'skills': 'ms word'}, {'id': '14259385793885776770164514431890372233', 'skills': 'powerpoint'}, {'id': '14259385793885776770164514431890372233', 'skills': 'access'}, {'id': '14259385793885776770164514431890372233', 'skills': 'sql'}]}\n",
      "{'total': 241, 'batches': 1, 'time': 0.06700444221496582}\n",
      "{'rows': [{'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-engineer-iii-e867612a74144501b4d70b51d5b2f7e9', 'id': '5533667039977703598659155174781970519', 'data': 'Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Seasoned Senior Data Engineer possessing indepth knowledge of RDBSM environments ETL techniques architecture data modeling and integration between systems Offering 10 years of background managing various aspects of development design and delivery of database solutions Analytical passionate and aspiring leader bringing proven communication and organizational abilities Seeking a fulltime remote position        Skills            Expertise in Microsoft SQL Server Management Studio 20052019     Microsoft Certified Professional Designation  Microsoft Database Fundamentals Designation  Temporary Tables DLL statements Loops Schema creations Common Table Expressions View Dynamic SQL scripts Merge statements Indices Performance Tuning Triggers Functions Store Procedures Joins Parametrization Cross Apply Outer Apply Automation Job Creation Scheduling Environment Variable Deployment Variables Store Procedures Pivots Performance Tuning     Expertise in BIDSVisual Studio     Extraction from various sources eg flat files DBs XML etc Dynamic SQL Automation Deployment Parameterization Looping ProgramProcess Executions Custom coding using Script Tasking Flat FileXML File Creation UTF8 conversion MergeSCD implementation     Adept Excel User     Pivots use of Macros Formulas Charts Tabular data from SSAS cubes     Data Architecture and Modeling     Use of Star and Snowflake Schema utilizing tools like Visio to create Flow Charts using data modeling tools like SQLDB to create denormalized structures creating business specific entities to create balance between efficiency and use case       Oracle     Familiarity and use of PLSQL corroborated financial reports using Oracle Transaction Business Intelligence OTBI and the UCM as well     Familiarity of Presentation Layer Tools     Salesforce Tableau Qlikview SSRS OTBI     Subject Matter Expert     Deep IT derived knowledge in business operations including Finance Accounting Marketing Underwriting Compliance and some Specialty Risk Programs     Other Pertinent Skills     Agile HybridKANANWaterfalls methodology Familiarity with AzureDevOps GitHub TFS Skilled Design Documentation     Personal Skills     Diligent Adaptive Organized Methodical Analytical HonestBlunt                       Experience       Data Engineer III       072016      Current     Crown Castle Usa Inc    –    Salt Lake City     UT            Development    Utilized various IDEs including Visual Studio SQL Server Management Studio Astera Centerprise to build ETL solutions  Created dynamic SQL to droprecreate objects  Utilized Activity Monitortype tools such as FogLight and Solarwinds DPA tool for bottlenecks  Created technical and operational audits within process to better catch issues  Performance tuned process cutting down an 8 hour process to 5 minutes  Created and ingested delimited files  Created pivot structures within SQL taking data and transposing it in the other direction eg horizontal data to vertical  Utilized VB and C languages to create custom Script Task components to use in ETL solutions    Integration    Integrated claims personal and commercial lines products  Extensively developed piloted and implemented integration solution between OLTP data and Salesforce created API pipeline delivering quote details to business thereby allowing them to make key decisions with agents  Created data consumption and delivery solutions for claims including though not limited to sending flat files to vendors containing company data receiving decompressing unpacking ingesting data creating a wrapper shell on a program that was called by SSIS solution dynamically recreating dll with parameterized inputs    Analytics    Researched financial disparities between Oracle and TSQL restructured format  Sequenced data dictionary from Oracle to extrapolate and originate data sources to better deliver solutions  Created pivoted OLAPTabulated SSAS structures to help display intersection of trends and data    Reporting    Created canned subscription and ad hoc reports for business stakeholders sectors including finance claims actuary legal statistical statutory marketing and underwriting  Substantiated differences between expected and actual results in control reports allowing business to see disparities    Architecture    Architected a SCD Type 1  2 and 3 denormalized and normalized structures  Created conceptual logical and physical data models by utilizing tools such as Visio SysDiagram and SQLDBM    Leadership    Led and organized scrumstatus meetings Provided guidance and focused delivery tasks to colleagues whilst assessing for gaps  Provided instructional and informational background on projects to new comers detailing them thoroughly on the historical and current SDLC processes lessons learned etc  Piloted groups to create best practices and code vettingreview within organization    Organizational    Incorporated various methodology including though not limited to Waterfall Agile KANBAN Hybrid  Created several detailed design documents some serving as a standard and template for other developers  Coordinated project management updates to determine project scopes and limitations  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Part of SWAT delivery service a team designed to tackle high velocity and profile requests in company Managed solution implementation for several highprofile projects from start to finish           Application Developer       122014      072016     Honeywell    –    Deer Park     TX            Development    Created ETL solutions using Visual Studio and SQL Studio Management Studio  Parsed and massaged data into Guidewire specific formats following tight edits  Promoted and automated ETL processes  Performance tuned existing queries to cut down on time reducing overall process from 2 hours to 8 minutes    Integration    Integrated commercial and personal lines Farm and Ranch insurance products into Guidewire Claims Center  Worked closely with business stakeholders to understand data whilst also pushing for systemoriginated corrections where fixes could not be done via ETL    Leadership    Mentored colleagues and fellow testers in TSQL scripting    Organizational    Utilized Agile Methodology and TFS to check inout code from repository  Created design documents detailing process and flow  Worked closely with business stakeholders to understand requirements and provide deliverables per their specifications    Educational    Actively sought out areas to increase insurance domain of knowledge  Enrolled and successfully completed AINS 24 offered by The Institutes  Informed and trained with Guidewire and SAP HANA software  Became Farm and Ranch SME for data           ETL Developer       032013      102014     American Homes 4 Rent    –    Cincinnati     OH            Development    Created complex integration solution to ingest and parse rowdelimited flat files using the FiServ Data model  Ingested outputted files from FNMA and FHLMC  Further refined this data using ETL based on businessend user requirements  Participated in code review sessions with colleague to assess bottlenecks strategize resource consumption kill deadlocks and tailor code to become more IO efficient  Created an SDLC lifecycle for process flow including environments for development testing and production  Installed and Upgraded SQL Server Studio    Integration    Delivered outputs using webservice calls fetching unique systemoriginated keys from OLTP system to use within TSQL    Leadership    Trained and mentored users on how to utilize TSQL and create simple SSIS packages  Crosstrained colleagues on ETL solution    Organizational    Created several design documents detailing pipeline of ETL solution caveatslimitations and future development requirements          Education and Training       Associate of Arts              Expected in   122011                Collin College      Frisco TX          GPA        Status           Summa Cum Laude Honors graduate  Phi Theta Kappa International Honor Society member  Sigma Kappa Delta National English Honor Society member  Student Leadership Academy graduate', 'resume_html': 'none', 'skills': \"['BI', 'Design', 'Business Intelligence', 'Tableau', 'Tableau', 'Tableau', 'Data Visualization', 'Dashboards', 'Database Design', 'Designing', 'Business Intelligence', 'ETL', 'Transform', 'Load', 'Extract', 'Transform', 'Load', 'data warehouse', 'trends', 'business requirements', 'business intelligence', 'evaluating trends', 'realtime data', 'Tableau', 'MS Excel', 'SSRS', 'Snowflake', 'Cassandra', 'Oracle', 'MS SQL Server', 'MS Access', 'Postgres', 'Amazon S3', 'SQL', 'Python', 'TSQL', 'HTML', 'CSS', 'Java', 'Salesforce', 'MS Word', 'MS Excel', 'Outlook', 'FrontPage', 'PowerPoint', 'BI', 'Reporting', 'dashboards', 'Tableau', 'Data Quality', 'Python', 'Cassandra', 'Snowflake', 'MEMSQL', 'warehouses', 'Tableau', 'SQL', 'Amazon S3', 'pipelines', 'Amazon S3', 'Snowflake', 'Extract', 'Load', 'Transform', 'Data Governance', 'Data Analysis', 'Data Mapping', 'Business Requirement', 'Data Validation', 'Tableau', 'Tableau', 'Snowflake', 'Cassandra', 'MemSQL', 'Oracle', 'AWS', 'Postgres', 'Excel', 'Salesforce', 'MS Access', 'BI', 'Tableau', 'Tableau', 'Dashboards', 'dashboards', 'visualizations', 'adhoc analysis', 'reporting', 'customized analysis', 'dashboards', 'Designing', 'developing', 'data warehouse', 'Redshift', 'BI', 'optimization', 'Designing', 'developing', 'prototyping', 'dashboards', 'dashboards', 'designing', 'Dashboard', 'reporting', 'dashboarding', 'Waterfall', 'Agile', 'statistical techniques', 'validate data', 'Develop', 'data collection', 'dashboards', 'Tableau', 'design', 'visualizations', 'Tableau', 'SAS', 'Data Integration', 'extracting', 'transforming', 'loading data', 'Data warehouse', 'Data Sets', 'business analysis', 'SAS', 'testing']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/senior-data-engineer-c2991ccc65f9419cbf914b7149f33d26', 'id': '274097087237322696861922203277862748921', 'data': 'Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Professional Summary       Having 9 years of Industry experience in ETL Tools such as DataStage  Informatica and SSI Packages  Having experience in Data Bricks Hive SQL Azure CICD pipeline Delta Lake Data Lake Hadoop File system Snowflake  Having experience in Building ETL pipe lines using Apache Spark Python  Excellent Experience in Designing Developing Documenting Testing of ETL jobs and mappings in Server and Parallel jobs using DataStageInformatica to populate tables in Data Warehouse DataMart ODS and Large Data sets  Experience in working on streaming data using IBM MQ and Kafka  Experience in working using AgileSCRUM and Waterfall Development Methodology  Experience in logging tickets in Service now Version control tools PVCS Azure DevOps CICD pipeline  Experience in Azure work environment  Work experience in IBM Master Data ManagementMDM Architecture  Working Experience on Azure Cloud  Experience in working with multiple Data Bases like Oracle SQL Server DB2 Netezza NOSQL Mongo DB Salesforce Snowflake DB  Experience in working on data migration from oracle 9i to 10g and DB2 to Netezza  Expertise in using DataStageInformatica to integrate with different Sources and Targets like Azure SQL database Oracle Mainframe systems Netezza Salesforce SOAP and REST services XML SQL Server and MongoDB  Experience in UNIX AIX and Linux server resource monitoring and load balancing  Ensured that user requirements are effectively and accurately communicated to the other members of the development team and Facilitate communications between business users developers and testing teams  Conducting internal and external reviews as well as formal walkthrough among various teams and documenting the proceedings  Excellent problemsolving and troubleshooting capabilities Quick learner highly motivated result oriented and an enthusiastic team player             Education       Sri Krishnadevaraya University    Kurnool AndhraPradesh           Expected in   052011     –      –       Bachelor of Engineering        Computer Science  Information Technology          GPA                   Skills         ETL Tools  DataStage Informatica Power Centre and SSIS Packages  Big Data Technologies  HiveSpark  HDFSKafka Sqoop  Database  Oracle SQL ServerDB2 Netezza  Mongo Snowflake  Programming Languages  UNIX Python PLSQL  Working experience in Agile Waterfall model and tracking in JIRRA and Microsoft Devops  Configuration Tools  PVCS  Microsoft TFS Azure CICD pipeline  Cloud Experience Azure      Job Scheduling Tools CA7 Control M  Operating System  Win XP 7 10 and UNIX  Adaptability  Data management  Organization and Time management  Teamwork                     Certifications      IBM Data Stage  Oracle  Informatica  Netezza          Work History       Factset Research Systems Inc      Senior Data Engineer   San Francisco     CA                   022021      Current     Bank Operational Data Distribution Hub is highly availability distribution center for operational data The servers have been set up to provide failover capabilities in the event of any issues which could cause the hardware to shut down The design of this system focuses on four main vendors We load data to  HDFS storage as well and built HIVE on top of this to analyze   Developed implemented supported and maintained data analytics protocols standards and documentation  Analyzed complex data and identified anomalies trends and risks to provide useful insights to improve internal controls  Contributed to internal activities for overall process improvements efficiencies and innovation  Communicated new or updated data requirements to global team  Explained data results clearly and discussed how it can be utilized to support project objectives  Planned and implemented security measures to safeguard vital business data  Created and implemented database designs and data models  Monitored incoming data analytics requests executed analytics and efficiently distributed results to support strategies  Built databases and table structures following OLAPOLTP architecture methodology for web applications           Cox Communications Inc      Lead Data Engineer   Dayton     OH                   012018      012021    Master Data ManagementMDM EQH is primary vehicle for customer selfservice for Life and Annuity products Displays current policy values statements confirmation notices and prospectuses Supports profile maintenance including address phone and email address changes financial profile and investment strategies Selfservice tools include performance financial transactions ACH payments and loans   Responsibilities    Leads programproject application engineering teams consisting of cross functional global and virtual groups directly supervises staff assigns responsibility to members monitors progress of daily activities  Monitor and manage programproject application engineering baseline to ensure activities are occurring as planned  scope budget and schedule and managing variances  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Proactively identify risks issues and problems on programsprojects  leading engineering and projectprogram team to develop risk management and issue management plans  I have saved 1 person monitoring work by performing optimization   Ability to clearly articulate problems and proposed options and solutions and apply judgment in implementing application engineering methodologies processes and practices to ensure security resilience maintainability and quality of MDM solutions  Analyses and defines detailed MDM processes tasks data flows and dependencies  Develop custom mapping functions  Participates in system and integration testing  Produces database code SQL stored procedures and any other database specific code solutions meeting technical specifications and business requirements according to established designs  Proactively resolved issues within team  Validated warehouse data structure and accuracy  Cooperated fully with product owners and enterprise architects to understand requirements  Collaborated with multifunctional roles to communicate and align development efforts  Mapped data between source systems and warehouses  Performed systems and data analysis using variety of computer languages and procedures  Documented data warehouse architecture to guarantee capacity met current and forecasted needs  Developed and modified programs to meet customer requirements  Quickly learned new skills and applied them to daily tasks improving efficiency and productivity  Provided global thought leadership in analytics solutions to benefit customers           Epam Systems Inc      Senior ETL Developer   Washington     DC                   082017      122017    Netezza Rewrite This project is about migrating around 70 Data Marts runs on Oracle to Netezza in 10 phases This includes redesigning DataStage jobs integrates with oracle to change it to Netezza and Informatica to DataStage migration   Role  Responsibilities     Requirement Analysis Creating mappings Unit Testing Defect Fixing Documentation and Status Reporting  Identifying Entities cardinality and developing Logical and Physical Data Model  Mentored newly hired employees offering insight into job duties and company policies for easier transition to job position  Prioritized and organized tasks to efficiently accomplish service goals  Analyzing existing process scripts and preparing design document with performance optimized approach  Performed impact analysis on every source and target tables  Responsible for estimation of Design Development and Unit testing  Analyzing dependent objects and data involved and updating efficient unit testing approach  Responsible for scheduling changes which includes changing node to new 115 server change in run time parameters change in predecessor or successor requirements and removing jobs  Have published Play book or Implementation plan for every release  Responsible for driving implementation and doing post implementation data checks  Resolved complex DataStage performance issues and other environment issues  Designed and Developed reusable components which can parse dsx and provide input and output SQLs used in DataStage code  Reviewed Netezza Deliverables and DataStage deliverables in every phase of project  Resolved Netezza SQL issues to Business users  Coordinated with downstream systems and worked on impacted system sign off review and production preparation           Epam Systems Inc      Senior ETL Developer   PA     State                   012016      072017    Financial Move Forward inforce Data Equitable Enterprise Data Warehouse EDW manages collection of components in both Mainframe Information DatabaseIDB and Distributed environments Open system Data WarehouseOSDW that include batch processes operational data stores business intelligence BI data marts and general data services to all IT lines of business   FMFInforce Data project consists of two phases  First phase involves migration of DB2 data to Netezza DB with help of integrated ETL tool Data Stage 87  Second phase contains Data Modelling and DataMart design of migrated tables in Netezza  First phase basically involves initial data load IDL of 400 Db2 tables to Netezza DB  Took care of installing Netezza client on UNIX box where Data stage client exists and ensured connectivity is good for designing data stage jobs Role  Responsibilities  Understanding requirements and coming up with high level design  Created Lowlevel design of mapping document  Created mappings and transformations as per business requirements  Writing reusable mapplets and Oracle PLSQL stored procedures  Unit test jobs according to test plans  Monitored debugged and scheduled mappings according to requirements  Improving performance of mapping execution thus reducing CPU and execution cost and time  Providing System Testing and User Testing Support IQA of Mappings  Capable of assisting team of developers both onshore and offshore to provide strategic plan for execution of this project  Ability to work with key team members to ensure solution meets business requirements  Provided Proof of Concept POC for technical approach regarding design of data stage jobs  Understanding requirements and coming up with technical design strategies with project team and business users  Contributed to detailed estimation of development work  Involved in Estimation of DBA Effort for this project  Designed Field level mapping template design based on business rules transformations and validations  Performed problem assessment resolution and documentation for new and existing database objects  Prepared Knowledge Transition documents which were appreciated by business IT people  Communicated with data architects programmers and engineers to keep projects on track', 'resume_html': 'none', 'skills': \"['business analysis', 'quality assurance', 'functional testing', 'critical thinking', 'project management', 'cryptography', 'threat analysis', 'Cloud', 'Microsoft Office', 'reporting', 'Cryptography', 'JAVA', 'Programming', 'Windows', 'business analysis', 'data integrity', 'optimization', 'testing', 'technical documentation', 'reporting', 'quality assurance', 'reporting', 'business analysis', 'Networking', 'Cryptography', 'data warehouse', 'JAVA', 'Programming', 'Linux', 'Access', 'Microsoft Office', 'Windows', 'optimization', 'project management', 'quality assurance', 'reporting', 'technical documentation', 'UNIX']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/aws-data-engineer-63b337f6d6bf36b19ae995691c672ca0', 'id': '270987204323396713165325793790364446993', 'data': 'Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases             Skills         ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub      ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                     Education and Training       Purdue University    West Lafayette     IN      Expected in   022022     –      –       Post Graduate         Data Engineering           GPA                    University of Texas At Austin    Austin     TX      Expected in   092021     –      –       Post Graduate         Data Science And Business Analytics           GPA                    Califonia State University     Fullerton CA           Expected in   122009     –      –       Bachelor of Arts        Business Administration And Management          GPA                     Experience       Deloitte      AWS Data Engineer   Rosslyn     NV                   012022      022022     Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process           Bank Of America Corporation      Data Engineer   Arcadia     CA                   012016      112019     Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark           Cumming Llc      Big Data Engineer  Hadoop Developer   Aliso Viejo     CA                   102013      122015   AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Fiserv      Python Developer    City     STATE                   092012      102013     AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes', 'resume_html': 'none', 'skills': \"['business requirements', 'data accuracy', 'project management', 'Excel', 'SQL', 'Data Mining', 'MS Office', 'Analyzing trends', 'VBA', 'Data Analysis', 'Optimization', 'Quality Assurance', 'Project Management', 'programming', 'data integrity', 'SQL', 'Excel', 'VBA', 'data quality', 'process redesign', 'maintaining', 'Excel', 'SQL', 'accurate data', 'data cleaning', 'data preparation', 'processing', 'statistical techniques', 'SQL', 'research data', 'Excel', 'data sources', 'data cleaning', 'reliability', 'excel', 'excel', 'spreadsheets', 'access', 'excel', 'processing', 'HIPAA', 'processing', 'Processing', 'Problem solving', 'Problem Solving', 'Excel', 'SQL', 'Data Mining', 'MS Office', 'Analyzing trends', 'VBA', 'Data Analysis', 'Optimization', 'Quality Assurance', 'Project Management', 'programming', 'data integrity', 'SQL', 'Excel', 'VBA', 'data quality', 'process redesign', 'maintaining', 'Excel', 'SQL', 'accurate data', 'implement', 'data cleaning', 'data preparation', 'processing', 'statistical techniques', 'SQL', 'research', 'research', 'Excel', 'data sources', 'data cleaning', 'reliability', 'excel', 'excel', 'spreadsheets', 'access', 'excel', 'processing', 'HIPAA', 'processing', 'Processing', 'consistent', 'Problem solving']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-analyst-data-engineer-5d0c7b4ff12f4323b51fb55c6284662a', 'id': '76199997604772823340207052419224154975', 'data': 'Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary       Logical Data Analyst skilled in requirement analysis software development and database management Selfdirected and proactive professional with 35 years of vast experience collecting cleaning and interpreting data sets Natural problemsolver possessing strong crossfunctional understanding of information technology and business processes  Strong knowledge in AWS cloud services like  ECS   EC2  infrastructure  S3  for storage Elastic MapReduce EMR   Athena  as query manager and  CloudWatch   Very well experienced with various visualization tools like  Tableau  by extracting data from various data sources  MasteringLeading in development of applicationstools using  Python  for 3 years  Worked on performance tuning and optimization to improve the efficiency in script executions  Good working experience loading Data Files in  AWS  Environment and Performed SQL Testing on AWS redshift databases  Exceptional ability to research analyze and convey complex technical information to diverse endusers at all levels               Skills         Data Validation  UNIX System  SQL  Python3  BI ToolsTableau Looker Datapoint  Data BasesSqlServer Postgres MYSQl PythonOracle      Amazon Web Services AWS  Servicenow  Jenkins  Pagerduty  Splunk  GIT                     Education and Training       University of Mary HardinBaylor    Belton     TX      Expected in   122017     –      –               Management Information Systems          GPA                    Auroras Technological And Research Institute    UppalIndia           Expected in   082015     –      –       Bachelor of Science        Electrical Electronics And Communications Engineering          GPA                   Certifications       Licensed AWS Solution Architect  2019           Experience       Management Decisions Inc      Data AnalystData Engineer   Reston     VA                   092021      Current     Worked in Banking industry under Risk Management sector to maintain various applications tools data pipelines which have both upstream and downstream applications  Saved at least 7 hoursweek of team effort by automating manual business tasks using python pandas within first 3 months of joining the team  Strong experience in implementing various tables and schemas in Amazon Redshift DB Snowflake DB also worked on migrating various tables from Redshift to Snowflake DB  Organized several empathy sessions with business users and established brand new high impact Tableau dashboards along with improving existing dashboards as per new user requirement which received immaculate user response  Working knowledge of Amazon’s Elastic Cloud Compute EC2 infrastructure for computational tasks Simple Storage Service S3 as storage mechanism  Managed timely flow of business intelligence information to users  Collected tracked and evaluated current business and market trend data  Proven ability to manage all stages of project development Strong Problem solving skills and Analytical skills and abilities to make balanced and independent decisions           Capital One      Data Analyst   City     STATE                   042019      082021     Involved in analysis design and documenting business reports such as Executive summaries Scorecards and drilldown reports  Worked on performance tuning and Query Optimization for increasing the efficiency of scripts  Developed monitored the workflows and responsible for performance tuning of staging and 3NF workflows  Analyzing and profiling data returned for data integrity and business decisions  Responsible for migrating legacy reports to a new platform by rebuilding them to meet current business requirements  Audited internal data and processes to identify and manage initiatives improving business performance  Assisted integration of internal and external data tools and products maintaining stability and performance across systems  Provided technical support for existing reports dashboards or other tools  Maintained or updated business intelligence tools databases or dashboards  Disseminated information regarding tools reports or metadata enhancements  Communicated with customers competitors and suppliers to stay abreast of industry or business trends           Dollar Shave Club      Data AnalystProduction Support Analyst   City     STATE                   052018      032019     Used JIRA Agile methodology extensively to track day to day scrum activities  Writing SQL scripts for selecting the data from servers and modifying data as need with python pandas and stored back to different data base servers Organized and facilitated sprint planning daily standup meetings Scrum of Scrum Sprint review Sprint retrospectives and other Scrumrelated meetings  Writing SQL scripts for selecting the data from servers and modifying data as need with python pandas and stored back to different data base servers Organized and facilitated sprint planning daily standup meetings Scrum of Scrum Sprint review Sprint retrospectives and other Scrumrelated meetings  Manipulating cleansing and processing data using python code and SQL  Troubleshooted Various production failures related to data loads  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Responsible for loading extracting and validation of client data Collected data from various databases to SQL server by extensively using Joins and Sub queries in SQL according to requirements of the dev team  Initiated daily status email to track the data loads in production environment', 'resume_html': 'none', 'skills': \"['Excel', 'pivot tables', 'SAS', 'data storage', 'office', 'data analysis', 'dashboard', 'SAS', 'Problem Solving', 'MS Project', 'explanation', 'Pivot tables', 'Waterfall', 'dashboard', 'SAS', 'Excel', 'MS Access', 'SAS', 'Excel', 'MS Access', 'MS Access', 'Regularization', 'MS Access', 'Regularization', 'MS Excel', 'Pivot tables', 'SAS', 'MS Excel', 'Pivot tables', 'SAS', 'SAS', 'Excel', 'SAS', 'Excel', 'Business Objects', 'SQL', 'Business Objects', 'SQL', 'SQL', 'Query Builder', 'SQL', 'Query Builder', 'MS Visio', 'SharePoint', 'OneNote', 'Office', 'MS Visio', 'MS SharePoint', 'MS OneNote', 'Apple Sheets', 'Apple Pages', 'Open Office', 'MS PowerPoint', 'MS PowerPoint', 'MS Word', 'MS Word', 'MS Excel', 'MS Excel', 'Programming', 'SQL', 'SAS', 'Java', 'VBNet', 'Programming', 'SQL', 'SAS', 'Java', 'VBNet', 'Windows', 'Android', 'MS Project', 'Access', 'Access', 'reporting', 'Java', 'VBnet', 'performance analysis', 'VBNet', 'Business Objects', 'Java', 'access', 'Access', 'MS Access', 'MS Excel', 'Excel', 'Office', 'MS PowerPoint', 'MS Project', 'SharePoint', 'Windows', 'MS Word', 'analysis', 'Pivot tables', 'reporting', 'SAS', 'SQL', 'Visio']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/sr-data-engineer-architect-898ff76e69d44c7c964261bd55776363', 'id': '305667889169402374245195369321522535673', 'data': 'Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary      Seasoned Data Architect adept at understanding mandates developing plans and implementing enterprisewide solutions Complex problemsolver with an innovative approach Ready to bring 1Oplus years of progressive experience and take on a challenging new role with growth potential        Skills           Data Management  Organizational Skills  Critical Thinking  Team Management  Problem Resolution  Customer Service      Relationship Building  Team Building  Supervision  Leadership  Planning  Organizing  Friendly Positive Attitude                       Experience       Sr Data Engineer   Architect        082016   to   Current     Honeywell    –    Baltimore     MD             Used statistical software to analyze and process large data sets  Followed industry innovations and emerging trends through scientific articles conference papers or selfdirected research  Distilled data to devise solutions related to budgeting staffing and marketing decisions  Recommended data analysis tools to address business issues  Provided global thought leadership in analytics solutions to benefit customers  Captured and shared bestpractice knowledge amongst developers community  Cleaned and manipulated raw data  Created graphs and charts detailing data analysis results  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Developed new functions and applications to conduct analyses  Assisted solution providers with definition and implementation of technical and business strategies  Tested validated and reformulated models to foster accurate prediction of outcomes  Contributed to maintaining AzureSQL SQL Sever and DB2 databases in conjunction with data development and software engineering teams  Adept in troubleshooting and identifying current issues and providing effective solutions  Promoted customer success in building and migrating applications software and services on Azure platform  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Sr ETL Developer        012016   to   082016     Millennium Health    –    City     STATE             Assessed code during testing stage to determine potential glitches and bugs  Conferred with project managers and other stakeholders to fully understand software design specifications and plan optimal development approaches  Employed integrated development environments IDEs  Devised automation backup and recovery protocols to preserve and safeguard data  Analyzed user needs and software requirements to determine design feasibility  Collaborated with support team to assist client stakeholders with emergent technical issues and develop effective solutions  Applied security measures into systems development supporting final products resistance to intrusion and exploitation  Designed customized technical solutions to meet functional specifications outlined by Millennium Health database customers  Worked closely with systems analysts engineers and programmers to understand limitations develop capabilities and resolve software problems  Collaborated with Architects to define data extraction methodologies and data source tracking protocols  Utilized established design patterns to expedite novel software creation and support consistent performance results  Integrated objectoriented design and development techniques into projects to support usability goals  Analyzed code and corrected errors to optimize output  Identified opportunities for process improvements to decrease in support calls  Defined and documented SSIS ETL data mapping plans using Windows PowerShell scripting and custom solution from vendors ie Pragmatic Works  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes  Programmed applications and tools using objectoriented languages with goals of code abstraction stability and reuse  Assisted in User Acceptance Testing for Millennium customers verifying ETL jobs complied with assigned parameters achieving success during execution phases  Combined rootlevel authentication and authorization technologies with ongoing system design to harden finished solutions  Applied prescribed policies to programming syntax in compliance with internal language policies  Modified existing software to correct errors adapt to newly implemented hardware or upgrade interfaces  Leveraged Agile methodologies to move development lifecycle rapidly through initial prototyping to enterprisequality testing and final implementation  Developed software for embedded systems coding solutions for both new installations and insitu hardware  Performed troubleshooting of postrelease software faults to support live service and installed software patch design  Developed requirements for system modifications and new system installations  Recommended improvements to facilitate team and project workflow  Coordinated testing and validation procedures through software development lifecycle  Managed endtoend operations of ETL data pipelines maintaining uptime of 96  Improved and corrected existing software and system applications  Applied Conceptual Logical and Physical  DimensionalRelational model designs to ETL tasks           Sr Database App   BI Developer       052013   to   102015     TransCanada Corporation    –    City     STATE             Analyzed and developed technical and functional specifications for databases  Developed and updated all documentation related to database technologies for department  Built integrations from multiple data sources including Salesforce and Pardot  Identified databases not reaching peak performance and determined ways to solve concerns  Applied various skills to evaluate design implement and optimize databases and database applications  Managed financial management systems customer and production databases and inventory production equipment and editing systems  Provided support to clients in understanding and manipulating data to obtain value through SQL and ETL technical processes and visual analytics tools  Managed all levels of internal analytics practice including ETL database administration report development and integration  Produced complex database project with zero issues due to effective troubleshooting  Developed designed and optimized data structures for analysis  Partnered with project management teams on development of scope and timelines  Assisted clients in understanding and manipulating data to gain value through SQL and ETL technical processes and visual analytics tools  Developed data models and database designs to plan projects  Developed and implemented security initiatives to protect important company data  Constructed database and warehouse streamlined disparate data sources and unverified queries into main source  Wrote scripts and processes for data integration and bug fixes  Planned designed and streamlined data structures for analysis  Supervised all levels of internal analytics practice including ETL database administration report creation and integration  Built database and warehouse including consolidating disparate data sources and unverified queries into central source  Created integrations from various data sources including Salesforce and ERD Systems           Database Application Developer        022011   to   042013     Barclays Plc    –    City     STATE             Modified existing software to correct errors adapt to newly implemented hardware or upgrade interfaces  Utilized established design patterns to expedite novel software creation and support consistent performance results  Employed integrated development environments IDEs  Developed logic flowcharts and diagrams to use in program coding and workflow planning  Developed software for embedded systems coding solutions for both new installations and insitu hardware  Identified opportunities for process improvements to decrease in support calls  Analyzed code and corrected errors to optimize output  Liaised with clients to clarify business challenges and objectives to optimize performance of existing systems  Trained and coached new hires and junior developers and shared insight into ways to meet tight deadlines and improve overall efficiency  Coordinated testing and validation procedures through software development lifecycle  Combined rootlevel authentication and authorization technologies with ongoing system design to harden finished solutions  Identified debugged and fixed system bottlenecks and problems  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes  Devised automation backup and recovery protocols to preserve and safeguard data  Applied innovative approaches to application design through creative inception and planning  Contributed to requirements gathering and design development meetings  Improved and corrected existing software and system applications  Leveraged Agile methodologies to move development lifecycle rapidly through initial prototyping to enterprisequality testing and final implementation  Assessed code during testing stage to determine potential glitches and bugs  Worked closely with systems analysts engineers and programmers to understand limitations develop capabilities and resolve software problems  Conferred with project managers and other stakeholders to fully understand software design specifications and plan optimal development approaches  Applied prescribed policies to programming syntax in compliance with internal language policies  Recommended strategies to maximize performance and lifespan of equipment involved in software installations  Developed requirements for system modifications and new system installations  Performed testing on user defined functions and triggers  Utilized best practices to identify and remedy bugs in applications within specific timeframe  Recommended improvements to facilitate team and project workflow  Optimized application process flow to improve performance  Programmed applications and tools using objectoriented languages with goals of code abstraction stability and reuse  Updated software upon release of vendor patches to mitigate vulnerabilities  Monitored equipment function to verify conformance with specifications  Analyzed user needs and software requirements to determine design feasibility  Applied application product support to contractors located internationally  Worked closely with brand and marketing teams across organizations to promote specific applications  Increased efficiency through task automation  Applied security measures into systems development supporting final products resistance to intrusion and exploitation  Assessed project scope to identify necessary requirements  Established clear system performance standards and wrote specifications  Collaborated with support team to assist client stakeholders with emergent technical issues and develop effective solutions  Integrated objectoriented design and development techniques into projects to support usability goals  Reviewed project requirements to identify customer expectations and resources needed to meet goals  Performed troubleshooting of postrelease software faults to support live service and installed software patch design          Education and Training       Bachelor of Science     Electrical Electronics Engineering     Expected in   072006     University Of Leicester      Leicestershire England           GPA               Accomplishments       Led team to achieve overhaul and enhancement of our Operational Data Store as well as upgrade to our Cognos Application earning recognition from upper management and financial reward  Negotiated with vendors saving company over US1M annually  Improved delivery of Data for data driven decisions by modernizing our applicationsystems realizing overall increase in customer satisfaction and cost efficiency         Activities and Honors       Member Alumni Association         Certifications       Certified Microsoft ProfessionalSQL Server 2008R2  2010  Certified Microsoft Azure Data Engineer  2020  Certified Microsoft Azure Architect  2020', 'resume_html': 'none', 'skills': \"['data analysis', 'reporting', 'user acceptance testing', 'Microsoft Access', 'Microsoft Excel', 'SAS', 'SQL Server', 'Cognos', 'Crystal', 'Business Objects', 'SQL', 'Tableau', 'Project Management', 'Data Analysis', 'Data Quality', 'Monitor', 'HIPAA', 'process analysis', 'SAS', 'SQL', 'MS Office', 'excel', 'access', 'claims analysis', 'access', 'project planning', 'testing', 'business requirements', 'reporting', 'analysis', 'reconciliation', 'Microsoft Access', 'reporting', 'Access', 'processing', 'Microsoft Access', 'Project Management', 'Project Management', 'Business Objects', 'Cognos', 'Crystal', 'Data Analysis', 'Microsoft Access', 'Microsoft Excel', 'Microsoft Office', 'Process Analysis', 'Project Management', 'Project Planning', 'User Acceptance Testing', 'Reporting', 'SAS', 'Reporting', 'SQL', 'Tableau', 'Tableau']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/aws-data-engineer-5cc9299938587076e54cae645ae21ec1', 'id': '83985006356839563643115453489150250520', 'data': 'Jessica    Claire                                 609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor   Home   555 4321000        Cell           resumesampleexamplecom                  Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases         Skills          ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                      Experience      012022   to   022022     AWS Data Engineer      Deloitte    –    Rosslyn     MA             Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Used Spark SQL for Scala  amp Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process  Configured computing networking and security systems within cloud environment  Implemented cloud policies managed technology requests and maintained service availability          012016   to   112019     Data Engineer      Verizon Communications    –    Irving     TX             Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts          102013   to   122015     Big Data Engineer  Hadoop Developer      Cumming Llc    –    Boston     MA           AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes          092012   to   102013     Python Developer       Fiserv    –    City     STATE             AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes          Education and Training      Expected in   022022     Post Graduate      Data Engineering      Purdue University      West Lafayette     IN     GPA               Expected in   092021     Post Graduate      Data Science And Business Analytics      University of Texas At Austin      Austin     TX     GPA               Expected in   122009     Bachelor of Arts     Business Administration And Management     Califonia State University       Fullerton CA          GPA', 'resume_html': 'none', 'skills': \"['Designing', 'Testing', 'monitoring', 'Designing', 'testing', 'Analysis', 'Reporting', 'Risk Assessment', 'Statistical Analysis', 'Problem Solving', 'Modeling', 'Project Management', 'Rapid Application Development', 'Adhoc testing', 'Backend testing', 'Reporting', 'Critical Thinking', 'Decision Making', 'Microsoft Access', 'Microsoft Excel', 'MS Office', 'Microsoft Project', 'PowerPoint', 'SharePoint', 'SQL', 'Tableau', 'Spotfire', 'Visio', 'CRM', 'SQL Server', 'AWS', 'Agile', 'data analysis', 'business requirements', 'design', 'developing', 'SQL', 'Designing', 'Drupal', 'Data Warehouses', 'research', 'cloud', 'MS project', 'analysis', 'Cisco', 'design', 'testing', 'technical documentation', 'Microsoft Project', 'designing', 'developing', 'Big Data', 'HDFS', 'access', 'access', 'project', 'MS project', 'research', 'Agile', 'Big Data', 'business analysis', 'CRM', 'data analysis', 'Data Warehouses', 'Designing', 'Drupal', 'access', 'Microsoft Access', 'Microsoft Excel', 'MS Office', 'PowerPoint', 'MS project', 'Microsoft Project', 'SharePoint', 'SQL', 'SQL Server', 'Tableau', 'technical documentation', 'Visio', 'Visual Studio']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/manager-student-and-resident-data-3e141e5ce83341f0b9b1ba584e2a5912', 'id': '278624286922027916746373616168944850452', 'data': 'Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary      Management professional seeking a director position  which allows me to utilize my strong administrative and technical experience managerial and interpersonal skills and quality assurance initiatives to maximize efficiency data procurement and customer satisfaction Resultsoriented handson professional with a highly successful record of accomplishments in quality process improvement survey management communications and career development             Key Skills         Continuous Quality Improvement Measures DMAIC  Survey Management  Data Analysis  Database Management      Customer Service  Outcomes Projections  Performance Tracking and Evaluation  Team Building                     Education       Indiana University               Expected in        –      –       Bachelor of Arts        Theater and Drama          GPA            Minor in English  Member Phi Beta Sigma           Accomplishments      Personally facilitated the creation of a new Graduate Medical Education GME Census online data application This included serving as a liaison between the American Medical Association AMA and the Association of American Medical Colleges AAMC creating and testing online technical design and functionality establishing new procedural standards for quality assurance and implementing new marketing constructs  Created all publishing promotional and help manuals for the Census with both an electronic delivery and hardcopy format  Since established in 2002 Census response rates have consistently grown by 20 to their present mark of 98    Spearheaded and supervised the collection monitoring and quality control of GME data from over 8900 US residency programs and 160000 residency training segments This includes personally maintaining multiple Access databases and coordinating and documenting all process flows between multiple departments and organizations Coordinated and participated  in all annual design and development changes of GME Track with the AMA GME Department and any AAMC staff Improved and streamlined processes which coincided with staff reductions of 70 due to reorganizations and departmental reconstructions  Despite losing personnel improved productivity and results by over 60    Directly took over and managed all student data procurement and quality without  increasing staff or resources despite these tasks being formerly held by a separate unit  Led multiple process reviews and implemented new standards which reduced the amount of manual processing by 70    Asked to be on multiple crossdepartmental and creative teams formed to address a variety of issues including departmental skills matrix development contract process improvement organizational strategic planning and various departmental improvement initiatives   Created a departmental electronic forum to capture common questions and encourage process review This forum led to several policy and process adjustments by the data and business divisions        Experience       American Medical Association      Manager Student and Resident Data   City     STATE                   112000      Current     Hired and developed staff on the use of the AMA Graduate Medical Education GME Database AMA physician Masterfile GME Track Online Census and departmental procedures  Provided professional and highly regarded GME Census customer service for all content and technical questions from both external and internal customers  Became the leader for all GMErelated data questions within the Division used consistently by divisional executives  Spearheaded and supervised the collection monitoring and quality control of all GME data  Managed all student data procurement and quality  Served as PC Coordinator providing PC technical and application support participating with rollouts of critical updates and testing new software and services as they applied to the department  Encouraged the AMA to become members of the American Society for Quality ASQ Became the direct contact between the AMA and ASQ facilitating organizational and departmental process improvement and creating crossdivisional teams to address organizational deficiencies and issues  Conducted cost schedule contract performance variance and risk analysis  Offered feedback to executivelevel management on the effectiveness of strategies selling programs and initiatives  Coached and mentored staff members by offering constructive feedback and taking interest in their longterm career growth           American Medical Association      Survey Management Specialist   City     STATE                   091998      112000     Collected and monitored large quantities of GME data from over 8000 residency programs and 100000 residents                                Provided ongoing quality control of all data received through direct use upkeep and modification of Access programming   Created and helped to implement powerful survey applications that serve to track update and collect data as well as insure a high response rate and data accuracy   Participated directly in training all new employees and temporary staff on the GME Database AMA physician Masterfile and departmental procedures   Provided constant customer service through phone support for all survey questions as well as questions regarding GME                   Skills      10 year advanced experience with Customer service Database development and integration Team leadership and Professional Development Survey marketing and creation and MS Access  Concentration on Process Improvement Quality quality assurance quality control Six Sigma and strategic planning', 'resume_html': 'none', 'skills': \"['Tableau', 'developing', 'Tableau', 'Tableau', 'dashboards', 'Tableau', 'Tableau', 'Tableau', 'Tableau', 'Tableau', 'design', 'monitoring', 'data profiling', 'BI', 'teams', 'Tableau', 'Windows', 'Active Directory', 'SQL', 'PLSQL', 'XML', 'Python', 'programming', 'Data modeling', 'Tableau', 'SharePoint', 'Performance tuning', 'SQL', 'PLSQL', 'Performance tuning', 'Tableau', 'Tableau', 'Tableau', 'data blending', 'dashboard', 'data analytics', 'Tableau', 'Waterfall', 'Decision making', 'ETL', 'agile', 'Software Development Life Cycle', 'SDLC', 'testing', 'Data Analysis', 'Data Validation', 'Data Cleansing', 'Data Verification', 'data mismatch', 'DB2', 'Oracle', 'Teradata', 'SQL Server', 'data modeling', 'Erwin', 'Dimensional Modeling', 'Snowflake', 'Data warehouse', 'ETL', 'Snowflake', 'data modeling', 'data modeling', 'Erwin', 'Modeling', 'Snowflake', 'Normalization', 'Process Analysis', 'Data modeling', 'data modeling', 'BI', 'Tableau', 'ETL', 'SQL Server', 'Microsoft Office', 'Word', 'PowerPoint', 'Excel', 'Microsoft Project', 'Microsoft Office', 'Oracle', 'SQL Server', 'DB2', 'Teradata', 'Programming', 'C', 'C', 'HTML', 'SQL', 'PLSQL', 'XML', 'Python', 'C', 'Data modeling', 'ERwin', 'ER Studio', 'MS Visio', 'SQL', 'Tableau', 'datasets', 'data modeling', 'time series analysis', 'modeling', 'MATLAB', 'programming', 'Collecting data', 'text mining', 'python', 'ETL', 'Oracle', 'Windows', 'VMware', 'Active Directory', 'Linux', 'Cisco', 'Exchange', 'MS Office', 'MS Visio', 'HTML', 'MS Access', 'SQL', 'JavaScript', 'AJAX', 'jQuery', 'Bootstrap', 'AngularJS', 'SQL Server', 'SSIS', 'SSRS', 'DB2', 'TSQL', 'Oracle', 'AspNet', 'C', 'JQuery', 'MATLAB', 'Cognos', 'MS Office', 'MS Visio', 'HTML', 'UML', 'C', 'Java', 'Visual Basic', 'VB', 'Windows XP', 'MS Access', 'SQL', 'NET', 'Agile', 'Tableau', 'Alteryx', 'Develop', 'Tableau', 'dashboards', 'Tableau', 'BI', 'project', 'design', 'develop', 'visualizations', 'POC', 'dashboards', 'dashboards', 'PLSQL', 'Design', 'Tableau', 'reporting', 'Business Intelligence', 'data analytics', 'visualization', 'reporting', 'OLAP', 'Processing', 'data mining', 'SQL', 'UNIX', 'Linux', 'Java', 'Tableau', 'Active Directory', 'Tableau', 'Design', 'develop', 'dashboards', 'Tableau', 'Access', 'transform', 'datasets', 'Develop', 'Tableau', 'data sources', 'Data Blending', 'design', 'develop', 'BI', 'design', 'data warehouse', 'visualization', 'big data', 'Design', 'visualizations', 'Dashboard', 'Cognos', 'Cognos', 'Tableau', 'business requirements', 'design', 'Reporting', 'reporting', 'Cognos', 'SQL', 'Data modeling', 'Design', 'CSS', 'HTML', 'JavaScript', 'HTML5', 'CSS', 'JavaScript', 'JQuery', 'Angularjs', 'Tableau', 'Tableau', 'HTML5', 'CSS', 'JavaScript', 'JQuery', 'Angularjs', 'Java', 'PLSQL', 'Windows', 'Oracle', 'Crystal', 'Excel', 'JAVA', 'Tomcat', 'MS Visio', 'SQL', 'Teradata', 'Cognos', 'BI', 'Tableau', 'Tableau', 'dashboard development', 'Tableau', 'data analysis', 'Statistics', 'Dashboard', 'JavaScript', 'HTML5', 'jQuery', 'SASS', 'AngularJS', 'database design', 'normalization', 'OLAP', 'analysis', 'Cognos', 'dashboards', 'Tableau', 'Cognos', 'reporting', 'Tableau', 'JavaScript', 'DML', 'SQL', 'visualizations', 'Tableau', 'data quality', 'access', 'SSIS', 'C', 'SSIS', 'PLSQL', 'SSIS', 'load', 'SSIS', 'data validation', 'loading', 'Data warehouse', 'SQL', 'Teradata', 'BI', 'tableau', 'reporting', 'loading', 'Business Intelligence', 'Tableau', 'Tableau', 'dashboards', 'R', 'data blending', 'dashboard', 'design', 'data modeling', 'Tableau', 'Tableau', 'Tableau', 'REST', 'Sharepoint', 'Exchange', 'transformation', 'code reviews', 'ETL', 'design', 'data processing', 'performance tuning', 'Tableau', 'SQL server', 'Oracle', 'SAP', 'design', 'SAP', 'designing', 'Business Objects', 'Crystal', 'Business Objects', 'reporting', 'Business Objects', 'SQL Server', 'SSIS', 'ETL', 'load', 'SQL Server', 'data quality', 'SQL Server Reporting Services', 'SSRS', 'Reporting', 'Business Intelligence', 'design', 'develop', 'Windows', 'Active Directory', 'SAP', 'SAP', 'Business Objects', 'PLSQL', 'SQL server', 'Office', 'Oracle', 'Windows', 'Excel', 'MS SQL Server', 'Active Directory', 'Oracle', 'Windows', 'VMware', 'Linux', 'Cisco', 'VMware', 'Exchange', 'MS Office', 'MS Visio', 'HTML', 'MS Access', 'JavaScript', 'AJAX', 'jQuery', 'Bootstrap', 'SASS', 'AngularJS', 'project', 'Develop', 'Project', 'Project', 'Business Requirement', 'Data Analysis', 'Data Mapping', 'design', 'designing', 'Data Validation', 'Data Modeling', 'Data Analysis', 'MS Visio', 'Modeling', 'Data Warehousing', 'monitoring', 'reporting', 'Tableau', 'Tableau', 'NET', 'AspNet', 'Active Directory', 'Agile', 'AJAX', 'reporting', 'big data', 'BI', 'Business Intelligence', 'Business Objects', 'C', 'C', 'Cisco', 'Cognos', 'Cognos', 'Crystal', 'CSS', 'Data Analysis', 'data processing', 'data mining', 'Data Modeling', 'Data Validation', 'data warehouse', 'Data Warehousing', 'designing', 'DML', 'ERwin', 'ETL', 'XML', 'modeling', 'HTML', 'HTML5', 'DB2', 'Java', 'JavaScript', 'JAVA', 'JQuery', 'JQuery', 'Linux', 'MATLAB', 'Access', 'MS Access', 'C', 'MS Excel', 'Excel', 'Exchange', 'Exchange', 'Microsoft Office', 'MS Office', 'Office', 'PowerPoint', 'Microsoft Project', 'Sharepoint', 'Windows', 'Windows XP', 'Word', 'Modeling', 'OLAP', 'Oracle', 'Oracle', 'PLSQL', 'PLSQL', 'Programming', 'Python', 'database design', 'reporting', 'SAP', 'SAP', 'SAP', 'Sql', 'MS SQL Server', 'SQL', 'SQL Server', 'SQL Server', 'Statistics', 'Tableau', 'Teradata', 'Tomcat', 'TSQL', 'UML', 'UNIX', 'VB', 'Visio', 'Visual Basic', 'Windows']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-specialist-3e8b2e5124574d3081485139c3bd36f3', 'id': '6361630242691360309919431676680490692', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market       Skills           Performance driven  10years combined experience in customer service patient services entry processing and scheduling  Microsoft Office  Excel intermediate to advanced Word intermediate Outlook advanced  PowerPoint intermediate  Typing at a speed of 100 wmp 10 key                         Experience       Data Entry Specialist       032016      Present     Elara Caring    –    Harker Heights     TX            Compile sort and verify the accuracy of data before it is entered  Compare data with source documents or reenter data in verification format to detect errors  Store completed documents in appropriate locations  Locate and correct data entry errors or report them to supervisors  Ensures that all invoices are completely entered in a timely and accurate manner  On assigned days all mail is broken down sorted in an accurate efficient and timely manner  Ensures that on assigned days mail is scanned in an accurate efficient and timely manner           Administrative Assistant       112015      022016     Qualtek    –    Pittsburgh     PA            Answer telephones and give information to callers take messages or transfer calls to appropriate individuals  Operate office equipment such as fax machines copiers and phone systems and use computers for spreadsheet word processing database management and other applications  Greet visitors or callers and handle their inquiries or direct them to the appropriate persons according to their needs  Set up and maintain paper and electronic filing systems for records correspondence and other material  Open read route and distribute incoming mail or other materials and answer routine letters  Complete forms in accordance with company procedures  Make copies of correspondence or other printed material  Compose type and distribute meeting notes routine correspondence and reports  Maintain scheduling and event calendars  Schedule and confirm appointments for clients customers or supervisors  Order and dispense supplies  Provide services to customers such as order placement or account information  Coordinate conferences and meetings  Operate electronic mail systems and coordinate the flow of information internally or with other organizations           SchedulerCustomer Service Representative       082015      022016     Eastern Metal Supply    –    Winston     FL            Obtain customers names addresses and billing information product numbers and specifications of items to be purchased and enter this information on order forms  Prepare invoices shipping documents and contracts  Inform customers by mail or telephone of order information such as unit prices shipping dates and any anticipated delays  Receive and respond to customer complaints  Check inventory records to determine availability of requested merchandise  Compute total charges for merchandise or services and shipping charges  Schedule appointments for customers needing repairs on their existing appliances           Receptionist       042015      082015     Laz Parking    –    Westminster     CO            Greet persons entering establishment determine nature and purpose of visit and direct or escort them to specific destinations  Transmit information or documents to customers using computer mail or facsimile machine  Hear and resolve complaints from customers or the public  Provide information about establishment such as location of departments or offices employees within the organization or services provided  Receive payment and record receipts for services  Schedule appointments and maintain and update appointment calendars  Keep a current record of staff members whereabouts and availability  Provide support for sales team in managing operation work flow  Demonstrate proficiency in telephone email fax and front desk reception within a high volume environment  Schedule appointments for appliances being delivered           Customer Service RepresentativeCashier       052014      042015     STOPNGO    –    City     STATE            Receive payment by cash check credit cards vouchers or automatic debits  Issue receipts refunds credits or change due to customers  Assist customers by providing information and resolving their complaints  Establish or identify prices of goods services or admission and tabulate bills using calculators cash registers or optical price scanners  Greet customers entering establishments  Answer customers questions and provide information on procedures or policies  Sell tickets and other items to customers  Process merchandise returns and exchanges  Maintain clean and orderly checkout areas and complete other general cleaning duties such as mopping floors and emptying trash cans  Stock shelves and mark prices on shelves and items  Request information or assistance using paging systems  Count money in cash drawers at the beginning of shifts to ensure that amounts are correct and that there is adequate change  Calculate total payments received during a time period and reconcile this with total sales  Monitor checkout stations to ensure that they have adequate cash available and that they are staffed appropriately  Supervise others and provide onthejob training  Keep periodic balance sheets of amounts and numbers of transactions           PullerProcessor       042013      122013     CTI PAPER    –    City     STATE            Read orders to ascertain catalog numbers sizes colors and quantities of merchandise  Update daily logs for tracking file movements  Obtain customers names addresses and billing information product numbers and specifications of items to be purchased and enter this information on order forms  Check inventory records to determine availability of requested merchandise  Review orders for completeness according to reporting procedures and forward incomplete orders for further processing  Confer with production sales shipping warehouse or common carrier personnel in order to expedite or trace shipments  File copies of orders received or post orders on records  Verify customer and order information for correctness checking it against previously obtained information as necessary  Prepare invoices shipping documents and contracts  Inform customers by mail or telephone of order information such as unit prices shipping dates and any anticipated delays  Compute total charges for merchandise or services and shipping charges           Patient Services AssistantTravel Coordinator       032005      012010     WILLIAM S MIDDLETON VETERANS MEMORIAL HOSPITAL    –    City     STATE            Coordinate communication between patients family members medical staff administrative staff or regulatory agencies  Interview patients or their representatives to identify problems relating to care  Maintain knowledge of community services and resources available to patients  Investigate and direct patient inquiries or complaints to appropriate medical staff members and follow up to ensure satisfactory resolution  Explain policies procedures or services to patients using medical or administrative knowledge  Answer applicants questions about benefits and claim procedures  Interview benefits recipients at specified intervals to certify their eligibility for continuing benefits  Compile record and evaluate personal and financial data in order to verify completeness and accuracy and to determine eligibility status  Utilize knowledge and skills of medical terminology for emergency department check ins admitting dictation records and eligibility  Coordinate admission processes and prepare agreement packets          Education and Training       High School Diploma              Expected in   Jun 1999                MADISON EAST HIGH SCHOOL      MADISON     WI     GPA        Status                  Bachelor of Arts       Business Administration       Expected in   Jun                ASHFORD UNIVERSITY FORBES SCHOOL OF BUSINESS      SAN DIEGO     CA     GPA        Status         Business Administration        Skills     10 key administrative Schedule appointments balance sheets benefits billing calculators cash registers catalog conferences contracts Make copies credit clients customer service data entry database management dictation electronic mail email facsimile machine fax machines fax filing financial forms inventory Prepare invoices Issue receipts letters notes managing mark materials medical terminology meetings Excel mail money Microsoft Office Outlook PowerPoint Word office equipment direct patient personnel phone systems copiers policies processes Read reception repairs reporting sales scanners scheduling shipping spreadsheet take messages telephone telephones Typing type word processing       Activities and Honors', 'resume_html': 'none', 'skills': \"['Oracle', 'MS Access', 'Redshift', 'S3', 'Software Products', 'Oracle', 'Oracle', 'Hadoop', 'Amazon Web services', 'Redshift', 'Programming', 'Oracle', 'PLSQL', 'SQL', 'Impala', 'SQL', 'Python', 'C', 'Visual Basic', 'MS Excel', 'VBA', 'Power Query', 'PLSQL Developer', 'Kibana', 'excel', 'Eclipse', 'MS Office', 'MS Project', 'MS Visio', 'Power Query', 'Excel', 'Putty', 'Tortoise SVN', 'GitHub', 'JIRA', 'Hadoop', 'Oracle', 'UNIX', 'Amazon Web Services', 'AWS', 'Redshift', 'business analysis', 'system analysis', 'design', 'Project management', 'Hadoop', 'HDFS', 'AWS', 'Oracle', 'Hive', 'Redshift', 'Hadoop', 'Oracle', 'Hadoop', 'data sources', 'XML', 'SDLC', 'analysis', 'design', 'testing', 'Regression', 'design', 'SQL', 'Stored Procedures', 'SQL', 'SQL', 'query optimization', 'Agile', 'Waterfall', 'Project management', 'Agile', 'Scrum', 'Oracle', 'UNIX', 'Analysis', 'Design', 'Estimation', 'Analysis', 'business requirements', 'develop', 'loading', 'UNIX', 'data science', 'data sets', 'reporting', 'Oracle', 'Analysis', 'Design', 'reporting', 'Oracle', 'UNIX', 'Analysis', 'Design', 'Estimation', 'Analysis', 'Requirement analysis', 'technical design', 'Technical design', 'optimization', 'monitoring', 'SQL', 'stored procedures', 'system testing', 'Integration testing', 'reporting', 'Agile', 'automation', 'Big Data', 'business analysis', 'C', 'Data Analysis', 'data modeling', 'Eclipse', 'XML', 'FTP', 'Java', 'MS Access', 'MS Excel', 'Excel', 'MS Office', 'MS Project', 'optimization', 'Oracle', 'Oracle', 'PLSQL', 'PLSQL', 'Programming', 'Project management', 'Python', 'reporting', 'Research', 'Scrum', 'SDLC', 'SQL', 'analysis', 'UNIX', 'Visio', 'Visual Basic', 'VBA']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/secretary-customer-service-sales-data-base-entry-46df311c156e477c8d7f5406e116de02', 'id': '147203991935502946470515237354155400269', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Career Overview      Dedicated personable and motivated secretarycustomer service representative My focus is to maintain customer satisfaction and contribute to company success I have extensive work experience in a variety of customer service settings such as hospitals businesses and retail         Core Strengths           Strong organizational skills  Active listening skills  Sharp problem solver  Courteous demeanor  Energetic work attitude  Telephone inquiries specialist  Customer service expert  Invoice processing  Adaptive team player  Openingclosing procedures  Quick learner   Have worked with many business specific computer programs       Data collection  Data entry  Documentation  Email  Internet research  Speaking  Telephone skills  Multitasking                        Accomplishments       Customer Service    Researched calmed and rapidly resolved client conflicts to prevent loss of key accounts    Customer Interface    Greeted customers upon entrance and handled all cash and credit transactions  Assisted customers over the phone regarding store operations product promotions and orders     Database Maintenance    Assisted in the managing of the company database and verified edited and modified customers’ information         Work Experience       SecretaryCustomer ServiceSalesData Base Entry       092012   to   062015     Taco Bell    –    Gravette     AR             Responsible for daily operations and overall finances of a small but busy satellite company which includes but is not limited to billing budgeting customer invoicing through QuickBooks payroll quarterly payroll and company taxes Knowledge in word and excel spreadsheets Created customer accounts revising as necessary  Developed highly empathetic client relationships  Computed accurate sales prices for purchase transactions  Resolved product issues and shared benefits of new technology  Expressed appreciation and invited customers to return  Managed quality communication and customer support for each client  Interacted with customers to followup on shipping statuses and expedited orders  Promptly responded to general inquiries from members staff and clients via mail email and fax  Guaranteed positive customer experiences and resolved customer complaints  Executed outbound calls to existing customer            Manager       2011   to   042012     Smart Cow    –    City     STATE             Managed team of 34 employees  Served as mentor to junior team members  Took necessary steps to meet customer needs and effectively resolve food or service issues  Communicated clearly and positively with employees  Resolved customer complaints in a postive manner  Assisted in important decisions on new products and new employee hire   Quickly and efficiently processed payments and made accurate change  Closely followed standard procedures for safe food preparation assembly and presentation to ensure customer satisfaction  Performed general maintenance duties including mopping floors washing dishes wiping counter tops and emptying traps           SecretaryCNA in MomBaby Unit       082009   to   112011     Exempla Lutheran Medical Center    –    City     STATE              CNA     Took vital signs of mothers and newborns for a floor of up to 20 patients per shift  Took and recorded patients temperature pulse and blood pressure  Performed 24 hr infant testing including PKU hearing and jaundice  Worked as part of team to ensure proper care and safety of myself  and  patient and newborns  Assisted physicians with the circumcision of newborns  Accurately identified patients with patient chart by verbalizing and checking patient bracelet  Cleaned and sterilized instruments and disposed of contaminated supplies     Secretary     Accurately documented all elements of inpatient information discharge instructions and followup care  Managed smooth and effective communication among physicians patients families and staff  Handled incoming and outgoing correspondence including mail email and faxes  Screened telephone calls and inquiries and directed them as appropriate  Devised and maintained office systems to efficiently deal with paper flow  Organized personal and professional calendars and supplied reminders of upcoming meetings and events when necessary  Flexible and trained to fillin as secretary in sister units such as Labor and Delivery  and  NICU  Actively maintained strict confidentiality and safeguarded all patientrelated information with HIPPA knowledge    Well trained in hospital specific computer programs such as Epic and CPN            Educational Background       Obtained Cosmetology License      Cosmetology     Expected in   2012     Empire Beauty School      Arvada     CO     GPA                Obtained CNA License     Nursing     Expected in   2009     Front Range Community College      Denver     CO     GPA                           Expected in   2008     Community College of Denver      Denver     CO     GPA        Completed necessary courses that contributed to nursing career          Obtained high school diploma     Basic     Expected in   2004     Arvada High School      Denver     CO     GPA                Skills       Patientfocused care  Excellent interpersonal skills  Compassionate and trustworthy caregiver  Detailoriented  Effectively interacts with patients and families  Medical terminology  Charting and record keeping', 'resume_html': 'none', 'skills': \"['Analysis', 'Reporting', 'SSRS', 'Crystal', 'CRM', 'technical documentation', 'project management', 'data quality', 'CRM', 'Excel', 'Excel', 'Visual Basic', 'technical documentation', 'project designs', 'SSRS', 'Crystal', 'Pivot table', 'VLOOKUP', 'Scrum', 'teams', 'Test Cases', 'SharePoint', 'assembly', 'technical assistance', 'assembly', 'gateway', 'analyzing data', 'business intelligence', 'business decisions', 'Pivot Table', 'Microsoft Excel', 'data integrity', 'storage', 'Technical Documentation', 'Design', 'UML', 'ERD']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-specialist-36e2b426632b41d08cf146119467bc1c', 'id': '97098914450695563702884660921271152925', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary     \\xa0Meticulous and detailoriented Executive AssistantOffice Administrator adept at implementing innovative practices and procedures to improve efficiency Organized and implemented an efficient work flow system that resulted in 30 increase in productivity Established good working relationships with students faculty staff and members of the public Selected to assume additional duties as Recording Secretary for committee meetings averaging 20 attendees \\xa0Executive Assistant with management experience and exceptional people skills Versed in  communication  and  organization  Desires a challenging role as a teamplayer        Core Qualifications         Microsoft Office Suite Google Docs Datatel Excels in area pf details\\xa0  Resultsoriented     Proficiency in supervision   \\xa0  Operations management   Clientfocused                       Experience       Data Entry Specialist       092015      102015     Elliot Davis    –    Nashville     TN            Prepared source data for computer entry by compiling and sorting information establishing entry priorities  Processed employee and account source documents by reviewing data for deficiencies resolving discrepancies by using standard procedures or returning incomplete documents to the team leader for resolution  Entered employee and account data by inputting alphabetic and numeric information on keyboard or optical scanner according to screen format  Maintained data entry requirements by following data program techniques and procedures           Executive Assistant       022015      072015     Xl Group    –    Miami     FL            Reviewed and provided comments on the adequacy of documents and took necessary steps to cure any deficiencies       Effectively controlled the release of proprietary and confidential information for general client lists  Represented Administrator through communication both verbal and electronic as well as in person  Extensive planning and preparation of meetings and events  Extensive creation and maintenance of records  Worked in liaison with College Deans offices organize faculty interviews  Organized coordinated and maintained Administrators calendar  Assisted prospective faculty with house hunting trips  Organized and coordinate prospective faculty moves  Arranged meetings with top level administrators  Coordinated travel arrangements for Administrator  Processed Sabbatical applications and various reports Processed faculty overloads and tuition reimbursements applications  Managed use and reconciled Administrators credit cards           Administrative AssistantOffice Administrator       2011      2015     Soriano Tax Services    –    City     STATE              Prepared correspondence accounting and financial documents for analysis  Provided onsite training      Provided tax preparation services represented clients before IRS resolved outstanding tax issues  Responded to inquiries maintain office database and files  Maintained and processed confidential information  Followed up on client inquires  Heavy telephone sales to secure clients           Administrative Secretary Vice President       082008      072011     California State University    –    City     STATE          Full time student at California State University Northridge majoring in Sociology         Office of Vice President       061997      032007     East Los Angeles College    –    City     STATE            Managed the daily operations of the Office of Academic Affairs  Prepared and distributed agendas of various committee meetings  Took transcribed and distributed minutes of various committee meetings  Maintained daily calendarschedule of administrator  Managed communication flow and work flow to administrators office  Established cross training system that improved efficiency of department by 50  Supervised trained clerical staff provided key input on performance evaluations Managed confidential personnel records  Served as official liaison between administrator and campus staff students and members of the public  Initiated coordinated and monitored hiring process for faculty and administrators  Composed correspondences reports presentations Proof read documents reports press releases for accuracy  Provided research support for reports and meetings  Communicated and implemented District policies  Monitored and tracked departments budget expenditures  Managed official travel system processed expense reports and reimbursements  Facilitated website updates on Office of Academic Affairs working in liaison with webmaster          Education       Bachelor of Arts       Sociology       Expected in   2011                California State\\t\\tCollege of Behavioral Sciences      Northridge     CA     GPA        Status           Coursework in  Sociology      NonProfit  Humanitarian Work   Assisted in the coordination of fund raising campaigns for Doctors without Borders St Josephs Indian School and Oxfam International         Certifications     EDD Certification 73 words per minute with 98 accuracy Reeswood Secretarial College Certificate in Shorthand 120 wpm Graduated with honors in the program       Interests     Volunteered at Lighthouse for Women and Children a homeless shelter in Oxnard California       Languages     French Basic Spanish Basic       Skills     Academic budget clerical credit client clients data entry database expense reports French fund raising hiring keyboard team leader meetings Microsoft Office Suite Office 98 personnel policies presentations press releases Profit read research sales scanner Secretarial Shorthand sorting Spanish \\xa0tax preparation telephone travel arrangements website       Additional Information       Volunteered at Lighthouse for Women and Children a homeless shelter in Oxnard California', 'resume_html': 'none', 'skills': \"['STATA', 'programming', 'R', 'programming', 'SAS', 'programming', 'GIS', 'NVIVO', 'Microsoft Office', 'analytical research', 'Project', 'Evaluation', 'statistical models', 'Research', 'data collection', 'data quality', 'statistical methods', 'analysis', 'Designing', 'data collection', 'data collection', 'Designing', 'GIS', 'Microsoft Office', 'programming', 'research', 'SAS', 'STATA']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-operator-3586d5320e6848b198a737749a7eba1b', 'id': '138536846257210096623495337605137733129', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary       Committed and motivated Administrative Assistant with exceptional customer service and decision making skills Strong work ethic professional demeanor and great initiative  Energetic and reliable Office Manager skilled with working with a diverse group of people  Executive Assistant who is skilled at multitasking and maintaining a strong attention to detail Employs professionalism and superior communication skills to meet client and company needs          Areas of Expertise           Word Excel Access Word Perfect  Operations management  Communication   Interpersonal       Time management  Flexible  Works well under pressure  Employee training and development                       Work Experience       Data Entry Operator       032014      082014     Iron Mountain Incorporated    –    Fort Myers     FL            Performed general data entry using SAP Microsoft Excel and Word    Performed a wide variety of secretarial tasks in support of the business   Answered phones and create notifications in the system   Contacted with internal and external customers  Collaborated with other administrative team members human resources and the finance department on special projects and events  Developed and managed thirdtier resolution process to resolve issues originating from the customer retention team           Secretary       2010      2013     Walt Disney Co    –    Auburn Hills     MI            Arranged appointments sales calendars trainings for the sales of department  Maintained the operations sales database  Customized sales reports and sales literature  Verified and logged in deadlines for responding to daily inquiries  Improved communication efficiency as primary liaison between departments clients and vendors           Assistant Manager       2008      2010     Sumitomo Electric Group    –    Mount Prospect     IL            Recruited hired scheduled and motivated a staff of up to 8 people  Adapted in communicating effectively with customers vendors and staff  Reached the monthly goals  Managed the daytoday tactical and longterm strategic activities within the business  Reduced and controlled expenses by improving resource allocation          Education       Associate       Arts       Expected in                   Community College of Philadelphia      Philadelphia     PA     GPA        Status          Arts          Certified with diploma              Expected in   1 2013                Notary Public                GPA        Status                 Professional Affiliations              Languages     English Spanish       Skills      Interpersonal data entry database English Languages Access Microsoft Excel Excel Word sales SAP secretarial Spanish phones Word Perfect', 'resume_html': 'none', 'skills': \"['business processes', 'data integrity', 'SAP', 'SAP', 'processing', 'budget analysis', 'resource analysis', 'Crystal', 'analyzing data', 'SAP', 'analysis', 'SAP', 'SAP', 'MicroSoft Office', 'Excel', 'Word', 'PowerPoint', 'Outlook', 'Visio', 'Access']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-customer-service-357f0974fe214cdfad27461bf1b36f96', 'id': '321556423433782918032074297826301460362', 'data': 'JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary       Energetic and reliable Retail Sales Associate skilled in highend merchandise environments  Personable and responsible Cashier with 5 years in retail and customer service Solid team player with upbeat positive attitude  Resultsdriven with proven ability to establish rapport with clients  Dedicated Customer Service Representative motivated to maintain customer satisfaction and contribute to company success         Highlights          Problem resolution  Selfstarter  Deadlineoriented  Microsoft Office  Employee training and development  Customer service expert  Openingclosing procedures  Telecommunication skills      Strong organizational skills  Active listening skills  Large cashcheck deposits expert  Energetic work attitude  Top sales performer  Invoice processing  Courteous demeanor  Sharp problem solver                       Experience        072013   to   Current   Data Entry Customer Service    A Duie Pyle Inc         Stoughton     MA           •Performed general clerical duties as needed such as completing forms and reports  •Confered with Assembly Technician Lead and Manager to determine progress of work and to provide order status  •Assembled various components within established assembly time standards while adhering to procedures and set specifications  •Used Method Instructions to assemble equipment  •Inspect parts to ensure that quality standards are maintained  •Provide responsibility for the quality of work  •May cross train to perform other duties on the production lines  •Performs simple calculations  •Clean and maintain work area  •Will be required to wear personal protective equipment relevant to work area            Other duties as needed and assigned           072010   to   062013   Data Entry Clerk    Ferguson         Red Bank     NJ           •Received and scans large stacks of documents with excellent attention to quality  •Scan to file of hard copy job following standard operating procedures  •Inspect finished work for accuracy  •Conscientious and consistent effort to quickly and accurately complete each task andor job is the companys productivity standard  •File records as needed  •Destroy documents as stated within the policy           2010   to   062012   Front Desk ClerkNight Manager    Western Inn         City     STATE           • Process guest registrations including collecting payment   • Complete shift reports   • Respond to guest needs special requests and complaints alert the appropriate manager to potential issues as needed   • Assist customer with making room keys  • Assist customers in various assignments including finding nearby attractions restaurants and etc   • Prepare coffee for guest  • Transmit and receive messages via telephone and fax machine   • Sort and rack incoming messages and mail   • Assist guests with requests and problems related to their stay at the property   •Resolve guest complaints ensuring guest satisfaction  •Maintain complete knowledge of or where to access to following information a all hotel featuresservices hours of operation b all room types numbers layout decor appointments and location c all room rates special packages and promotions d daily house count and expected arrivalsdepartures e room availability status for any given day f scheduled daily group activities  •Pick up count and maintain bank Secure bank at all times  •Read the log book daily and record all pertinent information in the log book  •Process currency exchange and payments to guest accounts  •Process adjustments rebates paid outs and credits as required  •Verified that all checks are closed and closes and logs any open check in the POS Point of Sale system  •Run Room  Tax verifying that all room rates posted  •Verify Cashiers Report to drop log and paperwork  •Record room statistics  •Close POS after all work was balanced  •Run end of day program and close day  •Check that interfaces are up and running  •Run daily Flash Reports and distribute accordingly  •Run morning reports and backup reports and distribute accordingly  •Print express check out folios and distribute  •Sign out and brief relief  •Review Night Audit checklist and verify that all work has been completed  •Restock all printers  •Fill out and deposit payment and corresponding checks  •Review status of assignments and any followup action with oncoming Supervisor  •Document maintenance needs on work orders and submit to ManagerSupervisor             102008   to   2010   Retail Sales Consultant    Sprint         City     STATE           •Provided a total sales solution to the customer regarding their wirelessmobility needs that includes selling the value for Sprints devices accessories and service plans maximizing customer connections saving the customer money personalizing the customer experience protecting their investment  •Delivered an outstanding store experience that improves customer loyalty and strengthens the Sprint Brand  •Met key performance objectives that include sales and customer satisfaction goals  •Made certain accurate customer account setup so they are ready to use when leaving the store  •Identified the right solutions to customer billing technical and or account issues  •Completed all courses in your curriculum path with the required time frame  •Complied with all operational policies and procedures including the Sprint Code of Conduct  •Promote innovation and friendly competition to deliver unparalleled customer experience           112006   to   112008   ExpeditorCashier    Macys         City     STATE           •Assist customers in all aspects of service fulfillment by demonstrating proficient use of proprietary devices and applications proactively create enhanced shopping experiences through the heightened use of tools technology and collaboration  •Determined customer needs based on personal features and other customer preference related factors  •Demonstrated knowledge of store products and services to build sales and minimize returns  •Maintained a professional attitude with sincerity and enthusiasm reflecting Macy’s commitment to our customer – the most important person in our stores  •  POS procedures  •Assisted customers in all aspects of service fulfillment by demonstrating proficient use of proprietary devices and applications proactively create enhanced shopping experiences through the heightened use of tools technology and collaboration  •Recovered shoe sales floor and scan inventory back into stock  •Maintained integrity of shoe inventory by ensuring accuracy of scanning and placement  •Receive and process new merchandise          Education        Expected in   2008   Associates       Network Support System    Everest Institute     Houston     TX      GPA                 Expected in   2006   High School Diploma           United Christian School     Houston     TX      GPA               Accomplishments       Volunteer at Mustang Center teaching English to adults and teens 2010  until        Skills       10Key Account Management Active Learning Calendaring Client Relations Computer Proficiency Coordination Creative Problem Solving Critical Thinking Customer Needs Assessment Customer Service Data Collection Data Entry Documentation Email Executive Management Support Filing Grammar Internet Research Report Transcription Research Scheduling Service Orientation Speaking Spreadsheets Telephone Skills Time Management Travel Arrangements Travel Booking Travel Planning Type 3040 WPM Typing Writing Letters and Memos Lotus Notes Microsoft Excel Microsoft Office Suite Microsoft Outlook Microsoft PowerPoint Microsoft Word Minute Taking MultiTask Management Organizational Skills Prioritization Proofreading Reading ComprehensionCash handlingProfessional and friendly Careful and active listener Multitasking Production Mechanical Assembler Packager Labeling Inventory', 'resume_html': 'none', 'skills': \"['Data privacy', 'Research', 'data analysis', 'cloud', 'Networking', 'user training', 'monitoring', 'statistical analysis', 'reporting', 'data collection', 'statistical analysis', 'project', 'algorithms', 'workflow diagrams', 'HIPAA', 'HIPAA', 'HIPAA', 'HIPAA', 'Windows XP', 'Office', 'Data privacy', 'Research', 'data analysis', 'cloud', 'Networking', 'user training', 'monitoring', 'statistical analysis', 'reporting', 'data collection', 'statistical analysis', 'project', 'algorithms', 'workflow diagrams', 'HIPAA', 'HIPAA', 'HIPAA', 'HIPAA', 'Windows XP', 'Office']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-entry-operator-381caf3c16b649328748d7e6c673a7d8', 'id': '283692866702135157759709078394118382144', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Energetic worker with a broad range of customer service and team leader skills Data Entry specialist adept at developing and maintaining databases Highly skilled at creating effective organizational and filing systems Dynamic Data Analyst trained in an IT environment who goes above and beyond given job responsibilities to achieve superior results and maintain companywide integrity        Highlights           Certified in 10key  Time management  Meticulous attention to detail  Resultsoriented  Selfdirected  Excellent communication skills  Strong problem solver  Filing and data archiving  HIPAA compliance      Advanced MS Office Suite knowledge  Resourceful  Strong interpersonal skills  Pleasant demeanor  Understands grammar  Customer serviceoriented  Advanced clerical knowledge  Critical thinker                       Accomplishments       Data Entry    Preserved an accuracy of 98 during 5 years of employment    Training    Successfully trained staff in all office systems and databases policies and procedures while focusing on minimizing errors and generating superior results    Reporting    Maintained status reports to provide management with updated information for client projects    Administration    Performed administration tasks such as filing developing spreadsheets faxing reports photocopying collateral and scanning documents for externaldepartmental use    Multitasking    Demonstrated proficiencies in telephone email fax and frontdesk reception within highvolume environment   Implementation   Assisted in implementation of new tracking system that resulted in improved patient care    OSHA Compliance    Properly disposed of daily biohazard waste in compliance with federal and local regulations   Documentation   Drafted documents for internal meetings          Experience       Data Entry Operator       082009      Current     Iron Mountain Incorporated    –    Essex Junction     VT            8000 key strokes per hour  Trained staff to operate new environmental health technology  Verified that information in the computer system was uptodate and accurate  Processed confidential medical information  Identified and resolved system and account issues  Developed and created a more effective filing system to accelerate paperwork processing  Trained new employees and explained protocols clearly and efficiently  Provided base level IT support to company personnel  Troubleshot hardware issues and worked with service providers to facilitate repairs           Tutor       032012      062012     Arizona State University    –    Tempe     AZ            Tutored college level students in the fields of reading writing and computers  Routinely met with students regarding inclass issues and learning interruptions to discuss solutions  Performed student background reviews to develop tailored lessons based on student needs  Taught Creative writing to a diverse class of 20 students  Developed and implemented interesting and interactive learning mediums to increase student understanding of course materials           VolunteerMentor       082007      052008     Boys And Girls Club Of Northeast Florida    –    City     STATE            Coordinated after school tutoring hours to help students in need of extra attention  Received high remarks for the creativity of classroom lesson plans and instructional techniques from students parents and faculty  Created and enforced childbased handson curriculum to promote student interest and receptive learning  Designed lesson plans focused on age and levelappropriate material  Developed interesting course plans to meet academic intellectual and social needs of students  Consistently met schedules and deadlines for all illustration projects  Worked alongside the entire development team in an energetic and creative environment          Education       Associate of Arts       Business Administration       Expected in   2014                Florida State College at Jacksonville      Jacksonville     Florida     GPA        Status          360 GPA  Member of Phi Theta Kappa Honor Society  Recipient of 2014 Academic Achievement Award  Coursework in Marketing Public Relations and Business Management           Skills      10Key  Customer Service  Data Entry  Microsoft Office Suite', 'resume_html': 'none', 'skills': \"['Data Quality', 'Oracle Forms', 'Google Drive', 'Google Docs', 'Google Sheets', 'Microsoft Office', 'Data Quality', 'data analysis', 'developing', 'data quality', 'data patterns', 'data linking', 'test scripts', 'interviews', 'SWOT analysis', 'data quality', 'data analysis', 'Microsoft Office', 'Oracle', 'research']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-management-services-coordinator-368330306fc74431af7e0137d3706178', 'id': '189067289296683940487677712533012870883', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  9XXX5    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary     Administrator with over 15 years of professional experience Skilled in all aspects of office administration organization of filing systems use of electronic office equipment handling multiline phone systems reception data entry coordinating with staff scheduling appointments banking and accounts receivable and payable Communication skills demonstrated through verbal and writing abilities client relations marketing expertise customer service skills training new employees and the ability to produce indepth reports and correspondence       Highlights           Confidential Correspondence and Data  Microsoft Excel  Microsoft Word  Microsoft Outlook  Microsoft PowerPoint  Data Entry  Document Creation and Maintenance  Editing and Proofreading  Information Resource  Knowledge of Office Equipment CopierFax  10Key Calculator  Agenda and Event Coordination  Business Correspondence  Client Services  Call Screening  Mail Distribution  Stocking and Supplies  Typing  Data Entry  Billing Processes  Purchasing and Inventory  Payroll and Accounts Administration                         Experience       Data Management Services Coordinator       2009   to   Current     Penguin Random House    –    City     STATE             Performs highly accurate and detailed data entry for end of month invoicing  Performs data entry for business account orders in a timely manner  Responsible for several monthly reports submitted to management   Compiled statistical information for special reports     Organized billing and invoice data     Updated departmental standard operating procedures and database to accurately reflect the current practices       Identified and resolved system and account issues         Crosstrained and provided backup for other data management representatives when needed       Resolved spreadsheet issues and shared benefits of new technology         Interacted with customers to followup on shipping status and expedited orders           Promptly responded to general inquiries from members staff and clients via mail email and fax             Assisted customers in finding outofprint items             Kept abreast of rapidly evolving technology             Provided accurate and appropriate information in response to customer inquiries             Dispersed incoming mail to correct recipients throughout the office               Organized files developed spreadsheets faxed reports and scanned documents               Received and screened a high volume of internal and external communications including email and mail               Created and maintained spreadsheets using advanced Excel functions and calculations to develop reports and lists                  ReceptionistCashier Supervisor       2008   to   2009     Koons Of Westminster    –    City     STATE              Assessed customer needs and responded to questions       Organized register supplies      Worked with customer service to resolve issues        Provided professional and courteous service at all times       Worked overtime shifts during busy periods       Monitored a \\xa0 team of  78  of professionals    Trained and mentored new cashiers      Hired 34 team members     Managed cashier shifts and breaks       Built and maintained productive relationships with employees       Greeted customers promptly and responded to questions       Documented performance issues       Counted and balanced cashier drawers       Worked in competitive team environment to exceed revenue quotas             OfficeProgram Assistant       2004   to   2008     General Dynamics Information    Technology    –    City     STATE             Maximized productivity by maintaining multiple calendars scheduling meetings tracking expenses and prioritizing phone calls for Program ManagersMaintained office equipment and ordered supplies  Prepared weekly spreadsheets monitoring more than 15 ongoing projectsOversaw status of projects by continually gathering information and followingup with Program Managers  Updated dynamic organizational charts and headcount spreadsheets  Answered multiline telephone system maintained appointment calendar filed personnel records and assisted Program Manager  Performed timely and highly accurate data entry to ensure fastest turnaround possible for end of month invoicing  Developed planned organized and administered policies and procedures for organization to ensure administrative and operational objectives were met  Implemented corrective action plan to solve problems  Established and maintained comprehensive and current record keeping system of activities and operational procedures in business office  Prepared reviewed and submitted reports concerning activities expenses budget government statutes and rulings and other items affecting business and program services  Consulted with staff and others in government business and private organizations to discuss issued coordinate activities and resolve problems  Prepared budget and directed and monitored expenditures of department funds  Directed and conducted studies and research and prepared reports and other publications relating to operational trends and program objectives and accomplishments           Loan Editor       2001   to   2004     BancFirst    –    City     STATE             Verified and examined information and accuracy of loan application and closing documents  Recorded applications for loan and credit loan information and disbursement of funds using computer  Accepted payment on accounts   Filed and maintained loan records    Presented loan and repayment schedule to customer   Calculated reviewed and corrected errors on interest principal payment and closing costs using computer and calculator    Contacted credit bureaus employers and other sources to check applicant credit and personal references    Assembled and compiled documents for closing such as title abstract insurance form loan form and tax receipt   Prepared and typed loan applications closing documents legal documents letters forms government notices and checks using computer    Interviewed loan applicant to obtain personal and financial data and to assist in filling out application    Complied with federal state and company policies procedures and regulations    Debited and credits accounts   Processed negotiable instruments such as checks and vouchers   Evaluated records for accuracy of balanced postings calculations and other records pertaining to business and operating transactions and reconciled and notes discrepancies   Recorded financial transactions and other account information to update and maintain accounting records          Education       Associate     Accounting     Expected in   2007     Ashworth University      Norcross     GA     GPA   GPA 355    Accounting GPA 355        Skills     10Key Calculator accounting administrative Billing budget Business Correspondence calculator charts closing credit Client Data Entry Editing Event Coordination Fax filling Financial forms funds government insurance Inventory invoicing legal documents letters notes meetings Microsoft Excel Mail office Microsoft Outlook Microsoft PowerPoint Microsoft Word Office Equipment Office Management organizational Payroll personnel Copier policies Processes Proofreading publications Purchasing record keeping research scheduling spreadsheets tax telephone phone Typing', 'resume_html': 'none', 'skills': \"['functional requirements', 'monitoring', 'excel', 'access', 'developing', 'Data Warehouse', 'monitor', 'Sharepoint', 'Sharepoint', 'data sharing', 'implement', 'Microsoft Word', 'Microsoft PowerPoint', 'Microsoft Excel', 'developing', 'analyzing data', 'Excel', 'Sharepoint', 'Sharepoint', 'data sharing', 'Adobe Photoshop', 'Data Warehouse', 'Access', 'Microsoft Excel', 'Excel', 'Microsoft Office', 'Outlook', 'Microsoft PowerPoint', 'Power Point', 'Microsoft Publisher', 'Word', 'Microsoft Word', 'Project Management', 'VISIO']\"}, {'profile': 'Data Engineer', 'url': 'https://www.livecareer.com/resume-search/r/data-warehouse-developer-35a285e749a74031816e915414d2ae8b', 'id': '14259385793885776770164514431890372233', 'data': 'Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Professional Summary       Proactive performancedriven professional with 105  years of experience in IT industry in Investment Banking domain   Proficient in the Integration of various data sources with multiple relational databases like Oracle11g Oracle10g9i Sybase into the staging area Data Warehouse which includes developing PLSQL Procedures Packages Triggers Bulk collections Cursors Views Objects and Performance Tuning of Data Warehouse environment  Data Warehousing ETL experience of using Informatica PowerCenter Client tools  Mapping Designer Repository manager Workflow ManagerMonitor and Server tool Repository Server manager   Proficient in UNIX shell scripting in automation of various processes Hands on experience in server setup to host files to automate the database refresh with help of Perlcgi scripts Developing web page applications using HTML5 CSS3 and Javascript for form validations Also using Ajax and PHP scripting to get back data from Oracle DB     Experience in using Automation Scheduling tools like Autosys and ControlM     Knowledge and hands on in Informatica ETL Tool for loading huge data files     Handson experience across all stages of Software Development Life Cycle SDLC including business requirement analysis data mapping build unit testing systems integration and user acceptance testing     Experience working in OnShoreOffshore Development Models     Strong time management skills in leading multiple projects and deadlines under minimal supervision     Recognized and highly appreciated for consistent success knowledge and flexibility          Areas of Expertise           Windows NTXP2007 UNIX MSDOS  ETL Tools Informatica Power Center 918171 Designer Workflow Manager Workflow Monitor Repository manager and Informatica Server  Databases Oracle 11g10g9i8i Sybase  Languages SQL PLSQL UNIX Shell scripts Perl HTML5 Java Scripting PHP  Scheduling Tools Autosys ControlM                         Work Experience       Data Warehouse Developer       032015      Current     22Nd Century Technologies       Bothell     WA            Customer Warehouse Environment CWE is a group in Fidelity focusing on providing analytical data to various business groups  This data helps the business community in increasing the Fidelity business in Retail and Institutional areas  The data constitutes of various customers accumulated from various sources  Major responsibilities include impact analysis support production support activities install planning and help answer business adhoc requests  Role and Responsibilities Providing resolution to the issues raised by users  Monitoring and providing batch support for the daily and monthly batches  Conduct regular meetings to check for weekly and monthly install  Provide month end batch support and provide oncall support  Enhancement of the existing application to provide more functionality  Identify long running process and tune the same  Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  Used SQLDeveloperToad for creating all types of PLSQL objects like tables Indexes packages triggers sequences and stored procedures  Optimized the existing long running processes to run faster with usage of Bulk Loader Hints and limits on bulk loader Used Explain Plans to analyze long running queries and degree of Parallel to make queries run faster  Create new web applications from HTML5 and CSS  Javascript for form validation Host the web applications on windows server 2012 R2 machine Used PHP as the server side language to psftp the files from users machine  Call oracle client functions ODBC to connect to db and get back the results on HTML forms           ETLData Warehouse Developer       072012      032015     22Nd Century Technologies        Burkeville     VA            The Strategic Investment Product team works on providing investment plans for individuals investment plan and guidance tool for retailers retirement plans 401k for organizations  The tasks involved as part of this are loading and processing the stocks and mutual fund details from different sources categorize and customize such that retail and individual investors get the accurate details which would help in investments  Role and Responsibilities Understand and analyze requirements follow up with business analysts and subject matter expert team for any clarification if required  DesignReview the Test Cases for Integration testing System testing and User Acceptance testing Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  From staging area data is processed and modified as per the requirement and then loaded to integration area through stored procedures and ETL Informatica mappings  Use Toad for developing all types of PLSQL objects like tables Indexes packages triggers sequences and stored procedures  Developed and modified Oracle packages stored procedures functions Scripts synonyms tables and indexes to implement business requirements and rules  Applied optimizer hints to tune the queries for faster performance  Worked on Performance tuning by using Explain plans and various hints  Worked on Table Partitioning and deploying various methods of indexing like local Indexes and Global Indexes on partitioned tables  Organize and analyze the behavior of each Investment Instrument like Stocks Bonds etc for a certain period and provide the complete detailed analysis to investors which would help them in planning their portfolio and the investments  Evaluate the performance of different funds which would be ideal for long term investing which is the primary requirement of a retirement plan 401k aimed at institutions  Project Title PARA reporting for an Investment Bank           Data Warehouse Developer       2009      062012     22Nd Century Technologies        Columbia     MO            Location Bangalore Tokyo and NY Description The department division deals in reporting profit and loss to back to the central system  It also involves doing the daily adjustments and sending the feed to different streams which will do further processing at their end  The data involved capital markets data like Securities Bonds Funds Repos etc  Role and Responsibilities Handling all user support requests from across the regions APAC UK  USA  Automating frequent process with shell scripting and Autosys scheduler  Handling major UATs independently  representing client from offshore office  Responsible for all Production Release across regions  Generate reports required by Japanese and US Federal Gov  Developed and modified Packages Functions Synonyms tables and indexes to implement business requirements and rules  Bulk loading of data done using utilities like BCP and Sybase Central  Code tuning or Query Optimization done using Explain Plans Hints  Analyzing and monitoring system performances using DBCC Trace on commands query plan outputs system  Handling various enhancements to the system this entailed writing new codes and changing some existing codes  Wrote new complex stored procedures in Sybase and optimized the existing code written in Sybase  Debugging the scripts and jobs in Production environment written in Shell and PERL  Handling and creating various Autosys jobs in Production environment  Providing daily status reports to the clients  Provided production support and 247 support for resolving the critical production issues  Involved in the solving the tickets that are raised by the end users  Responsible for creating PLSQL Programs and UNIX Scripts for Data Validation and Data Conversion  Project Title Basel 2 Risk Platform UK based bank           Data Warehouse Developer       012006      112008     Royal Bank Of Scotland        City     STATE            Location Bangalore Description FMIT is a dedicated IT division within RBS and we provide a fully managed service for Finance IT Business As Usual BAU team  Designed the tables indexes triggers stored procedures functions and packages to implement the requirement  Unix scripts is used to ftp the files from vendor sites update the data files before loading the data using ETLInformatica to process data from various vendors  Informatica Mappings are used in loading the target databases which is usually on different schema and the data is transformed in the process before loading to target  Analysis of Source data Oracle Source Objects and identifying the methods for loading data to target  Created and used External Tables for migrating flat file data into target  Responsible for the creation of Packages Functions and Procedures  Performed Testing of the Packages Procedures and Functions  Wrote scripts for creating tables Indexes Grants and Synonyms in different schemas and made modifications for the existing tables as per business logic  Preparation of test scripts for System and Integration Testing and Traceability Matrix for the assurance of complete coverage of system requirements  Develop bug fixes and enhancements  Production support of some of the applications of the bank which includes resolving issues of the overnight batch processes and daily users queries          Education       Bachelors       Telecommunication       Expected in   2005                VTU                GPA        Status         Telecommunication        Professional Affiliations              Skills     Bonds capital markets CSS client clients Data Conversion Data Validation Databases Debugging ETL Finance forms ftp Funds Grants HTML HTML5 PHP indexing Informatica investments Japanese Javascript Java Scripting logic meetings office Windows NT works MSDOS ODBC Operating Systems Optimization Oracle db Developer PLSQL PLSQL PERL processes profit and loss reporting requirement Retail Scheduling Securities Shell Scripts Shell scripts shell scripting SQL Strategic Sybase Tables user support Toad UNIX Unix scripts utilities validation web applications windows server Workflow written', 'resume_html': 'none', 'skills': \"['data analysis', 'statistical analysis', 'R', 'SAS', 'Microsoft Office', 'Word', 'Excel', 'Power Point', 'spreadsheets', 'data platform', 'data analysis', 'MS Excel', 'data validation', 'spreadsheets', 'accurate', 'data analysis', 'pivot tables', 'MS Excel', 'analysis', 'Research', 'analysis', 'Statistical analysis', 'R studio', 'Statistics', 'SAS', 'SAS', 'MS Word', 'PowerPoint', 'Excel', 'Access', 'R Studio', 'SAS', 'SQL']\"}]}\n",
      "{'total': 15, 'batches': 1, 'time': 0.011986970901489258}\n",
      "{'rows': [{'profile': 'Data Engineer', 'skills': 'bi'}, {'profile': 'Data Engineer', 'skills': 'design'}, {'profile': 'Data Engineer', 'skills': 'business intelligence'}, {'profile': 'Data Engineer', 'skills': 'tableau'}, {'profile': 'Data Engineer', 'skills': 'data visualization'}, {'profile': 'Data Engineer', 'skills': 'dashboards'}, {'profile': 'Data Engineer', 'skills': 'database design'}, {'profile': 'Data Engineer', 'skills': 'designing'}, {'profile': 'Data Engineer', 'skills': 'etl'}, {'profile': 'Data Engineer', 'skills': 'transform'}, {'profile': 'Data Engineer', 'skills': 'load'}, {'profile': 'Data Engineer', 'skills': 'extract'}, {'profile': 'Data Engineer', 'skills': 'data warehouse'}, {'profile': 'Data Engineer', 'skills': 'trends'}, {'profile': 'Data Engineer', 'skills': 'business requirements'}, {'profile': 'Data Engineer', 'skills': 'evaluating trends'}, {'profile': 'Data Engineer', 'skills': 'realtime data'}, {'profile': 'Data Engineer', 'skills': 'ms excel'}, {'profile': 'Data Engineer', 'skills': 'ssrs'}, {'profile': 'Data Engineer', 'skills': 'snowflake'}, {'profile': 'Data Engineer', 'skills': 'cassandra'}, {'profile': 'Data Engineer', 'skills': 'oracle'}, {'profile': 'Data Engineer', 'skills': 'ms sql server'}, {'profile': 'Data Engineer', 'skills': 'ms access'}, {'profile': 'Data Engineer', 'skills': 'postgres'}, {'profile': 'Data Engineer', 'skills': 'amazon s3'}, {'profile': 'Data Engineer', 'skills': 'sql'}, {'profile': 'Data Engineer', 'skills': 'python'}, {'profile': 'Data Engineer', 'skills': 'tsql'}, {'profile': 'Data Engineer', 'skills': 'html'}, {'profile': 'Data Engineer', 'skills': 'css'}, {'profile': 'Data Engineer', 'skills': 'java'}, {'profile': 'Data Engineer', 'skills': 'salesforce'}, {'profile': 'Data Engineer', 'skills': 'ms word'}, {'profile': 'Data Engineer', 'skills': 'outlook'}, {'profile': 'Data Engineer', 'skills': 'frontpage'}, {'profile': 'Data Engineer', 'skills': 'powerpoint'}, {'profile': 'Data Engineer', 'skills': 'reporting'}, {'profile': 'Data Engineer', 'skills': 'data quality'}, {'profile': 'Data Engineer', 'skills': 'memsql'}, {'profile': 'Data Engineer', 'skills': 'warehouses'}, {'profile': 'Data Engineer', 'skills': 'pipelines'}, {'profile': 'Data Engineer', 'skills': 'data governance'}, {'profile': 'Data Engineer', 'skills': 'data analysis'}, {'profile': 'Data Engineer', 'skills': 'data mapping'}, {'profile': 'Data Engineer', 'skills': 'business requirement'}, {'profile': 'Data Engineer', 'skills': 'data validation'}, {'profile': 'Data Engineer', 'skills': 'aws'}, {'profile': 'Data Engineer', 'skills': 'excel'}, {'profile': 'Data Engineer', 'skills': 'visualizations'}, {'profile': 'Data Engineer', 'skills': 'adhoc analysis'}, {'profile': 'Data Engineer', 'skills': 'customized analysis'}, {'profile': 'Data Engineer', 'skills': 'developing'}, {'profile': 'Data Engineer', 'skills': 'redshift'}, {'profile': 'Data Engineer', 'skills': 'optimization'}, {'profile': 'Data Engineer', 'skills': 'prototyping'}, {'profile': 'Data Engineer', 'skills': 'dashboard'}, {'profile': 'Data Engineer', 'skills': 'dashboarding'}, {'profile': 'Data Engineer', 'skills': 'waterfall'}, {'profile': 'Data Engineer', 'skills': 'agile'}, {'profile': 'Data Engineer', 'skills': 'statistical techniques'}, {'profile': 'Data Engineer', 'skills': 'validate data'}, {'profile': 'Data Engineer', 'skills': 'develop'}, {'profile': 'Data Engineer', 'skills': 'data collection'}, {'profile': 'Data Engineer', 'skills': 'sas'}, {'profile': 'Data Engineer', 'skills': 'data integration'}, {'profile': 'Data Engineer', 'skills': 'extracting'}, {'profile': 'Data Engineer', 'skills': 'transforming'}, {'profile': 'Data Engineer', 'skills': 'loading data'}, {'profile': 'Data Engineer', 'skills': 'data sets'}, {'profile': 'Data Engineer', 'skills': 'business analysis'}, {'profile': 'Data Engineer', 'skills': 'testing'}, {'profile': 'Data Engineer', 'skills': 'quality assurance'}, {'profile': 'Data Engineer', 'skills': 'functional testing'}, {'profile': 'Data Engineer', 'skills': 'critical thinking'}, {'profile': 'Data Engineer', 'skills': 'project management'}, {'profile': 'Data Engineer', 'skills': 'cryptography'}, {'profile': 'Data Engineer', 'skills': 'threat analysis'}, {'profile': 'Data Engineer', 'skills': 'cloud'}, {'profile': 'Data Engineer', 'skills': 'microsoft office'}, {'profile': 'Data Engineer', 'skills': 'programming'}, {'profile': 'Data Engineer', 'skills': 'windows'}, {'profile': 'Data Engineer', 'skills': 'data integrity'}, {'profile': 'Data Engineer', 'skills': 'technical documentation'}, {'profile': 'Data Engineer', 'skills': 'networking'}, {'profile': 'Data Engineer', 'skills': 'linux'}, {'profile': 'Data Engineer', 'skills': 'access'}, {'profile': 'Data Engineer', 'skills': 'unix'}, {'profile': 'Data Engineer', 'skills': 'data accuracy'}, {'profile': 'Data Engineer', 'skills': 'data mining'}, {'profile': 'Data Engineer', 'skills': 'ms office'}, {'profile': 'Data Engineer', 'skills': 'analyzing trends'}, {'profile': 'Data Engineer', 'skills': 'vba'}, {'profile': 'Data Engineer', 'skills': 'process redesign'}, {'profile': 'Data Engineer', 'skills': 'maintaining'}, {'profile': 'Data Engineer', 'skills': 'accurate data'}, {'profile': 'Data Engineer', 'skills': 'data cleaning'}, {'profile': 'Data Engineer', 'skills': 'data preparation'}, {'profile': 'Data Engineer', 'skills': 'processing'}, {'profile': 'Data Engineer', 'skills': 'research data'}, {'profile': 'Data Engineer', 'skills': 'data sources'}, {'profile': 'Data Engineer', 'skills': 'reliability'}, {'profile': 'Data Engineer', 'skills': 'spreadsheets'}, {'profile': 'Data Engineer', 'skills': 'hipaa'}, {'profile': 'Data Engineer', 'skills': 'problem solving'}, {'profile': 'Data Engineer', 'skills': 'implement'}, {'profile': 'Data Engineer', 'skills': 'research'}, {'profile': 'Data Engineer', 'skills': 'consistent'}, {'profile': 'Data Engineer', 'skills': 'pivot tables'}, {'profile': 'Data Engineer', 'skills': 'data storage'}, {'profile': 'Data Engineer', 'skills': 'office'}, {'profile': 'Data Engineer', 'skills': 'ms project'}, {'profile': 'Data Engineer', 'skills': 'explanation'}, {'profile': 'Data Engineer', 'skills': 'regularization'}, {'profile': 'Data Engineer', 'skills': 'business objects'}, {'profile': 'Data Engineer', 'skills': 'query builder'}, {'profile': 'Data Engineer', 'skills': 'ms visio'}, {'profile': 'Data Engineer', 'skills': 'sharepoint'}, {'profile': 'Data Engineer', 'skills': 'onenote'}, {'profile': 'Data Engineer', 'skills': 'ms sharepoint'}, {'profile': 'Data Engineer', 'skills': 'ms onenote'}, {'profile': 'Data Engineer', 'skills': 'apple sheets'}, {'profile': 'Data Engineer', 'skills': 'apple pages'}, {'profile': 'Data Engineer', 'skills': 'open office'}, {'profile': 'Data Engineer', 'skills': 'ms powerpoint'}, {'profile': 'Data Engineer', 'skills': 'vbnet'}, {'profile': 'Data Engineer', 'skills': 'android'}, {'profile': 'Data Engineer', 'skills': 'performance analysis'}, {'profile': 'Data Engineer', 'skills': 'analysis'}, {'profile': 'Data Engineer', 'skills': 'visio'}, {'profile': 'Data Engineer', 'skills': 'user acceptance testing'}, {'profile': 'Data Engineer', 'skills': 'microsoft access'}, {'profile': 'Data Engineer', 'skills': 'microsoft excel'}, {'profile': 'Data Engineer', 'skills': 'sql server'}, {'profile': 'Data Engineer', 'skills': 'cognos'}, {'profile': 'Data Engineer', 'skills': 'crystal'}, {'profile': 'Data Engineer', 'skills': 'monitor'}, {'profile': 'Data Engineer', 'skills': 'process analysis'}, {'profile': 'Data Engineer', 'skills': 'claims analysis'}, {'profile': 'Data Engineer', 'skills': 'project planning'}, {'profile': 'Data Engineer', 'skills': 'reconciliation'}, {'profile': 'Data Engineer', 'skills': 'monitoring'}, {'profile': 'Data Engineer', 'skills': 'risk assessment'}, {'profile': 'Data Engineer', 'skills': 'statistical analysis'}, {'profile': 'Data Engineer', 'skills': 'modeling'}, {'profile': 'Data Engineer', 'skills': 'rapid application development'}, {'profile': 'Data Engineer', 'skills': 'adhoc testing'}, {'profile': 'Data Engineer', 'skills': 'backend testing'}, {'profile': 'Data Engineer', 'skills': 'decision making'}, {'profile': 'Data Engineer', 'skills': 'microsoft project'}, {'profile': 'Data Engineer', 'skills': 'spotfire'}, {'profile': 'Data Engineer', 'skills': 'crm'}, {'profile': 'Data Engineer', 'skills': 'drupal'}, {'profile': 'Data Engineer', 'skills': 'data warehouses'}, {'profile': 'Data Engineer', 'skills': 'cisco'}, {'profile': 'Data Engineer', 'skills': 'big data'}, {'profile': 'Data Engineer', 'skills': 'hdfs'}, {'profile': 'Data Engineer', 'skills': 'project'}, {'profile': 'Data Engineer', 'skills': 'visual studio'}, {'profile': 'Data Engineer', 'skills': 'data profiling'}, {'profile': 'Data Engineer', 'skills': 'teams'}, {'profile': 'Data Engineer', 'skills': 'active directory'}, {'profile': 'Data Engineer', 'skills': 'plsql'}, {'profile': 'Data Engineer', 'skills': 'xml'}, {'profile': 'Data Engineer', 'skills': 'data modeling'}, {'profile': 'Data Engineer', 'skills': 'performance tuning'}, {'profile': 'Data Engineer', 'skills': 'data blending'}, {'profile': 'Data Engineer', 'skills': 'data analytics'}, {'profile': 'Data Engineer', 'skills': 'software development life cycle'}, {'profile': 'Data Engineer', 'skills': 'sdlc'}, {'profile': 'Data Engineer', 'skills': 'data cleansing'}, {'profile': 'Data Engineer', 'skills': 'data verification'}, {'profile': 'Data Engineer', 'skills': 'data mismatch'}, {'profile': 'Data Engineer', 'skills': 'db2'}, {'profile': 'Data Engineer', 'skills': 'teradata'}, {'profile': 'Data Engineer', 'skills': 'erwin'}, {'profile': 'Data Engineer', 'skills': 'dimensional modeling'}, {'profile': 'Data Engineer', 'skills': 'normalization'}, {'profile': 'Data Engineer', 'skills': 'word'}, {'profile': 'Data Engineer', 'skills': 'c'}, {'profile': 'Data Engineer', 'skills': 'er studio'}, {'profile': 'Data Engineer', 'skills': 'datasets'}, {'profile': 'Data Engineer', 'skills': 'time series analysis'}, {'profile': 'Data Engineer', 'skills': 'matlab'}, {'profile': 'Data Engineer', 'skills': 'collecting data'}, {'profile': 'Data Engineer', 'skills': 'text mining'}, {'profile': 'Data Engineer', 'skills': 'vmware'}, {'profile': 'Data Engineer', 'skills': 'exchange'}, {'profile': 'Data Engineer', 'skills': 'javascript'}, {'profile': 'Data Engineer', 'skills': 'ajax'}, {'profile': 'Data Engineer', 'skills': 'jquery'}, {'profile': 'Data Engineer', 'skills': 'bootstrap'}, {'profile': 'Data Engineer', 'skills': 'angularjs'}, {'profile': 'Data Engineer', 'skills': 'ssis'}, {'profile': 'Data Engineer', 'skills': 'aspnet'}, {'profile': 'Data Engineer', 'skills': 'uml'}, {'profile': 'Data Engineer', 'skills': 'visual basic'}, {'profile': 'Data Engineer', 'skills': 'vb'}, {'profile': 'Data Engineer', 'skills': 'windows xp'}, {'profile': 'Data Engineer', 'skills': 'net'}, {'profile': 'Data Engineer', 'skills': 'alteryx'}, {'profile': 'Data Engineer', 'skills': 'poc'}, {'profile': 'Data Engineer', 'skills': 'visualization'}, {'profile': 'Data Engineer', 'skills': 'olap'}, {'profile': 'Data Engineer', 'skills': 'html5'}, {'profile': 'Data Engineer', 'skills': 'tomcat'}, {'profile': 'Data Engineer', 'skills': 'dashboard development'}, {'profile': 'Data Engineer', 'skills': 'statistics'}, {'profile': 'Data Engineer', 'skills': 'sass'}, {'profile': 'Data Engineer', 'skills': 'dml'}, {'profile': 'Data Engineer', 'skills': 'loading'}, {'profile': 'Data Engineer', 'skills': 'r'}, {'profile': 'Data Engineer', 'skills': 'rest'}, {'profile': 'Data Engineer', 'skills': 'transformation'}, {'profile': 'Data Engineer', 'skills': 'code reviews'}, {'profile': 'Data Engineer', 'skills': 'data processing'}, {'profile': 'Data Engineer', 'skills': 'sap'}, {'profile': 'Data Engineer', 'skills': 'sql server reporting services'}, {'profile': 'Data Engineer', 'skills': 'data warehousing'}, {'profile': 'Data Engineer', 'skills': 's3'}, {'profile': 'Data Engineer', 'skills': 'software products'}, {'profile': 'Data Engineer', 'skills': 'hadoop'}, {'profile': 'Data Engineer', 'skills': 'amazon web services'}, {'profile': 'Data Engineer', 'skills': 'impala'}, {'profile': 'Data Engineer', 'skills': 'power query'}, {'profile': 'Data Engineer', 'skills': 'plsql developer'}, {'profile': 'Data Engineer', 'skills': 'kibana'}, {'profile': 'Data Engineer', 'skills': 'eclipse'}, {'profile': 'Data Engineer', 'skills': 'putty'}, {'profile': 'Data Engineer', 'skills': 'tortoise svn'}, {'profile': 'Data Engineer', 'skills': 'github'}, {'profile': 'Data Engineer', 'skills': 'jira'}, {'profile': 'Data Engineer', 'skills': 'system analysis'}, {'profile': 'Data Engineer', 'skills': 'hive'}, {'profile': 'Data Engineer', 'skills': 'regression'}, {'profile': 'Data Engineer', 'skills': 'stored procedures'}, {'profile': 'Data Engineer', 'skills': 'query optimization'}, {'profile': 'Data Engineer', 'skills': 'scrum'}, {'profile': 'Data Engineer', 'skills': 'estimation'}, {'profile': 'Data Engineer', 'skills': 'data science'}, {'profile': 'Data Engineer', 'skills': 'requirement analysis'}, {'profile': 'Data Engineer', 'skills': 'technical design'}, {'profile': 'Data Engineer', 'skills': 'system testing'}, {'profile': 'Data Engineer', 'skills': 'integration testing'}, {'profile': 'Data Engineer', 'skills': 'automation'}, {'profile': 'Data Engineer', 'skills': 'ftp'}, {'profile': 'Data Engineer', 'skills': 'project designs'}, {'profile': 'Data Engineer', 'skills': 'pivot table'}, {'profile': 'Data Engineer', 'skills': 'vlookup'}, {'profile': 'Data Engineer', 'skills': 'test cases'}, {'profile': 'Data Engineer', 'skills': 'assembly'}, {'profile': 'Data Engineer', 'skills': 'technical assistance'}, {'profile': 'Data Engineer', 'skills': 'gateway'}, {'profile': 'Data Engineer', 'skills': 'analyzing data'}, {'profile': 'Data Engineer', 'skills': 'business decisions'}, {'profile': 'Data Engineer', 'skills': 'storage'}, {'profile': 'Data Engineer', 'skills': 'erd'}, {'profile': 'Data Engineer', 'skills': 'stata'}, {'profile': 'Data Engineer', 'skills': 'gis'}, {'profile': 'Data Engineer', 'skills': 'nvivo'}, {'profile': 'Data Engineer', 'skills': 'analytical research'}, {'profile': 'Data Engineer', 'skills': 'evaluation'}, {'profile': 'Data Engineer', 'skills': 'statistical models'}, {'profile': 'Data Engineer', 'skills': 'statistical methods'}, {'profile': 'Data Engineer', 'skills': 'business processes'}, {'profile': 'Data Engineer', 'skills': 'budget analysis'}, {'profile': 'Data Engineer', 'skills': 'resource analysis'}, {'profile': 'Data Engineer', 'skills': 'data privacy'}, {'profile': 'Data Engineer', 'skills': 'user training'}, {'profile': 'Data Engineer', 'skills': 'algorithms'}, {'profile': 'Data Engineer', 'skills': 'workflow diagrams'}, {'profile': 'Data Engineer', 'skills': 'oracle forms'}, {'profile': 'Data Engineer', 'skills': 'google drive'}, {'profile': 'Data Engineer', 'skills': 'google docs'}, {'profile': 'Data Engineer', 'skills': 'google sheets'}, {'profile': 'Data Engineer', 'skills': 'data patterns'}, {'profile': 'Data Engineer', 'skills': 'data linking'}, {'profile': 'Data Engineer', 'skills': 'test scripts'}, {'profile': 'Data Engineer', 'skills': 'interviews'}, {'profile': 'Data Engineer', 'skills': 'swot analysis'}, {'profile': 'Data Engineer', 'skills': 'functional requirements'}, {'profile': 'Data Engineer', 'skills': 'data sharing'}, {'profile': 'Data Engineer', 'skills': 'microsoft word'}, {'profile': 'Data Engineer', 'skills': 'microsoft powerpoint'}, {'profile': 'Data Engineer', 'skills': 'adobe photoshop'}, {'profile': 'Data Engineer', 'skills': 'power point'}, {'profile': 'Data Engineer', 'skills': 'microsoft publisher'}, {'profile': 'Data Engineer', 'skills': 'data platform'}, {'profile': 'Data Engineer', 'skills': 'accurate'}, {'profile': 'Data Engineer', 'skills': 'r studio'}]}\n",
      "{'total': 241, 'batches': 1, 'time': 0.019064664840698242}\n",
      "{'cat': 'BD-ML-AI', 'rows': [{'id': '5533667039977703598659155174781970519', 'skills': 'bi'}, {'id': '5533667039977703598659155174781970519', 'skills': 'design'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business intelligence'}, {'id': '5533667039977703598659155174781970519', 'skills': 'tableau'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data visualization'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboards'}, {'id': '5533667039977703598659155174781970519', 'skills': 'database design'}, {'id': '5533667039977703598659155174781970519', 'skills': 'designing'}, {'id': '5533667039977703598659155174781970519', 'skills': 'etl'}, {'id': '5533667039977703598659155174781970519', 'skills': 'transform'}, {'id': '5533667039977703598659155174781970519', 'skills': 'load'}, {'id': '5533667039977703598659155174781970519', 'skills': 'extract'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data warehouse'}, {'id': '5533667039977703598659155174781970519', 'skills': 'trends'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business requirements'}, {'id': '5533667039977703598659155174781970519', 'skills': 'evaluating trends'}, {'id': '5533667039977703598659155174781970519', 'skills': 'realtime data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms excel'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ssrs'}, {'id': '5533667039977703598659155174781970519', 'skills': 'snowflake'}, {'id': '5533667039977703598659155174781970519', 'skills': 'cassandra'}, {'id': '5533667039977703598659155174781970519', 'skills': 'oracle'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms sql server'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms access'}, {'id': '5533667039977703598659155174781970519', 'skills': 'postgres'}, {'id': '5533667039977703598659155174781970519', 'skills': 'amazon s3'}, {'id': '5533667039977703598659155174781970519', 'skills': 'sql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'python'}, {'id': '5533667039977703598659155174781970519', 'skills': 'tsql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'html'}, {'id': '5533667039977703598659155174781970519', 'skills': 'css'}, {'id': '5533667039977703598659155174781970519', 'skills': 'java'}, {'id': '5533667039977703598659155174781970519', 'skills': 'salesforce'}, {'id': '5533667039977703598659155174781970519', 'skills': 'ms word'}, {'id': '5533667039977703598659155174781970519', 'skills': 'outlook'}, {'id': '5533667039977703598659155174781970519', 'skills': 'frontpage'}, {'id': '5533667039977703598659155174781970519', 'skills': 'powerpoint'}, {'id': '5533667039977703598659155174781970519', 'skills': 'reporting'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data quality'}, {'id': '5533667039977703598659155174781970519', 'skills': 'memsql'}, {'id': '5533667039977703598659155174781970519', 'skills': 'warehouses'}, {'id': '5533667039977703598659155174781970519', 'skills': 'pipelines'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data governance'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data mapping'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business requirement'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data validation'}, {'id': '5533667039977703598659155174781970519', 'skills': 'aws'}, {'id': '5533667039977703598659155174781970519', 'skills': 'excel'}, {'id': '5533667039977703598659155174781970519', 'skills': 'visualizations'}, {'id': '5533667039977703598659155174781970519', 'skills': 'adhoc analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'customized analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'developing'}, {'id': '5533667039977703598659155174781970519', 'skills': 'redshift'}, {'id': '5533667039977703598659155174781970519', 'skills': 'optimization'}, {'id': '5533667039977703598659155174781970519', 'skills': 'prototyping'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboard'}, {'id': '5533667039977703598659155174781970519', 'skills': 'dashboarding'}, {'id': '5533667039977703598659155174781970519', 'skills': 'waterfall'}, {'id': '5533667039977703598659155174781970519', 'skills': 'agile'}, {'id': '5533667039977703598659155174781970519', 'skills': 'statistical techniques'}, {'id': '5533667039977703598659155174781970519', 'skills': 'validate data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'develop'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data collection'}, {'id': '5533667039977703598659155174781970519', 'skills': 'sas'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data integration'}, {'id': '5533667039977703598659155174781970519', 'skills': 'extracting'}, {'id': '5533667039977703598659155174781970519', 'skills': 'transforming'}, {'id': '5533667039977703598659155174781970519', 'skills': 'loading data'}, {'id': '5533667039977703598659155174781970519', 'skills': 'data sets'}, {'id': '5533667039977703598659155174781970519', 'skills': 'business analysis'}, {'id': '5533667039977703598659155174781970519', 'skills': 'testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'business analysis'}, {'id': '274097087237322696861922203277862748921', 'skills': 'quality assurance'}, {'id': '274097087237322696861922203277862748921', 'skills': 'functional testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'critical thinking'}, {'id': '274097087237322696861922203277862748921', 'skills': 'project management'}, {'id': '274097087237322696861922203277862748921', 'skills': 'cryptography'}, {'id': '274097087237322696861922203277862748921', 'skills': 'threat analysis'}, {'id': '274097087237322696861922203277862748921', 'skills': 'cloud'}, {'id': '274097087237322696861922203277862748921', 'skills': 'microsoft office'}, {'id': '274097087237322696861922203277862748921', 'skills': 'reporting'}, {'id': '274097087237322696861922203277862748921', 'skills': 'java'}, {'id': '274097087237322696861922203277862748921', 'skills': 'programming'}, {'id': '274097087237322696861922203277862748921', 'skills': 'windows'}, {'id': '274097087237322696861922203277862748921', 'skills': 'data integrity'}, {'id': '274097087237322696861922203277862748921', 'skills': 'optimization'}, {'id': '274097087237322696861922203277862748921', 'skills': 'testing'}, {'id': '274097087237322696861922203277862748921', 'skills': 'technical documentation'}, {'id': '274097087237322696861922203277862748921', 'skills': 'networking'}, {'id': '274097087237322696861922203277862748921', 'skills': 'data warehouse'}, {'id': '274097087237322696861922203277862748921', 'skills': 'linux'}, {'id': '274097087237322696861922203277862748921', 'skills': 'access'}, {'id': '274097087237322696861922203277862748921', 'skills': 'unix'}, {'id': '270987204323396713165325793790364446993', 'skills': 'business requirements'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data accuracy'}, {'id': '270987204323396713165325793790364446993', 'skills': 'project management'}, {'id': '270987204323396713165325793790364446993', 'skills': 'excel'}, {'id': '270987204323396713165325793790364446993', 'skills': 'sql'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data mining'}, {'id': '270987204323396713165325793790364446993', 'skills': 'ms office'}, {'id': '270987204323396713165325793790364446993', 'skills': 'analyzing trends'}, {'id': '270987204323396713165325793790364446993', 'skills': 'vba'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data analysis'}, {'id': '270987204323396713165325793790364446993', 'skills': 'optimization'}, {'id': '270987204323396713165325793790364446993', 'skills': 'quality assurance'}, {'id': '270987204323396713165325793790364446993', 'skills': 'programming'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data integrity'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data quality'}, {'id': '270987204323396713165325793790364446993', 'skills': 'process redesign'}, {'id': '270987204323396713165325793790364446993', 'skills': 'maintaining'}, {'id': '270987204323396713165325793790364446993', 'skills': 'accurate data'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data cleaning'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data preparation'}, {'id': '270987204323396713165325793790364446993', 'skills': 'processing'}, {'id': '270987204323396713165325793790364446993', 'skills': 'statistical techniques'}, {'id': '270987204323396713165325793790364446993', 'skills': 'research data'}, {'id': '270987204323396713165325793790364446993', 'skills': 'data sources'}, {'id': '270987204323396713165325793790364446993', 'skills': 'reliability'}, {'id': '270987204323396713165325793790364446993', 'skills': 'spreadsheets'}, {'id': '270987204323396713165325793790364446993', 'skills': 'access'}, {'id': '270987204323396713165325793790364446993', 'skills': 'hipaa'}, {'id': '270987204323396713165325793790364446993', 'skills': 'problem solving'}, {'id': '270987204323396713165325793790364446993', 'skills': 'implement'}, {'id': '270987204323396713165325793790364446993', 'skills': 'research'}, {'id': '270987204323396713165325793790364446993', 'skills': 'consistent'}, {'id': '76199997604772823340207052419224154975', 'skills': 'excel'}, {'id': '76199997604772823340207052419224154975', 'skills': 'pivot tables'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sas'}, {'id': '76199997604772823340207052419224154975', 'skills': 'data storage'}, {'id': '76199997604772823340207052419224154975', 'skills': 'office'}, {'id': '76199997604772823340207052419224154975', 'skills': 'data analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'dashboard'}, {'id': '76199997604772823340207052419224154975', 'skills': 'problem solving'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms project'}, {'id': '76199997604772823340207052419224154975', 'skills': 'explanation'}, {'id': '76199997604772823340207052419224154975', 'skills': 'waterfall'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms access'}, {'id': '76199997604772823340207052419224154975', 'skills': 'regularization'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms excel'}, {'id': '76199997604772823340207052419224154975', 'skills': 'business objects'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sql'}, {'id': '76199997604772823340207052419224154975', 'skills': 'query builder'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms visio'}, {'id': '76199997604772823340207052419224154975', 'skills': 'sharepoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'onenote'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms sharepoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms onenote'}, {'id': '76199997604772823340207052419224154975', 'skills': 'apple sheets'}, {'id': '76199997604772823340207052419224154975', 'skills': 'apple pages'}, {'id': '76199997604772823340207052419224154975', 'skills': 'open office'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms powerpoint'}, {'id': '76199997604772823340207052419224154975', 'skills': 'ms word'}, {'id': '76199997604772823340207052419224154975', 'skills': 'programming'}, {'id': '76199997604772823340207052419224154975', 'skills': 'java'}, {'id': '76199997604772823340207052419224154975', 'skills': 'vbnet'}, {'id': '76199997604772823340207052419224154975', 'skills': 'windows'}, {'id': '76199997604772823340207052419224154975', 'skills': 'android'}, {'id': '76199997604772823340207052419224154975', 'skills': 'access'}, {'id': '76199997604772823340207052419224154975', 'skills': 'reporting'}, {'id': '76199997604772823340207052419224154975', 'skills': 'performance analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'analysis'}, {'id': '76199997604772823340207052419224154975', 'skills': 'visio'}, {'id': '305667889169402374245195369321522535673', 'skills': 'data analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'reporting'}, {'id': '305667889169402374245195369321522535673', 'skills': 'user acceptance testing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft access'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft excel'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sas'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sql server'}, {'id': '305667889169402374245195369321522535673', 'skills': 'cognos'}, {'id': '305667889169402374245195369321522535673', 'skills': 'crystal'}, {'id': '305667889169402374245195369321522535673', 'skills': 'business objects'}, {'id': '305667889169402374245195369321522535673', 'skills': 'sql'}, {'id': '305667889169402374245195369321522535673', 'skills': 'tableau'}, {'id': '305667889169402374245195369321522535673', 'skills': 'project management'}, {'id': '305667889169402374245195369321522535673', 'skills': 'data quality'}, {'id': '305667889169402374245195369321522535673', 'skills': 'monitor'}, {'id': '305667889169402374245195369321522535673', 'skills': 'hipaa'}, {'id': '305667889169402374245195369321522535673', 'skills': 'process analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'ms office'}, {'id': '305667889169402374245195369321522535673', 'skills': 'excel'}, {'id': '305667889169402374245195369321522535673', 'skills': 'access'}, {'id': '305667889169402374245195369321522535673', 'skills': 'claims analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'project planning'}, {'id': '305667889169402374245195369321522535673', 'skills': 'testing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'business requirements'}, {'id': '305667889169402374245195369321522535673', 'skills': 'analysis'}, {'id': '305667889169402374245195369321522535673', 'skills': 'reconciliation'}, {'id': '305667889169402374245195369321522535673', 'skills': 'processing'}, {'id': '305667889169402374245195369321522535673', 'skills': 'microsoft office'}, {'id': '83985006356839563643115453489150250520', 'skills': 'designing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'monitoring'}, {'id': '83985006356839563643115453489150250520', 'skills': 'analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'reporting'}, {'id': '83985006356839563643115453489150250520', 'skills': 'risk assessment'}, {'id': '83985006356839563643115453489150250520', 'skills': 'statistical analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'problem solving'}, {'id': '83985006356839563643115453489150250520', 'skills': 'modeling'}, {'id': '83985006356839563643115453489150250520', 'skills': 'project management'}, {'id': '83985006356839563643115453489150250520', 'skills': 'rapid application development'}, {'id': '83985006356839563643115453489150250520', 'skills': 'adhoc testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'backend testing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'critical thinking'}, {'id': '83985006356839563643115453489150250520', 'skills': 'decision making'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft access'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft excel'}, {'id': '83985006356839563643115453489150250520', 'skills': 'ms office'}, {'id': '83985006356839563643115453489150250520', 'skills': 'microsoft project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'powerpoint'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sharepoint'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sql'}, {'id': '83985006356839563643115453489150250520', 'skills': 'tableau'}, {'id': '83985006356839563643115453489150250520', 'skills': 'spotfire'}, {'id': '83985006356839563643115453489150250520', 'skills': 'visio'}, {'id': '83985006356839563643115453489150250520', 'skills': 'crm'}, {'id': '83985006356839563643115453489150250520', 'skills': 'sql server'}, {'id': '83985006356839563643115453489150250520', 'skills': 'aws'}, {'id': '83985006356839563643115453489150250520', 'skills': 'agile'}, {'id': '83985006356839563643115453489150250520', 'skills': 'data analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'business requirements'}, {'id': '83985006356839563643115453489150250520', 'skills': 'design'}, {'id': '83985006356839563643115453489150250520', 'skills': 'developing'}, {'id': '83985006356839563643115453489150250520', 'skills': 'drupal'}, {'id': '83985006356839563643115453489150250520', 'skills': 'data warehouses'}, {'id': '83985006356839563643115453489150250520', 'skills': 'research'}, {'id': '83985006356839563643115453489150250520', 'skills': 'cloud'}, {'id': '83985006356839563643115453489150250520', 'skills': 'ms project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'cisco'}, {'id': '83985006356839563643115453489150250520', 'skills': 'technical documentation'}, {'id': '83985006356839563643115453489150250520', 'skills': 'big data'}, {'id': '83985006356839563643115453489150250520', 'skills': 'hdfs'}, {'id': '83985006356839563643115453489150250520', 'skills': 'access'}, {'id': '83985006356839563643115453489150250520', 'skills': 'project'}, {'id': '83985006356839563643115453489150250520', 'skills': 'business analysis'}, {'id': '83985006356839563643115453489150250520', 'skills': 'visual studio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tableau'}, {'id': '278624286922027916746373616168944850452', 'skills': 'developing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboards'}, {'id': '278624286922027916746373616168944850452', 'skills': 'design'}, {'id': '278624286922027916746373616168944850452', 'skills': 'monitoring'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data profiling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'bi'}, {'id': '278624286922027916746373616168944850452', 'skills': 'teams'}, {'id': '278624286922027916746373616168944850452', 'skills': 'windows'}, {'id': '278624286922027916746373616168944850452', 'skills': 'active directory'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'plsql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'xml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'python'}, {'id': '278624286922027916746373616168944850452', 'skills': 'programming'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sharepoint'}, {'id': '278624286922027916746373616168944850452', 'skills': 'performance tuning'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data blending'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboard'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data analytics'}, {'id': '278624286922027916746373616168944850452', 'skills': 'waterfall'}, {'id': '278624286922027916746373616168944850452', 'skills': 'decision making'}, {'id': '278624286922027916746373616168944850452', 'skills': 'etl'}, {'id': '278624286922027916746373616168944850452', 'skills': 'agile'}, {'id': '278624286922027916746373616168944850452', 'skills': 'software development life cycle'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sdlc'}, {'id': '278624286922027916746373616168944850452', 'skills': 'testing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data validation'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data cleansing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data verification'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mismatch'}, {'id': '278624286922027916746373616168944850452', 'skills': 'db2'}, {'id': '278624286922027916746373616168944850452', 'skills': 'oracle'}, {'id': '278624286922027916746373616168944850452', 'skills': 'teradata'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql server'}, {'id': '278624286922027916746373616168944850452', 'skills': 'erwin'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dimensional modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'snowflake'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data warehouse'}, {'id': '278624286922027916746373616168944850452', 'skills': 'modeling'}, {'id': '278624286922027916746373616168944850452', 'skills': 'normalization'}, {'id': '278624286922027916746373616168944850452', 'skills': 'process analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'microsoft office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'word'}, {'id': '278624286922027916746373616168944850452', 'skills': 'powerpoint'}, {'id': '278624286922027916746373616168944850452', 'skills': 'excel'}, {'id': '278624286922027916746373616168944850452', 'skills': 'microsoft project'}, {'id': '278624286922027916746373616168944850452', 'skills': 'c'}, {'id': '278624286922027916746373616168944850452', 'skills': 'html'}, {'id': '278624286922027916746373616168944850452', 'skills': 'er studio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms visio'}, {'id': '278624286922027916746373616168944850452', 'skills': 'datasets'}, {'id': '278624286922027916746373616168944850452', 'skills': 'time series analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'matlab'}, {'id': '278624286922027916746373616168944850452', 'skills': 'collecting data'}, {'id': '278624286922027916746373616168944850452', 'skills': 'text mining'}, {'id': '278624286922027916746373616168944850452', 'skills': 'vmware'}, {'id': '278624286922027916746373616168944850452', 'skills': 'linux'}, {'id': '278624286922027916746373616168944850452', 'skills': 'cisco'}, {'id': '278624286922027916746373616168944850452', 'skills': 'exchange'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms access'}, {'id': '278624286922027916746373616168944850452', 'skills': 'javascript'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ajax'}, {'id': '278624286922027916746373616168944850452', 'skills': 'jquery'}, {'id': '278624286922027916746373616168944850452', 'skills': 'bootstrap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'angularjs'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ssis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ssrs'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tsql'}, {'id': '278624286922027916746373616168944850452', 'skills': 'aspnet'}, {'id': '278624286922027916746373616168944850452', 'skills': 'cognos'}, {'id': '278624286922027916746373616168944850452', 'skills': 'uml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'java'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visual basic'}, {'id': '278624286922027916746373616168944850452', 'skills': 'vb'}, {'id': '278624286922027916746373616168944850452', 'skills': 'windows xp'}, {'id': '278624286922027916746373616168944850452', 'skills': 'net'}, {'id': '278624286922027916746373616168944850452', 'skills': 'alteryx'}, {'id': '278624286922027916746373616168944850452', 'skills': 'develop'}, {'id': '278624286922027916746373616168944850452', 'skills': 'project'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visualizations'}, {'id': '278624286922027916746373616168944850452', 'skills': 'poc'}, {'id': '278624286922027916746373616168944850452', 'skills': 'reporting'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business intelligence'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visualization'}, {'id': '278624286922027916746373616168944850452', 'skills': 'olap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'processing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mining'}, {'id': '278624286922027916746373616168944850452', 'skills': 'unix'}, {'id': '278624286922027916746373616168944850452', 'skills': 'access'}, {'id': '278624286922027916746373616168944850452', 'skills': 'transform'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data sources'}, {'id': '278624286922027916746373616168944850452', 'skills': 'big data'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business requirements'}, {'id': '278624286922027916746373616168944850452', 'skills': 'css'}, {'id': '278624286922027916746373616168944850452', 'skills': 'html5'}, {'id': '278624286922027916746373616168944850452', 'skills': 'crystal'}, {'id': '278624286922027916746373616168944850452', 'skills': 'tomcat'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dashboard development'}, {'id': '278624286922027916746373616168944850452', 'skills': 'statistics'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sass'}, {'id': '278624286922027916746373616168944850452', 'skills': 'database design'}, {'id': '278624286922027916746373616168944850452', 'skills': 'analysis'}, {'id': '278624286922027916746373616168944850452', 'skills': 'dml'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data quality'}, {'id': '278624286922027916746373616168944850452', 'skills': 'load'}, {'id': '278624286922027916746373616168944850452', 'skills': 'loading'}, {'id': '278624286922027916746373616168944850452', 'skills': 'r'}, {'id': '278624286922027916746373616168944850452', 'skills': 'rest'}, {'id': '278624286922027916746373616168944850452', 'skills': 'transformation'}, {'id': '278624286922027916746373616168944850452', 'skills': 'code reviews'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data processing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sap'}, {'id': '278624286922027916746373616168944850452', 'skills': 'designing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business objects'}, {'id': '278624286922027916746373616168944850452', 'skills': 'sql server reporting services'}, {'id': '278624286922027916746373616168944850452', 'skills': 'office'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms sql server'}, {'id': '278624286922027916746373616168944850452', 'skills': 'business requirement'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data mapping'}, {'id': '278624286922027916746373616168944850452', 'skills': 'data warehousing'}, {'id': '278624286922027916746373616168944850452', 'skills': 'ms excel'}, {'id': '278624286922027916746373616168944850452', 'skills': 'visio'}, {'id': '6361630242691360309919431676680490692', 'skills': 'oracle'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms access'}, {'id': '6361630242691360309919431676680490692', 'skills': 'redshift'}, {'id': '6361630242691360309919431676680490692', 'skills': 's3'}, {'id': '6361630242691360309919431676680490692', 'skills': 'software products'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hadoop'}, {'id': '6361630242691360309919431676680490692', 'skills': 'amazon web services'}, {'id': '6361630242691360309919431676680490692', 'skills': 'programming'}, {'id': '6361630242691360309919431676680490692', 'skills': 'plsql'}, {'id': '6361630242691360309919431676680490692', 'skills': 'sql'}, {'id': '6361630242691360309919431676680490692', 'skills': 'impala'}, {'id': '6361630242691360309919431676680490692', 'skills': 'python'}, {'id': '6361630242691360309919431676680490692', 'skills': 'c'}, {'id': '6361630242691360309919431676680490692', 'skills': 'visual basic'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms excel'}, {'id': '6361630242691360309919431676680490692', 'skills': 'vba'}, {'id': '6361630242691360309919431676680490692', 'skills': 'power query'}, {'id': '6361630242691360309919431676680490692', 'skills': 'plsql developer'}, {'id': '6361630242691360309919431676680490692', 'skills': 'kibana'}, {'id': '6361630242691360309919431676680490692', 'skills': 'excel'}, {'id': '6361630242691360309919431676680490692', 'skills': 'eclipse'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms office'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms project'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ms visio'}, {'id': '6361630242691360309919431676680490692', 'skills': 'putty'}, {'id': '6361630242691360309919431676680490692', 'skills': 'tortoise svn'}, {'id': '6361630242691360309919431676680490692', 'skills': 'github'}, {'id': '6361630242691360309919431676680490692', 'skills': 'jira'}, {'id': '6361630242691360309919431676680490692', 'skills': 'unix'}, {'id': '6361630242691360309919431676680490692', 'skills': 'aws'}, {'id': '6361630242691360309919431676680490692', 'skills': 'business analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'system analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'design'}, {'id': '6361630242691360309919431676680490692', 'skills': 'project management'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hdfs'}, {'id': '6361630242691360309919431676680490692', 'skills': 'hive'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data sources'}, {'id': '6361630242691360309919431676680490692', 'skills': 'xml'}, {'id': '6361630242691360309919431676680490692', 'skills': 'sdlc'}, {'id': '6361630242691360309919431676680490692', 'skills': 'analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'regression'}, {'id': '6361630242691360309919431676680490692', 'skills': 'stored procedures'}, {'id': '6361630242691360309919431676680490692', 'skills': 'query optimization'}, {'id': '6361630242691360309919431676680490692', 'skills': 'agile'}, {'id': '6361630242691360309919431676680490692', 'skills': 'waterfall'}, {'id': '6361630242691360309919431676680490692', 'skills': 'scrum'}, {'id': '6361630242691360309919431676680490692', 'skills': 'estimation'}, {'id': '6361630242691360309919431676680490692', 'skills': 'business requirements'}, {'id': '6361630242691360309919431676680490692', 'skills': 'develop'}, {'id': '6361630242691360309919431676680490692', 'skills': 'loading'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data science'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data sets'}, {'id': '6361630242691360309919431676680490692', 'skills': 'reporting'}, {'id': '6361630242691360309919431676680490692', 'skills': 'requirement analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'technical design'}, {'id': '6361630242691360309919431676680490692', 'skills': 'optimization'}, {'id': '6361630242691360309919431676680490692', 'skills': 'monitoring'}, {'id': '6361630242691360309919431676680490692', 'skills': 'system testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'integration testing'}, {'id': '6361630242691360309919431676680490692', 'skills': 'automation'}, {'id': '6361630242691360309919431676680490692', 'skills': 'big data'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data analysis'}, {'id': '6361630242691360309919431676680490692', 'skills': 'data modeling'}, {'id': '6361630242691360309919431676680490692', 'skills': 'ftp'}, {'id': '6361630242691360309919431676680490692', 'skills': 'java'}, {'id': '6361630242691360309919431676680490692', 'skills': 'research'}, {'id': '6361630242691360309919431676680490692', 'skills': 'visio'}, {'id': '147203991935502946470515237354155400269', 'skills': 'analysis'}, {'id': '147203991935502946470515237354155400269', 'skills': 'reporting'}, {'id': '147203991935502946470515237354155400269', 'skills': 'ssrs'}, {'id': '147203991935502946470515237354155400269', 'skills': 'crystal'}, {'id': '147203991935502946470515237354155400269', 'skills': 'crm'}, {'id': '147203991935502946470515237354155400269', 'skills': 'technical documentation'}, {'id': '147203991935502946470515237354155400269', 'skills': 'project management'}, {'id': '147203991935502946470515237354155400269', 'skills': 'data quality'}, {'id': '147203991935502946470515237354155400269', 'skills': 'excel'}, {'id': '147203991935502946470515237354155400269', 'skills': 'visual basic'}, {'id': '147203991935502946470515237354155400269', 'skills': 'project designs'}, {'id': '147203991935502946470515237354155400269', 'skills': 'pivot table'}, {'id': '147203991935502946470515237354155400269', 'skills': 'vlookup'}, {'id': '147203991935502946470515237354155400269', 'skills': 'scrum'}, {'id': '147203991935502946470515237354155400269', 'skills': 'teams'}, {'id': '147203991935502946470515237354155400269', 'skills': 'test cases'}, {'id': '147203991935502946470515237354155400269', 'skills': 'sharepoint'}, {'id': '147203991935502946470515237354155400269', 'skills': 'assembly'}, {'id': '147203991935502946470515237354155400269', 'skills': 'technical assistance'}, {'id': '147203991935502946470515237354155400269', 'skills': 'gateway'}, {'id': '147203991935502946470515237354155400269', 'skills': 'analyzing data'}, {'id': '147203991935502946470515237354155400269', 'skills': 'business intelligence'}, {'id': '147203991935502946470515237354155400269', 'skills': 'business decisions'}, {'id': '147203991935502946470515237354155400269', 'skills': 'microsoft excel'}, {'id': '147203991935502946470515237354155400269', 'skills': 'data integrity'}, {'id': '147203991935502946470515237354155400269', 'skills': 'storage'}, {'id': '147203991935502946470515237354155400269', 'skills': 'design'}, {'id': '147203991935502946470515237354155400269', 'skills': 'uml'}, {'id': '147203991935502946470515237354155400269', 'skills': 'erd'}, {'id': '97098914450695563702884660921271152925', 'skills': 'stata'}, {'id': '97098914450695563702884660921271152925', 'skills': 'programming'}, {'id': '97098914450695563702884660921271152925', 'skills': 'r'}, {'id': '97098914450695563702884660921271152925', 'skills': 'sas'}, {'id': '97098914450695563702884660921271152925', 'skills': 'gis'}, {'id': '97098914450695563702884660921271152925', 'skills': 'nvivo'}, {'id': '97098914450695563702884660921271152925', 'skills': 'microsoft office'}, {'id': '97098914450695563702884660921271152925', 'skills': 'analytical research'}, {'id': '97098914450695563702884660921271152925', 'skills': 'project'}, {'id': '97098914450695563702884660921271152925', 'skills': 'evaluation'}, {'id': '97098914450695563702884660921271152925', 'skills': 'statistical models'}, {'id': '97098914450695563702884660921271152925', 'skills': 'research'}, {'id': '97098914450695563702884660921271152925', 'skills': 'data collection'}, {'id': '97098914450695563702884660921271152925', 'skills': 'data quality'}, {'id': '97098914450695563702884660921271152925', 'skills': 'statistical methods'}, {'id': '97098914450695563702884660921271152925', 'skills': 'analysis'}, {'id': '97098914450695563702884660921271152925', 'skills': 'designing'}, {'id': '138536846257210096623495337605137733129', 'skills': 'business processes'}, {'id': '138536846257210096623495337605137733129', 'skills': 'data integrity'}, {'id': '138536846257210096623495337605137733129', 'skills': 'sap'}, {'id': '138536846257210096623495337605137733129', 'skills': 'processing'}, {'id': '138536846257210096623495337605137733129', 'skills': 'budget analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'resource analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'crystal'}, {'id': '138536846257210096623495337605137733129', 'skills': 'analyzing data'}, {'id': '138536846257210096623495337605137733129', 'skills': 'analysis'}, {'id': '138536846257210096623495337605137733129', 'skills': 'microsoft office'}, {'id': '138536846257210096623495337605137733129', 'skills': 'excel'}, {'id': '138536846257210096623495337605137733129', 'skills': 'word'}, {'id': '138536846257210096623495337605137733129', 'skills': 'powerpoint'}, {'id': '138536846257210096623495337605137733129', 'skills': 'outlook'}, {'id': '138536846257210096623495337605137733129', 'skills': 'visio'}, {'id': '138536846257210096623495337605137733129', 'skills': 'access'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data privacy'}, {'id': '321556423433782918032074297826301460362', 'skills': 'research'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data analysis'}, {'id': '321556423433782918032074297826301460362', 'skills': 'cloud'}, {'id': '321556423433782918032074297826301460362', 'skills': 'networking'}, {'id': '321556423433782918032074297826301460362', 'skills': 'user training'}, {'id': '321556423433782918032074297826301460362', 'skills': 'monitoring'}, {'id': '321556423433782918032074297826301460362', 'skills': 'statistical analysis'}, {'id': '321556423433782918032074297826301460362', 'skills': 'reporting'}, {'id': '321556423433782918032074297826301460362', 'skills': 'data collection'}, {'id': '321556423433782918032074297826301460362', 'skills': 'project'}, {'id': '321556423433782918032074297826301460362', 'skills': 'algorithms'}, {'id': '321556423433782918032074297826301460362', 'skills': 'workflow diagrams'}, {'id': '321556423433782918032074297826301460362', 'skills': 'hipaa'}, {'id': '321556423433782918032074297826301460362', 'skills': 'windows xp'}, {'id': '321556423433782918032074297826301460362', 'skills': 'office'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data quality'}, {'id': '283692866702135157759709078394118382144', 'skills': 'oracle forms'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google drive'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google docs'}, {'id': '283692866702135157759709078394118382144', 'skills': 'google sheets'}, {'id': '283692866702135157759709078394118382144', 'skills': 'microsoft office'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data analysis'}, {'id': '283692866702135157759709078394118382144', 'skills': 'developing'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data patterns'}, {'id': '283692866702135157759709078394118382144', 'skills': 'data linking'}, {'id': '283692866702135157759709078394118382144', 'skills': 'test scripts'}, {'id': '283692866702135157759709078394118382144', 'skills': 'interviews'}, {'id': '283692866702135157759709078394118382144', 'skills': 'swot analysis'}, {'id': '283692866702135157759709078394118382144', 'skills': 'oracle'}, {'id': '283692866702135157759709078394118382144', 'skills': 'research'}, {'id': '189067289296683940487677712533012870883', 'skills': 'functional requirements'}, {'id': '189067289296683940487677712533012870883', 'skills': 'monitoring'}, {'id': '189067289296683940487677712533012870883', 'skills': 'excel'}, {'id': '189067289296683940487677712533012870883', 'skills': 'access'}, {'id': '189067289296683940487677712533012870883', 'skills': 'developing'}, {'id': '189067289296683940487677712533012870883', 'skills': 'data warehouse'}, {'id': '189067289296683940487677712533012870883', 'skills': 'monitor'}, {'id': '189067289296683940487677712533012870883', 'skills': 'sharepoint'}, {'id': '189067289296683940487677712533012870883', 'skills': 'data sharing'}, {'id': '189067289296683940487677712533012870883', 'skills': 'implement'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft word'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft powerpoint'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft excel'}, {'id': '189067289296683940487677712533012870883', 'skills': 'analyzing data'}, {'id': '189067289296683940487677712533012870883', 'skills': 'adobe photoshop'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft office'}, {'id': '189067289296683940487677712533012870883', 'skills': 'outlook'}, {'id': '189067289296683940487677712533012870883', 'skills': 'power point'}, {'id': '189067289296683940487677712533012870883', 'skills': 'microsoft publisher'}, {'id': '189067289296683940487677712533012870883', 'skills': 'word'}, {'id': '189067289296683940487677712533012870883', 'skills': 'project management'}, {'id': '189067289296683940487677712533012870883', 'skills': 'visio'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'statistical analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'r'}, {'id': '14259385793885776770164514431890372233', 'skills': 'sas'}, {'id': '14259385793885776770164514431890372233', 'skills': 'microsoft office'}, {'id': '14259385793885776770164514431890372233', 'skills': 'word'}, {'id': '14259385793885776770164514431890372233', 'skills': 'excel'}, {'id': '14259385793885776770164514431890372233', 'skills': 'power point'}, {'id': '14259385793885776770164514431890372233', 'skills': 'spreadsheets'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data platform'}, {'id': '14259385793885776770164514431890372233', 'skills': 'ms excel'}, {'id': '14259385793885776770164514431890372233', 'skills': 'data validation'}, {'id': '14259385793885776770164514431890372233', 'skills': 'accurate'}, {'id': '14259385793885776770164514431890372233', 'skills': 'pivot tables'}, {'id': '14259385793885776770164514431890372233', 'skills': 'analysis'}, {'id': '14259385793885776770164514431890372233', 'skills': 'research'}, {'id': '14259385793885776770164514431890372233', 'skills': 'r studio'}, {'id': '14259385793885776770164514431890372233', 'skills': 'statistics'}, {'id': '14259385793885776770164514431890372233', 'skills': 'ms word'}, {'id': '14259385793885776770164514431890372233', 'skills': 'powerpoint'}, {'id': '14259385793885776770164514431890372233', 'skills': 'access'}, {'id': '14259385793885776770164514431890372233', 'skills': 'sql'}]}\n",
      "{'total': 0, 'batches': 1, 'time': 0.021219730377197266}\n"
     ]
    }
   ],
   "source": [
    "#delete_cv_skill_link()\n",
    "#delete_cvs()\n",
    "#delete_skills(\"BD-ML-AI\")\n",
    "#delete_cv_profile_link()\n",
    "\n",
    "setup_db(\"livecareer-final-data-DE-IT2.csv\")\n",
    "#refresh_matcher_and_scores(conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
