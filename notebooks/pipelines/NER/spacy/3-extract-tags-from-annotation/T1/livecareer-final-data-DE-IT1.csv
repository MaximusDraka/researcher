profile|url|id|data|resume_html|skills
Data Engineer|https://www.livecareer.com/resume-search/r/data-engineer-2803c6ab603a4a0681e372b5316189e3|280519406331163022609303386259277165013|Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Professional Summary      Energetic Data Engineer in developing robust code for highvolume businesses Strong decisionmaker with 6 years of experience in Data engineering and help firms in designing and executing solutions for complex business problems involving large scale data warehousing realtime analytics and reporting solutions Ability to translate business questions and concerns into specific quantitative questions that can be answered with available data using sound methodologies        Skills           Python 3x R SQL  Hadoop Apache Spark  Hive Pig Kafka Sqoop Oozie  Teradata Snowflake      Amazon S3EMRLambda  Git Jenkins Splunk  MS Office  Microsoft Visual CNET                       Work History       Data Engineer       042020   to   Current     Avanade    –    Bangor     ME             Combining data from multiple source systems Profile Systematics etc and multiple platforms Snowflake OneLake Hubs and computing canonical goldstar metrics “once and for all” to cut operational costs  Develop Spark jobs to transform data and apply business transformation rules to loadprocess data across enterprise and application specific layers  Experience in buildingoperatingmaintaining fault tolerant and scalable data processing integrations using AWS  Configured S3 buckets with various life cycle policies to archive the infrequently accessed data based on requirement  Good working experience on submitting the Spark jobs which shows the metrics of the data which is used for Data Quality Checking  Working on building efficient data pipelines that transform high volume data into a format used for analytical fraud prevention and ML use cases  Extensively used Splunk Search Processing Language SPL queries Reports Alerts and Dashboards  Excellent knowledge of source control management concepts such as Branching Merging LabelingTagging and Integration with tool like Git  Performing Data Quality checks like row count schema validation Hash key validation for all data movement between applications           Data Engineer       082019   to   042020     Avanade    –    Burlington     NC             Responsible for designing and developing various analytical solutions for gaining analytical insights into large data sets by ingesting and transforming these datasets in the Big Data environment using technologies like Spark Sqoop Oozie HIVE  Scheduling jobs to automate the process for regular executing jobs worked on using Oozie  Developed Oozie workflow schedulers to run multiple Hive and Pig jobs that run independently with time and data availability  Familiar with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining  machine learning and advanced data processing  All the projects which I have worked for are Open Source Projects and has been tracked using JIRA  Parallel copying of files between various clusters using Distcp Kafka in Hadoop           Data Scientist Intern       092018   to   122018     Ascend Learning    –    Memphis     TN             Acquire clean integrate analyze and interpret disparate datasets using a variety of statistical data analysis and data visualization methodologies reporting and authoring findings where appropriate  Developed Linear Mixed Effects Models for Boston Edfi dataset to estimate the teacher contribution to the student test scores  Random intercept model which utilized a preposttest design and included fixed effects for student demographics and a growth model where students were nested by time random slope and intercept and teacher was tested  Generated various Clustering models for entire West Virginia department of education and evaluated cluster’s performance           Big Data Engineer       012014   to   072017     Verizon    –                      Identify customers digital analytical needs and engage with customer and principal architects daily understand the business requirements for big data analytical solutions and break down large scale requirements into detailed system specifications  Moving data to HDFS framework using SQOOP from Teradata SQL Server  Good working experience on Hadoop tools related to Data warehousing like Hive Pig and also involved in extracting the data from these tools on to the cluster using Sqoop  Solved performance issues in Hive with understanding of joins groups bucketing partitions and working on them using HiveQL  Deployed programs written in PySpark to run Spark MLlib for analytics and reduced customers churn rate by 25          Education       Master of Science     Business Analytics     Expected in   4 2020     The University Of Texas At Dallas      Richardson     TX     GPA               Websites Portfolios Profiles        httpswwwlinkedincominJessicaClaire    httpwwwgithubcomJessicaClaire                  Skills       Python 3x R SQL  Hadoop Apache Spark  Hive Pig Kafka Sqoop Oozie  Teradata Snowflake    Amazon S3EMRLambda  Git Jenkins Splunk  MS Office  Microsoft Visual CNET         Work History       Data Engineer     042020   to   Current     Capital One Financial Corp   –   Wilmington     DE      Combining data from multiple source systems Profile Systematics etc and multiple platforms Snowflake OneLake Hubs and computing canonical goldstar metrics “once and for all” to cut operational costs  Develop Spark jobs to transform data and apply business transformation rules to loadprocess data across enterprise and application specific layers  Experience in buildingoperatingmaintaining fault tolerant and scalable data processing integrations using AWS  Configured S3 buckets with various life cycle policies to archive the infrequently accessed data based on requirement  Good working experience on submitting the Spark jobs which shows the metrics of the data which is used for Data Quality Checking  Working on building efficient data pipelines that transform high volume data into a format used for analytical fraud prevention and ML use cases  Extensively used Splunk Search Processing Language SPL queries Reports Alerts and Dashboards  Excellent knowledge of source control management concepts such as Branching Merging LabelingTagging and Integration with tool like Git  Performing Data Quality checks like row count schema validation Hash key validation for all data movement between applications           Data Engineer     082019   to   042020     Verizon Wireless   –   Tampa     FL      Responsible for designing and developing various analytical solutions for gaining analytical insights into large data sets by ingesting and transforming these datasets in the Big Data environment using technologies like Spark Sqoop Oozie HIVE  Scheduling jobs to automate the process for regular executing jobs worked on using Oozie  Developed Oozie workflow schedulers to run multiple Hive and Pig jobs that run independently with time and data availability  Familiar with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining  machine learning and advanced data processing  All the projects which I have worked for are Open Source Projects and has been tracked using JIRA  Parallel copying of files between various clusters using Distcp Kafka in Hadoop           Data Scientist Intern     092018   to   122018     Hoonuit   –   Minneapolis     MN      Acquire clean integrate analyze and interpret disparate datasets using a variety of statistical data analysis and data visualization methodologies reporting and authoring findings where appropriate  Developed Linear Mixed Effects Models for Boston Edfi dataset to estimate the teacher contribution to the student test scores  Random intercept model which utilized a preposttest design and included fixed effects for student demographics and a growth model where students were nested by time random slope and intercept and teacher was tested  Generated various Clustering models for entire West Virginia department of education and evaluated cluster’s performance           Big Data Engineer     012014   to   072017     Tata Consultancy Services   –              Identify customers digital analytical needs and engage with customer and principal architects daily understand the business requirements for big data analytical solutions and break down large scale requirements into detailed system specifications  Moving data to HDFS framework using SQOOP from Teradata SQL Server  Good working experience on Hadoop tools related to Data warehousing like Hive Pig and also involved in extracting the data from these tools on to the cluster using Sqoop  Solved performance issues in Hive with understanding of joins groups bucketing partitions and working on them using HiveQL  Deployed programs written in PySpark to run Spark MLlib for analytics and reduced customers churn rate by 25|none|['data warehousing', 'Python', 'R', 'SQL', 'Hadoop', 'Apache', 'Spark', 'Hive', 'Pig', 'Kafka', 'Sqoop', 'Oozie', 'Teradata', 'Snowflake', 'S3EMRLambda', 'Git', 'Jenkins', 'Splunk', 'MS Office', 'Snowflake', 'Spark', 'AWS', 'Spark', 'ML use cases', 'Splunk', 'Git', 'Big Data', 'Spark', 'Sqoop', 'Oozie', 'HIVE', 'Scheduling jobs', 'Hive', 'data ingestion', 'pipeline design', 'Hadoop', 'data modeling', 'data mining', 'machine learning', 'JIRA', 'Distcp', 'Kafka', 'Hadoop', 'data analysis', 'data visualization', 'Big Data', 'business requirements', 'big data', 'Teradata', 'SQL Server', 'Hadoop', 'Data warehousing', 'Hive', 'Hive', 'PySpark', 'Spark', 'Python', 'R', 'SQL', 'Hadoop', 'Apache', 'Spark', 'Hive', 'Pig', 'Kafka', 'Sqoop', 'Oozie', 'Teradata', 'Snowflake', 'S3EMRLambda', 'Git', 'Jenkins', 'Splunk', 'MS Office', 'Snowflake', 'Spark', 'AWS', 'Spark', 'Splunk', 'Git', 'Big Data', 'Spark', 'Sqoop', 'Oozie', 'HIVE', 'Scheduling jobs', 'Hive', 'Hadoop', 'data modeling', 'data mining', 'machine learning', 'JIRA', 'Distcp', 'Kafka', 'Hadoop', 'data analysis', 'data visualization', 'Clustering', 'Big Data', 'business requirements', 'big data', 'HDFS', 'Teradata', 'SQL Server', 'Hadoop', 'Data warehousing', 'Hive', 'Hive', 'PySpark', 'Spark']
Data Engineer|https://www.livecareer.com/resume-search/r/data-engineer-3fb6a76ff6b44876ada5d6b8ca0e51f7|101004532999732917696975093886375431500|Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Involving in various projects related to Data Modeling Data Analysis Design and Development for Data warehousing environments Practical understanding of the Data modeling Dimensional  Relational concepts like StarSchema Modeling Snowflake Schema Modeling Fact and Dimension tables Comprehensive knowledge and experience in process improvement normalizationdenormalization data extraction data cleansing data manipulation Exceptional troubleshooting skills with ETL Technologies        Highlights         Hadoop Hive Avro Kafka MapReduce Looker Programming Languages SQL Java Scala PHP Shell Script HTML Database Tools Aster Database PostgreSQL MySQL Operating Systems Linux Unix Microsoft Windows                       Accomplishments              Experience       Data Engineer       082012      Current     Avanade    –    Greenville     NC            Experience with full development cycle of a Data Warehouse including requirements gathering design implementation and maintenance  Data modeling based on Kimball methodology developed and architected ETL processes for different projects RMA inventory purchase order etc Reengineer some of the current ETL processes to streamline the data acquisition and integration process using our homegrown ETL tools  Built eventdriven data pipeline that comprises multiple steps to gather high volume and velocity data from both push based and pull based sources which includes design and implement web service to collect data JSON object over http request convert data in JSON format into Avro then feed into Kafka land data in Kafka on Hadoop and Aster  Designed and built Looker API using Scala which makes other teams access the data in data warehouse more easily and gracefully  Establish and maintain SQL queries and routines  Write adhoc queries based upon the schema understanding for diverse needs of our business users           Java Intern       022012      032012     Dominion Enterprises    –    Fredericksburg     VA            Implemented code for small features  bugfixes in Java           PHP Developer       062011      092011     Meetoncruise    –    City     STATE            Implemented the backend logic using PHP  Utilized JQuery and AJAX to provide dynamic and interactive user interface  Designed and implemented data model in MySQL database to support the website          Education       Master of Science       Electrical and Computer Engineering       Expected in   2012                Polytechnic Institute of New York University      Brooklyn     NY     GPA        Status         Electrical and Computer Engineering         Bachelor of Science       Automation Engineering       Expected in   2010                Nanjing University of Aeronautics and Astronautics      Nanjing     Jiangsu     GPA        Status         Automation Engineering        Skills     streamline ad AJAX API data acquisition Data modeling data warehouse Database engineer ETL features HTML http PHP inventory Java JQuery JSON Linux logic access Microsoft Windows MySQL Operating Systems PostgreSQL processes Programming requirements gathering Shell Script SQL Unix user interface website|none|['Data Modeling', 'Data Analysis', 'Data warehousing', 'Data modeling', 'Modeling', 'Snowflake', 'Modeling', 'ETL', 'Hadoop', 'Hive', 'Kafka', 'MapReduce', 'Looker', 'Programming', 'SQL', 'Java', 'Scala', 'PHP', 'Shell', 'HTML', 'PostgreSQL', 'MySQL', 'Linux', 'Unix', 'Windows', 'Data modeling', 'ETL', 'ETL', 'ETL', 'JSON', 'JSON', 'Kafka', 'Kafka', 'Hadoop', 'Aster', 'Looker', 'Scala', 'access', 'SQL', 'Java', 'Java', 'PHP', 'PHP', 'JQuery', 'AJAX', 'MySQL', 'Automation', 'Automation', 'AJAX', 'Data modeling', 'ETL', 'HTML', 'PHP', 'Java', 'JQuery', 'JSON', 'Linux', 'access', 'Windows', 'MySQL', 'PostgreSQL', 'Programming', 'Shell', 'SQL', 'Unix']
Data Engineer|https://www.livecareer.com/resume-search/r/data-engineer-71257baffbce48d4a96368645e3008da|26624075482424204881256882949061507743|Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary      Over 5 years of engineering and finance experience as a Data Developer with cross platform integration experience using Hadoop and Spark architecture Handson experience configuring as well as installing Hadoop Ecosystem  HDFS MapReduce Pig Hive Oozie Flume HBase Spark Sqoop Flume and Oozie Strong understanding of various Hadoop services MapReduce and YARN architecture Vast knowledge in importing as well as exporting data in HDFS using SQOOP Automated transfer of data from Hbase by developing Map Reduce jobs Expertise in analysis using PIG HIVE and MapReduce Experience in HDFS data storage and support for running mapreduce jobs Involved in Infrastructure set up and installation of HDP stack on Amazon Cloud Experienced with ingesting data from RDBMS such as SQL Teradata into HDFS using Sqoop and Oracle Expertise in Hadoop architecture HDFS Mapreduce Oozie Sqoop Spark Hive Zookeeper and NoSQL databases Deployed and configured clusters in Cloudera Manager Set up backups and disaster recovery for Data Node and Name Node metadata as well as sensitive data on clusters Expertise in implementing and designing HDFS access controls directory and file permissions user authorization that facilitates stable secure access for multiple users in a large multitenant cluster Knowledge on exporting as well as importing stream data into HDFS using Flume Spark and Kafka messaging systems Utilized various schedulers on the Job tracker to share resources within clusters for Map Reduce jobs such as FIFO scheduler Fair Scheduler and Capacity Scheduler Monitored jobs with YARN and provisioned configured and installed HBase Kafka Hive Oozie Sqoop Ranger Storm Flume Spark as well as maintained total architecture AWS services for cloud migration such as S3 Redshift EMR Glue Athena and DynamoDB Expertise in Vertica DB architecture High Availability and column orientation Great knowledge of Agile and Scrum methodologies Behavior Driven Development Domain Driven DesignTest Driven Development and continuous integration as well as delivery Skilled in defining functional and gathering user interface requirements for applications and websites Expertise in real time analysis by utilizing RDD Datasets Data Frames and Streaming API in Apache Spark Expertise in using Spark Resilient distributed datasets and dataframe APIs over the Cloudera platform to perform analytics on Hive data by integrating Hadoop with Kafka Expertise in uploading Click stream data from Kafka to HDFS Expert in utilizing Kafka for messaging and publishing subscriber based messaging systems  Worked with NoSQL databases such as Cassandra HBASE PostgreSQL MongoDB Redis DynamoDB        Skills           SQL transactional replications  SAN technologies  Reverse engineering skills  Warehouse models  Security Protocols  Enterprise information architecture      Quality analysis  Deep learning  Data mining  Data analytics  Critical thinking                       Experience       Data Engineer       072021   to   Current     Principal Financial Group    –    Pittsburgh     PA            Analyze design and build Modern data solutions using Azure PaaS service to support visualization of data  Understand current Production state of application and determine the impact of new implementation on existing business processes  Extract Transform and Load data from Sources Systems to Azure Data Storage services using a combination of Azure Data Factory TSQL Spark SQL and USQL Azure Data Lake Analytics Data Ingestion to one or more Azure Services  Azure Data Lake Azure Storage Azure SQL Azure DW and processing the data in In Azure Databricks  Created Pipelines in ADF using Linked  ServicesDatasetsPipeline to Extract Transform and load data from different sources like Azure SQL Blob storage Azure SQL Data warehouse writeback tool and backwards  Developed Spark applications using Pyspark and  SparkSQL for data extraction transformation and  aggregation from multiple file formats for analyzing  transforming the data to uncover insights into the  customer usage patterns  Responsible for estimating the cluster size monitoring  and troubleshooting of the Spark databricks cluster  Experienced in performance tuning of Spark  Applications for setting right Batch Interval time  correct level of Parallelism and memory tuning   Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Contributed to maintaining  Type  databases in conjunction with data development and software engineering teams  Assisted solution providers with definition and implementation of technical and business strategies  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Data Engineer       072020   to   062022     Principal Financial Group    –    Portland     OR             Worked on MongoDB by using CRUD Create Read Update and Delete Indexing Replication and Sharding features  Involved in designing the row key in HBase to store Text and JSON as key values in the HBase table and designed row key in such a way to getscan it in sorted order  Oozie was integrated with the Hadoop stack which consists of MapReduce Hive Sqoop and Pig and Unix shell scripts  While working on Hive tables used Hive QL designed and Implemented PartitioningStatic Dynamic Buckets on Hive  Performed Cache and Persist as well as Checkpointing when utilizing Spark Streaming APIs to build common learning data models to get near realtime data from Kafka and persisted into Cassandra  Conducted cluster coordination services with Zookeeper and monitored the workload capacity planning and job performance through Cloudera Manager  Built applications by utilizing Maven and integrated with Continuous Integration servers such as Jenkins to build jobs  Deployed maintained and configured Test and multinode Dev Kafka Clusters as well as handled clusters and implemented data ingestion for real time processing in Kafka  Created cubes in Talend for various aggregation types of data from PostgreSQL and MS SQL server to visualize data  Monitored Name Node health status in Hadoop as well as the number of Data Nodes and Task trackers running along with automating jobs to pull data from various MySQL data sources to push result set data to HDFS  Created story telling dashboards through Tableau Desktop to publish on Tableau Server and integrated GitHub for version control tools to maintain the versions in projects  Deployed Spark applications in python and utilized Datasets and DataFrames in Spark SQL for processing data faster  Loaded transactional data with Sqoop from Teradata created managed and external tables in Hive and worked with semistructured and structured data of 5 Petabytes in size  Constructed MapReduce jobs to validate clean and access data and worked with Sqoop jobs with incremental load to populate and load into Hive External tables  Designed strategies to optimize distribution of weblog data over clusters in addition to exporting and importing stored web log data into Hive and HDFS through Sqoop  Responsibilities of building scalable data solutions that are distributed through Hadoop and Cloudera as well as developed and designed automated test scripts in Python  Integrated Apache Storm with Kafka to perform web analytics and to perform clickstream data from Kafka to HDFS  Developed SQL scripts and designed solutions to implement Spark with Hive Generic UDFs for incorporating business logic within Hive queries  Developed data pipelines in AWS using S3 EMR Redshift to extract data from weblogs to store into HDFS  Transmitted streaming data from Kafka to HBase Hive and HDFS by integrating Apache Storm and wrote Pig scripts for transforming raw data from various data sources to form baseline data  Participated in Agile meetings Ford Credit Customer Data domain conducted daily scrum meetings and spring planning  Expanded and optimized data pipelines and architecture as well as optimized data flow and collection  Created pipelines from scratch using PySpark and scheduled jobs using Airflow  Stream processed data in Kafka streaming wrote Producer Consumer Connector and Streams API to handle stream of records subscription of topics consume input and build reusable producers and consumers  Worked with unstructured datasets such as IoT sources sensor data XML and JSON document sources  Worked with on prem clusters as well as clusters on the cloud and used GCP Big Query Data Fusion and DataFlow  Scaled up architecture with Google Kubernetes and set up load balancing  Worked on various optimization techniques in Spark such as cache and persist using accumulators  bucketing and partitioning garbage collection tuning data serialization windowing functions and broadcast variables  Used numerous Spark transformations such as groupByKeyreduceByKey flatMap filter sample union etc  Utilized Dataproc to spin up clusters Dataprep for data analysis and Dataflow for streaming data using Apache Beam  Dealt with VPC controls on Google Cloud Platform and CMEK encryption for data security  Environment Hadoop HDFS HBase SparkPython and Scala Azure Databricks Scala Hive Kafka MapReduce Sqoop ETL Java Python PostgreSQL SQL Server Teradata UnixLinux           Big Data Developer       052017   to   052020     Cognizant Technology Solutions    –    Mount Laurel     NJ             Performed query tuning in HiveQL as well as performance tuning transformations in Pyspark using Spark RDDs and Python  Used lambda functions to create a Serverless Data intake pipeline on AWS  Using python constructed a Spark Streaming pipeline to receive realtime data from Apache Kafka and store it in DynamoDB  Implemented Apache Spark data processing module to handle data from multiple RDBMS and Streaming sources then compiled Apache Spark applications using Scala and Python  Extensive experience designing and scheduling multiple Spark Streaming  batch Jobs in Python pyspark and Scala  Achieved highthroughput scalable faulttolerant stream processing of live data streams using Apache Spark Streaming  Involved with the use for creating and saving data frames using various Python modules with pyspark  Sqooped data and performed Hive queries for data ingestion from relational databases to analyze historical data  Experienced with Elastic MapReduce EMR as well as setting up environments on amazon AWS EC2 instances for pipelines in AWS  Expertise in handling Hive queries using Spark SQL such as window functions and aggregations  Ran Spark applications on Docker using EMR and used AWS Glue data catalog as the metastore in Spark SQL  Configured different File Formats like Avro parquet for HIVE querying and processing based on business logic  Utilized Sequence files RC files Map side joins bucketing partitioning for Hive performance enhancement and storage improvement  Implemented Hive UDF to implement business logic and performed extensive data validation using Hive  Involved in loading the structured and semi structured data into spark clusters using Spark SQL and Data Frames API  Utilized AWS CloudWatch to monitor the performance environment instances for operational and performance metrics during load testing  Scripting Hadoop package installation and configuration to support fully automated deployments  Involved in chefinfra maintenance including backupsecurity fix on Chef Server  Deployed application updates using Jenkins  Installed configured and managed Jenkins  Triggering the SIT environment build of the client remotely through Jenkins  Deployed and configured Git repositories with branching forks tagging and notifications  Worked on MongoDB database concepts such as locking transactions indexes Shading replication schema design  Viewing the selected issues of web interface using SonarQube  Developed a fully functional login page for the companys user facing website with complete UI and validations  Installed Configured and utilized AppDynamics Tremendous Performance Management Tool in the whole JBoss Environment Prod and NonProd  Installed and installed Hive in a Hadoop cluster and assisted business usersapplication teams in finetuning their HIVE QL for optimal performance and efficient use of cluster resources  Utilized Oozie workflow for ETL Process for critical data feeds across the platform  Configured Ethernet bonding for all Nodes to double the network bandwidth  Configured Kerberos Security Authentication protocol for existing clusters  Constructed the use of Zookeeper failover controller ZKFC and Quorum Journal nodes for high availability for significant production clusters and automatic failover controller created  Installation and deployment of many Apache Hadoop nodes on an AWS EC2 system as well as development of Pig Latin scripts to replace the old traditional process with Hadoop and data feeding to AWS S3  Experience with AWS CloudFront including the creation and management of distributions that provide access to an S3 bucket or an HTTP server running on EC2 instances  Developed Python scripts UDFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop And Developed enterprise application using Python  Constructed Spark application performance optimization including determining the appropriate Batch Interval time Parallelism Level and Memory Tuning  Experience and handson knowledge in Akka and LIFT Framework  Used PostgreSQL and NoSQL database and integrated with Hadoop to develop datasets on HDFS  Environment HDFS Map Reduce Hive 110 Kafka Hue 390 Pig Flume Oozie Sqoop Apache Hadoop 26 Spark SOLR Storm Cloudera Manager Red Hat MySQL Prometheus Docker Puppet YARN SparkSQL Python Amazon AWS Elastic Search Tableau Linux  Key Skills SQL Apache hive Apache SparkDatabricks Jupyter Notebook Anaconda Python Django Pandas Flask Keras NumPy Scikitlearn MatPlotLib Tensorflow  Time Series Forecasting AB testing Bayesian methods PowerBI MicrosoftWord Excel Powerpoint Java Data Visualization Analytical Skills Cost Accounting Corporate Finance Knowledge Statistical AnalysisRStudio          Education and Training       Bachelor’s     Computer Science     Expected in        Uttara Institute of Business and Technology                GPA|none|['Hadoop', 'Spark', 'Hadoop', 'HDFS', 'MapReduce', 'Pig', 'Hive', 'Oozie', 'Flume', 'HBase', 'Spark', 'Sqoop', 'Flume', 'Oozie', 'Hadoop', 'Hbase', 'PIG', 'HIVE', 'RDBMS', 'SQL', 'Teradata', 'Oracle', 'Hadoop', 'Mapreduce', 'Oozie', 'Sqoop', 'Spark', 'Hive', 'Zookeeper', 'NoSQL', 'access', 'access', 'HDFS', 'Flume', 'Spark', 'Kafka', 'HBase', 'Kafka', 'Hive', 'Oozie', 'Sqoop', 'Ranger', 'Storm', 'Flume', 'Spark', 'AWS', 'Redshift', 'DynamoDB', 'Agile', 'Scrum', 'Apache', 'Spark', 'Spark', 'Hive', 'Hadoop', 'Kafka', 'Kafka', 'HDFS', 'Kafka', 'NoSQL', 'Cassandra', 'HBASE', 'PostgreSQL', 'MongoDB', 'Redis', 'DynamoDB', 'SQL', 'Deep learning', 'Data mining', 'Critical thinking', 'Azure', 'Azure', 'Azure', 'Spark', 'SQL', 'Azure', 'Azure', 'Azure', 'Azure', 'Azure', 'SQL', 'Azure', 'Azure', 'Databricks', 'ADF', 'Azure', 'SQL', 'Azure', 'SQL', 'Spark', 'Pyspark', 'Spark', 'databricks', 'Spark', 'MongoDB', 'HBase', 'JSON', 'HBase', 'Hadoop', 'Hive', 'Unix', 'shell', 'Hive', 'Hive', 'Hive', 'Spark', 'Kafka', 'Cassandra', 'Maven', 'Jenkins', 'Kafka', 'Kafka', 'PostgreSQL', 'MS SQL server', 'Hadoop', 'MySQL', 'Tableau', 'Tableau', 'Spark', 'python', 'Spark', 'SQL', 'Teradata', 'Hive', 'access', 'Hive', 'Hive', 'Hadoop', 'Python', 'Apache', 'Kafka', 'Kafka', 'SQL', 'Spark', 'Hive', 'Hive', 'AWS', 'Redshift', 'Kafka', 'HBase', 'Hive', 'Apache', 'Agile', 'scrum', 'spring', 'PySpark', 'Airflow', 'Kafka', 'IoT', 'XML', 'JSON', 'GCP', 'Kubernetes', 'optimization', 'Spark', 'Spark', 'data analysis', 'Apache', 'Google Cloud Platform', 'Hadoop', 'HBase', 'Scala', 'Azure', 'Databricks', 'Scala', 'Hive', 'Kafka', 'ETL', 'Java', 'Python', 'PostgreSQL', 'SQL Server', 'Teradata', 'Big Data', 'Pyspark', 'Spark', 'Python', 'AWS', 'python', 'Spark', 'Apache', 'Kafka', 'DynamoDB', 'Apache', 'Spark', 'RDBMS', 'Apache', 'Spark', 'Scala', 'Python', 'Spark', 'Python', 'pyspark', 'Scala', 'Apache', 'Spark', 'Python', 'pyspark', 'Hive', 'amazon AWS', 'AWS', 'Hive', 'Spark', 'SQL', 'Spark', 'Docker', 'AWS', 'Spark', 'SQL', 'HIVE', 'Hive', 'Hive', 'Hive', 'spark', 'Spark', 'SQL', 'AWS', 'testing', 'Hadoop', 'Jenkins', 'Jenkins', 'Jenkins', 'Git', 'MongoDB', 'Hive', 'Hadoop', 'HIVE', 'ETL', 'Apache', 'Hadoop', 'AWS', 'Hadoop', 'AWS', 'AWS', 'access', 'Python', 'Spark', 'Python', 'Spark', 'optimization', 'PostgreSQL', 'NoSQL', 'Hadoop', 'Hive', 'Kafka', 'Apache', 'Hadoop', 'Spark', 'MySQL', 'Docker', 'Python', 'Amazon AWS', 'Tableau', 'Linux', 'SQL', 'Apache', 'hive', 'Apache', 'Jupyter', 'Python', 'Django', 'Pandas', 'Flask', 'Keras', 'NumPy', 'Tensorflow', 'Time Series Forecasting', 'testing', 'PowerBI', 'Excel', 'Powerpoint', 'Java', 'Data Visualization']
Data Engineer|https://www.livecareer.com/resume-search/r/data-engineer-b9cc3bb45c274243bede854fa43fe37c|272474282630622662458174313553127824080|Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      I am an Automation Developer with a love for troubleshooting and electronics This passion for automation started at Micro Center in 2017 where I began finding ways to simplify my reports to upper management These reports took several hours to compile and took the time I could have spent guiding my agents Eventually I started finding ways to gather that data through an API After discovering I could collect this information and use APIs to simplify tasks I started programming software to do these things This new skill helped change how I approached my teams problems  complaints and eventually led to me creating tools to improve our efficiency and productivity Since then my passion has grown into a career that keeps me wanting to learn more        Skills           Systems Engineering  Lab Test Technician  Cybersecurity  Data Engineering      API Design  Deployment  Software Developer Python NodeJS PHP Bash Git Etc  Automation  Disaster Recovery                       Experience       Data Engineer       062021      112023     Bank Of America Corporation    –    Aurora     IL            Deployed Linux cloud servers and applications to meet the needs of the company PostgreSQL Metabase Apache Spark TableAU  Secured servers and applications using ACLsﬁrewallsIDSIPS and following standard security practices  Built several ETLs in PythonPHPJavaScript for several ticketing platforms  Designed disaster recovery plans for our systems  Constructed complex SQL queries to aid in reporting and automation  Tested data regularly to ensure accurate reporting  Managed several PostgreSQL databases  Automated reports and redundant tasks through the use of APIs and databases  Created and implemented complex business intelligence solutions  Created conceptual logical and physical data models for use in different business areas  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified protected and leveraged existing data  Planned and installed database management system software upgrades to enhance systemic performance           Test Engineering Technician       082020      062021     General Atomics    –    Houlka     MS            Traveled daily to integrators to provide quality assurance for PowerSpec systems and review the build process  Compiled reports for integrators to log and classify defects  Answered product questions for PowerSpec systems  Regularly assisted neighboring departments on build projects and quality controlled parts for buyers  Worked directly with vendors for BIOS updates and patches on PowerSpec systems  Created and deployed system images for PowerSpec products  Built extensive QC reports through rigorous testing methodologies including benchmarking components and testing for compatibility  Performed problem solving and resolution for technical quality inspection and customer quality issues  Developed and maintained solutions for integration and testing phases  Tested functionality performance and compliance of each product against design specifications to maintain strong development standards and high customer satisfaction  Completed unit and regression tests on software and individual modules  Created and optimized automated testing tools for repetitive tasks  Created and maintained database of common and known testing defects  Worked with offsite teams to complete timely tests and facilitate smooth product releases  Promoted high customer satisfaction by resolving problems with knowledgeable and friendly service           Call Center Lead       012019      082020     Metlife    –    Omaha     NE            Encouraged team members to improve productivity and service levels by modeling correct behaviors and coaching employees  Resolved team support issues with efficient approach to keep call center operating smoothly and customers satisfied with services  Routinely updated guides to reduce questions on the ﬂoor  Provided accurate and detailed reports for management  Handled customer escalations and ensured they went to the appropriate team  Maintained Zendesk and the ticketing procedures to ensure both a smooth and efficient experience for my agents  Worked with management to update legacy procedures and remove redundant processes through automation  Managed customer concerns with calm demeanor and knowledgeable service  Created an efficient workflow in order to provide a better experience for consumers  Kept records of customer interactions or transactions thoroughly recording details of inquiries  Worked directly with stores and customer relations team to improve customer retention  Built and maintained call center server to house reports and automations           Technical Consultant       092017      012019     Applied Systems Inc    –    University Park     IL            Troubleshot and resolved problems with programs and systems  Utilized knowledge of applications programming and systems functionality to assist employees with technical needs  Assisted customers with various types of technical issues via email live chat and telephone  Handled customer service issues by providing guidance or escalating for advanced support  Served as first point of contact for escalated technical service calls emails and live chat  Troubleshot hardware issues and worked with service providers to facilitate repairs for end users  Developed and maintained strong relations with customers to meet quality expectations  Documented customer complaints and inquiries for use in technical documentation and bug tracking  Reviewed support cases for technical and troubleshooting accuracy and identified needed process improvements  Maintained uptodate case documentation for future reference  Demonstrated advanced product knowledge to solve customer issues  Delivered remote assistance for technical issues using screen sharing mouse and keyboard control and other tools          Education and Training       High School Diploma       Electronic Classroom       Expected in   062012                Electronic Classroom of Tomorrow      Columbus OH          GPA        Status   |none|['C', 'Automation', 'automation', 'programming', 'Cybersecurity', 'API Design', 'Python', 'NodeJS', 'PHP', 'Bash', 'Git', 'Automation', 'Aurora', 'Linux', 'PostgreSQL', 'Apache', 'Spark', 'TableAU', 'SQL', 'automation', 'PostgreSQL', 'database management system', 'quality assurance', 'problem solving']
Data Engineer|https://www.livecareer.com/resume-search/r/data-engineer-2c2f724ef72e40e2b35028b711268d0e|60738515287105690973581748617742576500|Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary       Data Engineering  with experience in Design Development Implementation and support of  Data Warehousing  for over  8 years  Experienced in complete  Software Development Life Cycle SDLC   software Testing Life Cycle STLC   SDLC methodologies  Extensively worked on  Informatica Designer Tools  Source Analyze Warehouse Designer Mapping Designer Mapplet Designer and Transformation Developer and Workflow Manager Tools Task Developer Worklet and Workflow Designer Workflow Monitor and informatica Power exchange Experience in working with using Informatica in  SAP HANA   Oracle   MS SQL   Teradata  and  DB2 environments  Hands on experience in migrating on premise ETL to  Google Cloud PlatformGCP  using cloud native tools such as  BIG Query   Cloud Data Proc   Google Cloud Storage   Composer   Practical understanding of the Data modeling concepts like  Star  Schema Modeling snowflake Schema Modeling Fact and Dimension tables  Also experience in Optimizing Database querying data manipulation using SQL and  PLSQL  in Oracle flat files and SQL server database Experienced in implementing  Change Data Capture CDC  using informatica Power Center for Oracle and SAP Systems Experienced in debugging mapping by analyzing the data flow and evaluating transformations Experienced in performance tuning of data flow through source target sessions and mappings  identifying and resolving performance bottlenecks at various stages  using techniques like Database tuning and Session Partitioning Experienced in test strategy developing test plan details test cases and writing test scripts by decomposing business requirements and developing test scenarios to support quality deliverables Involved in test planning and execution for various Test Phases  Unit Test   System and User Acceptance Testing  Expert in  analyzing designing developing installing configuring  and  deploying  MS SQL Server suite of products with Business Intelligence in SQL Server Reporting Services SQL Server Analysis Services and SQL Server Integration Services Performed data profiling data cleansing data conversion exception handling and data matching using  informatica IDQ   Excellent communication skills good organizational skills selfmotivated hardworking ability to grasp quickly and learn fast and open to new technologies         Skills  Tools           ETL Tools Informatica Power Center 1051 Power Exchange 961901 Informatica Data Quality 961 Power connect for SAP BW power connect for JMS power connect for IBM MQ series power connect for Mainframes DTS MDM ERWIN  Scheduling TIDAL UC4 CONTROLM AUTOSYS OpCon  Oracle SAP HANA MS SQL Server Snowflake MongoDB Teradata DB2 AWS  Python Java SQL UNIX Unix Shell Scripts HTML XML JSON Microsoft Office Apache Spark Kafka Kubernetes Hive Scala  Data analysis Data management Data warehouse Big Data PLSQL RDBMS NoSQL Vertica Spark Kafka Oozie Maven      Debugging Coding Designing Quality analysis ETL SAP BW  AWS Cloud Tools EC2 Elastic Loadbalancers Elastic Container Service Docker Containers S3 Elastic Beanstalk Cloud Front Elastic Files System RDS Dynamo DB DMS VPC Direct Connect Route53 Cloud Watch Cloud Trail Cloud Formation IAM EMR ELB Lambda functions REST API Airflow Data Pipeline RedShift  Google Cloud Platform GCP Cloud Storage Big Query Composer Cloud Dataproc Cloud SQL Cloud Functions Cloud PubSub Dataflow AI Building Blocks Looker Cloud Data Fusion Dataprep Firestone  Azure Azure Storage Database Azure Data Factory Azure Analysis Services                       Experience       Data Engineer       082019      Current     Lockheed Martin    –    Eagan     MN            Responsible in Identifying all the Legacy systems analyze their  data models  mathematical and  Scientific models and all the business components to be migrated to Arkansas Integrated Eligibility system ARIES  NexGen solution   Business and Data Mapping Analyst worked with Client to gather business and functional requirements  Experience in building largescale data pipelines and datacentric applications using Big Data tooling like Hadoop Spark Hive and Airflow in a production setting  Experience in working with Rest APIs for data extraction  Designing and coordinating with  Informatica Power center  admin to set up services users Groups database components like tables indexes procedures and synonyms Unix groups and User access for ARIES system  Designing all the components of ARIES system and step by step conversion approach using the  Ralph Kimball  and  Bill Inmom  data warehouse design methodologies to determine the feasibility of the design within the time and cost constraints  Development of  ETL  Extract Transform and Load code components like mappings Workflows Sessions and database objects like stored procedures functions and  Unix shell scripts  from business requirements and design plan  Designing and developing the mathematical models and analytical reports in  Cognos  analyze  Developing integrations using  Informatica Cloud  Data Integration  IICS  –  CDI  Service  For Google Analytics  Responsible Identifying the performance bottleneck in the ARIES systems during Integrated  Testing  IT and  System Testing  ST and PT  Performance Testing  phases and implement Performance tuning techniques  Confer with the scrum master client partners Business Managers and adhere to the  agile   Experience in multiple database technologies such as traditional RDBMS MS SQL Server Oracle MySQL PostgreSQL MPP AWS Redshift Snowflake Teradata Distributed Processing Spark Hadoop EMR NoSQL MongoDB DynamoDB Cassandra Neo4J Titan  Conduct code review sessions with peer developers to ensure code quality  Wrote scripts and processes for  data integration  and bug fixes  Utilized  Python  to handle debugging and automation scripting tasks  Created and implemented complex business intelligence solutions  Create  Managing buckets on  S3  and store DB and log backup upload images for CND server  Setup databases on  Amazon RDS  or  EC2 Instances  as per requirements  Handson experience with snowflake utilities SnowSQL SnowPipe Big data model techniques using python  Expert in migrating data from various systems into Salesforce CRM using ETL tools   Informatica   Hands on Experience in  Data Management Data Security Data Modeling Data Quality Workflow Automation Formulas  Validations   Collaborated with Legacy team to define data extraction methodologies and data source tracking protocols  Used  SparkSQ L to read parquet data and create tables in  Hive  using  Scala API   Hands on experience with data ingestion tools like  Sqoop Kafka Flume   Used various  spark transformations  and  Actions  for cleansing the input data  Installed and configured  Apace Airflow  for workflow management and created workflow in python  Worked on Tableau Desktop versions 782 Tableau Reader and Server  Build data pipelines in Airflow in GCP for ETL related jobs using different airflow operators  Experience in GCP Dataproc GCS Cloud functions BigQuery           BIETL Developer       112018      072019     Wells Fargo    –    Loveland     CO            Analyzing the source data coming from Mainframe sources and working with business users and developers to develop the Model  Created xsd and used it in  xml generator Transformation   Also called  WSDL  files using web consumer transformation in  Informatica   Effectively and efficiently communicated systems solutions to business problems to team members business unit representatives management and other impacted project teams  Analyze and modify existing stored procedures functions and queries in order to integrate them into previously built reporting application utilizing  SSIS  and  SQL Server Management Studio   Change ETL process from  importing flat files  and  COBOL  as source for previously built application utilizing informatica Power center 96 to read data from existing Reporting Server SQL Server databases  Interface with report users to determine requirements for new and existing reports  Develop test and deploy  ETL  jobs with reliable errorexception handling and rollback framework Manage automation of file processing as well as all ETL processes within a job  workflow   Worked as a Data Analysts to match the current data mappings with old mainframe mappings  Created Design Documents Context Diagrams on every Projects and did code review  Created Reusable lookup to send emails Notification to Users  This Lookup was used by multiple developer in multiple Projects  STOP and START the  Netezza  appliances in case of issues  Prepared best practices documentation for teams in writing  NOSQL   Conducted sessions to help explain teams about the  Netezza  architecture and design  As a part of Informatica decommission objects Project Used  SVN  to archive the ETL Code and delete the unwanted mappings and workflows  Extracted data from  flat file  and staged into a single place and applied business logic to load them in the  Teradata database   Lead in few complex Projects and Performed Unit testing and created  QA documents   Participated in solution brainstorming and provided technical instruction and coaching to others within organization  Developed technical project deliverables  Created spreadsheets using  Power BI  and  Power Pivot  by importing data from the sources directly  Created visually impactful dashboards in Excel and  Tableau  for data reporting using  PivotTables  and  VLOOKUP   Involved in migrating ETL code lower environments to like dev testLoad to production environment  Used  Debugger  for debugging Mappings  Created  Tidal  Jobs and  Runbooks  to schedule jobs in Tidal  Scheduling ETL Jobs using  UC4 scheduler   Migrated ETL code using Deployment Groups from Dev to Prod environments  Created SSIS package for loading the data coming from various interfaces like OMS Orders Adjustments and Objectives and also used multiple transformation in SSIS to collect data from various sources  Worked on SSIS Package DTS ImportExport for transferring data from Database Oracle and Text format data to SQL Server  Created  SSIS  packages for File Transfer from one location to the other using FTP task  Manage and document our platform infrastructure This can go from installing a new Consul server to resolving performance issues in a MongoDB cluster through setting up a continuous integration pipeline           Informatica Developer       122017      102018     Cognizant Technology Solutions    –    Horsham     PA            Involved in all the phases of the development like Analysis Design Coding  Unit Testing  System Testing and UAT  Extracted data from  heterogenous sources  and performed complex transformations to load data into the target systems  Resolved various performance issues by examining the logs current design and  removing the bottlenecks   Created reusable ETL components which need to be run at the mapping session and workflow levels  Wrote complex SQL queries and performed extensive data analysis in  Oracle 11g   Peer reviewed developers code and ensured they fall into the enterprise guidelines  Worked extensively with session parameters Mapping Parameters Mapping Variables and Parameter files for Incremental Loading  Created Informatica mappings to load Payment flow BT cash data Currency conversion data from  SAP HANA DSL layer  to  SAP HANA DPL layerDLL SAP BA layer  Implemented SAP BW and  BOBJ  with different SAP data source  Used  Qlik  and Informatica for  ETL   reporting  Worked closely with  SAP ABAP  team to create logical systems RFC destinations and to create tRFC port for RFC destinations  Followed best practices that were defined at the enterprise level and also peer reviewed the code for the same  Created and reviewed scripts to create new tables queries for new enhancements and bug fixes in the existing  data warehouse   Used Debugger and various other techniques like tracing to fix the defects errors and data issues  Extensively worked with various Lookup caches like  Static cache Dynamic cache and Persistent cache   Involved in developing the Deployment groups for deploying the code between various environment Dev QA  Worked in the Data Integration Team to perform data and application integration with a goal of moving more data more effectively efficiently and with high performance to assist in businesscritical projects coming up with huge data extraction  Migrated data from SQL Server to  Netezza using NZMigrate utility   Experience in integration of various data sources like  Oracle DB2 SQL server csv XML and Flat Files  into staging area  Developed code to extract transform and load ETL data from inbound flat files and various databases into outbound  flat files  and  XML  files using complex business logic  Involved in deploying objects from DEV to UATPROD during monthlyquarterly releases  Developed Slowly Changing Dimension Mappings for  Type 1  SCD  and  Type 2 SCD  Monitored and improved query performance by creating views indexes and sub queries Extensively involved in enhancing and managing Unix Shell Scripts  Developed workflow dependency in  Informatica  using Event Wait Task Command Wait  Involved in  L3 Support  by fixing load failures and defects during peak and offPeak hours  Working with Informatica and  SAP ECC   We are extracting data in Informatica from  SAP ECC  via  Business Content Data sources  Design the ETL architecture for the conversion of source from legacy to new  ERP SAP   Built Sqoop scripts to extract data from the SAP  Responsible for data integration  Epicor and SAP  and data management  Validated DW data with standard  SAP reports            ETL Informatica Developer       082013      122015     ObjectOne Information Systems    –    City     STATE           · Responsible for developing ETL mappings for the reporting requirement  for data feeds  · Created mapplets and many other reusable components reduce redundancy  · Provided the best solution for their requirement Help the Clients in designing of the system and process to make it more robust and meaningful  · Created the ETL component and test scripts Involved in client meetings for their inconsistent requirements  · Sourced data from  SAP HANA  using  SAP Adapter  in Informatica Target Tables  · End to end processing from the source to target Load the data from various File systems to the  DWH  and  DM  Maintain the Versioning for the objects by using  VSS   · Successfully conducted Load Testing and Performance Testing  · Resolve the issues coming from the end to end processing Perform the enhancements if required by the business users requirement  for data feeds  · Involved in preparing Source to Target mappings and Application Design Document Worked closely with all upstream and downstream application owners to make sure interface agreement documents are clear  · Successfully conducted Load Testing and Performance Testing Automated regression test suite for actuate reporting and true portal  · Used  ALM  to track and report system defects and bugs writing modification request for the bugs in the application and helped developers to track the problem and resolve the technical issues         Education and Training       Master of Science       Computer And Information Systems       Expected in   122017                New England College      Henniker     NH     GPA        Status   |none|['C', 'Data Warehousing', 'Software Development Life Cycle', 'software Testing', 'exchange', 'SAP HANA', 'Oracle', 'SQL', 'Teradata', 'DB2', 'ETL', 'Google Cloud', 'Google Cloud', 'Data modeling', 'Modeling', 'snowflake', 'Modeling', 'SQL', 'Oracle', 'SQL server', 'Power Center', 'Oracle', 'SAP', 'business requirements', 'Testing', 'MS SQL Server', 'SQL Server Reporting Services', 'SQL Server Analysis Services', 'SQL Server Integration Services', 'ETL', 'Power Center', 'Exchange', 'SAP', 'JMS', 'DTS', 'MDM', 'ERWIN', 'TIDAL', 'UC4', 'CONTROLM', 'AUTOSYS', 'OpCon', 'Oracle', 'SAP HANA', 'MS SQL Server', 'Snowflake', 'MongoDB', 'Teradata', 'DB2', 'AWS', 'Python', 'Java', 'SQL', 'UNIX', 'Unix', 'Shell', 'HTML', 'XML', 'JSON', 'Microsoft Office', 'Apache', 'Spark', 'Kafka', 'Kubernetes', 'Hive', 'Scala', 'Data analysis', 'Big Data', 'RDBMS', 'NoSQL', 'Vertica', 'Spark', 'Kafka', 'Oozie', 'Maven', 'ETL', 'SAP', 'AWS Cloud', 'Docker', 'REST', 'Airflow', 'Data Pipeline', 'RedShift', 'Google Cloud Platform', 'GCP', 'Cloud Storage', 'Big Query', 'SQL', 'Looker', 'Azure', 'Azure', 'Azure', 'Data Factory', 'Azure', 'Big Data', 'Hadoop', 'Spark', 'Hive', 'Airflow', 'Rest', 'Power center', 'Unix', 'access', 'ETL', 'Unix', 'shell', 'business requirements', 'Cognos', 'Testing', 'Testing', 'Testing', 'scrum', 'agile', 'RDBMS', 'MS SQL Server', 'Oracle', 'MySQL', 'PostgreSQL', 'MPP', 'AWS', 'Redshift', 'Snowflake', 'Teradata', 'Spark', 'Hadoop', 'EMR', 'NoSQL', 'MongoDB', 'DynamoDB', 'Cassandra', 'Neo4J', 'Python', 'automation', 'snowflake', 'Big data', 'python', 'Salesforce', 'CRM', 'ETL', 'Data Modeling', 'Automation', 'Hive', 'Scala', 'Kafka', 'spark', 'Airflow', 'python', 'Tableau', 'Tableau', 'Airflow', 'GCP', 'ETL', 'airflow', 'GCP', 'BigQuery', 'xml', 'SSIS', 'SQL Server', 'ETL', 'COBOL', 'Power center', 'SQL Server', 'ETL', 'automation', 'ETL', 'NOSQL', 'SVN', 'ETL', 'Teradata', 'testing', 'spreadsheets', 'Power BI', 'Excel', 'Tableau', 'ETL', 'ETL', 'ETL', 'SSIS', 'SSIS', 'SSIS', 'Oracle', 'SQL Server', 'SSIS', 'MongoDB', 'Testing', 'Testing', 'UAT', 'ETL', 'SQL', 'data analysis', 'Oracle', 'SAP HANA', 'SAP HANA', 'SAP', 'SAP', 'SAP', 'Qlik', 'ETL', 'SAP ABAP', 'SQL Server', 'Oracle', 'DB2', 'SQL server', 'XML', 'ETL', 'XML', 'Unix', 'Shell', 'SAP', 'SAP', 'ETL', 'SAP', 'SAP', 'SAP', 'SAP', 'ETL', 'ETL', 'ETL', 'SAP HANA', 'SAP', 'Load Testing', 'Performance Testing', 'Load Testing', 'Performance Testing']
Data Engineer|https://www.livecareer.com/resume-search/r/data-engineer-e1345820d6384278b986c6b6dfd4e897|150574715265165963952823813869633129863|Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary       Over 3 years of professional IT experience in  Data Engineering  and  Data Analytics  using various languages and tools like  SQL Python BigHadoop   Extensive experience on Data Engineering field including Ingestion Datalake Datawarehouse Reporting and Analytics  Strong knowledge and experience on Data Analysis Data Lineage Big Data pipelines Data quality Data Reconciliation Data transformation rules Data flow diagram including Data replication Data integration and Data orchestration tools  Knowledge and experience on AWS services like RedshiftRedshift spectrumS3GlueAthena Lambdacloudwatch and EMRs like HIVE Presto  Extensively used ETL methodology for performing Data Migration Extraction Transformation and loading using  Talend  and designed data conversions from wide variety of source systems  Experienced in Data Ingestion projects to inject data into  Data lake  using multiple sources systems using Talend Bigdata  Good technical Skills in  SQL Server   ETL  Development using  Informatica tool   Expertise in writing  SQL   PLSQL  to integrate of complex OLTP and OLAP database models and data marts worked extensively on  Oracle SQL SERVER   Experience in all the life cycle phases of the projects on  large data sets  and experience with performance  tuning  and  troubleshooting   Ability to work effectively with associates at all levels within the organization  Strong background in mathematics and have very good analytical and problemsolving skills         Skills           Big Data Ecosystems  HDFS MapReduce Spark Kafka Hive Airflow Stream Sets HBase Flume Zookeeper Nifi Sentry Ranger  Scripting Language  Python PowerShell Scripting Pig Latin HiveQL  Cloud Environment  Amazon Web Services AWS Microsoft Azure  NoSQL Database  Database  MySQL Oracle Teradata MS SQL SERVER PostgreSQL DB2  Version Control  Git SVN Bitbucket  ETL Tools  Tableau Microsoft Excel Informatica Power BI R Google Data Studio                        Experience       Data Engineer       012022      Current     Honeywell    –    Michigan     ND            Worked on Architecture Design for Multistate implementation or deployment  Implement One time Data Migration of Multistate level data from SQL server to Snowflake by using Python and SnowSQL  Day today responsibility includes developing ETL Pipelines in and out of data warehouse develop major regulatory and financial reports using advanced SQL queries in snowflake  Stage the API or Kafka Datain JSON file format into Snowflake DB by FLATTENing the same for different functional services  Build Docker Images to run airflow on local environment to test the Ingestion as well as ETL pipelines  BuildingMaintaining Docker container clusters managed by Kubernetes Utilization of Kubernetes and Docker for the runtime environment of the CICD system to build test and deploy  Created Airflow DAGs to schedule the Ingestions ETL jobs and various business reports  Created  Airflow  Scheduling scripts in Pythonx  Cluster capacity planning along with operations team and management team and Cluster maintenance as well as creation and removal of nodes  HDFS  support and maintenance  Strong knowledge of  Rack awareness  topology in the  Hadoop cluster   Involved in Loading data from  LINUX file system  to  Hadoop Distributed File System   Responsible for building scalable distributed data solutions using  Hadoop   Experience in managing and reviewing  Hadoop log files   Data migration from  RDMS  to  Hadoop  using  Sqoop  for analysis and implemented  Oozie  jobs for automatic data imports from source  Created  HBase  tables to store various data formats of  PII  data coming from different portfolios  Strong in Exporting the analyzed and processed data to the  Relational databases  using  Sqoop  for visualization and for generation of reports for the team           Data Engineer       012021      122021     Honeywell    –    Nashville     TN            Prepared ETL design document which consists of the database structure change data capture Error handling restart and refresh strategies  Worked with different feeds data like JSON CSV XMLDAT and implemented Data Lake concept  Developed Informatica design mappings using various transformations  Designing and building multiterabyte full endtoend Data Warehouse infrastructure from the ground up on Confidential  Redshift  for large scale data handling Millions of records every day  Optimizing and tuning the  Redshift  environment enabling queries to perform up to 100x faster for Tableau and SAS Visual Analytics  Wrote various data normalization jobs for new data ingested into  Redshift   Advanced knowledge on Confidential  Redshift  and MPP database concepts  Migrated on premise database structure to Confidential  Redshift  data warehouse  Developed UDF in Scala to implement the business logic  Developed spark applications in Scala on distributed environment to load huge number of JSON files with different schema in to Hive tables  Most of the infrastructure is on AWS used   AWS EMR  Distribution for Hadoop   AWS S3  for raw file storage   AWS EC2  for Kafka  Used  AWS Lambda  to perform data validation filtering sorting or other transformations for every data change in a DynamoDB table and load the transformed data to another data store  Created  Airflow  Scheduling scripts in Python  Programmed  ETL functions  between Oracle and Amazon Redshift           SQL Developer       062019      122023     Ihs Markit    –    Southfield     MI            Gathered business requirements and converted them into new TSQL stored procedures in visual studio for database project  Performed unit tests on all code and packages  Analyzed requirement and impact by participating in Joint Application Development sessions with business client online  Performed and automated SQL Server version upgrades patch installs and maintained relational databases  Performed front line code reviews for other development teams  Modified and maintained SQL Server stored procedures views adhoc queries and SSIS packages used in the search engine optimization process  Updated existing and created new reports using Microsoft SQL Server Reporting Services Team consisted of 2 developers  Created files views tables and data sets to support Sales Operations and Analytics teams  Monitored and tuned database resources and activities for SQL Server databases          Education       Master of Science       Data Analytics Engineering       Expected in   122022                George Mason University      Fairfax     VA     GPA        Status           38 GPA           Bachelor of Science       Electronics  Communication Engineering       Expected in   062019                KL University      Guntur          GPA        Status   |none|['SQL', 'Python', 'Data Analysis', 'Big Data', 'AWS', 'HIVE', 'ETL', 'SQL Server', 'ETL', 'SQL', 'Oracle', 'SQL SERVER', 'mathematics', 'Big Data', 'Spark', 'Kafka', 'Hive', 'Airflow', 'HBase', 'Python', 'PowerShell', 'Pig', 'Latin', 'HiveQL', 'Amazon Web Services', 'AWS', 'Azure', 'NoSQL', 'MySQL', 'Oracle', 'Teradata', 'MS SQL SERVER', 'PostgreSQL', 'DB2', 'Git', 'SVN', 'Bitbucket', 'ETL', 'Tableau', 'Microsoft Excel', 'Power BI', 'R', 'SQL server', 'Snowflake', 'Python', 'ETL', 'SQL', 'snowflake', 'Kafka', 'JSON', 'Snowflake', 'Docker', 'airflow', 'ETL', 'Docker', 'Kubernetes', 'Kubernetes', 'Docker', 'Airflow', 'ETL', 'Airflow', 'Hadoop', 'LINUX', 'Hadoop', 'Hadoop', 'Hadoop', 'Hadoop', 'HBase', 'ETL', 'JSON', 'CSV', 'XMLDAT', 'Redshift', 'Redshift', 'Tableau', 'SAS', 'Redshift', 'Redshift', 'Redshift', 'Scala', 'spark', 'Scala', 'JSON', 'Hive', 'AWS', 'AWS', 'Hadoop', 'AWS', 'AWS', 'Kafka', 'AWS', 'DynamoDB', 'Airflow', 'Python', 'ETL', 'Oracle', 'Redshift', 'SQL Developer', 'business requirements', 'SQL Server', 'SQL Server', 'SSIS', 'optimization', 'SQL Server Reporting Services', 'SQL Server']
Data Engineer|https://www.livecareer.com/resume-search/r/data-science-data-engineer-intern-bd32a7569cbb4dd2a9f164b4801eef35|266612002933194965787459684177947415296|Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market           Skills         Machine LearningData Mining  NLP  Linear Regression neural networksdeep learning Naive Bayes SVM Logistic Regression  decision trees Kmean KNN N Grams edit distance gradient descent  Statistical Programming  Packages  R Python NumPy Matplotlib scikitlearn pandas ggplot2 Shiny dplyr caret e1071keras  Business Intelligence  Visualization  Tableau Qlik View Qlik Sense Excel OBIEE  Hadoop Ecosystem Components  Spark Hive Sqoop Flume Kafka Impala  Databases  Oracle MySql PostgreSql  Oracle ERP  Fusion Middleware  Oracle EBusiness Suite 11i  R12 Oracle SOA Oracle Service Bus  Other Languages  Tools  SQL PLSQL core java Scala RStudio Jupyter SqlDeveloper Toad Atom  Certifications  Tableau SQL  PLSQL                       Education and Training       University of Utah David Eccles School of Business    Salt Lake City     Utah      Expected in   August 2017     –      –       Master of Science        Information Systems          GPA           Information Systems Data Science and Analytics specialization   Recipient of 15000 Graduate Fellowship from David Eccles School of Business Academic Capstone Project  Big Data  Building statistical regression and classification models cleaning exploring data and developing interactive web interface using R Shiny which helps the company to classify clients loan type and predicting the amount of loan they will take in future Kaggle  House Prices Predictions  Applied different machine learning simple and advanced models on housing predictors for predicting the sales prices of houses used imputation methods for filling missing and null values in the data set Independent Study  Research  Apache Spark using Scala and Python Rajeev Gandhi Memorial College of Engineering and Technology          India                        Expected in   May 2012     –      –       Bachelor of Science        Computer Science and Engineering          GPA           Computer Science and Engineering Designed Hand Draw Shape Recognition interface which helps the user to invoke desired application just by drawing the shape linked to the application          Experience       Envestnet      Data Science  Data Engineer Intern   Secaucus     NJ                   012017      Present     Working on Big Data Ingestion using Sqoop for transferring data from multiple MySql database servers to transient storage in amazon EMR Hcatalog and using Hive to transfer data to persistent storage in amazon S3 bucket  Developing Sqoop and Hive scripts for data ingestion  Using R and spark in amazon EMR for filtering exploring analyzing providing insights on data and developing reports           First American Corporation      Oracle Technical Consultant  Data Analyst   City          India              022013      072016     Created SQL scripts for daily extracts adhoc requests  reporting and for analyzing large data sets  Designed ER diagrams conceptual models logical and physical models created database objects  Tables Indexes Sequences and Views  Developed Oracle Business Intelligence reports created and modified Oracle database objects  Tables Views and Indexes which increased the performance of Oracle Business Intelligence reports by 60 in production environment  Prepared SQL  Loader scripts for loading data from other systems into oracle ERP system worked with onsite business team in performing data fixes  Created PLSQL interfaces for doing business validation transferring data between ERP modules and loading data to base tables           CMC Limited      Data Analyst Intern   City          India              112012      012013     gt Developed SQL scripts worked on oracle 11i database and oracle reports          Skills     Academic ad Apache Big Data Business Intelligence Draw clients Data Mining Databases database EBusiness edit ERP filling drawing java Machine Learning Excel Middleware MySql NLP networks neural Oracle Oracle database PLSQL PostgreSql Programming Python reporting Research sales servers scripts SQL SQLLoader Tableau Tables Toad type validation View|none|['NLP', 'Linear Regression', 'Naive Bayes', 'SVM', 'Logistic Regression', 'decision trees', 'Kmean', 'KNN', 'N Grams', 'distance gradient descent', 'R', 'Python', 'NumPy', 'Matplotlib', 'scikitlearn', 'pandas', 'ggplot2', 'Shiny', 'dplyr', 'caret', 'Tableau', 'Qlik', 'Qlik', 'Excel', 'Hadoop', 'Spark', 'Hive', 'Sqoop', 'Flume', 'Kafka', 'Impala', 'Oracle', 'MySql', 'PostgreSql', 'Oracle', 'Oracle', 'Oracle', 'Oracle', 'SQL', 'PLSQL', 'java', 'Scala', 'RStudio', 'Jupyter', 'Tableau', 'SQL', 'Big Data', 'R', 'Spark', 'Scala', 'Python', 'Big Data', 'MySql', 'Hive', 'Hive', 'R', 'spark', 'SQL', 'SQL', 'SQL', 'oracle', 'oracle', 'Apache', 'Big Data', 'Data Mining', 'java', 'Machine Learning', 'Excel', 'MySql', 'NLP', 'Oracle', 'Oracle', 'PostgreSql', 'Programming', 'Python', 'Research', 'SQL', 'Tableau']
Data Engineer|https://www.livecareer.com/resume-search/r/senior-data-engineer-ffd843d6febb4217800488911db2a3f8|137893745859917600174161396390961745079|Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Professional Summary      Business Intelligence Consultant with a 10year career in data warehousing business intelligence reporting and data management architecture Progressive developer and technical team lead with a strength in design  development as well as driving performance reducing inefficiencies and cutting costs Possess comprehensive knowledge and hands on experience in both ETL and Reporting tools Knowledge of Credit Card account life cycle management in Finance domain Functional knowledge of Collections and Recovery operations Expertise in planning executing and spearheading various SDLC and Agile Scrum projects in compliance to quality standards Known for effective communication with excellent relationship building  interpersonal skills strong analytical problem solving  organizational abilities Proven track record of taking ownership and diving deeper into issues to identify the root cause and troubleshoot towards resolution        Skills           ETL Tools  Ab Initio  Scripting Languages  Unix shell scripting Python SparkScala  Cloud Services Microsoft Azure Cloud Services Data Lake  DataBricks and Data Pipelines Azure Data Factory  BI Reporting Skills  Tableau Desktop Tableau Server Power BI Reporting  SAP Business Objects SAP BOUniverse Designer Tool SAP BO XI WEBI ReportingSAP BO XI Deski Reporting  Database  Oracle  Teradata Hive      Scheduler tools  Autosys ControlM  Relational Database Management  Business Intelligence Reporting  Project Management  Data Analysis and Visualization                       Work History      062017   to   Current     Senior Data Engineer      Thoughtworks    –    Memphis     TN            A multitenure loyalty program across all Williams Sonoma brands – the first time that customers can earn points by purchasing across the multiple brands WSI Pottery Barn West Elm etc I work in the customer data warehouse CDW which is Teradata and work with their 3rd party marketing systems vendor There are various data points and integrations with the data warehouse which holds all customer information points transactions loyalty IDs etc   Worked with Requirements Analysts and PDMs to identify understand and document business needs for data flow Analyze requirementsUser stories at business meetings and strategize impact of requirements on different platformsapplications Worked with Project Management in creation of project estimates for each Agile story  Engaged in projects that uses Scala and Azure DatabricksDatafactoryDatalake to extract process and load Adobe Clickstream data received for Williams Sonoma ECOM website Deployed custom codes in Scala to read JSON extract CSV files from Adobe add column headers handle badmalformed records handle invalid records before finally writing cleansed data into ParquetDelta file format partitioned by date and hour  Build and review design deliverables Perform dependency analysis between new objects created in Ab Initio graphs Conduct IT and UAT testing of each of these graphs Record errors reported during Unit and Integrated testing  Prepare production Implementation Plan to deploy codes successfully to production environment  Coordinate daily standups short meetings where teammates talk through whats being worked on yesterday what was done today and if blocked Involved in variety of other scrum meetings such as planning meetings where team pulls in stories requirements grooming meeting to look at backloglist of stories and flush out timeline and work expected demo show work that was completed in sprint etc  Reporting  Visualization in Tableau and Power BI  Design and deploy rich Graphic visualizations with Drill Down and Drop down menu option Prepare Dashboards using calculations parameters Apply various reporting objects like Filters prompts Calculated fields Groups Parameters Work on the development of Dashboard reports for the Key Performance Indicators for the top management          032010   to   062017     Senior DeveloperTechnical Team Lead      Tech Mahindra Synchrony Financial Services    –    City     STATE            Synchrony Financial is a consumer financial services company offering consumer financing products including credit promotional financing and loyalty programs and installment lending through Synchrony Bank its wholly owned subsidiary Synchrony is the largest provider of private label credit cards in the US The company provides private label credit cards for such brands as Amazon JC Penney CheapOAir OneTravel Sams Walmart Lowe’s Guitar Center Gap BP We as Data warehouse solution providers store this credit card holder information from the instance a credit card is applied till the account is closedcharged off   Understanding the full scope of requirements from business Estimating time and effort involved from gathering project requirements till project implementation  Answering technical queries driving product initiatives and metric collection and analysis  CoOrdinating between project stake holders – Business Client and offshore teams during all phases of project  Preparing design documentation design reviews development performing code reviews to ensure coding standards are followed testing and deployment of application enhancements  Experience in Ab Initio toolsets including the following  o Parallel and serial flow batch graphs conditional components other components like reformat normalize and join lookup and graphs processing ASCII and EBCDIC layouts  o Knowledge on Abinitio architecture including the GDE Cooperating system EME and other related items  o Fine tuning of Abinitio graphs based on performance enhancement by proper usage of memory in the components  o Well versed with various Ab Initio components such as Join Rollup Partition Departition Dedup sorted Scan Normalize DeNormalize  o Expertise knowledge improving Performance and Troubleshooting of the AbInitio graphs and monitoring ABInitio run time statistics  o Experience with advanced Abinitio metaprogramming air commands and other admin related tasks including the creation of save and performing migration from one server to the other  Establish support such as acquiring conditioned test data support from third partyteam etc for Integration Testing to simulate production environment and to ensure correctness and quality of buildparameter set up  Getting sign off from IT managers for deploying parameterset up onto production environment  Documenting implementation plan and Hour by Hour plan listing down the tasks in sequence of their execution on the date of implementation  Coordinate release activities across multiple teams          Education      Expected in   4 2008     Bachelors     Information Technology     College of Engineering Bhubaneswar      India          GPA               Accomplishments      • Tableau Desktop Specialist Certified No expiration  These are Open Badges that I have been awarded and that attest to my skills  httpswwwyouracclaimcombadges556948fdd8114f8097d245ab6c530d9clinkedinprofile   o Tableau Desktop Specialist title use their foundational knowledge of Tableau Desktop and data analytics to solve problemsDesktop Specialists can connect to prepare explore and analyze data and share their insights        Skills       ETL Tools  Ab Initio  Scripting Languages  Unix shell scripting Python SparkScala  Cloud Services Microsoft Azure Cloud Services Data Lake  DataBricks and Data Pipelines Azure Data Factory  BI Reporting Skills  Tableau Desktop Tableau Server Power BI Reporting  SAP Business Objects SAP BOUniverse Designer Tool SAP BO XI WEBI ReportingSAP BO XI Deski Reporting  Database  Oracle  Teradata Hive    Scheduler tools  Autosys ControlM  Relational Database Management  Business Intelligence Reporting  Project Management  Data Analysis and Visualization         Work History      062017   to   Current     Senior Data Engineer       Kforce Inc Williams Sonoma Inc   –   San Francisco     CA     A multitenure loyalty program across all Williams Sonoma brands – the first time that customers can earn points by purchasing across the multiple brands WSI Pottery Barn West Elm etc I work in the customer data warehouse CDW which is Teradata and work with their 3rd party marketing systems vendor There are various data points and integrations with the data warehouse which holds all customer information points transactions loyalty IDs etc   Worked with Requirements Analysts and PDMs to identify understand and document business needs for data flow Analyze requirementsUser stories at business meetings and strategize impact of requirements on different platformsapplications Worked with Project Management in creation of project estimates for each Agile story  Engaged in projects that uses Scala and Azure DatabricksDatafactoryDatalake to extract process and load Adobe Clickstream data received for Williams Sonoma ECOM website Deployed custom codes in Scala to read JSON extract CSV files from Adobe add column headers handle badmalformed records handle invalid records before finally writing cleansed data into ParquetDelta file format partitioned by date and hour  Build and review design deliverables Perform dependency analysis between new objects created in Ab Initio graphs Conduct IT and UAT testing of each of these graphs Record errors reported during Unit and Integrated testing  Prepare production Implementation Plan to deploy codes successfully to production environment  Coordinate daily standups short meetings where teammates talk through whats being worked on yesterday what was done today and if blocked Involved in variety of other scrum meetings such as planning meetings where team pulls in stories requirements grooming meeting to look at backloglist of stories and flush out timeline and work expected demo show work that was completed in sprint etc  Reporting  Visualization in Tableau and Power BI  Design and deploy rich Graphic visualizations with Drill Down and Drop down menu option Prepare Dashboards using calculations parameters Apply various reporting objects like Filters prompts Calculated fields Groups Parameters Work on the development of Dashboard reports for the Key Performance Indicators for the top management          032010   to   062017     Senior DeveloperTechnical Team Lead       Tech Mahindra Synchrony Financial Services   –   Chicago     IL     Synchrony Financial is a consumer financial services company offering consumer financing products including credit promotional financing and loyalty programs and installment lending through Synchrony Bank its wholly owned subsidiary Synchrony is the largest provider of private label credit cards in the US The company provides private label credit cards for such brands as Amazon JC Penney CheapOAir OneTravel Sams Walmart Lowe’s Guitar Center Gap BP We as Data warehouse solution providers store this credit card holder information from the instance a credit card is applied till the account is closedcharged off   Understanding the full scope of requirements from business Estimating time and effort involved from gathering project requirements till project implementation  Answering technical queries driving product initiatives and metric collection and analysis  CoOrdinating between project stake holders – Business Client and offshore teams during all phases of project  Preparing design documentation design reviews development performing code reviews to ensure coding standards are followed testing and deployment of application enhancements  Experience in Ab Initio toolsets including the following  o Parallel and serial flow batch graphs conditional components other components like reformat normalize and join lookup and graphs processing ASCII and EBCDIC layouts  o Knowledge on Abinitio architecture including the GDE Cooperating system EME and other related items  o Fine tuning of Abinitio graphs based on performance enhancement by proper usage of memory in the components  o Well versed with various Ab Initio components such as Join Rollup Partition Departition Dedup sorted Scan Normalize DeNormalize  o Expertise knowledge improving Performance and Troubleshooting of the AbInitio graphs and monitoring ABInitio run time statistics  o Experience with advanced Abinitio metaprogramming air commands and other admin related tasks including the creation of save and performing migration from one server to the other  Establish support such as acquiring conditioned test data support from third partyteam etc for Integration Testing to simulate production environment and to ensure correctness and quality of buildparameter set up  Getting sign off from IT managers for deploying parameterset up onto production environment  Documenting implementation plan and Hour by Hour plan listing down the tasks in sequence of their execution on the date of implementation  Coordinate release activities across multiple teams|none|['data warehousing', 'ETL', 'Agile', 'Scrum', 'problem solving', 'ETL', 'Unix', 'shell', 'Python', 'Azure', 'DataBricks', 'Azure', 'Data Factory', 'BI', 'Tableau', 'Tableau', 'Power BI', 'SAP', 'SAP', 'SAP', 'BO XI', 'WEBI', 'BO XI', 'Oracle', 'Teradata', 'Hive', 'Project Management', 'Data Analysis', 'Teradata', 'Project Management', 'Agile', 'Scala', 'Azure', 'Scala', 'JSON', 'UAT', 'testing', 'scrum', 'Tableau', 'Power BI', 'ETL', 'Unix', 'shell', 'Python', 'Azure', 'DataBricks', 'Azure', 'Tableau', 'Tableau', 'Power BI', 'SAP', 'SAP', 'SAP', 'Oracle', 'Teradata', 'Hive', 'Project Management', 'Data Analysis', 'Teradata', 'Project Management', 'Agile', 'Scala', 'Azure', 'Scala', 'JSON', 'UAT', 'scrum', 'Tableau', 'Power BI', 'Integration Testing']
Data Engineer|https://www.livecareer.com/resume-search/r/data-support-services-engineer-fd6014dbd6d04903b67269cf3fc6cd59|12880653418622435105799992611550629230|Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Desktop and Server Support Application Server Technician Offer my Windows Server 20122008 R22003 Windows XP78 Mac OS X Linux Application Server Microsoft Office 2010 and Office Mac 2011 Active Directory and Network LANWAN switches and routers experience Desktop and server hardware and software technical support experience Strong analytical troubleshooting communication and customer service skills  Skilled Windows System Administrator offering 10 years of experience building and maintaining multiplatform technology services with a solid understanding of current Windows Mac and UNIX application systems       Highlights           Systems	Windows XP78 Desktop	Max OS X 108 109 1010	iO78 mobile and tablet  Windows 20032008 Server	Android mobile		Red Hat Enterprise Linux  Unix web applications	LANWAN  Mobile Phone	iPhoneiPad Apple	Blackberry		Android  Applications	Microsoft Office 20072010	Office Mac 2010		Final Cut 7 and 10  Microsoft Office 365	Pages OS X		Numbers OS X  Adobe Creative Suite cloud	Adobe Acrobat XXI	Adobe Lighthouse 45  Parallels	WebEx  Utilities	Symantec Antivirus	Voltage Email Encryption	Barracuda Backup Cloud  PGP DesktopServer	Symantec Ghost  Server Applications	Active Directory	PowerShell Scripting	WINS	DFS  DHCP	FilePrint Services	LDAP	Group Policy GP  Exchange 2010	NTFS security		HyperV	DNS                         Accomplishments       Requirements Analysis    Completed business requirements analysis including the evaluation of systems specifications for the Soundview Throgs Neck Community Health Center     Strategy and Planning    Developed and communicated Electronic Health Record security policies and standards to all users  Established policies and procedures for medical record documentation     IT Training    Successfully trained all employees to use Electronic Health Record and Billing systems     Network Support    Acted as first point of contact for all major technical issues including power outages system failures and disaster recovery  Oversaw infrastructure of three offices and acted as support for helpdesk technicians of Yeshiva University          Experience       Data Support Services Engineer       072013      092013     Montefiore Medical Center formally SVTNCMHC Health Care Services    –    City     STATE            Managed the daytoday IT operations for the Montefiore Medical Center  Assisted in the migration of technology services from Yeshiva Universitys servers to Montefiores servers including computers user logins data files printer settings software applications and email archives  Provided QA testing in the migration of the centers Electronic Health Record Mindlinc and Billing IMA system  Trained all staff in the use of Montefiores technical services including clerical registration billing electronic health record and emailprintingdata file usage           Technical System Support Engineer       072003      072013     Yeshiva University Health Care Services Soundview Throgs Neck Community Health Center SVTNCMHC    –    City     STATE            Desktops Mobile Phones Printers and Application Servers Provided all levels of enduser desktop server mobile phonetablet and printer technical support for 2 healthcare center locations in the Bronx  Backend technical support for all server and network services  Responsible for managing 15 application servers including 4 domain controllers  Deployed and maintained a Windows 2008R2 and 2003 Active Directory environment  Managed all aspects of server and application security using Active Directory LDAP and Linux  Planned and implemented the physical deployment and migration of Windows 7 desktops from Windows XP  Consulted daily with the executive clinical and administrative staff concerning the overall quality and possible improvement of technology systems for the medical center  Trained all staff in the use of SVTNYeshiva Universitys technical services including clerical registration billing electronic health record and emailprintingdata file usage  Provided QA support and testing of our internal and external billing system IMA  Work closely with outside vendors to design and maintain the EHR Billing and Backup applications  Implemented and maintained the centers data backup system using Barracuda Cloud Backup  Identified designed and implemented the requirements for the centers disaster recovery system  Responsible for managing and maintaining the centers audiovideo conference room system          Education       Graduate Certificate       Digital Media and Project Management       Expected in                   The New School      New York     NY     GPA        Status                  BBA       Computer Information Systems       Expected in                   Baruch College City University of New York      New York     NY     GPA        Status                  Skills      Active Directory administrative Adobe Adobe Acrobat Antivirus Apple audio Backup Billing billing system clerical Encryption Desktops DHCP disaster recovery DNS Email Ghost LAN LDAP Linux Mac managing Max Exchange Microsoft Office Office Windows 7 Windows Windows XP migration Enterprise network OS printer Printers quality QA Red Hat Servers Scripting Symantec technical support Phones Phone Unix Utilities video WAN web applications|none|['Windows', 'Windows', 'Mac', 'OS X', 'Linux', 'Microsoft Office', 'Office', 'Mac', 'Active Directory', 'Windows', 'Windows', 'Mac', 'UNIX', 'Windows', 'Windows', 'Android', 'Red Hat Enterprise', 'Linux', 'Unix', 'Android', 'Microsoft Office', 'Office', 'Mac', 'Microsoft Office', 'Adobe Creative Suite', 'Active Directory', 'PowerShell', 'WINS', 'DFS', 'DHCP', 'Exchange', 'DNS', 'business requirements', 'QA testing', 'Windows', 'Active Directory', 'Active Directory', 'Linux', 'Windows 7', 'Windows XP', 'Project Management', 'Active Directory', 'DHCP', 'DNS', 'Linux', 'Mac', 'Exchange', 'Microsoft Office', 'Office', 'Windows 7', 'Windows', 'Windows XP', 'Unix']
Data Engineer|https://www.livecareer.com/resume-search/r/aws-data-engineer-fcef99cc1f1c4bdeb25cbb8d5b3ac32c|339690346324715490168338998099130748299|Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases         Skills          ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                      Experience       AWS Data Engineer       012022      022022     Accenture Contractor Jobs    –    Rochester     NY            Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Used Spark SQL for Scala  amp Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process  Configured computing networking and security systems within cloud environment  Implemented cloud policies managed technology requests and maintained service availability           Data Engineer       012016      112019     Verizon    –    Beaverton     OR            Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Big Data Engineer  Hadoop Developer       102013      122015     Two95 International Inc    –    Boca Raton     FL          AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Python Developer        092012      102013     Fiserv    –    City     STATE            AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes          Education and Training       Post Graduate        Data Engineering        Expected in   022022                Purdue University      West Lafayette     IN     GPA        Status                  Post Graduate        Data Science And Business Analytics        Expected in   092021                University of Texas At Austin      Austin     TX     GPA        Status                  Bachelor of Arts       Business Administration And Management       Expected in   122009                Califonia State University       Fullerton CA          GPA        Status   |none|['Big Data', 'Hadoop', 'Big Data', 'Data Warehouse', 'Data Mart', 'Data Visualization', 'Hadoop', 'programming', 'Hadoop', 'Hadoop', 'Apache', 'Spark', 'HDFS', 'Sqoop', 'PIG', 'Hive', 'HBase', 'Oozie', 'Flume', 'NiFi', 'Kafka', 'MapReduce', 'Spark', 'Spark', 'algorithms', 'Hadoop', 'Spark', 'Spark', 'PySpark', 'Scala', 'Sqoop', 'Flume', 'Hive', 'HIVE', 'Hadoop', 'Hadoop', 'Hive', 'Hive', 'Json', 'XML', 'Airflow', 'Airflow', 'Python', 'Bash', 'Google Cloud', 'Google Cloud', 'SQL', 'NoSQL', 'MongoDB', 'HBase', 'Cassandra', 'SQL server', 'PostgreSQL', 'Java', 'MongoDB', 'HBase', 'Phoenix', 'SQL', 'HBase', 'RDBMS', 'ETL', 'SQL Server', 'SQL Server', 'SSIS', 'XML', 'AWS', 'automation', 'automation', 'GIT', 'Ansible', 'modeling', 'Snowflake', 'modeling', 'JSON', 'SOAP web services', 'python', 'Sublime', 'Big Data', 'Hadoop', 'Hive', 'HBase', 'Oozie', 'Flume', 'NiFi', 'Kafka', 'Zookeeper', 'Yarn', 'Apache', 'Spark', 'Mahout', 'Sparklib', 'Oracle', 'MySQL', 'SQL Server', 'MongoDB', 'Cassandra', 'DynamoDB', 'PostgreSQL', 'Teradata', 'Cosmos', 'Python', 'PySpark', 'Scala', 'Java', 'C', 'C', 'Shell', 'Perl', 'SQL', 'AWS', 'Azure', 'Django', 'REST', 'PyCharm', 'Eclipse', 'SQL Developer', 'SQL', 'SQL Server', 'SQL', 'Eclipse', 'Postman', 'SVN', 'Git', 'GitHub', 'Windows', 'Ubuntu Linux', 'Network Security', 'Modeling', 'Modeling', 'Modeling', 'Snowflake', 'Modeling', 'Apache', 'Airflow', 'Tableau', 'ggplot2', 'matplotlib', 'SSRS', 'Power BI', 'Machine Learning', 'NLP', 'Clustering', 'AWS', 'access', 'AWS', 'AWS', 'DynamoDB', 'testing', 'Hive', 'Pig', 'MapReduce', 'access', 'AWS', 'Redshift', 'machine learning', 'algorithms', 'python', 'AWS', 'AWS', 'DynamoDB', 'Spark', 'SQL', 'Scala', 'Python', 'Spark', 'PySpark', 'SQL Server', 'SSIS', 'Teradata', 'ETL', 'RDBMS', 'Spark', 'Hive', 'Data blending', 'Alteryx', 'SQL', 'Tableau', 'Tableau', 'Kibana', 'Elasticsearch', 'AWS', 'Apache', 'Airflow', 'AWS', 'AWS', 'Redshift', 'Lambda', 'Boto3', 'DynamoDB', 'SageMaker', 'Apache', 'Spark', 'HBase', 'Apache', 'Kafka', 'HIVE', 'SQOOP', 'Map Reduce', 'Snowflake', 'Apache', 'Python', 'SSRS', 'Tableau', 'networking', 'Azure', 'SQL', 'Cassandra', 'Azure', 'SQL', 'Azure', 'Azure', 'Kubernetes', 'Spark', 'Cassandra', 'Azure', 'SQL', 'Azure', 'Spark', 'Azure', 'Spark', 'SQL', 'SQL', 'Mongo', 'SQL Server Reporting Services', 'SSRS', 'Power BI', 'Databricks', 'Spark', 'Databricks', 'ADF', 'Azure', 'Azure', 'SQL', 'use cases', 'Azure', 'ADF', 'Azure', 'Azure', 'Spark', 'Data bricks', 'Azure', 'SQL', 'Azure', 'ADF', 'SSRS', 'Power BI', 'Azure', 'Azure', 'Apache', 'Spark', 'use cases', 'data analysis', 'data analysis', 'Big Data', 'Hadoop', 'Hadoop', 'Spark', 'Kafka', 'HIVE', 'ETL', 'HIVE', 'Spark', 'Spark', 'Spark', 'SQL', 'RDBMS', 'Java', 'Hive', 'XML', 'JSON', 'Hive', 'Hive', 'Business Requirements', 'ETL', 'Python', 'Snowflakes', 'SnowSQL', 'SQL', 'Snowflake', 'SQL Server Reporting Services', 'SSRS', 'Spark', 'Cassandra', 'Cassandra', 'Apache', 'Hadoop', 'Scala', 'Kafka', 'Hive', 'HBase', 'AWS', 'PySpark', 'Spark', 'Scala', 'Hive', 'JSON', 'AWS', 'Cassandra', 'PySpark', 'Apache', 'Spark', 'HBase', 'Apache', 'Kafka', 'HIVE', 'Apache', 'oozie', 'Zookeeper', 'ETL', 'Snowflake', 'Apache', 'Python', 'Java', 'SSRS', 'Hadoop', 'Hadoop', 'data analysis', 'Python', 'AWS', 'LAMBDA', 'Datadog', 'CloudTrail', 'Ansible', 'MySQL', 'Python', 'Git', 'Jenkins', 'DynamoDB', 'Docker', 'Kubernetes', 'design patterns', 'Python']
Data Engineer|https://www.livecareer.com/resume-search/r/voice-data-network-engineer-22138788e64d42bdb03363162c2bd70a|285353067357438442010566533524756651993|Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       Home   555 4321000    Cell       resumesampleexamplecom              Summary      Technical Customer Service Specialist with a vast knowledge of web applications software and framework seeking to assist clients in all troubleshooting endeavors Outstanding networking adept at developing customer relationships looking for an opportunity to bring exceptional team building skills to a management position in a vibrant customer service department          Highlights           RoutersHubsSwitches        Juniper Nexus Cisco 2600 2800 3600 3700 3800 7200 2900 3500  Authentication TACACS  RADIUS  Network Analyzers Ethereal Sniffer Pro  Network Technologies        STP RSTP HSRP VRRP IRB Port Channel Cisco ASA 5500 SA  Operating Systems Windows20002003 Win98ME2000 proxp  Routing Protocols TCPIP RIP EIGRP OSPF and BGP  WAN Technologies T1E1 DS3 FrameRelay DSL MPLS leasedline  LAN Technologies 8021q ISL Patching up BackupRestoration DNS Infoblox DHCP  VOIP SIPH323 MGCPTDMSS7 Avaya  Voice gateways                         Accomplishments       10 years of IT experience in design development implementation troubleshooting and maintenance of mediumto large scale Network infrastructure Experience in Static Routing and configuring dynamic Routing Protocols  RIP v1 and v2 IGRP EIGRP OSPF and BGP Expertise in network protocols Switching Routing and Security technologies Experience on Cisco Catalyst Series 6500 Switches and Virtual switching system  Deployed the switches in high availability configuration  Good knowledge of Spanning Tree Protocol  IEEE 8021d IEEE 8021w IEEE 8021s PVST Well experienced in VLAN implementation VTP domain settings Stacking Load balance and Redundancy technologiesXRRP VRRP HSRP PAGP LACP Configuration of Access Control Lists ACL Quality of  ServiceQoS VPN NATPAT policies Experience in implementation of  PPP ISDN HDLC Frame Relay  T1 DS3 MPLS Installation and maintenance of RADIUS TACACS    Cisco Secure ACS Good understanding of the OSI Reference Model and the TCPIP Model Network Management CiscoWorks LMS ProCurve Manager Plus PCM  SonicwallOpen View Network analysis and troubleshooting tools  Sniffer Pro Wireshark Agilent Network and analysis tools Experience working with Cisco Nexus 2148 Fabric Extender and Nexus 5000 series to provide a Flexible Access Solution for a datacenter access architecture  Experience configuring Virtual Device Context in Nexus 7010  Experience convert PIX rules over to the Cisco ASA solution  Worked on Extensively on Cisco Firewalls Cisco PIX 506E515E525  ASA     550055105540 Series  Proficient in the operation and administration of Juniper NetScreenISGSSGbased firewalls Juniper SRXseries services gateways Juniper JMMXseries routers Juniper SAseries SSL VPN appliances F5 loadbalancing products LTM GTM FirePass and various Cisco IOSbased platform  Assisted in converting PIX rules over to the Cisco ASA solution  Good knowledge on VOIP protocols like H323 SIP MGCP and SS7 and interfacing of TDM to VOIP system    Worked on Avaya  Voice gateways for VOIP implementation using Cisco Catalyst switches Experience in System Software installations Implementation of DNS TCPIP DHCP and IAS in Network environments Windows 2003 Windows 2000 and Windows 9598 Sound knowledge of networking methods techniques and processes Extensive knowledge of installing advanced configurations on Cisco Call Manager platforms Possess good working knowledge of TCPIP networks Comprehensive knowledge of Cisco network protocols and transport systems Indepth knowledge of Cisco Unified Communications like UCM UCCX MPE Unity NSTS and DRSN Well experienced in Planning Designing implementation and management of mediumtolarge scale of enterprise networks         Experience       Voice  Data Network Engineer       012015   to   Current     Criterion Systems    –    Washington     DC             Responsible for presales and postsales support for the design and implementation of call center and corporate voice solutions  Implemented call routing call management and computer telephony integration  Configured installed and supported Cisco Unified Communication Manager CallManager 8x Cisco Messaging Systems UnityUnity Connection 8x CER VoiceGateways Cisco routers and switches  Performed system upgrades and patches based on request schedule or events  Created dialplans route patterns route groups route lists calling search space partitions  Patched and upgraded Cisco Unified Communications applications  Created Voice Mail boxes call handlers create and reset passwords MWIs administered and maintained Active Directory on Unity  Used tools such as Advanced Settings Tool Message Store manager DOH PropTest Services Tools Depot  Checked Outlook accounts in Exchange servers to view warning and limits text messages associated w Unity Voice Mail created Subscriber services skill sets and prompts for UCCX deployment  Created Subscriber services skill sets prompts for UCCX deployment  Established and maintained collaborative relationships with customers recognized for consistently providing excellent customer service and retention  Instrumental in developing telecommunications departmental Service Level Agreements SLAs and standard processes which optimized daily operations           Network Engineer       072012   to   112014     Criterion Systems    –    Beltsville     MD             Performed responsibilities of assisting voice engineer in designing and installing Cisco network equipment  Handle first level of designing and installing Cisco Call Manager and associated Voice applications  Responsible for the administration configuration and implementation of IP Telephony systems  Handle the tasks of designing and maintaining documentation related to network layout  Perform responsibilities of maintaining security policies on Cisco firewall solutions  Monitor and ensure that the requirements of high availability are in place for Cisco environment  Manage team tasks against SLAs as well as work with project plans adhering to project timelines resource constraints and within budget  Primary support for Cisco Unity Unity Connection Unity Express AIM and NMCUE Call Manager VOIP Migration Project Involved in migrating Lehmans Branches to centralized VOIP platform that will replace the existing Standalone phone system  Cisco router 3745 series and switch series 3560 are used to support to add VLANs to all the routers and switches to support the VOIP phones  Worked on Avaya Gateway equipment is added to the network to support the ports from Cisco Switches across the network  Successfully performed installation and configuration for IPSEC VPN on Cisco routers PIX series VPN Concentrator 3000 and ASA 5500 security appliances  Worked extensively in Configuring Monitoring and Troubleshooting Ciscos ASA 5500PIX security appliance Failover DMZ zoning  configuring     VLANsroutingNATing with the firewalls as per the design  Assisted in migration of centralized voice mail system based on Cisco Unity connection and support interfacing of legacy TDM PBX of the sited in voice network system  Familiar with H323 IOS gateways and SRST telephony design and implementation and call manager deployment in the voice network  Hands on experience on implementation and troubleshooting of ACLs and QoS for the networks Involved in supporting voice infrastructure including call routing the voice gateways and its troubleshooting methods  Worked on updating CATOS for that switches and IOS for the routers to support the VOIP standalone system for 6500 Multilayer switch and 3500 switch series  HP Openview SNMP network node manager is used to monitor the network and in the times of green zones and DMZ zones           Sr Voice Data Analyst       2012   to   062012     Criterion Systems    –    Germantown     MD             Troubleshoot network software and hardware issues Remote corrective actions supporting Monitored all functions around the voice and data network  Responsibly monitorered network and Tier 23 support  Interfacework with repairrestoration of remote site equipment Assisting field technicians in maintenance of remote site equipment Responsibly cross trained other members of the Network Engineering team to broaden the knowledge base within the team  Managed emergency change needs of the business  Proven ability to learn new systems as required to effectively support customer needs  Troubleshoot wireless problems using different RF analysis tools  Handle and prioritize high call volumes and customer inquiries  Other duties as assigned by the Director of Telecommunications           Network Engineer       102009   to   112011     MORGAN STANLEY    –    City     STATE             Involved in Network Redesign for branch for data environment  Convert Branch primary WAN circuit T1 to MPLS and branch router security migration  Performing BGP Changes on branch router and headend router  Conversions to BGP WAN routing  Which will be to convert WAN routing from OSPF to BGP OSPF is used for local routing only which involves new WAN links  Replace network hardware with new 7200 routers and 3825 switches in all branch locations  Design and implementation of GRE for multicast and unicast communication on an existing IP VPN Managed and engineered 58 JuniperCheckpoint firewalls  Experience on designing and troubleshooting of complex BGP and OSPF routing problems  Involved in designing and implementing QOS and policy map to 3800 series routers for all the branches Worked closely with network monitoring staff and will do network design testingcertification prior to production implementation  Implements configurations and implementation instructions into change management system and insures that all approvals and processes are adhered to compliant with Audit  Worked on Layer 2 protocols such as VTP STP RSTP PVST MST and other VLAN troubleshooting issues and configuring switches from scratch and deployment Experience in branch relocation connect workstation servers etc  Rack and stack preconfigured new hardware and connect the circuits  Worked with carrier to test and turnup circuits  Worked with carriers like ATT Verizon Wind stream and Level3 for deploying WAN circuits and implementation of MPLS cloud Performs system level documentation on platforms and assists in project tracking and documentation  Experience with developing network design documentation and presentations using VISIO  Troubleshoots with problems regarding the network changes and migrations           Network Engineer       112006   to   092009     PITNEY BOWES    –    City     STATE             Responsible for Design integration configuration maintenance performance monitoring and security of network infrastructure including local area networks LAN wide area networks WAN  firewalls DHCP DNS Installing the Network devices in datacenter environment and clearly articulate complex network designs and drawings through documentation Visio as well as verbal training sessions  Experience in Configuring SitetoSite and Remote Site VPNs NATPAT policies  Managing Cisco Secure ACS for TACACS  RADIUS authentications  Monitoring customer data networks and providing fault isolation and remote troubleshooting  Experience on designing and troubleshooting of EIGRP routing issues  Responsible for the management of network at the client environment  Supporting and performing projects for the client WAN environment at a global level  Implementation of network system upgrades and modifications including planning testing scheduling and coordination  Ensures that change management and defined security procedures for all network systems are executed in accordance with customer policies and procedures  Interacting with Carriers for installation of new WAN circuits at Customer premises and makes sure circuit installed with no issues and ready to use before users move in to the branch  Providing Teir3 technical support for LANWAN issues and oncall for technical escalation on a rotational basis Remedy Ticketing system  Well experienced in troubleshooting bug related issues with help of Cisco TAC service  Providing networking services coordinate tasks and ensure their execution and documentation in accordance with established corporate standards           Systems  Network Engineer       052005   to   092006     WESTERN UNION    –    City     STATE              Resolved customer complaints and concerns with strong verbal and negotiation skills  Responsible for clients new branches turn up and managing the network environment  Installation of Operating Systems application softwares and Troubleshooting hardware issues Management of WAN connected over Frame relay and DSL Management of Pix Firewall  Cisco Routers and switches infrastructure Configuration of routers and switches for OSPF EIGRP VLANs STP Trunks Ether Channel Load Balancing Configuration of Routers and Firewalls for sitetosite IPSEC VPN tunnels Implemented Secured Wireless Network based on WPA and MAC Authentication  technologies Integration of AVAYA IPPhones with Switches in VLAN environment  Policy configuration and management of firewalls  DNS changes as per client requests  Planning about new product requirements Contacting Vendors for product purchases  Contacting Service providers to troubleshoot issues on WAN connectivity  Involved in planning and designing team for network upgradeschanges  Configuring NAT Policies Global VPN Client Policies Establishing SiteSite VPN Connections Maintenance of DHCP Server  Enabling highavailability and highcapacity features such as Spanning Tree and link aggregation on Switches Responsible for service request tickets generated by the helpdesk in all phases such as troubleshooting maintenance upgrades patches fixes and all around technical support Basic Configurations and Maintenance of various Switches Routers Firewalls and Wireless Access Points at different client locations Responsible for interaction with third party technicians on site Performed network support involving daily wiring maintenance of cabling switch maintenance network port reenabling and cable room Documentation of all new software installation procedures troubleshooting procedures Responsible for implementation of new network infrastructure includes Cabling Switching Routing Establishing MAN Connectivity between  their different braches Involved in Loop back tests and troubleshooting issues during establishment of T1 Line Dynamic VLAN implementation using IEEE 8021x protocol and RADIUS Server Integration of Active Directory Services ADS with RADIUS Server and creation of remote access polices on RADIUS Implementation of Access Control ListsACLs  QoS Implementation of Portfast on all access switches and setting bridge priorities on L3 Switches Environment Cisco 2900 3560 1800 3700 3800 7200 2900 3500          Education       Bachelor of Technology          Expected in   2003     Goa Institute of Technology                GPA               Certifications      CCNP Cisco Certified Network Professional  CCNA Cisco Certified Network Associate        Skills     Active Directory ADS articulate AVAYA Backup Basic BGP budget cable Cabling call center change management Cisco router Cisco Cisco Routers hardware network systems computer telephony integration Client excellent customer service DOH designing DHCP Documentation DNS DSL EIGRP engineer fast features FirewallsFrameRelay gateways Gateway HP Openview Hubs IP local area networks LAN layout leasedline MAC Director Managing Messaging Access Exchange Outlook 2000 Windows2000 Win98 Migration Network Engineering network design network hardware network support Network networking networks Operating Systems OSPF PBX phone system Policies presentations processes project plans Protocols Express RIP router Routers Routing sales sales support scheduling servers Service Level Agreements SLA SNMP software installation SS7 Store manager Switches switch Cisco Switches T1 TCPIP TDM technical support telecommunications Telephony Phones Troubleshoot Troubleshooting upgrades view VPN VISIO voice and data Voice Mail VOIP WAN wiring|none|['HSRP', 'OSPF', 'BGP', 'MPLS', 'DNS', 'DHCP', 'OSPF', 'BGP', 'HSRP', 'MPLS', 'Network Management', 'Wireshark', 'DNS', 'DHCP', 'Windows', 'Windows', 'Windows', 'networking', 'Active Directory', 'Outlook', 'Exchange', 'IPSEC', 'MPLS', 'BGP', 'BGP', 'OSPF', 'BGP', 'OSPF', 'multicast', 'BGP', 'OSPF', 'MPLS', 'VISIO', 'DHCP', 'DNS', 'OSPF', 'IPSEC', 'MAC', 'DNS', 'DHCP', 'Active Directory', 'CCNA', 'Active Directory', 'BGP', 'DHCP', 'DNS', 'MAC', 'Exchange', 'Outlook', 'OSPF']
Data Engineer|https://www.livecareer.com/resume-search/r/data-engineer-ii-30ba17cad1264ff7923637259cc2cb9f|148616979939238102062955431006346284182|Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Selfmotivated Data Engineer offering 5 years of leadership experience across various industries Methodical with significant experience in data mining and statistical analysis Excellent problemsolver with a history of automating processes and driving operational enhancements Effective at making important team decisions focused on moving products through planning preproduction and production phases        Skills           Data Integrity Validation and Analysis  Database Programming and SQL  Product Planning      Analytical Problem Solving  Strong Work Ethic  Application Support                       Experience       Data Engineer II        032021      Current     Usaa    –    Argyle     TX            Work with stakeholders to identify improvements to processes and data delivery using technical aptitude to deliver ad hoc tools reporting and analytics with a focus on the needs of the Residential Portfolio  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts  Maintain JIRA projects and workflows to meet project metrics  Work with Residential Portfolio to reduce CIP Construction InProgress by 69 across Residential Cox Business and Network Transformation programs by developing clear processes to mitigate risk providing visibility through enhanced reporting and database aligning  Build and validate the hypothesis of product ideas and infrastructure improvements for Fiber Reinforcement New Build Rebuild and Expansion Projects  Provide for database management to support the integrity of data for enterprisesupported tools and applications  Work with software development team to design and customize applications SiteTracker applications and trackers to meet market exceptions  Utilized existing reporting to perform data blending as needed to serve business needs  Develop processes to improve workflow and department efficiency  Designed and developed data quality dashboards with visualization in Tableau Power BI and Salesforce  Work with the development team to define and implement customer change requests to enhance product functionality  Complete pilot testing implement best data practices and optimize workflow to enhance performance and drive efficiencies in the organization to meet or exceed customer and business expectations  Utilize WATTS and SiteTracker experience as a user andor report development  Identify strengths and weaknesses of existing reports suggests areas of improvement and help enhance existing data reports to meet evolving requirements  Interpret and analyze data using exploratory mathematic and statistical techniques based on scientific methods           ANALYTICS CONSULTANT       032019      042021     Syndigo    –    Boise     ID            Partnered with WBS developers to automate manual processes decreasing errors and saving time creating a 6 increase in margins 2021 fiscal year  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Worked to create UI mockups and prototypes that illustrate how sites function and look like  Provided executives with analytics and decisionsupport tools used a basis for reorganization consolidation and relocation strategies  Executed tests collected and analyzed resulting data and identified trends and insights to achieve maximum ROI in paid search campaigns  Evaluated project requirements and content standards for each project to produce copy in line with a creative structure  Recommended changes to website architecture content and linking to improve SEO positions           DATA QUALITY ANALYST       052018      092020     Illumina    –    Virginia     MN            Worked mainly with wellknown engineering clients to provide customers with quality products  Dashboard development for project projection project closing and project profit  Responsible for estimating negotiating and agreeing on budgets and times lines while overseeing the production process  Responsible for drafted proposals per specifications and estimation skeleton with a focus on Data Centers  Developed RPAs that work alongside ERP to run cost analytics and tracked quality data metrics  Provide code compliance knowledge and enforcement for both owner and contractor  Drive and own backlog grooming and management prioritize iteration and drive acceptance testing and delivery of iterations  Defining road maps and prioritizing backlogs of work to meet with a vision of providing services that meet customers standards  Prioritized functionality backlog after golive to resteer team to achieve the desired ROI  Performed root cause analysis based on rework data to create corrective and preventive measures throughout the product development  Collaborated with Business Analysts and the software development team to identify and convert business goals into data requirements  Developed and streamlined productivity tracker to provide 34month predictions  Responsible for managing the progression of a project from RFP to  CO          Education and Training       Master of Science       Computer Science  Analytics        Expected in                   Georgia Tech Master of Science in Computer       Atlanta GA          GPA        Status                  Bachelor of Science       Civil Engineering   Industrial Microbiology       Expected in                   University of Georgia      Athens     GA     GPA        Status                 Activities and Honors       GatherUp  Nonprofit organization working with the community to provide recourses for further growth  NAMIC  Marketing  2021 2H Synergy Award|none|['data mining', 'statistical analysis', 'Programming', 'SQL', 'Problem Solving', 'JIRA', 'data blending', 'Tableau', 'Power BI', 'Salesforce', 'pilot testing']
Data Engineer|https://www.livecareer.com/resume-search/r/data-center-cloud-engineer-architect-8eec0f1efb0947ffab14c9580532f9d1|326757138740783914221199641260286969063|Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Summary      Senior system engineer with 20 years of experience in software design development and architecture Full software development life cycle requirement gathering prototyping design implementation testing release and maintenance Has strong customer interfacing experience Has strong project lead and vendor management experience Capable of working with various teams to drive requirement gathering analysis and application design Polished communication and presentation skills able to demo products and solutions to both internal and external audience        Highlights          Performance and scalability optimization  Development environment software  Complex problem solver  Strong decision maker  Excellent communicator     Strong in C C  Microsoft Visual C  PHP MySQL   Linux Scripting   Cloudera Hadoop Certified  Ceph storage clustering  Apache Mesos   Docker and Linux Containers                       Accomplishments      Promoted to Lead Engineer after 18 months of employment at Dell  Promoted to strategist position at Dell after 5 years of employment  Holder of several US issued patents  US patent 20090100194  httpwwwfaqsorgpatentsapp20090100194   US patent 20130086262  httpwwwfaqsorgpatentsapp20130086262   US patent 20120198349  httpwwwfaqsorgpatentsapp20120198349         Experience      022013   to   072015     Data Center Cloud EngineerArchitect      Abbott Laboratories    –    Madison     MS             Solid experience with Mesosphere and the latest offering of DCOS Data Center Operating System Currently integrating DCOS on cloud hardware using Chef as a configuration management tool   Experience with Data Center Cloud Servers Responsible for designing and building a rack level softwarehardware solution for hyperscale cloud customers   Responsible for evaluating ARM 64 bit as a server solution in a data center   Evaluating CoreOS and Docker containers as a solution for Dell servers in a data center   Experience with Ceph as a cold storage server solution  Currently working as a solution architect to support the sales team by providing technical guidance on Dell cloud servers  Responsible for proposing solutions to customers and responding to customer RFI and RFP  Travel to customer site to present about Dell cloud servers          2007   to   022013     Principal Engineer      Johnson  Johnson    –    Athens     GA             Worked on competitive analysis to compare the Cisco UCS solution to Dells Active System solution  Worked on automating OS deployment using WSMAN and Dells life cycle controller The solution was intended to help services team in the field to have a quick tool to deploy the Active System solution  Worked on integrated solutions using PowerEdge servers PowerConnect switches and Compellent storage array The integrated solution is intended to build a business ready configuration called vStart  Was Responsible for the integration of Hadoop Big Data with HPC clustering stack  Was the lead technical engineer to lead an engineering group of 10 The team was responsible for developing custom solutions to address unique customer requirements  Was responsible to support the sales team in EMEA by providing technical insight into Dell products  Developed plugins for Microsoft SCOM System Center Operation Manager Most development was done using C Visual Basic scripting and XML  Traveled to Dell India to train group of engineers on Dell system management products  Interface and interlock between several teams to lock down requirements and drive to results  Worked with Dell marketing on new feature requirements  Developed a Windows and Linux solution to monitor Dell systems for storage and BMC alerts and send SNMP traps to a system management console          032000   to   2007     Senior Software Engineer      Transcore    –    Hayward     CA             Developed firmware using C and C to authenticate users via Active Directory using industry standard LDAP protocol  Have a good working knowledge of USB protocols Worked with CATC to debug several USB devices  Developed a USB Linux kernel mode mass storage stack to make a USB slave device appear as a USB disk  Was the lead engineer of a group that developed Voice over IP clientserver application that allowed chat over the network The technology used GSM610 Audio Codec and Win32 Wave API  Developed Internet Explorer plugin using ActiveX technology to be used with Dell Remote Access Cards remote media feature  Worked on defining a protocol to send SCSI commands over  TCPIP The protocol was used as the basis for the remote media feature on Dells Remote Management Card DRAC   Developed application software for remote KVM keyboard video and mouse to provide the console redirection feature for Dells Remote Management Card DRAC   Was responsible for the integration of a high performance serial device driver using Microsoft DDK for Windows NT as well as Windows 20002003 following the WDM architecture          041999   to   032000     Software Engineer      Transcore    –    Miramar     FL             Was responsible for the design implementation and integration of tools such as compiler linker and assembler in an integrated development environment IDE  The IDE is used by firmware programmers to develop on Zilogs family of microcontrollers  This project required intense use of C MFC and Multithreaded Programming          031997   to   031999     Software Engineer      Transcore    –    Peachtree Corners     GA             Was the primary engineer responsible for developing Windows 98 and Windows NT device driver using WDM Win32 Driver Model Microsofts SDK and Microsofts DDK Driver Development Kit to allow host to target communication between Kodaks digital camera and the PC  Successfully developed application and interface software using C and object oriented techniques that allowed interfacing a digital camera to the IEEE 1394 high performance serial bus  Was part of the team to develop firmware embedded software for Kodaks high end digital camera  Was part of the team to develop the camera SDK software development kit using multithreaded concepts and Visual C the SDK was used by outside developers to control and acquire images from the Kodak digital camera          111994   to   031997     Software Engineer      ULTRA SCANCALSPAN CORPORTATION    –    City     STATE             Was part of the team responsible for coding and maintaining fingerprint match software using C and C on a UNIX workstation  Developed Windows 95 driver using C to allow communication with an ultrasonic fingerprint scanner from a PC over the parallel port  Integrated fingerprint match software into several applications using Visual C MFC and the Win32 SDK Software Development Kit  Designed and implemented firmware using Assembly and C to interface the MOTOROLA DSP56166 to an ultrasonic fingerprint scanner  Successfully developed firmware to allow transfer of images from a scanner to the PC over the parallel and the serial port Assembly and C were used in this project          Education      Expected in        BS     Electrical and Computer Engineering     State University of New York      Buffalo     New York     GPA   with Cum Laude GPA 3640            Skills      Win32 API ActiveX Big Data IPMI WSMAN C  C Visual C Visual Basic Visual Source Safe ClearCase Linux gnu tools  storage clustering competitive analysis  Database Dell servers device drivers XML Windows Audio API  PHP ASP  JAVA LDAP Active Directory MySQL Microsoft SQL MFC API Windows 2000  Windows 2008 Windows 20012  networking object oriented  Programming Red Hat Linux Ubuntu Linux  technical sales  SCSI protocols  Storage technology  scripting SNMP   UNIX USB VB scripting Voice over IP|none|['software development life cycle', 'requirement gathering', 'prototyping', 'testing', 'optimization', 'C', 'C', 'C', 'PHP', 'MySQL', 'Linux', 'Scripting', 'Cloudera', 'Hadoop', 'clustering', 'Apache', 'Mesos', 'Docker', 'Linux', 'Docker', 'Hadoop', 'Big Data', 'C', 'Visual Basic', 'XML', 'Windows', 'Linux', 'C', 'Active Directory', 'Linux', 'KVM', 'C', 'UNIX', 'Assembly', 'Assembly', 'Big Data', 'C', 'C', 'Visual Basic', 'Visual Source Safe', 'ClearCase', 'Linux', 'clustering', 'XML', 'Windows', 'PHP', 'JAVA', 'LDAP', 'Active Directory', 'MySQL', 'SQL', 'Windows', 'Windows', 'Windows', 'networking', 'Programming', 'Red Hat', 'Linux', 'Ubuntu Linux', 'UNIX', 'VB']
Data Engineer|https://www.livecareer.com/resume-search/r/data-protection-engineer-95cf566a964e41eaa5970c3e1d68ce54|88130048337986624128176699613407932497|JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Profile     Professional and dedicated worker committed to delivering high quality performance and superior customer service to insure team success Utilize strong ethics customer relations and training  Over 8 years of experience in Storage management including designing installing configuring  and administering TSM  Netbackup Experience in both UNIX and Windows environment       Core Qualifications           Extensive knowledge of TSM administration procedures    Adept at server software installation and maintenance      Familiar with UNIX and Perl scripting     Good problem solving skills                          Professional Experience        072015   to   Present   Data Protection Engineer    Bmo         Pleasant Prairie     WI            TSM Implementation and administration TSM 6X 5X Implementation and administration on AIX Linux Solaris Windows platforms Tivoli data Protection for Oracle SQL implementation Bare metal Recovery implementation IBM 3592 EO5 LTO2LTO3 and LTO4  DataDomain configuration Tivoli Storage Management Disaster Recovery Implementation Installing TSM client on Unix and Win 200x servers TSM tuning for faster backup and restore Server to server configuration Expertise in TSM DR implementation and recovery Lanfree configuration  on UnixAixLinuxHPUX TSM library manager and library client configuration and Implementation Managing 8 TSM server environment with 2000 clients different site with Data Doman and physical library TSM administration and node backup centralized scheduling and reporting Avamar Client registration and configuration Avamar Client backup and restore Participate in SRT Schedules Retention and Targets yearly review with customers            042012   to   052015   Data Protection Engineer    Bmo         Portage     WI            TSM 6X 5X Implementation and administration on AIX Linux Solaris WinTel platforms Tivoli data Protection for Oracle SQL implementation Bare metal Recovery implementation IBM 3592 EO5 LTO2LTO3 and LTO4  DataDomain configuration Tivoli Storage Management Disaster Recovery Implementation Installing TSM client on Unix and Win 200x servers TSM tuning for faster and backup and restore Server to server configuration Expertise in TSM DR implementation and recovery Lanfree configuration  on UnixAixLinuxHPUX TSM library manger and library client configuration and Implementation Managing 10 TSM servers environment with 3000 clients different site with Data Doman and physical library            042007   to   052012   IT Specialist    Autodesk Inc         Colorado     TX            Netbackup 5x 6x implementation and administration on Window and Unix Netbackup client installation and configuration Provided  247 oncall support rotation Responsible for troubleshooting and daily backup issues Monitored and maintained tape volumes Troubleshot ACSLStape library software  issues Wrote a Runbook for both TSM and Netbackup Netbackup to TSM migration Netbackup client implementation Backup and restore using TSM Tivoli data Protection for Oracle SQL implementation            122006   to   042007   System Administrator    Advantest America Corporation         Fremont     CA            Coordinated hardware and software installations and upgrades to insure work is performed in accordance with Agency policy  Coordinated and monitored troubleshooting to isolate and diagnose command system problems  Daily system checks of all TSM servers tape library devices and server backup operations  Improved processes by writing scripts to automate task that is done on regular base            042000   to   062005   Design Automation Engineer    Intel Corp         Multiple Cities     NJ            Extensive experience in gate level simulation  Evaluated and QA new simulation tools  Assisted Design Engineers with tool issues they encountered and solve the problem  Interfaced between Intel and tool vendors to resolve issues and new releases  Provided automation and made improvements to existing flows  Trained new users on Gate Level Simulation tools and any new methodology  Experience in test vector generation test bench writing and regression running  Supported GLS and RTL VTPSIM Vector Test Pattern Simulation for all CPD Components Platform Division and tool owner for VTPSIM  Supported Design Engineering Environment for all Chipset Engineering including Software and License installations  Responsible for Local DBA Database Administration for TIBETBug tracking tool  Developed utility to generate indicator data for managers  Created Auto clone feature for Design Group which enabled them to submit bugs in two different projects at the same time  Provided training for Design Product Design Automation and Marketing Engineers on bug tracking tool  Setup training class for Design Automation Group            051997   to   042000   Test Engineer    Drs Technologies         Johnstown     PA            Sole author of STIL2S9K tool that generated all S9K test mode patterns for Camino MCH MTH and Carmel MCH  This tool was the first tool that enabled PCG Platform Components  Group to generate scan transition fault vectors with multiple timing sets  Kafka tool owner and support  This tool converts VCD Variable Change Dump file to STIL Standard Test Interface Language format  It was the first tool that enabled PCG to convert VCD to STIL  Worked with Teradyne Engineers to develop a STIL2J973 vector generation tool for J973 testers  Ran tests and gave feedback to the vendor  Helped out Product Engineers in generating test mode and functional vectors and validated the pattern before silicon arrived  Site owner for software tools Fabio and Purify  These tools were required to be run on the products test tape for white paper process          Education        Expected in   1994   Bachelor of Science       Computer Engineering    California State University     Sacramento     CA      GPA       Computer Engineering        Skills     AIX Agency automate Automation Backup hardware Client clients Database Administration DBA Disaster Recovery functional HP UX IBM Intel Linux Managing 8 Marketing Win WinTel Windows Window 2000 migration Oracle processes QA reporting scheduling server configuration servers scripts Simulation Solaris SQL Tivoli troubleshooting TSM TSM 6X Unix upgrades Netbackup Netbackup 5x author|none|['UNIX', 'Windows', 'UNIX', 'Perl', 'problem solving', 'Linux', 'Windows', 'Oracle', 'SQL', 'Unix', 'Linux', 'Oracle', 'SQL', 'Unix', 'Unix', 'Oracle', 'SQL', 'Automation', 'Kafka', 'Automation', 'Linux', 'Windows', 'Oracle', 'SQL', 'Unix']
Data Engineer|https://www.livecareer.com/resume-search/r/senior-software-engineer-data-manager-92600f55fb9a4018a4a7688a4607e44f|191782290901319920686020512632969913854|JC     Jessica    Claire                      Montgomery Street       San Francisco     CA    94105             555 4321000                 resumesampleexamplecom                         Summary      Senior Software Engineer with 26 years of experience as an enterpriselevel database web applications developer technical lead and project manager  Expert Microsoft SQL database administrator with SQL server integration services SSIS and SharePoint experience  ​  Proven leader and strategic thinker able to architect cuttingedge software solutions and lead development teams through the entire software development life cycle SDLC  Recipient of NASAs Space Flight Awareness Honoree award for significant contributions in the preservation and management of Lunar sample data         Skills           Databases Microsoft SQL Server 20002014 MongoDB MySQL  Languages SQL Microsoft NET C and VB ColdFusion 11 Java JavaScript PowerShell Python CC  Development ToolsMethodologies SDLC Agile  Waterfall Methodologies Object Oriented Programming OOP Application Programming Interfaces API MVC Visual Studio Eclipse Git Subversion UML SSIS SSRS HTML5 CSS SASSLESS web services REST jQuery Bootstrap Accessibility508 Compliance Website Security UX Design  Graphic DesignMultimedia  Adobe Creative Suite CC Photoshop Illustrator InDesign Dreamweaver Premiere Pro Lightroom Inkscape  Software  Adobe Acrobat Pro Microsoft SharePoint Visio and Project R                         Experience        012007   to   Present   Senior Software EngineerData Manager    Ssi Schaefer Systems International North America         Columbus     OH            Designed and developed web database applications to showcase NASAcurated astromaterials sample collections including comprehensive searchable databases with sample processing and availability information scientific analyses references and multimedia photos 3D models videos for use by space and planetary scientists  Technologies used included Adobe ColdFusion Microsoft NET C JavaScriptJQuery SVG REST various APIs and Microsoft SQL among others  Architected managed and developed internal Enterpriselevel databases and applications using both traditional waterfall and agile methodologies  Led teams of 25 developers and interns both internal and in distributed teams and provided technical mentorship  Significant expertise as database architect and administrator with experience creating SQL server integration services SSIS and reporting service SSRS solutions and optimizing query performance  Regularly employed scripting technologies such as PowerShell and Python to implement work automation and provide data visualization  As webmaster for the directorate public websites was a recognized expert in all aspects of website management including the administration of IIS and Apache web servers ColdFusion and Java application servers and SQL servers  Maintained adherence to NASA and government standards and regulations accessibility508 compliance information privacy websiteapplication security  Developed and maintained relationships and collaborations within NASA and global research and technology communities that leveraged crossfunctional expertise to improve the quality and accessibility and transfer of scientific data and support NASAs Open Data initiatives            012001   to   012007   IT Consultant    Splunk         Aldie     VA            Windows 10 Windows Server 2003  2012 R2 Linux IIS 758 Apache ColdFusion Server Designed and developed custom ecommerce and informational websites for small to mediumsized clients using various technologies including Adobe ColdFusion and Flash Microsoft Visual Basic and ASP Java JavaScript MySQL and Microsoft SQL on ApacheLinux and IISWindows systems  Provided website management and system administration services to consumers and small businesses  Coordinated full range of project development from initial proposal to final delivery            011998   to   012001   IT Program Manager    Polar Tank         Houston     TX            Managed enterprise software projects with budgets in excess of 3M which were consistently completed within budgetary and time constraints  Prepared proposals managed budgets and developed business system requirements and functional specifications for proposed projects  Supervised 10 business analysts project managers application developers and contractors during entire software development lifecycle  Designed and implemented relational and OLAP databases for integration with web applications and developed middletier components using Microsoft Visual Basic ASP ADO and COMCOM technologies using the Microsoft Visual Studio IDE and SQL Server 2000 with Analysis Server  Experienced using ERWin Data Modeler and UML for database design  Successfully completed global implementation of wide area networks and computer installations in 20 international locations by directing internal resources and local and international external vendors            011994   to   011998   DSP Marketing Programs Manager    TEXAS INSTRUMENTS         City     STATE            Designed and implemented intranetextranet web applications for the sales organization customers and product distributors that greatly reduced marketing support and training costs for the division  Web applications were implemented in C and Perl on a UNIX platform for the NCSA Mosaic browser and for Netscape Navigator  Managed team of 34 developers            011993   to   011994   Technology Transfer Team Leader    SEMATECH         City     STATE            Managed an average of 30 projects a month for the Lithography division to fulfill technology transfer requirements of consortium            011991   to   011993   Design Engineer    IBM CORPORATION         City     STATE            Designed 486based computer motherboards and peripheral interfaces for IBMs PC Value Line          Education and Training        Expected in   1991   MS       Electrical Engineering    Solid State Devices  Circuits  University of Michigan     Ann Arbor     MI      GPA       Electrical Engineering        Awards              Skills     Microsoft NET 3D ADO Adobe Creative Suite Adobe Acrobat Adobe Dreamweaver Photoshop Premiere Agile Apache API ASP automation budgets C ColdFusion COM COM CSS clients data visualization database applications Databases database architect database design delivery directing ecommerce Eclipse ERWin extranet Flash functional government Graphic Design UX HTML5 IBM IDE IIS IIS 75 Illustrator InDesign Java JavaScript JQuery Linux marketing C Microsoft SharePoint Microsoft SQL SQL Server 2000 Windows 2000 MongoDB motherboards Multimedia MVC MySQL Enterprise Netscape Navigator networks Object Oriented Programming OOP OLAP Perl Programming project development proposals proposal Python quality reporting research sales scientific SDLC servers scripting software development Microsoft SQL Server SQL SQL server system administration UML UNIX Visio Microsoft Visual Basic VB Microsoft Visual Studio Visual Studio Web applications web servers Web Development Website websites website management webmaster Windows Server 486       Additional Information       AWARDS JETS Superior Performance Team Award 2014  Apollo Sample Curation Team NASA Group Achievement Awards 2014  Apollo Sample Curation Team  2013  Hayabusa Curation Team  Stardust Interstellar Curation Team JSC Directors Innovation Group Achievement Award 2013  Antarctic Meteorite Curation Team NASA Space Flight Awareness Awards 2009  Honoree  GRANTS 2016 NASA PDART CoPI Creating and Serving Novel Data Products of Astromaterials Combining ImageBased 3D Reconstructions and XRay CT Data of Astromaterials 2015 NASA PDART CoPI MoonDB Restoration and Synthesis of Lunar Petrological Data         Websites Portfolios Profiles|none|['SQL', 'SQL server integration services', 'SSIS', 'SharePoint', 'software development life cycle', 'SQL Server', 'MongoDB', 'MySQL', 'SQL', 'C', 'VB', 'Java', 'JavaScript', 'PowerShell', 'Python', 'Agile', 'Waterfall', 'Object Oriented Programming', 'OOP', 'Programming', 'Eclipse', 'Git', 'Subversion', 'UML', 'SSIS', 'SSRS', 'HTML5', 'CSS', 'REST', 'jQuery', 'Bootstrap', 'Adobe Creative Suite', 'Photoshop', 'Illustrator', 'InDesign', 'Dreamweaver', 'SharePoint', 'Visio', 'R', 'multimedia', 'C', 'REST', 'SQL', 'waterfall', 'agile', 'SQL server integration services', 'SSIS', 'SSRS', 'PowerShell', 'Python', 'automation', 'data visualization', 'Apache', 'Java', 'SQL', 'research', 'Splunk', 'Windows', 'Windows', 'Linux', 'Apache', 'Visual Basic', 'Java', 'JavaScript', 'MySQL', 'SQL', 'Visual Basic', 'Microsoft Visual Studio', 'SQL Server', 'Perl', 'UNIX', 'Adobe Creative Suite', 'Agile', 'Apache', 'automation', 'C', 'ColdFusion', 'CSS', 'data visualization', 'Eclipse', 'Graphic Design', 'HTML5', 'Java', 'JavaScript', 'JQuery', 'Linux', 'C', 'SharePoint', 'SQL', 'SQL Server', 'Windows', 'MongoDB', 'Multimedia', 'MySQL', 'Object Oriented Programming', 'OOP', 'OLAP', 'Perl', 'Programming', 'Python', 'research', 'SQL Server', 'SQL', 'SQL server', 'UNIX', 'Visio', 'Visual Basic', 'VB', 'Microsoft Visual Studio', 'Web Development', 'Windows']
Data Engineer|https://www.livecareer.com/resume-search/r/sr-data-engineer-867553e4eb4548f0bdb1253aa98a392b|217835661821555841839767419203542304004|Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Professional Summary       Over 8 years of progressive professional experience in analysis Design Development and Implementation as a Sr Data Engineer and Data ModelerData Analyst  Strong experience in Data Migration Data Cleansing Transformation Integration Data Import and Data Export  Good knowledge in Data Quality  Data Governance practices  processes  Highly Qualified with about 5 years of experience in building Big data applications creating data lakes to manage structured and unstructured data  Well versed with Agile with Scrum Waterfall Model and Testdriven Development TDD methodologies  Server Reporting Services SSRS and SQL Server Integration Services SSIS  Worked with Java based ETL tool Talend and Implemented Integration solutions for cloud platforms with Informatica Cloud  Extensive experience in setting up CI CD pipelines using tools such as Jenkins Github Nexus and Maven  Experience developing On  premise and Real Time processes  Experience on Migrating SQL database to Azure data lake Azure data lake analytics Azure SQL database Data Bricks and Azure SQL Data Warehouse and Controlling and granting database access and migrating on premise databases to Azure Data Lake Store using Azure Data Factory  Strong experience in Big Data technologies like Spark Spark SQL Pyspark Hadoop HDFS Hive Sqoop Flume Kafka Spark Streaming  Experience in job workflow scheduling and monitoring tools like Oozie M2  Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa  Experience in designing dashboards reports performing adhoc analysis and visualizations using Tableau Power BI  Handson experience on NoSQL databases like Snowflake HBase Cassandra and Mongo DB  Experience in building and architecting multiple Data pipelines end to end ETL and ELT process for Data ingestion and transformation in GCP and coordinating tasks among the team  Expertise in DBMS concepts  Experience in GCP Dataproc GCS Cloud functions Big Table and Big Query  Experience in writing and executing unit system integration and UATscripts in data warehouse projects  Worked on different file formats like Parquet Avro ORC and Flat files  Designing star schema Snowflake schema for Data Warehouse ODS architecture  Experience with Apache Spark ecosystem using SparkCore SQL Data Frames and RDDs  Experienced in data manipulation using python  Handson experience working at Amazon Web Services AWS using Elastic Map Reduce Redshift and EC2 for data processing AWS glue RDS S3 Lambda  Proficient in installing configuring and using Apache Hadoop ecosystems such as MapReduce Hive Pig Flume Yarn HBase Sqoop Spark Storm Kafka Oozie and Zookeeper  Experience in integrating Kafka with Spark streaming for high speed data processing  Extensive experience in developing and driving strategic direction of SAP operating system SAP ECC and SAP business intelligence SAP BI system  Develop effective working relationships with client teams to understand and support requirements develop tactical and strategic plans to implement technology solutions and effectively manage client expectations  An excellent team member with an ability to perform individually good interpersonal relations strong communication skills hardworking and high level of motivation             Skills         Technical Skills  HadoopSpark Ecosystem  Hadoop MapReduce Hiveimpala YARN Kafka Flume Sqoop Oozie Zookeeper Spark  Databases Oracle My Sql SQL Server PostgreSQL HBase Snowflake Cassandra Mongo DB  Cloud computing  Amazon Web Services AWS Amazon Redshift MS Azure Azure blob storage Azure Data Factory Azure Synapse  Google cloud PlatformBig Query Big Table Dataproc      BI Tools Business Objects XI Tableau 91 Power BI  Query Languages SQL PLSQL TSQL  Scripting Languages Unix Python  Operating Systems Linux Windows Ubuntu Unix  SDLC Methodology Agile Scrum Waterfall UML                     Education       CBIT    Hyderabad           Expected in   2013     –      –       Bachelor’s        Computer Science          GPA                     Work History       Humana Inc      Sr Data Engineer   Riverton     WY                   102020      Current     Involved in requirements gathering analysis design development change management deployment  Involved in development of real time streaming applications using PySpark Apache Flink Kafka Hive on distributed Hadoop Cluster  Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Milib  Extracted data from heterogeneous sources and performed complex business logic on network data to normalize raw data which can be utilized by BI teams to detect anomalies  Designed and developed Flink pipelines to consume streaming data from Kafka and applied business logic to massage and transform and serialize raw data  Developed a common Flink module for serializing and deserializing AVRO data by applying schema  Developed Spark streaming pipeline to batch real time data detect anomalies by applying business logic and write anomalies to Hbase table  Implemented layered architecture for Hadoop to modularize design  Developed framework scripts to enable quick development  Designed reusable shell scripts for Hive Sqoop Flink and PIG jobs  Standardize error handling logging and metadata management processes  Indexed processed data and created dashboards and alerts in splunk to be utilized action by support teams  Responsible for operations and support of Big data Analytics platform Splunk and Tableau visualization  Managed developed and designed dashboard control panel for customers and Administrators using Tableau PostgresQL and REST API calls  Designed and Developed applications using Apache Spark Scala Python Nifi S3 AWS EMR on AWS cloud to format cleanse validate create schema and build data stores on S3  Developed CICD pipelines to automate build and deploy to Dev QA and production environments  Supported production jobs and developed several automated processes to handle errors and notifications  Also tuned performance of slow jobs by improving design and configuration changes of PySpark jobs  Created standard report Subscriptions and Data Driven Report Subscriptions  Tools Hadoop Map Reduce Spark Spark MLLib Tableau SQL Excel PIG Hive AWS PostgresQL Python PySpark Flink SQL Server 2012 TSQL CICD Git XML Tableau           Cognizant Technology Solutions      Sr GCP Data Engineer   Lake Forest     CA                   012018      092020     Developed multi cloud strategies in better using GCP for its PAAS and Azure for its SAAS  Involved in loading and transforming large sets of the structured semistructured dataset and analysing them by running Hive queries  Developed custom python program including CICD rules for Google cloud data catalog for metadata management  Design and develop spark job with Scala to implement end to end data pipeline for batch processing  Do fact dimensional modeling and proposed solution to load it  Processing data with Scala spark spark SQL and load in hive partition tables in parquet file format  Develop spark job with partitioned RDD like hash range custom for faster processing  Develop and deploy the outcome using spark and Scala code in Hadoop cluster running on GCP  Develop near real time data pipeline using flume Kafka and spark stream to ingest client data from their web log server and apply transformation  Performs data analysis and design and creates and maintains large complex logical and physical data models and metadata repositories using ERWIN and MB MDR  Assist service developers in finding relevant content in the existing reference models  Like Access Excel CV Oracle flat files using connectors tasks and transformations provided by AWS Data Pipeline  Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries  Worked on developing Pyspark script to encrypting the raw data by using Hashing algorithms concepts on client specified columns  Responsible for Design Development and testing of the database and Developed Stored Procedures Views and Triggers  Developed Pythonbased API RESTful Web Service to track revenue and perform revenue analysis  Compiling and validating data from all departments and Presenting to Director Operation  Develop SQOOP script and SQOOP job to ingest data from client provided database in batch fashion on incremental basis  Use DISTCP to load files from S3 to HDFS and Processing cleansing and filtering data using Scala Spark Spark SQL HIVE Impala Query and Load in Hive tables for data scientists to apply their ML algorithms and generate recommendations as part of data lake processing layer  Build data pipelines in airflow in CP for ETL related jobs using different airflow operators both old and newer operators  Created Big Query authorized views for row level security or exposing the data to other teams  Good knowledge in using cloud shell for various tasks and deploying services           Honeywell      Big Data Engineer   Altamonte Springs     FL                   062016      122017     Implemented a generic ETL framework with high availability for bringing related data for Hadoop  Cassandra from various FRANCHI sources using spark  Experienced in using Platfora a data visualization tool specific for Hadoop and created various Lens and Viz boards for a realtime visualization from hive tables  Queried and analysed data from Cassandra for quick searching sorting and grouping through COL  Implemented various Data Modelling techniques for Cassandra  Joined various tables in Cassandra using spark and Scala and ran analytics on top of them  Participated in various upgrades and troubleshooting activities across enterprises  Knowledge in performance troubleshooting and tuning Hadoop clusters  Applied Spark advanced procedures like text analytics and processing using inmemory processing  Implemented Apache Drill on Hadoop to join data from SQL and No SQL databases and store it in Hadoop  Created architecture stack blueprint for data access with NoSQL Database Cassandra  Brought data from various sources into Hadoop and Cassandra using Kafka  Experienced in using Tidal enterprise scheduler and Ooze Operational Services for coordinating the cluster and scheduling workflows  Applied spark streaming for real time data transforming  Created multiple dashboards in tableau for multiple business needs  Installed and configured Hive and written Hive UDFs and used piggy bank a repository of UDFs for Pig Latin  Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access  Exported the analysed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Using Tableau  Implemented Composite server for the data virtualization needs and created multiple views for restricted data access using a REST API  Devised and led the implementation of next generation architecture for more efficient data ingestion and processing  Created and implemented various shell scripts for automating the jobs  Implemented Apache Sentry to restrict the access on the hive tables on a group level  Employed AVRO format for the entire data ingestion forecaster operation and less space utilization  Experienced in managing and reviewing Hadoop log files  Worked in an Agile environment and used rally tools to maintain the user stories and tasks  Worked with Enterprise data support teams to install Hadoop updates patches version upgrades as required and fixed problems which were raised after the upgrades  Implemented test scripts to support testdriven development and continuous integration  Used Spark for Parallel data processing and better performances  Tools Map Reduce HDFS Hive pig Impala Cassandra spark Scala solr Java SQL Tableau PIG Zookeeper Sqoop Teradata CentOS Pentaho           ITC InfoTech      Data Engineer   City          India              062014      112015     Worked on configuration and monitoring Hadoop cluster using Cloudera distribution  Involved in migrating data from on prem Cloudera cluster to AWS EC2 instances deployed on EMR cluster and developed ETL pipeline to extract logs and store in AWS S3 Data Lake and further processed it using PySpark  Moved files between HDFS and AWS S3 and worked with S3 bucket in AWS on a regular basis  Responsible for developing data pipelines using Flume Sqoop and Pig to extract the data from weblogs and store them in HDFS  Migrated data between various data sources like Teradata Oracle and MySQL to HDFS by using Sqoop  Used HCatalog to access Hive table metadata from MapReduce and Pig code  Developed a data pipeline using Kafka and Storm for streaming data and to store it into HDFS  Used Informatica Powercenter for cleaning managing and integrating data from different sources for ETL and loaded into a single warehouse repository  Used Impala to read write and query the Hadoop data in HDFS from HBase and constructed Impala scripts to reduce query response time  Analysed data stored in S3 buckets using SQL PySpark and stored the processes data in Redshift and validated data sets by implementing Spark components  Performed ETL operations using Python SQL on many data sets to obtain metrics  Prepared data according to analyst requirements on the extracted data using Pandas and NumPy modules in Python  Involved in designing and developing automation test scripts using Python  Involved in writing multiple python scripts to extract data from different API’s  Created HBase tables using Shell to load large sets of data from different databases  Involved in scheduling Time based Oozie workflow engine to run multiple Hive and Pig jobs  Developed flow XML files using Apache NiFi to process and ingest data into HDFS  Worked on performance tuning of Apache NiFi workflow to optimize the data ingestion speeds  Responsible for collecting and aggregating large amounts of log data using Flume and staging it into HDFS for further analysis  Worked on integration of Apache Storm with Kafka to perform web analytics and upload streaming data from Kafka to HBase and Hive  Responsible for developing data pipelines using Apache Kafka by implementing Kafka producers and consumers  Used Hive optimization techniques like partitioning and bucketing to provide better performance with HiveQL queries  Loaded large amounts of data to HBase using MapReduce jobs  Worked on developing UDFs to work with Hive and wrote our tests in Scala  Used zookeeper to maintain configurations across clusters and for better synchronization grouping and reliable distributed coordination  Worked with Kerberos and Apache sentry for security and authorization on Hadoop  Used Git for version control  Tools Cloudera CDH4 Hadoop 2x HDFS MapReduce Yarn Sqoop Hive AWS EC2 S3 Redshift Impala Spark Pig SQL HBase Kafka Zookeeper Flume Oozie HCatalog NiFi Storm Informatica Python MySQL Scala Teradata Oracle Git          Skills       Technical Skills  HadoopSpark Ecosystem  Hadoop MapReduce Hiveimpala YARN Kafka Flume Sqoop Oozie Zookeeper Spark  Databases Oracle My Sql SQL Server PostgreSQL HBase Snowflake Cassandra Mongo DB  Cloud computing  Amazon Web Services AWS Amazon Redshift MS Azure Azure blob storage Azure Data Factory Azure Synapse  Google cloud PlatformBig Query Big Table Dataproc    BI Tools Business Objects XI Tableau 91 Power BI  Query Languages SQL PLSQL TSQL  Scripting Languages Unix Python  Operating Systems Linux Windows Ubuntu Unix  SDLC Methodology Agile Scrum Waterfall UML         Work History       ALCON     Sr Data Engineer   Fort Worth     TX    102020      Current     Involved in requirements gathering analysis design development change management deployment  Involved in development of real time streaming applications using PySpark Apache Flink Kafka Hive on distributed Hadoop Cluster  Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Milib  Extracted data from heterogeneous sources and performed complex business logic on network data to normalize raw data which can be utilized by BI teams to detect anomalies  Designed and developed Flink pipelines to consume streaming data from Kafka and applied business logic to massage and transform and serialize raw data  Developed a common Flink module for serializing and deserializing AVRO data by applying schema  Developed Spark streaming pipeline to batch real time data detect anomalies by applying business logic and write anomalies to Hbase table  Implemented layered architecture for Hadoop to modularize design  Developed framework scripts to enable quick development  Designed reusable shell scripts for Hive Sqoop Flink and PIG jobs  Standardize error handling logging and metadata management processes  Indexed processed data and created dashboards and alerts in splunk to be utilized action by support teams  Responsible for operations and support of Big data Analytics platform Splunk and Tableau visualization  Managed developed and designed dashboard control panel for customers and Administrators using Tableau PostgresQL and REST API calls  Designed and Developed applications using Apache Spark Scala Python Nifi S3 AWS EMR on AWS cloud to format cleanse validate create schema and build data stores on S3  Developed CICD pipelines to automate build and deploy to Dev QA and production environments  Supported production jobs and developed several automated processes to handle errors and notifications  Also tuned performance of slow jobs by improving design and configuration changes of PySpark jobs  Created standard report Subscriptions and Data Driven Report Subscriptions  Tools Hadoop Map Reduce Spark Spark MLLib Tableau SQL Excel PIG Hive AWS PostgresQL Python PySpark Flink SQL Server 2012 TSQL CICD Git XML Tableau           USAA     Sr GCP Data Engineer   San Antonio     TX    012018      092020     Developed multi cloud strategies in better using GCP for its PAAS and Azure for its SAAS  Involved in loading and transforming large sets of the structured semistructured dataset and analysing them by running Hive queries  Developed custom python program including CICD rules for Google cloud data catalog for metadata management  Design and develop spark job with Scala to implement end to end data pipeline for batch processing  Do fact dimensional modeling and proposed solution to load it  Processing data with Scala spark spark SQL and load in hive partition tables in parquet file format  Develop spark job with partitioned RDD like hash range custom for faster processing  Develop and deploy the outcome using spark and Scala code in Hadoop cluster running on GCP  Develop near real time data pipeline using flume Kafka and spark stream to ingest client data from their web log server and apply transformation  Performs data analysis and design and creates and maintains large complex logical and physical data models and metadata repositories using ERWIN and MB MDR  Assist service developers in finding relevant content in the existing reference models  Like Access Excel CV Oracle flat files using connectors tasks and transformations provided by AWS Data Pipeline  Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries  Worked on developing Pyspark script to encrypting the raw data by using Hashing algorithms concepts on client specified columns  Responsible for Design Development and testing of the database and Developed Stored Procedures Views and Triggers  Developed Pythonbased API RESTful Web Service to track revenue and perform revenue analysis  Compiling and validating data from all departments and Presenting to Director Operation  Develop SQOOP script and SQOOP job to ingest data from client provided database in batch fashion on incremental basis  Use DISTCP to load files from S3 to HDFS and Processing cleansing and filtering data using Scala Spark Spark SQL HIVE Impala Query and Load in Hive tables for data scientists to apply their ML algorithms and generate recommendations as part of data lake processing layer  Build data pipelines in airflow in CP for ETL related jobs using different airflow operators both old and newer operators  Created Big Query authorized views for row level security or exposing the data to other teams  Good knowledge in using cloud shell for various tasks and deploying services           T  Mobile     Big Data Engineer   Bellevue     WA    062016      122017     Implemented a generic ETL framework with high availability for bringing related data for Hadoop  Cassandra from various FRANCHI sources using spark  Experienced in using Platfora a data visualization tool specific for Hadoop and created various Lens and Viz boards for a realtime visualization from hive tables  Queried and analysed data from Cassandra for quick searching sorting and grouping through COL  Implemented various Data Modelling techniques for Cassandra  Joined various tables in Cassandra using spark and Scala and ran analytics on top of them  Participated in various upgrades and troubleshooting activities across enterprises  Knowledge in performance troubleshooting and tuning Hadoop clusters  Applied Spark advanced procedures like text analytics and processing using inmemory processing  Implemented Apache Drill on Hadoop to join data from SQL and No SQL databases and store it in Hadoop  Created architecture stack blueprint for data access with NoSQL Database Cassandra  Brought data from various sources into Hadoop and Cassandra using Kafka  Experienced in using Tidal enterprise scheduler and Ooze Operational Services for coordinating the cluster and scheduling workflows  Applied spark streaming for real time data transforming  Created multiple dashboards in tableau for multiple business needs  Installed and configured Hive and written Hive UDFs and used piggy bank a repository of UDFs for Pig Latin  Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access  Exported the analysed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Using Tableau  Implemented Composite server for the data virtualization needs and created multiple views for restricted data access using a REST API  Devised and led the implementation of next generation architecture for more efficient data ingestion and processing  Created and implemented various shell scripts for automating the jobs  Implemented Apache Sentry to restrict the access on the hive tables on a group level  Employed AVRO format for the entire data ingestion forecaster operation and less space utilization  Experienced in managing and reviewing Hadoop log files  Worked in an Agile environment and used rally tools to maintain the user stories and tasks  Worked with Enterprise data support teams to install Hadoop updates patches version upgrades as required and fixed problems which were raised after the upgrades  Implemented test scripts to support testdriven development and continuous integration  Used Spark for Parallel data processing and better performances  Tools Map Reduce HDFS Hive pig Impala Cassandra spark Scala solr Java SQL Tableau PIG Zookeeper Sqoop Teradata CentOS Pentaho           ITC InfoTech     Data Engineer   Pune         062014      112015     Worked on configuration and monitoring Hadoop cluster using Cloudera distribution  Involved in migrating data from on prem Cloudera cluster to AWS EC2 instances deployed on EMR cluster and developed ETL pipeline to extract logs and store in AWS S3 Data Lake and further processed it using PySpark  Moved files between HDFS and AWS S3 and worked with S3 bucket in AWS on a regular basis  Responsible for developing data pipelines using Flume Sqoop and Pig to extract the data from weblogs and store them in HDFS  Migrated data between various data sources like Teradata Oracle and MySQL to HDFS by using Sqoop  Used HCatalog to access Hive table metadata from MapReduce and Pig code  Developed a data pipeline using Kafka and Storm for streaming data and to store it into HDFS  Used Informatica Powercenter for cleaning managing and integrating data from different sources for ETL and loaded into a single warehouse repository  Used Impala to read write and query the Hadoop data in HDFS from HBase and constructed Impala scripts to reduce query response time  Analysed data stored in S3 buckets using SQL PySpark and stored the processes data in Redshift and validated data sets by implementing Spark components  Performed ETL operations using Python SQL on many data sets to obtain metrics  Prepared data according to analyst requirements on the extracted data using Pandas and NumPy modules in Python  Involved in designing and developing automation test scripts using Python  Involved in writing multiple python scripts to extract data from different API’s  Created HBase tables using Shell to load large sets of data from different databases  Involved in scheduling Time based Oozie workflow engine to run multiple Hive and Pig jobs  Developed flow XML files using Apache NiFi to process and ingest data into HDFS  Worked on performance tuning of Apache NiFi workflow to optimize the data ingestion speeds  Responsible for collecting and aggregating large amounts of log data using Flume and staging it into HDFS for further analysis  Worked on integration of Apache Storm with Kafka to perform web analytics and upload streaming data from Kafka to HBase and Hive  Responsible for developing data pipelines using Apache Kafka by implementing Kafka producers and consumers  Used Hive optimization techniques like partitioning and bucketing to provide better performance with HiveQL queries  Loaded large amounts of data to HBase using MapReduce jobs  Worked on developing UDFs to work with Hive and wrote our tests in Scala  Used zookeeper to maintain configurations across clusters and for better synchronization grouping and reliable distributed coordination  Worked with Kerberos and Apache sentry for security and authorization on Hadoop  Used Git for version control  Tools Cloudera CDH4 Hadoop 2x HDFS MapReduce Yarn Sqoop Hive AWS EC2 S3 Redshift Impala Spark Pig SQL HBase Kafka Zookeeper Flume Oozie HCatalog NiFi Storm Informatica Python MySQL Scala Teradata Oracle Git|none|['Data Governance', 'Big data', 'data lakes', 'Agile', 'Scrum', 'Waterfall', 'SSRS', 'SQL Server Integration Services', 'SSIS', 'Java', 'ETL', 'Jenkins', 'Github', 'Nexus', 'Maven', 'SQL', 'Azure', 'Azure', 'Azure', 'SQL', 'Data Bricks', 'Azure', 'SQL', 'access', 'Azure', 'Azure', 'Big Data', 'Spark', 'Spark', 'SQL', 'Pyspark', 'Hadoop', 'HDFS', 'Hive', 'Sqoop', 'Flume', 'Kafka', 'Spark', 'Tableau', 'Power BI', 'NoSQL', 'Snowflake', 'HBase', 'Cassandra', 'Mongo', 'ETL', 'GCP', 'DBMS', 'GCP', 'Dataproc', 'GCS', 'Snowflake', 'Spark', 'SQL', 'python', 'Amazon Web Services', 'AWS', 'Redshift', 'AWS', 'Hadoop', 'Hive', 'HBase', 'Spark', 'Kafka', 'Kafka', 'Spark', 'SAP', 'SAP', 'SAP', 'SAP', 'Hadoop', 'Kafka', 'Spark', 'Oracle', 'Sql', 'SQL Server', 'PostgreSQL', 'HBase', 'Snowflake', 'Cassandra', 'Mongo', 'Cloud computing', 'Amazon Web Services', 'AWS', 'Redshift', 'Azure', 'Azure', 'Azure', 'Data Factory', 'Azure', 'Synapse', 'Google cloud', 'Tableau', 'Power BI', 'SQL', 'PLSQL', 'TSQL', 'Unix', 'Python', 'Linux', 'Windows', 'Unix', 'Agile', 'Scrum', 'Waterfall', 'PySpark', 'Apache Flink', 'Kafka', 'Hive', 'Hadoop', 'Apache', 'Spark', 'Python', 'Big Data', 'Machine learning', 'machine Learning', 'use cases', 'Spark', 'Kafka', 'Spark', 'Hbase', 'Hadoop', 'shell', 'Hive', 'Sqoop', 'Flink', 'PIG', 'splunk', 'Big data', 'Splunk', 'Tableau', 'Tableau', 'PostgresQL', 'REST', 'Spark', 'Scala', 'Python', 'Nifi', 'S3', 'AWS', 'AWS cloud', 'PySpark', 'Data Driven', 'Hadoop', 'Spark', 'Spark', 'MLLib', 'Tableau', 'SQL', 'Excel', 'PIG', 'Hive', 'AWS', 'PostgresQL', 'Python', 'PySpark', 'SQL Server', 'TSQL', 'Git', 'XML', 'Tableau', 'GCP', 'GCP', 'PAAS', 'Azure', 'SAAS', 'Hive', 'python', 'Google cloud', 'spark', 'Scala', 'modeling', 'Scala', 'spark', 'spark', 'SQL', 'hive', 'spark', 'spark', 'Scala', 'Hadoop', 'GCP', 'Kafka', 'spark', 'data analysis', 'Access', 'Excel', 'Oracle', 'AWS', 'Spark', 'SQL', 'PySpark', 'SQL', 'Pyspark', 'algorithms', 'Scala', 'Spark', 'Spark', 'SQL', 'HIVE', 'Hive', 'algorithms', 'airflow', 'ETL', 'airflow', 'shell', 'Big Data', 'ETL', 'Hadoop', 'Cassandra', 'spark', 'data visualization', 'Hadoop', 'hive', 'Cassandra', 'Cassandra', 'Cassandra', 'spark', 'Scala', 'Hadoop', 'Spark', 'Apache Drill', 'Hadoop', 'SQL', 'SQL', 'Hadoop', 'access', 'NoSQL', 'Cassandra', 'Hadoop', 'Cassandra', 'Kafka', 'spark', 'tableau', 'Hive', 'Hive', 'HIVE', 'Tableau', 'REST', 'shell', 'Apache Sentry', 'Hadoop', 'Agile', 'Hadoop', 'Spark', 'Hive', 'Cassandra', 'spark', 'Scala', 'Java', 'SQL', 'Tableau', 'Teradata', 'Hadoop', 'AWS', 'ETL', 'AWS', 'PySpark', 'AWS', 'AWS', 'Teradata', 'Oracle', 'MySQL', 'Hive', 'Kafka', 'ETL', 'Hadoop', 'HBase', 'SQL', 'PySpark', 'Redshift', 'Spark', 'ETL', 'Python', 'SQL', 'Pandas', 'NumPy', 'Python', 'automation', 'Python', 'python', 'HBase', 'Shell', 'Hive', 'XML', 'Apache NiFi', 'Apache NiFi', 'Apache Storm', 'Kafka', 'Kafka', 'HBase', 'Hive', 'Kafka', 'Kafka', 'Hive', 'optimization', 'HBase', 'Hive', 'Scala', 'Hadoop', 'Git', 'Hadoop', 'Hive', 'AWS', 'Redshift', 'Spark', 'SQL', 'HBase', 'Kafka', 'Python', 'MySQL', 'Scala', 'Teradata', 'Oracle', 'Git', 'Hadoop', 'Kafka', 'Spark', 'Oracle', 'Sql', 'SQL Server', 'PostgreSQL', 'HBase', 'Snowflake', 'Cassandra', 'Mongo', 'Cloud computing', 'Amazon Web Services', 'AWS', 'Redshift', 'Azure', 'Azure', 'Azure', 'Azure', 'Google cloud', 'Tableau', 'Power BI', 'SQL', 'Unix', 'Python', 'Linux', 'Windows', 'Unix', 'Agile', 'Scrum', 'Waterfall', 'PySpark', 'Apache', 'Kafka', 'Hive', 'Hadoop', 'Spark', 'Python', 'Big Data', 'Machine learning', 'machine Learning', 'use cases', 'Spark', 'Kafka', 'Spark', 'Hbase', 'Hadoop', 'shell', 'Hive', 'splunk', 'Big data', 'Splunk', 'Tableau', 'Tableau', 'PostgresQL', 'REST', 'Spark', 'Scala', 'Python', 'AWS', 'AWS cloud', 'PySpark', 'Data Driven', 'Hadoop', 'Spark', 'Spark', 'Tableau', 'SQL', 'Excel', 'Hive', 'AWS', 'PostgresQL', 'Python', 'PySpark', 'SQL Server', 'Git', 'XML', 'Tableau', 'GCP', 'GCP', 'Azure', 'Hive', 'python', 'Google cloud', 'spark', 'Scala', 'modeling', 'Scala', 'spark', 'spark', 'SQL', 'hive', 'spark', 'spark', 'Scala', 'Hadoop', 'GCP', 'Kafka', 'spark', 'data analysis', 'Access', 'Excel', 'Oracle', 'AWS', 'Spark', 'SQL', 'PySpark', 'SQL', 'Pyspark', 'algorithms', 'testing', 'Scala', 'Spark', 'Spark', 'SQL', 'HIVE', 'Hive', 'algorithms', 'airflow', 'ETL', 'airflow', 'shell', 'Big Data', 'ETL', 'Hadoop', 'Cassandra', 'spark', 'data visualization', 'Hadoop', 'hive', 'Cassandra', 'Cassandra', 'Cassandra', 'spark', 'Scala', 'Hadoop', 'Spark', 'Apache', 'Hadoop', 'SQL', 'SQL', 'Hadoop', 'access', 'NoSQL', 'Cassandra', 'Hadoop', 'Cassandra', 'Kafka', 'spark', 'tableau', 'Hive', 'Hive', 'HIVE', 'access', 'Tableau', 'access', 'REST', 'shell', 'Apache', 'access', 'hive', 'Hadoop', 'Agile', 'Hadoop', 'Spark', 'Hive', 'Cassandra', 'spark', 'Scala', 'Java', 'SQL', 'Tableau', 'Teradata', 'Hadoop', 'AWS', 'ETL', 'AWS', 'PySpark', 'AWS', 'AWS', 'Teradata', 'Oracle', 'MySQL', 'access', 'Hive', 'Kafka', 'ETL', 'Hadoop', 'HBase', 'SQL', 'PySpark', 'Redshift', 'Spark', 'ETL', 'Python', 'SQL', 'Pandas', 'NumPy', 'Python', 'automation', 'Python', 'python', 'HBase', 'Shell', 'Hive', 'XML', 'Apache NiFi', 'Apache NiFi', 'Apache', 'Kafka', 'Kafka', 'HBase', 'Hive', 'Apache', 'Kafka', 'Kafka', 'Hive', 'optimization', 'HBase', 'Hive', 'Scala', 'Apache', 'Hadoop', 'Git', 'Hadoop', 'Hive', 'AWS', 'Redshift', 'Spark', 'SQL', 'HBase', 'Kafka', 'Python', 'MySQL', 'Scala', 'Teradata', 'Oracle', 'Git']
Data Engineer|https://www.livecareer.com/resume-search/r/aws-data-engineer-7289f190da885bd0d8ec5349e6934932|239182503578527980007051234933087188268|Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases         Skills          ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                      Experience       AWS Data Engineer       012022   to   022022     Deloitte    –    Gilbert     AZ             Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Used Spark SQL for Scala  amp Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process  Configured computing networking and security systems within cloud environment  Implemented cloud policies managed technology requests and maintained service availability           Data Engineer       012016   to   112019     Cognizant Technology Solutions    –    Hatboro     PA             Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Big Data Engineer  Hadoop Developer       102013   to   122015     Novogradac  Co Llp    –    Long Beach     CA           AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Python Developer        092012   to   102013     Fiserv    –    City     STATE             AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes          Education and Training       Post Graduate      Data Engineering      Expected in   022022     Purdue University      West Lafayette     IN     GPA                Post Graduate      Data Science And Business Analytics      Expected in   092021     University of Texas At Austin      Austin     TX     GPA                Bachelor of Arts     Business Administration And Management     Expected in   122009     Califonia State University       Fullerton CA          GPA|none|['Big Data', 'Hadoop', 'Big Data', 'Data Visualization', 'Hadoop', 'Map Reduce', 'Hadoop', 'Hadoop', 'Spark', 'Hive', 'HBase', 'Kafka', 'programming', 'Spark', 'Spark', 'optimization', 'algorithms', 'Hadoop', 'Spark', 'Spark', 'PySpark', 'Scala', 'Hive', 'HIVE', 'Hadoop', 'Hadoop', 'Hive', 'Hive', 'Json', 'XML', 'Airflow', 'Airflow', 'Python', 'Bash', 'Google Cloud', 'Google Cloud', 'SQL', 'NoSQL', 'MongoDB', 'HBase', 'Cassandra', 'SQL server', 'PostgreSQL', 'Java', 'MongoDB', 'HBase', 'Phoenix', 'SQL', 'HBase', 'RDBMS', 'ETL', 'SQL Server', 'SQL Server', 'SSIS', 'XML', 'AWS', 'automation', 'automation', 'GIT', 'Ansible', 'modeling', 'Snowflake', 'modeling', 'JSON', 'SOAP web services', 'python', 'Sublime', 'Big Data', 'Hadoop', 'HDFS', 'Sqoop', 'PIG', 'Hive', 'HBase', 'Oozie', 'Flume', 'Kafka', 'Zookeeper', 'Yarn', 'Spark', 'Oracle', 'MySQL', 'SQL Server', 'MongoDB', 'Cassandra', 'DynamoDB', 'PostgreSQL', 'Teradata', 'Programming', 'Python', 'PySpark', 'Scala', 'Java', 'C', 'C', 'Shell', 'Perl', 'SQL', 'AWS', 'Azure', 'Django', 'REST', 'Eclipse', 'SQL Developer', 'SQL', 'Query Analyzer', 'SQL Server', 'SQL', 'Eclipse', 'Postman', 'SVN', 'Git', 'Windows', 'Ubuntu Linux', 'MacOS', 'Network Security', 'Kerberos', 'Database Modelling', 'Dimension Modeling', 'ER Modeling', 'Star Schema', 'Modeling', 'Snowflake', 'Modeling', 'Apache', 'Airflow', 'Tableau', 'ggplot2', 'matplotlib', 'SSRS', 'Power BI', 'Machine Learning', 'NLP', 'Clustering', 'AWS', 'access', 'AWS', 'S3', 'AWS', 'Lambda', 'DynamoDB', 'testing', 'HDFS', 'Hive', 'Pig', 'MapReduce', 'access', 'AWS', 'Redshift', 'S3', 'machine learning', 'algorithms', 'python', 'AWS', 'AWS', 'DynamoDB', 'Spark', 'SQL', 'Scala', 'Python', 'Spark', 'PySpark', 'SQL Server', 'SSIS', 'Teradata', 'ETL', 'RDBMS', 'Spark', 'Hive', 'Data blending', 'Alteryx', 'SQL', 'Tableau', 'Tableau', 'Kibana', 'Elasticsearch', 'AWS', 'Apache', 'Airflow', 'AWS', 'AWS', 'S3', 'RDS', 'Redshift', 'Lambda', 'Boto3', 'DynamoDB', 'SageMaker', 'Apache', 'Spark', 'HBase', 'Apache', 'Kafka', 'HIVE', 'SQOOP', 'Map Reduce', 'Snowflake', 'Apache', 'Python', 'SSRS', 'Tableau', 'networking', 'Azure', 'SQL', 'Cassandra', 'Azure', 'SQL', 'Azure', 'Azure', 'Kubernetes', 'Spark', 'Cassandra', 'Azure', 'SQL', 'Azure', 'Spark', 'Azure', 'Spark', 'SQL', 'SQL', 'Mongo', 'SQL Server Reporting Services', 'SSRS', 'Power BI', 'Databricks', 'Spark', 'Databricks', 'ADF', 'Azure', 'Azure', 'SQL', 'use cases', 'Azure', 'ADF', 'Azure', 'Azure', 'Spark', 'Azure', 'SQL', 'Databrick', 'Azure', 'Synapse', 'Cosmos', 'ADF', 'SSRS', 'Power BI', 'Azure', 'Azure', 'Apache', 'Spark', 'use cases', 'data analysis', 'data analysis', 'Big Data', 'Hadoop', 'Hadoop', 'Spark', 'Kafka', 'HIVE', 'ETL', 'HIVE', 'Spark', 'Spark', 'Spark', 'SQL', 'RDBMS', 'Java', 'Hive', 'XML', 'JSON', 'Hive', 'Hive', 'Business Requirements', 'ETL', 'Python', 'SQL', 'Snowflake', 'SQL Server Reporting Services', 'SSRS', 'Spark', 'Cassandra', 'Cassandra', 'Apache', 'Hadoop', 'Scala', 'Kafka', 'Hive', 'HBase', 'AWS', 'PySpark', 'Spark', 'Scala', 'Hive', 'JSON', 'AWS', 'Cassandra', 'PySpark', 'Apache', 'Spark', 'HBase', 'Apache', 'Kafka', 'HIVE', 'Apache', 'ETL', 'Snowflake', 'Apache', 'Python', 'Java', 'SSRS', 'Hadoop', 'Hadoop', 'data analysis', 'Python', 'AWS', 'Ansible', 'MySQL', 'Python', 'Git', 'Jenkins', 'DynamoDB', 'Cloud Watch', 'Docker', 'Kubernetes', 'design patterns', 'Python']
Data Engineer|https://www.livecareer.com/resume-search/r/senior-data-center-lab-engineer-99a816296ff7460daf6f5c4230dd5baf|65679956705363143962195955010725210777|Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary      Resultsdriven IT management professional with 20 years of experience in diverse industries including Infrastructure and data center management Expertise includes team leadership technical architecture training  recruiting mentoring  development disaster recovery planning asset management inventory control VAR experience contract to hire experience and information protection analysis Knowledge of Enterprise Project Lifecycle methodology with a emphasis in Infrastructure engineering with strong background in project management product support including breakfix to the board level More than 15 years in IT project management skilled in installation configuration migration and implementation of rack systems cable management cable testing server power and cooling infrastructure including retrofitting commissioning and decommissioning  data centers from the ground up Dynamic resourceful and extremely driven individual with a deep passion for creating and delivering programs and solutions that empower a team company and customer to meet and exceed desired expectations               Highlights         Enterprise platforms  Experienced in infrastructure design  Forecasting specialist  Knowledge of Product Lifecycle Management PLM  Supplier interface  Performance criteria tracking  Endtoend product lifecycles  Collaborative  Inventory tracking  Vendor management         Project tracking  Hardware and software upgrade planning  Product requirements documentation  Selfdirected  Budgeting and resource management  MS Visio  Decisive  Cost reduction  Colo experience and management  Staffing augmentation hiring and training                     Education       Training Technologies    Rancho Cucamonga     CA      Expected in   1997     –      –               A CNA certification          GPA            Coursework in Computer Networking and Information Technology         Certifications       A  certified 9E8DTT2557  Network  certification  Microsoft Certified Professional 1267994 Windows NT 40  Microsoft Certified Professional 1269364 Windows 2000   7073 NT Server 40  7067 Windows 2000 Professional  70270 Windows SQL 70  Administration Microsoft Networking Essentials 7058   Hewlett Packard authorized technician  SD0436  Compaq authorized technician  Dell authorized technician  25933  Network Analysis with Sniffer  Interconnecting Cisco Network Devices  Light Brigade fiber certification           Accomplishments       Responsible for managing the infrastructure project commissioning of a 428000 Sq Ft data center from a empty building to completion including assembling the infrastructure team required to install the equipment racks cabling power successfully ahead of schedule  Responsible for managing the decommissioning project of a DR data center successfully transferring the entire contents to another DR data center including the inventory and assets  Successfully Increased core system availability to 100 by developing standards in house breakfix ability redundant hot swap equipment allowing immediate replacement within minutes instead of hours and implementing best practices  Successfully transitioned a mainframe data center to a standard open system data center including the high volume printers successfully without error  under budget and ahead of schedule  Reduced the incidence of IT issues by 50 globally by designing a training improvement program for the infrastructure engineers increasing their skill sets and knowledge with industry certifications related to their daily tasks  Reduced costs and delays by internally completing breakfix on the hardware instead of contracting out the breakfix function The quality of the repairs also increased because of pride of ownership and the expectation that it be repaired correctly the first time                          Experience       Apex Systems      Senior Data Center Lab Engineer   Middleton     WI                   072016      Current       Insight Global contract position      Install and perform repairs to hardware and peripheral equipment following design and installation specifications including copperfiber cabling for connectivity This includes Dell HP servers and blades Cisco switches and firewalls HP switches  EMC arrays F5 and related KVM devices etc     Provided feet and hands support for commissioningdecommissioning breakfix to the board level cabling installation and testing and rack installation for the Barrington Chicago Milwaukee and Wauwatosa data centers as required     Conduct computer diagnostics firmware upgrades to investigate and resolve problems and provide technical assistance and support to product developers and programmers       Design maintain and audit inventory according to present and forth coming projects      Provide space power and cooling direction  and installation according to project hardware and software requirements      Provide quarterly data center hardware audits using Visio to maintain FDA standards and requirements                Accenture      Senior Data Center Engineer   Fort Harrison     MT                   102015      072016      Apex contract position   Provided daily commissioning and decommissioning of various types of equipment into racks based on HP Service Manager customer tickets including required copperfiber cables including cable management  Provided vendor escorts to install andor provide breakfix of customer equipment  Provided breakfix for servers switches firewalls and other related equipment to the board level and beyond  Provided hands and feet services for offsite engineers to troubleshoot install repair upgrade or anything required to return their equipment to operational status  Maintained the cleanliness of the data center including the floors and both the interior and exterior of each rack being installed in the data center  Set up and configure servers and blade servers in the lab racks prior to going live on the data center floor according to owner requirements  Participate in bridge calls requiring assistance for failed equipment andor connections until corrected as the onsite engineer  Maintain and monitor the power and cooling systems and related alarms      ​          Cincinnati Bell      IBM Global Remote Data Center Manager   West Chester     OH                   022015      062015      Compunnel contract position   Manage IBM Colo for Baxter pharmaceutical customer daily request for connectivity and hardware installation remotely  Project management to both decommission old customer equipment and to commission customers design in the COLO for new racks equipment and connectivity  Managed customer split between Baxter pharmaceutical and Baxalta pharmaceutical and ensure that the connectivity and equipment is moved and reinstalled in the COLO ensuring to separate companies  Worked directly with customer network administration to complete customer request for network connectivity and hardware trouble shooting including issuing the proper TCPIP and VMWare scopes  Provided mentoring and trouble shooting experience with onsite remote hands engineers to complete customer new orders for cable connectivity and hardware installation and trouble shooting       ​          Engility Corporation      Infrastructure Manager   Fort Meade     MD                   052009      062014     Directed and managed the infrastructure and engineers to bring the data online from a empty building to a fully operational 428000 Sq Ft production data center   Managed daytoday activities of 12 infrastructure technicians and DR data center which included handling daily ITSM tickets for  power server switch and router installations decommissioning and connectivity issues  Conducted performance reviews of engineers and scheduling of PTO ECC and on call 24X7 coverage  Directed engineers course of action resolving Internet connectivity general software and hardware issues  Directed handson management of racking stacking decommissions inventory asset management cable tracing cable clean up cable testing and new circuit installations including Roadm and dark fiber  Redesigned recruiting process requiring stringent technical knowledge  and experience and training for seven data centers including the training for engineers colo space and remote hands  This involved the following Windows operating systems from XP thru Windows 7 Server 2003 Cisco Nexus 7000 6500 3750s and 2800 series Net scout Netbotz 500 Compaq RF code HP and Dell servers Liebert CRACs and PDUs Terminal server using Cisco 2600 and Cisco Airports for wireless  Participated in the weekly change control and risk meetings collaborating with all levels of management from director to CTO  Hired trained and mentored 75 new infrastructure engineers  Managed the 75 person local data center infrastructure team allocating resources to ongoing projects and enforcing deadlines  Collaborated with the global team to resolve IT support cases in the data centers           Sentinel Technologies Inc      Self employed   Bloomington     IL                   032008      052009     Assisted residential and small business owners with daily hardware and software support  This included breakfix reimaging network connectivity wireless networks cabling and network installation and maintenance  This involved the following Windows from 98 thru Vista Microsoft Office 97 thru 2007 Linksys and DLink routers and switches phone support           Granite Telecommunications Llc      Premise Technician   Chicago     IL                   092007      032008     Installed UVerse into customers homes and businesses on a daily basis which involved making cross connects at the VRAD and then completing the connection using a balun at the customers NID  Cat5coax was then installed throughout the home or business which was terminated tested and certified and connected to a television andor computer  This involved the following Windows from 98 thru Vista Microsoft Office 97 thru 2007 Linksys and DLink routers and switches including trouble tickets           Amerisourcebergen Corporation  Corporate      System Integration Manager   Dothan     AL                   102006      042007      CompuCom contract position   Managed contract account for GE Healthcare including budget profit margin hiring recruiting and mentoring  This included the projects for each of my teams in cooperation with GE managers which consisted of 45 technicians ranging from tier one to designer  Responsible for the daily operation of GE end users and responded to help desk tickets to meet SOP and SLAs which ranged from windowsUNIX server issues and everything in between  This involved Windows XP and Server 2000 and 2003 UNIX 9 10 and Linux on HP Compaq Dell and Sun servers               ​     ​          ZurichFarmers Insurance      Facility Data Center Manager   City     STATE                   062006      092006      Teksystems contract position   Installed server racks and servers into the data center and maintained and monitored their connectivity  Also maintained the Liebert 750KVA UPS and Liebert CRAC units using Liebert sitescan 30  This included the Liebert PDUs and halon fire suppression system on a 247 oncall basis  This involved Windows 2000 Server UNIX and HP AUX           Wisconsin Department Of Corrections      SIS Project Manager   City     STATE                   042006      052006      Teksystems contract position   Managed the project to transfer all ownership and technical support of the Department of Corrections computer systems to the Department of Engineering which included transferring eight thousand and five hundred Microsoft Outlook accounts and six hundred and fifty print servers  This involved Outlook 2003 and process management           JP MorganChase      Data Center Engineer   City     STATE                   2006      042006      Teksystems contract position   Installed and deinstalled servers on a daily basis and installed either the Windows server OS or the UNIX OS depending upon the requirement using a script  This also involved breakfix on Dell Compaq and HP servers to the board level and maintenance and monitoring of the Liebert power and cooling systems  Collaborated with the global team to decommission the data center and transfer the assets           Allergan Pharmaceutical      NOC Manager   City     STATE                   2005      042005      Compucom contract position   Managed the daily operation of 55 tier one and tier two technicians that responded to help desk tickets for end user assistance ranging from file and folder issues to server connectivity issues  This also involved managing the SLAs SOPs and SarbanesOxley audits to ensure FDA regulation compliance  Handled the recruiting hiring and mentoring of the staff which included vendor management           Publicis Groupe Leo Burnett USA Inc      Data Center Manager   City     STATE                   012002      2005     Directed and managed the transition from a main frame data center to a traditional open systems data center including printing services  Managed three global data centers and participated in the building and project management of a fourth data center spanning Chicago Michigan New York and Europe  Directed the management of 9 analysts 2 networktelecom technicians and 3 electricians which included training and mentoring  I also managed and was responsible for calculating the cooling and power needs of the data centers using Liebert UPS and CRACs  This involved the daily breakfix of all hardware within the data centers and the maintenance and management of the AS390 mainframe on a daily basis including transitioning from the mainframe to open systems solely  This involved Dell Compaq and HP Sun HP AUX servers and Panduit racks using Windows NT 40 and 2000 Novell 411  Managed help desk ticket and phone support queue          Skills       15 years of experience in recruiting hiring training and mentoring infrastructure engineers  10 years of experience managing the daily functioning of 247 data centers  18 years of experience performing breakfix to the board level on servers desktops laptops and most printers  10 years of experience upgrading commissioning and decommissioning global data centers|none|['project management', 'project management', 'MS Visio', 'Networking', 'Windows', 'Windows', 'Windows', 'Windows', 'SQL', 'Networking', 'KVM', 'technical assistance', 'Visio', 'Project management', 'VMWare', 'Windows 7', 'Nexus', 'Windows', 'Microsoft Office', 'Windows', 'Microsoft Office', 'Windows XP', 'UNIX', 'Linux', 'Windows', 'UNIX', 'Microsoft Outlook', 'Outlook', 'Windows', 'UNIX']
Data Engineer|https://www.livecareer.com/resume-search/r/lead-data-engineer-7dc7cc010ca540828e6322bd6605e6e6|94356944266639456107307305550356935516|Jessica    Claire                                 609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor   Home   555 4321000        Cell           resumesampleexamplecom                  Summary      · Seasoned data engineer professional adept at understanding mandates developing plans leading and implementing enterprisewide solutions Complex problemsolver with an innovative approach A proven track record with experience in all facets of data engineering from ETL to data consumption  Extensive experience in Hadoop and cloud computing platforms with focus on automated CICD Strong programming skills in design and implementation using JavaJ2EE Scala SQL and other scripting languages Excellent leadership management qualities with excellent communication and interpersonal skills        Skills          · Design and Create scalable platforms and products in data realm including ETL in hadoop ecosystem data services and SQLNoSql databases  · Create and review high level design documentation architecture diagrams  · Design and create automated CICD for various data platform both onprem and cloud ecosystems    · Lead and participate in all aspects of SDLC collaborating with product owners engineering devops and delivery teams to align and deliver products  · Test software products to ensure that the software products developed by the engineering team meet the companys quality and standards  · Guide and mentor engineering team on technical matters including design architecture coding practices                      Experience      022020   to   Current     Lead Data Engineer      Transamerica Life Insurance Company    –    Charlotte     NC            · Managing 3 scrum teams of ETL services and devOps across data platforms  · Oversaw and develop the Largescale real data processing pipelines to handle transaction and real time data using kafka and spark to various databases as part of Data Management Platform team  · Leading migration from hbase to aerospike and solr to elasticsearch which including 30TB historical data migration using spark  · Collaborate with devOps network infra teams to create data plaform architecture and establish project plans and timeline  · Design and create seamless CICD pipelines and provide solutions to all application both onprem and cloud applications And also manage build engineering team  · Managing entitlement of the applications and tools extensively working with security risk and architecture teams to define and implement RBACs across data platform  · Working closely with security infrastructure and vendors to remediate software and infrastructure vulnerabilities including in house and third party applications part of data platform   Recipient of Values Champions Award for 2021 at Early Warning         112012   to   022020     Senior Data Engineer      Change Healthcare    –    Kennesaw     GA            · Developed Batch and streaming spark application in scala to load data kafka hbase and solr on cloudera based Hadoop platform in DMP  · Explored with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Data Frame PairRDDs Spark YARN  · Developed ETL pipelines using Streamsets to move data from kenisiskafka to various datastores  · Extensive experience in designing and implementation of continuous integration continuous delivery continuous deployment through Chef ansible bamboo gitlab and harness  · Setup full CICD pipelines so that each commit a developer makes will go through standard process of software lifecycle and gets tested well enough before it can make it to the production  · Created Streamsets pipelines to consume data from Amazon Kinesis and Redhsift for data processing         042011   to   092012     Software Engineer      General Motors    –    West Chester     OH            · Involved in analyzing system specifications designing and development for multiple J2EE wholesale applications  · Performed a shakeout test to the code migrated to UAT for the new customer setups defects and for the enhancements  · Performed acceptance testing and conducted functional testing for the customer using WebMethods and UNIX environment  · Extensively worked on PM Online application project from scratch  · Conducted full lifecycle software development from planning to deployment and maintenance  · Reviewed and modified unit and integration tests to improve software quality and reliability  · Performed backend testing using SQL queries to validate the data in the backend database Used SQL to validate backend database changes deletes and update         122007   to   032011     Software Developer      Walt Disney Co    –    Cincinnati     OH            · Delivered code to meet functional or technical specifications  · Designed frontend and backend solutions for testdriven development  · Participated in code review meetings providing input on bugs inefficiencies and potential solutions to emergent issues  · Modified existing software systems to enhance performance and add new features  · Performed regression and performance tests for updated systems         Education and Training      Expected in        Bachelor of Science     Electrical Engineering     University of South Alabama      Mobile     AL     GPA|none|['ETL', 'Hadoop', 'cloud computing', 'programming', 'Scala', 'SQL', 'ETL', 'hadoop', 'scrum', 'ETL', 'kafka', 'spark', 'hbase', 'solr', 'elasticsearch', 'spark', 'spark', 'scala', 'kafka', 'hbase', 'Hadoop', 'Spark', 'optimization', 'algorithms', 'Hadoop', 'Spark', 'Spark', 'SQL', 'Data Frame', 'PairRDDs', 'Spark', 'YARN', 'ETL', 'ansible', 'bamboo', 'gitlab', 'UAT', 'acceptance testing', 'functional testing', 'UNIX', 'SQL', 'SQL']
Data Engineer|https://www.livecareer.com/resume-search/r/technical-support-engineer-data-protection-advisory-division-682fb3cc81704e3d87a843b304cfc62a|142168863301654727446038493985454191825|Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Summary     Data Protection Consultant with 10 years of experience with a diverse background in Hyperconverged  storage technologiesBuilding  Implementing  providing thoughtful solutions to external customers clients are best equipped to deal with Disaster situations        Experience      02XXX6   to        Technical Consultant      Perficient    –    Saint Louis     MO           Designed  delivered  deployed  migrated complex solutions and providing architectural recommendations on Data Protection technologies for vendors like DELL EMC  Rubrik  Cohesity and their integration with cloud providers  AWS  different applications like SQLOracle ExchangeDeveloped Run books for various proceduresProvided  participated with vendors to investigate root cause analysisProvided strategic direction and ensure the delivery of technical based projectsDevelop Service proposals for each scope of work for various vendors​        05XXX3   to   02XXX6     Delivery Specialist      Amobee    –    Manila     AR           Performed necessary storage infrastructure maintenance and necessary data migration as required Document articles on product behavior and set guidelines for the sameManage troubleshoot Integrate Implement as well as support DELL Provided proactive recommendations to external client for improving their use of DELL EMC Data Protection solutions         07XXX1   to   05XXX3     SYSTEMS ENGINEER      Servicenow    –    Hamburg     NY             Managed multiple DELL EMC Networker Environments for various clients  Performed upgrading  trouble shooting various Data Protection issues  Proactively manage space requirements for backup environments across the company  Understand and support the application level backups and restores of a robust exchange environment  Created Backup reports to discuss storage utilization every week           ​             102008   to   082009     TECHNICAL SUPPORT EXECUTIVE      Softcell Technologies Limited    –    City     STATE           As a member of Security team performed installation of Symantec Endpoint Protection AntivirusParticipated in maintenance  activities for the Lifecyle of productHelped customer to provide best possible solutions to prevent malicious attacks on workstations​        Education      Expected in   XXX1     Master of Science     Telecommunication Management     Stevens Institute of Technology      Hoboken     New Jersey     GPA   GPA 3389        Telecommunication Management GPA 3389          Expected in   2007     Bachelor of Engineering     Electronics and Telecommunication     Ramrao Institute of Technology      Mumbai     Maharashtra     GPA   GPA 34        Electronics and Telecommunication GPA 34          Expected in   2004     Diploma     Industrial Electronics     Bharati Vidyapeeth Institute of Technology      Mumbai     Maharastra     GPA   GPA 38        Industrial Electronics GPA 38|none|[]
Data Engineer|https://www.livecareer.com/resume-search/r/senior-data-engineer-5e86ce6ea1a64e939ce3a7158956de13|130969556747859829302611690656487220649|Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary      Practical Database Engineer possessing indepth knowledge of data manipulation techniques and computer programming paired with expertise in integrating and implementing new software packages and new products into system Offering 12year background managing various aspects of development design and delivery of database solutions Techsavvy and independent professional bringing outstanding communication and organizational abilities Hardworking and reliable Data Engineer with strong ability in building Data pipelines Highly organized proactive and punctual with teamoriented mentality        Skills           Requirements Gathering  Analysis and Modeling  Data Warehousing  SQL Reporting  Business Intelligence      Data Management  Microsoft SQL  Database Analysis  SQL Tuning  Critical Thinking                       Experience       Senior Data Engineer       122017      Current     Splunk    –    Dallas     GA            Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Worked as part of project teams to coordinate database development and determine project scopes and limitations  Trained nontechnical users and answered technical support questions  Collected outlined and refined requirements led design processes and oversaw project progress  Created conceptual logical and physical data models for use in different business areas  Wrote and coded logical and physical database descriptions specifying identifiers of database to management systems  Applied Conceptual Logical and Physical  DimensionalRelational model designs to ETL tasks  Managed endtoend operations of ETL data pipelines maintaining uptime of 95  Assisted in User Acceptance Testing for accountingmarketing users verifying ETL jobs complied with assigned parameters achieving desired results  Worked successfully with diverse group of coworkers to accomplish goals and address issues related to our products and services  Worked closely with team members to deliver project requirements develop solutions and meet deadlines           Senior Database Consultant       022013      122017     AmazonCom Inc    –    Lewisville     TX            Created Informatica mappingsETL’s using  Informatica Power Center  to load the data from Oracle MySQL databases to SQL Server 2016 databases hosted on AWS cloud  Performed the data manipulations using various  Informatica  Transformations like Expression Lookup Update Strategy Router and SQL transformation  Designed  Informatica  workflows with many sessions with Event with task Event raise task and Email task Scheduled the created workflowsjobs using  Informatica Scheduler   Created multiple  TSQL  objects mainly Stored procedures and Functions for new and existing software application requirements  Created multiple Linked Servers for ease of the users to execute commands on remote servers  Developed new code and finetuned existing Stored procedures to improve performance while utilizing the SQL Server Profiler and Database engine tuning wizard  Created SSIS packages to migrate data from legacy systems such as Oracle MySQL HP Vertica SQL Server flat files to centralized IT destination Created SSIS packages utilizing different SSIS transformations like  Script component Merge Join Look Up  and implemented error handling and logging  Performed unit testing and QA testing at various levels of the ETL’s and actively involved in team code reviews  Developed Report Models using report builder and distributed reports in multiple formats using SQL Server Reporting Services SSRS in Business intelligence development studio BIDS and SQL Server Data ToolsSSDT Created  Parameterized Cascaded Drilldown Crosstab and Drillthrough Reports  using SSRS 2008 R22012  Created Ad hoc Queries in TSQL Stored Procedures and Views to store the data of third parties and use them in SSRS reports to generate reports on the fly Worked with multivalued parameters for parameterized reports in SSRS  Developed VBA scripts for periodic Financial Daily Weekly reports generated directly from Excel utilizing Power Pivot options in Excel  Managed report delivery based on  Time driven and Data driven subscriptions  and  report security  for providing SSRS report  server level folder level and item level  permissions to various users across the organization based on role based access controlRBAC  Worked on resolving any performance issues with SSIS packages and SSRS reports and fine tune them for better performance  Worked on cleaning up users on SQL server and audit the access level Revoke Inappropriate access setting up AD groups and grant access based on the created groups  Worked on database  performance  and  maintenance  duties  such as  tuning   backup   restoration   Provide OnCall support for Production job failures Resolve and close the time critical Incidents in an appropriate way Perform root cause analysis create Problem report and work on any subsequent code changes to stop them from reoccurring           Database Consultant       082011      022013     AmazonCom Inc    –    Lithia Springs     GA            Involved in gathering business requirements definition and design of the data sourcing and data flows data quality analysis working in conjunction with the data warehouse architect on the development of logical data models  Using  TSQL  created complex  Stored Procedures Functions Cursors Tables and Views  and used other SQL joins and statements for applications in SQL Server 20052008  Developed standalone  SSIS  packages to extract data from different sources like  SQL Server Flat Files Excel Oracle and Sybase  transform and load the data onto required databases  Using  SSIS  transformations filtered bad data from different sources using  Derived Column Lookup Fuzzy lookup and Conditional split   Involved in creation of Technical Specs Design Documents Implementation Documents and Unit testingTest case test plan documents and maintained Issue logs  Using Script Component in  SSIS  wrote  C  NET  code to generate dynamic file names and create a text files Debugged and verified the values getting stored in  variables  in runtime using  C  code  Created dynamic  SSIS  packages through Variables and Script task components C  and VBNET and scheduled the packages using SQL Server Agent to process and load the data  Scheduled the package based on the Enterprise Calendar through SQL Server Agent Created several onetime and recurring jobs for package scheduling  Created XML file for package configurations and implemented parentchild package configuration in  SSIS  packages Implemented error and event handling precedence Constraints Break Points data grid and Logging in  SSIS  packages  Using  SSRS  developed multiple types of reports including Sub Reports Parameterized and Drill Down Reports using global variables expressions and functions based on the requirements provided  Deployed  SSRS  rdl reports on to the report server and created time driven subscriptions on the deployed reports  Created cubes with multiple fact measures groups and multiple dimension hierarchies based on the reporting needs using  SSAS   Modified existing dimensions and created new dimensions as per new business requirements  Created GUI interfaces using CNET and NET Framework to simplify the database access to end business users Enhanced existing GUI interfaces based on new requirements and improved efficiency  Resolved and closed production incident tickets generated because of failure of Daily Jobs          Education and Training       Master of Science       Electrical Engineering       Expected in   012011                California State University  Sacramento      Sacramento     CA     GPA        Status   |none|['Data Warehousing', 'SQL', 'Business Intelligence', 'Data Management', 'SQL', 'Database Analysis', 'SQL', 'Critical Thinking', 'Splunk', 'ETL', 'ETL', 'Testing', 'ETL', 'Power Center', 'Oracle', 'MySQL', 'SQL Server', 'AWS cloud', 'SQL', 'SQL Server', 'SSIS', 'Oracle', 'MySQL', 'SQL Server', 'SSIS', 'SSIS', 'unit testing', 'QA testing', 'ETL’s', 'SQL Server Reporting Services', 'SSRS', 'SQL Server', 'SSRS', 'TSQL', 'SSRS', 'SSRS', 'VBA', 'Excel', 'Excel', 'Data driven', 'SSRS', 'access', 'SSIS', 'SSRS', 'SQL server', 'business requirements', 'SQL', 'SQL Server', 'SSIS', 'SQL Server', 'Flat Files', 'Excel', 'Oracle', 'SSIS', 'SSIS', 'C', 'NET', 'C', 'SSIS', 'C', 'SQL Server', 'SQL Server', 'XML', 'SSIS', 'SSIS', 'SSRS', 'SSRS', 'SSAS', 'business requirements']
Data Engineer|https://www.livecareer.com/resume-search/r/system-services-representative-data-center-engineer-08ef2cab178d43c78255173d2136d97d|303008830723647812981083487132193910900|Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Experience      012015   to   Present     System Services Representative Data Center Engineer      Bridger Steel    –    Evansville     WY             Hired and trained by IBM to perform multiple tasks including hardware breakfix warranty repair in a multivendor environment  I am responsible for setting up coordinating and monitoring the operation of server equipment at two Delta Airline datacenters located at the airport  I run diagnostic tests to detect machine malfunctions  Independently handle high impact critical ticketsincidents  I use my connectivity skills to connect to the servers management port with SSH or Telnet  Other duties include but not limited to the following Rack stack cable configure and provision servers  Perform installation of wiring patch panel cables network switches hubs and KVMs  Deploy Cisco Layer 23 networking equipment  Installupgradereplacedeinstall servers devices and network components  Resolve issues  Execute planned changes  Use ServiceNow to follow change management requests until completion  Be accountable for ensuring a high level of client satisfaction with service delivery  Perform required site inspections  Perform audits  Problem analysis and remediation          012012   to   012014     IT Manager      City Of Atlanta    –    City     STATE             Hired by City of Atlanta to ensure reliability and availability of City of Atlanta data centers  Provided ongoing identification diagnosis and resolution of issues for users and City departments  Collaborated with internal teams and vendors at all technical levels to troubleshoot and resolve issues  Managed Windows Update Services for enterprise wide patches and security updates  Provided Exchange 2003 and 2010 administration  Upgraded maintained and installed servers and switch equipment  Provided server searches for open records act requests and litigation discovery searches  Involved in domain migration  Installed and maintained the security system for new and existing Windows servers  Proactively monitored production systems with a sense of urgency when issues arose  Worked with enterprise network loadbalancers Juniper Palo Alto and Cisco devices  Experience working with multiple server hardware platforms including IBM Dell Sun EMC and HP  Performed data center security monitoring  Successful citywide migration of Windows XP to Windows 7  Successfully implemented security software for City of Atlanta open records requests          012003   to   012012     Remote System x Server Technical Support Specialist      IBM    –    City     STATE             Hired by IBM to ensure reliability and availability of servers for IBM customers  This role participates in remote technical support of IBM hardware and software products andor systems and include the following  Provided remote troubleshooting and analysis assistance for installation or reinstallation usage and configuration questions  Provided answers for general usage and operation questions  Provided problem determination  problem source Identification  Reviewed diagnostic information to assist in isolation of a problem cause which could include assistance interpreting traces and dumps  Identify known defects and fixes to resolve problems  Identify suspected defects and engage development teams to assist in resolution  Helped with questions regarding product documentation related to the supported products  Interpreted online manuals regarding IBM code and application interfaces  Collaborated with other support centers and business units to provide seamless problem resolution  Demonstrated proficiency in the hardware and software platform supported by maintaining applicable technical certifications  Provided technical support service delivery within established guidelines demonstrating soft skills and technical skills that contribute to client satisfaction  Demonstrated excellent oral and written communication skills  Acquired industry certification and skills training  Exceeded customer satisfaction and case resolution metrics  Supported product lines including eSeries xSeries Intellistation Blade Center AIX iDataPlex fiber switches QLOGIC Emulex Cisco Brocade Quantum tape backup libraries  Provided additional support for FastT DASD fiber channel cabling ServeRAID Manager and management processors          Education      Expected in   2011     Associates of Applied Science Degree     Cyber Security     Chattahoochee Technical College      Marietta     GA     GPA       Cyber Security        Expected in        Security Certified Professional Network Certified Professional Server Certified Professional A Certified Professional Cisco Certified Technician                           GPA               Expected in        Blade eSeries xSeries AIX NAS SAN Certified          IBM University                GPA               Summary     Committed to ongoing professional development with CompTIA A Network Server and Security certifications  Also extensive academic training in network administration tracking intrusion detection firewall configuration OS administration cloud computing High level of technical proficiency with network utilities Master level of providing upperlevel support to management Master level of providing remote troubleshooting support to ensure continual operations of critical customer network systems Master experience in fastpaced high volume call environment with 12 years experience supporting IBM xSeries Blade Center Lenovo EMC AS400 DASD fiber NAS SAN RAID network appliances and tape library products Exceptional root cause analysis skills        Highlights         MS Server 2008 MS Server 2012 MS Windows 7 MS Windows 10 VMware Redhat Enterprise UNIX Networking  TCPIP SSL TLS SSH Telnet FTP HTTPS DHCP DNS WPA2 Ping Tracert TACACS Kerberos RADIUS RAS NAS IDSIPS Firewalls ToolsApplications  Solarwinds WireShark Snort Tera Term PuTTY MS Windows Security Templates MS Windows Security Update Service Microsoft Exchange 20032010 MS Windows Active Directory LDAP Terminal Services MS Windows Access Control MegaRAID Microsoft Office Lotus Notes Visual Basic 2010                       Skills     A Certified Active Directory AIX tape backup cables cable cabling change management Cisco Cisco Certified excellent oral hardware client customer satisfaction DASD delivery Dell DHCP diagnosis documentation DNS Firewalls FTP HP hubs IBM IBM hardware IDS LDAP litigation Lotus Notes Access Exchange Microsoft Exchange 2003 Microsoft Office Windows 7 Windows MS Windows 7 MS Windows Windows XP migration Enterprise NAS Network Networking Problem analysis problem resolution processors RAS Redhat SAN SSH servers SSL Sun switches switch TCPIP technical support Technician Telnet troubleshoot troubleshooting UNIX upgrade Visual Basic wiring written communication skills|none|['Windows', 'Exchange', 'Windows', 'Windows XP', 'Windows 7', 'cloud computing', 'Windows 7', 'VMware', 'UNIX', 'Networking', 'DHCP', 'DNS', 'WireShark', 'PuTTY', 'Exchange', 'Windows', 'Active Directory', 'Microsoft Office', 'Visual Basic', 'Active Directory', 'DHCP', 'DNS', 'Access', 'Exchange', 'Exchange', 'Microsoft Office', 'Windows 7', 'Windows 7', 'Windows XP', 'UNIX', 'Visual Basic']
Data Engineer|https://www.livecareer.com/resume-search/r/senior-data-engineer-03773a6bfb094914960c12c4167839e5|55652676729314852703085888775946776785|Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    Home   555 4321000    Cell       resumesampleexamplecom              Summary       Strong IT experience in AWS cloud computing Bigdata  Data warehousing  Strong Business Domain Knowledge in Sales Segmentation  Territory Planning  Strong Coding  SQL experience in Python and Redshift  Having an experience in Agile and deliver the results based on stories and task assigned on the sprint  Implementation Knowledge in Serverless Architecture using AWS Cloud Computing  Having Good Hands on experience in GIT as well as CICD pipeline Integration  Having Good Hands on Experience in building Data Model based on requirements         Skills           Programming Languages Java Python Unix Unix Shell Scripting  Cloud Technologies Amazon Web Services AWS  Version Control AWS CodeCommit GIT GitHub Repositories      Continuous IntegrationContinuous DeliveryCICD AWS CodeDeploy AWS CodePipeline  Databases and SQL Redshift Oracle Netezza NoSQL DDB  Elasticsearch  Big Data Technologies Hadoop Hive Ozzie AirFlow Spark PySpark Scoop Flume                       Experience       Senior Data Engineer       012018   to   Current     Splunk    –    Bloomington     MN             Collaborated with product owners to gather requirement to help build a solution for Sales Segmentation  Identified key use cases and associated reference architectures for market segments and industry verticals  Worked as part of project teams to coordinate database and pipeline development and determine project scopes and limitations  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts  Developed and managed enterprisewide big data environments  Specified user access levels for each database segment to protect database integrity and company information  Developed and implemented security initiatives to protect important company data  Established hardware requirements and devised storage capacity solutions and choose wisely on the AWS services by outweigh the requirement and business usecase           Big Data Engineer       122015   to   122017     Nike Inc    –    Gurnee     IL             Perform as a Big Data Developer and work in various phases like Data Ingestion Data Integration and Transformation  Create a Sqoop import command and pull the data both tables and all tables in DBs from MySQL DB to HDFS as part of Data Ingestion  Creates a MapReduce Programs as well as Spark programs to parse analyze and implement the solutions based on its customer need  Perform operations to filter the records from DB boundary query incremental update or insert in the Sqoop import commands  Perform operations to insert the data directly to Hive tables Change the delimiters and end line character and Change the formats like textFile AvroDataFile ORC file into HDFS systems using Sqoop Import commands  Perform Sqoop Eval function to evaluate the data in MYSQL  Perform Sqoop Export function to export the data from HDFS and loaded it into MYSQL and understand the delimiter of the file define the no of mapper to perform etc  Create a Flume config file to ingest the data from various source systems like Spool Directory NetCat Exec Sequence etc and loaded it into Avro HDFS etc  Define a proper channelsMemory File etc and loaded all the data comes from Source to Sinks  Used the interceptor to modify the data comes from Source like Filtering Regex Include the Timestamp Search and Replace etc  Create a MapReduce Program using Eclipse IDE and execute it through jar file  Create a Mapper Class and using Map method generate the key value pair as nodewithdate system utilization by writing in Context application  Create a Reducer Class and using reduce method generate the final output of system utilization per nodewithdate  Implements custom based writable class to get the two values in the mapOutputValue class  Implements custom based partitioner class to assign a appropriate data to the right reducerBasically try to achieve multilevel group by function with MR limitations  Implements Pattern Matching Logic for semistructured Data and routed the good and bad records separately  Create a custom based input format as well as record reader to parse the XML file  Handson in creating the Broadcast Variables  Handson in SparkSQL DataFrame creation DF via class object in Scala Convert DF to RDD TempTable Schema creation and So on           ETL Developer       092009   to   042015     Allegis Group    –    Jacksonville     FL             Created and implemented complex business intelligence solutions  Created conceptual logical and physical data models for use in different business areas  Developed and managed enterprisewide data analytics environments  Identified protected and leveraged existing data  Monitored multiple databases to keep track of all company inventory  Assisted maintenance team with completion of preventive maintenance and unscheduled service needs  Monitored installations to ensure compliance with local codes and industry best practices          Education and Training       Master of Science     Information Technology     Expected in        Amity University      Noida India          GPA                Bachelor of Engineering          Expected in   052008     College of Engineering Guindy Anna University      Chennai India          GPA|none|['AWS cloud', 'Data warehousing', 'SQL', 'Python', 'Redshift', 'Agile', 'AWS Cloud', 'GIT', 'Programming', 'Java', 'Python', 'Unix', 'Unix', 'Shell', 'Amazon Web Services', 'AWS', 'Version Control', 'AWS', 'CodeCommit', 'GIT', 'GitHub', 'AWS', 'CodeDeploy', 'AWS', 'SQL', 'Redshift', 'Oracle', 'Netezza', 'NoSQL', 'DDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'Ozzie', 'AirFlow', 'Spark', 'PySpark', 'Splunk', 'use cases', 'big data', 'access', 'AWS', 'Big Data', 'Big Data', 'MySQL', 'HDFS', 'Spark', 'Hive', 'MYSQL', 'MYSQL', 'Eclipse', 'XML', 'Scala', 'ETL']
Data Engineer|https://www.livecareer.com/resume-search/r/aws-data-engineer-f1007ddd5382d9514a28eef265cea9d0|311813237829986595668673300748081977825|Jessica  Claire                             resumesampleexamplecom                      555 4321000                                         100 Montgomery St 10th Floor                                                                                                                                                                                                           Summary       Dynamic and motivated IT professional with around 15 years of experience as principal software developer having expertise in designing data intensive applications using Spark Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API and worked explicitly on Scala   Handled ingestion of data from different data sources into HDFS using Sqoop and perform transformations using Hive Map Reduce and then loading data into HDFS  Experience of Partitions bucketing concepts in Scala and worked on fine tuning the sparkscala and sql queries to optimize performance   Experience with different file formats like Avro  parquet  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow  Experienced with using most common Operators in Airflow  Python Operator Bash Operator  Handson experience in handling database issues and connections with SQL databases such as Netezza Oracle Redshift and PostgreSQL   Experience in designing and creating RDBMS Tables Views User Created Data Types Stored Procedures  Expert in designing ETL data flows using creating mappingsworkflows to extract data from Netezza  Oracle Servers  Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on code versioning automation tools like GIT  Experienced with JSON based RESTful web services  Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases             Skills        ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                    Education and Training       Purdue University    West Lafayette     IN      Expected in   022022     –      –       Post Graduate         Data Engineering           GPA                    University of Texas At Austin    Austin     TX      Expected in   092021     –      –       Post Graduate         Data Science And Business Analytics           GPA                    Califonia State University     Fullerton CA           Expected in   122009     –      –       Bachelor of Arts        Business Administration And Management          GPA                     Experience       Deloitte      AWS Data Engineer   Rosslyn     CA                   012022      022022     Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon RDS  Used Spark SQL for Scala Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like S3  HDFS into Spark RDD and perform computations using sparkscalasparksql to generate the output response  Creating an automated alert notifications to identify and notify the idle EMR and EC2 clusters in our application regions to reduce the cost for EC2 and EMR resources  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Developed Grafana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Environment AWS EMR S3 RDS Redshift Lambda Apache Spark HIVE SQOOP Map Reduce Python  Assessed organization technology infrastructure and managed cloud migration process  Implemented cloud policies managed technology requests and maintained service availability           Splunk      Data Engineer   Memphis     TN                   012016      112019     Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           General Dynamics      Big Data Engineer  Hadoop Developer   Bossier City     MA                   102013      122015   AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Fiserv      Python Developer    City     STATE                   092012      102013     AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes|none|['Spark', 'Big Data', 'Data Visualization', 'Spark', 'Spark', 'optimization', 'algorithms', 'Hadoop', 'Spark', 'Scala', 'Hive', 'Scala', 'sql', 'Json', 'XML', 'Airflow', 'Airflow', 'Python', 'Bash', 'SQL', 'Oracle', 'Redshift', 'PostgreSQL', 'RDBMS', 'ETL', 'Oracle', 'XML', 'AWS', 'automation', 'GIT', 'JSON', 'Big Data', 'Hadoop', 'MapReduce', 'HDFS', 'Sqoop', 'PIG', 'Hive', 'HBase', 'Oozie', 'Flume', 'NiFi', 'Kafka', 'Zookeeper', 'Yarn', 'Apache', 'Spark', 'Oracle', 'MySQL', 'SQL Server', 'MongoDB', 'Cassandra', 'DynamoDB', 'PostgreSQL', 'Teradata', 'Programming', 'Python', 'PySpark', 'Scala', 'Java', 'C', 'C', 'Shell', 'Perl', 'SQL', 'AWS', 'Azure', 'Django', 'REST', 'PyCharm', 'Eclipse', 'Visual Studio', 'SQL Developer', 'SQL', 'SQL Server', 'SQL', 'Eclipse', 'Postman', 'SVN', 'Git', 'Windows', 'Ubuntu Linux', 'Network Security', 'Kerberos', 'Modeling', 'Modeling', 'Star Schema', 'Modeling', 'Snowflake', 'Modeling', 'Apache', 'Airflow', 'Tableau', 'ggplot2', 'matplotlib', 'SSRS', 'Power BI', 'Machine Learning', 'NLP', 'Clustering', 'AWS', 'access', 'AWS', 'Redshift', 'AWS', 'AWS', 'S3', 'Spark', 'SQL', 'Scala', 'Python', 'Spark', 'ETL', 'RDBMS', 'Spark', 'Hive', 'Elasticsearch', 'AWS', 'Redshift', 'Apache', 'Spark', 'HIVE', 'Python', 'Splunk', 'Azure', 'SQL', 'Cassandra', 'Azure', 'SQL', 'Azure', 'Azure', 'Kubernetes', 'Spark', 'Cassandra', 'Azure', 'SQL', 'Azure', 'Spark', 'Azure', 'Spark', 'SQL', 'SQL', 'Mongo', 'SQL Server Reporting Services', 'SSRS', 'Power BI', 'Databricks', 'Spark', 'Databricks', 'ADF', 'Azure', 'Azure', 'SQL', 'use cases', 'Azure', 'ADF', 'Azure', 'Azure', 'Spark', 'Azure', 'SQL', 'Azure', 'ADF', 'SSRS', 'Power BI', 'Azure', 'Azure', 'Apache', 'Spark', 'use cases', 'data analysis', 'data analysis', 'Big Data', 'Hadoop', 'Hadoop', 'Spark', 'Kafka', 'HIVE', 'ETL', 'HIVE', 'Spark', 'Spark', 'Spark', 'SQL', 'RDBMS', 'Java', 'Hive', 'XML', 'JSON', 'Hive', 'Hive', 'Business Requirements', 'ETL', 'Python', 'SQL', 'Snowflake', 'SQL Server Reporting Services', 'SSRS', 'Spark', 'Cassandra', 'Cassandra', 'Apache', 'Hadoop', 'Scala', 'Kafka', 'Hive', 'HBase', 'AWS', 'PySpark', 'Spark', 'Scala', 'Hive', 'JSON', 'AWS', 'Cassandra', 'PySpark', 'Apache', 'Spark', 'HBase', 'Apache', 'Kafka', 'HIVE', 'Apache', 'ETL', 'Snowflake', 'Apache', 'Python', 'Java', 'SSRS', 'Hadoop', 'Hadoop', 'data analysis', 'Python', 'AWS', 'Ansible', 'MySQL', 'Python', 'Git', 'Jenkins', 'DynamoDB', 'Docker', 'Kubernetes', 'design patterns', 'Python']
Data Engineer|https://www.livecareer.com/resume-search/r/sr-data-analyst-support-engineer-0690548375b14b17a0ecaf5aa825cc34|71493749037436587054693787398602463147|JC     Jessica    Claire                                        100 Montgomery St 10th Floor           555 4321000                 resumesampleexamplecom                         Summary       7 years of experience as a BI developer with a proven track record in Business Intelligence BI Data Warehouse DWH and Data Analytics related consulting projects  Proven ability to identify business needs and develop valuable solutions to drive accuracy and process efficiency  Experienced in developing implementing documenting monitoring and maintaining the data warehouse extracts transformations and ETL process in various industries like Financial Health Care and Retail Industry with  Sales Marketing Inventory Management Supply Chain and Finance  domains    Delivered BI reporting solutions in Power BI Reporting Services SSRS  Expertise in  Data Warehousing Architectures  including  ETL  design  Staging   Transformations   Deltachange Data  capture  StarSchemas  Cubes and  History  loading  Skilled in writing  TSQL Queries  Dynamicqueries subqueries and joins for generating  Stored Procedures Triggers Userdefined Functions  Views and Cursors  Experience with all phases of software development life cycle SDLC and Agile methodologies  Have delivered as a team member team lead and as an independent consultant on medium to large scale projects         Skills           DataBusinessSystems  Tools MS SQL Server IntegrationSSIS Analysis ServicesSSAS Metl  Databases SQL Server 20122016Netezza 72 Oracle 11g Azure  Development Skills TSQL DAX MDX C      BI Reporting Tools Power BI Reporting Services Services SSRS  Data Quality and Standards  Agile Methodology  Programming Languages   TSQL C VB Scripting                       Education        Expected in   052013   Bachelor of Science       Information Technology    GMR Institute of Technology     Rajam India           GPA               Experience        052020   to   Current   Sr Data AnalystSupport Engineer    Honeywell         Gladstone     MO            Client  Citi Bank Irving Tx   Working as Level2 Support Engineer for Data Pipelines the Retail Customer Applications and Azure Data Staging  Analyzed large amounts of data to identify trends and find patterns signals and hidden stories within data  Developing and supporting DevOps Repos and code pipelines for all ETLs Tasks and Jobs  Developed  Standard  ETL  integrations  to extract CommunityLoanLocation data from Mainframes system and implemented Business Rules to confirm the data  Experience in creating and managing SSAS tabular models creating dimension and fact tables  Responsible for data administration tasks  maintenance plans   performance tuning   backup and security  for MS SQL Server systems  Primarily involved in data migration using SQL SQL Azure Azure storage and azure data factory SSIS Powershell  Integrated Custom Visuals based on business requirements using Power BI desktop  Developed complex SQL queries using stored procedures common table expressions CTEs temporary table to support Power BI and SSRS reports  Developed complex calculated measures using Data Analysis Expression language DAX  Embedded Power BI reports on the Salesforce portal page and also managed access of reports and data for individual users using Roles  Provided continued maintenance and development of bug fixes for the existing and new Power BI Reports            082018   to   052020   Sr BI Data Developer    Butler Technical Group         Opa Locka     FL            Led the development of Data Mart for supply chainmarketing departments along with analysis of existing data warehouse for performance improvements  Worked on all phases of data warehouse development life cycle from gathering requirements implementation testing training and support  Interviewed Business Users of the Data Warehouse and the Business Analysts to understand and troubleshoot the existing bottlenecks and problem areas  Developed applications and designed processes for transformation and data management from companywide databases  Developed automated data dictionary for Enterprise Data Warehouse using SSRS and SQL  Completed a thorough analysis of the issues with the cube processing and query processing to identify bottleneck for the existing facts and dimensions  Designed and developed ETL packages to facilitate incremental load process  Created aggregates partitions attribute relationships and user defined hierarchies with the goal to reduce the cube processing time and query processing time  Created stored procedures and performed index analysis for slow performing SSRS reports to bring down the report rendering time  Developed Enterprise wide cascading reports in SSRS that were used throughout the agency to monitor the performance of the cube logging of the nightly ETL loads and also to monitor most active users of the cube  Created robust and highperforming ETL mappings using  SSIS  and  Metl   Migrated existing selfservice reports and adhoc reports to Power BI  Developed custom calculated measures using DAX in Power BI to satisfy business needs  Assisted users in accessing databases troubleshooting malfunctions and removing systembased barriers to task completion            112016   to   072018   Sr ETL Developer    Kairos Technologies Inc         City     STATE            Client  Cvs Caremark Irving Tx      Worked on ambitious project to develop an Enterprise Wide Drug plan Record system data warehouse connecting Insurance companies and Drug Manufacturers for CVS Caremark  Worked with Business Intelligence Manager to plan BI strategies create BI road map and budget plans to deliver on organizations Business Intelligence needs  Assessed the impact of current business processes on users and stakeholders  Developed multiple dashboards analytical reports KPIs and Interactive reports based on business requirements using SSRS to support executive business needs  Transferred data from various data sourcesbusiness systems including MS Excel MS Access Flat filesCOSMOS etc to SQL Server using SSIS  Implemented Query Optimization and performance tuning for slow performing SQL queries  Involved in writing complex TSQL queries and stored procedures to perform data transformations and to support SSRS Reports  Involved in creating User Security and Roles in reporting services at both parent level and report level  Supported maintenance of Data Quality Services DQS knowledge base and development of Master Data Services database            072013   to   042016   Business Data Analyst    Tata Consultancy Services         City     STATE            Client  Sun Trust Bank US   Designed Implemented and aided EndtoEnd data migration project using Microsoft SSIS  Assisted in creation of internal applications for marketing and customer relations departments including financial reporting tools and reports  Designed SSIS Packages to extract data from various data sources such as Excel spreadsheet and flat files and load data into destination databases for further Data Analysis and Reporting  Performed rigorous business case analyses and proposed various process improvements  Used Excel and Access applications for visualizing the data functionality  Led an initiative to convert all excel files which were used in inhouse timeexpense reporting wherein I automated the process to find discrepancies in timesheets by creating tables views using stored procedures    Client   Marks and SpencerUK   Improved business direction by prioritizing customers and implementing changes based on collected feedback  Analyzed open orders backlog and sales data to provide sales team with insights  Created tables and define their relations by using foreign key and primary key relationship  Involved in creating database objects using stored procedures Triggers Functions Views and TSQL Joins using TSQL  Performed normalization of database to reduce redundancy and achieve maximum system performance  Wrote custom TSQL stored procedures and triggers to improve performance preserve referential integrity and provide additional application functionality  Worked as ETL Developer for an EndtoEnd implementation of Operation Data Warehousing Project  Responsible for data integration and transformation using SSIS  Formulated and documented the physical ETL process design based on business requirements and system specifications with strong ETL design skills  Created source to target mappings transformations lookups aggregations expressions  Involved in scheduling reports creating snapshot reports and subscription for the reports Using SSRS|none|['ETL', 'Power BI', 'SSRS', 'Data Warehousing', 'ETL', 'software development life cycle', 'Agile', 'MS SQL Server', 'SQL Server', 'Oracle', 'Azure', 'DAX', 'C', 'Power BI', 'SSRS', 'Agile', 'Programming', 'C', 'VB', 'Azure', 'ETL', 'SSAS', 'MS SQL Server', 'SQL', 'SQL', 'Azure', 'Azure', 'azure', 'SSIS', 'Powershell', 'business requirements', 'Power BI', 'SQL', 'Power BI', 'SSRS', 'Data Analysis', 'DAX', 'Power BI', 'Salesforce', 'access', 'Power BI', 'testing', 'SSRS', 'SQL', 'ETL', 'SSRS', 'SSRS', 'ETL', 'ETL', 'SSIS', 'Power BI', 'DAX', 'Power BI', 'ETL', 'business requirements', 'SSRS', 'MS Excel', 'MS Access', 'SQL Server', 'SSIS', 'Optimization', 'SQL', 'SSRS', 'SSIS', 'SSIS', 'Excel', 'Data Analysis', 'Excel', 'Access', 'excel', 'ETL', 'Data Warehousing', 'SSIS', 'ETL', 'business requirements', 'ETL', 'SSRS']
Data Engineer|https://www.livecareer.com/resume-search/r/senior-data-engineer-ffd843d6febb4217800488911db2a3f8|137893745859917600174161396390961745079|Jessica    Claire               Montgomery Street       San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK      Home   555 4321000        Cell           resumesampleexamplecom                  Professional Summary      Business Intelligence Consultant with a 10year career in data warehousing business intelligence reporting and data management architecture Progressive developer and technical team lead with a strength in design  development as well as driving performance reducing inefficiencies and cutting costs Possess comprehensive knowledge and hands on experience in both ETL and Reporting tools Knowledge of Credit Card account life cycle management in Finance domain Functional knowledge of Collections and Recovery operations Expertise in planning executing and spearheading various SDLC and Agile Scrum projects in compliance to quality standards Known for effective communication with excellent relationship building  interpersonal skills strong analytical problem solving  organizational abilities Proven track record of taking ownership and diving deeper into issues to identify the root cause and troubleshoot towards resolution        Skills           ETL Tools  Ab Initio  Scripting Languages  Unix shell scripting Python SparkScala  Cloud Services Microsoft Azure Cloud Services Data Lake  DataBricks and Data Pipelines Azure Data Factory  BI Reporting Skills  Tableau Desktop Tableau Server Power BI Reporting  SAP Business Objects SAP BOUniverse Designer Tool SAP BO XI WEBI ReportingSAP BO XI Deski Reporting  Database  Oracle  Teradata Hive      Scheduler tools  Autosys ControlM  Relational Database Management  Business Intelligence Reporting  Project Management  Data Analysis and Visualization                       Work History      062017   to   Current     Senior Data Engineer      Thoughtworks    –    Memphis     TN            A multitenure loyalty program across all Williams Sonoma brands – the first time that customers can earn points by purchasing across the multiple brands WSI Pottery Barn West Elm etc I work in the customer data warehouse CDW which is Teradata and work with their 3rd party marketing systems vendor There are various data points and integrations with the data warehouse which holds all customer information points transactions loyalty IDs etc   Worked with Requirements Analysts and PDMs to identify understand and document business needs for data flow Analyze requirementsUser stories at business meetings and strategize impact of requirements on different platformsapplications Worked with Project Management in creation of project estimates for each Agile story  Engaged in projects that uses Scala and Azure DatabricksDatafactoryDatalake to extract process and load Adobe Clickstream data received for Williams Sonoma ECOM website Deployed custom codes in Scala to read JSON extract CSV files from Adobe add column headers handle badmalformed records handle invalid records before finally writing cleansed data into ParquetDelta file format partitioned by date and hour  Build and review design deliverables Perform dependency analysis between new objects created in Ab Initio graphs Conduct IT and UAT testing of each of these graphs Record errors reported during Unit and Integrated testing  Prepare production Implementation Plan to deploy codes successfully to production environment  Coordinate daily standups short meetings where teammates talk through whats being worked on yesterday what was done today and if blocked Involved in variety of other scrum meetings such as planning meetings where team pulls in stories requirements grooming meeting to look at backloglist of stories and flush out timeline and work expected demo show work that was completed in sprint etc  Reporting  Visualization in Tableau and Power BI  Design and deploy rich Graphic visualizations with Drill Down and Drop down menu option Prepare Dashboards using calculations parameters Apply various reporting objects like Filters prompts Calculated fields Groups Parameters Work on the development of Dashboard reports for the Key Performance Indicators for the top management          032010   to   062017     Senior DeveloperTechnical Team Lead      Tech Mahindra Synchrony Financial Services    –    City     STATE            Synchrony Financial is a consumer financial services company offering consumer financing products including credit promotional financing and loyalty programs and installment lending through Synchrony Bank its wholly owned subsidiary Synchrony is the largest provider of private label credit cards in the US The company provides private label credit cards for such brands as Amazon JC Penney CheapOAir OneTravel Sams Walmart Lowe’s Guitar Center Gap BP We as Data warehouse solution providers store this credit card holder information from the instance a credit card is applied till the account is closedcharged off   Understanding the full scope of requirements from business Estimating time and effort involved from gathering project requirements till project implementation  Answering technical queries driving product initiatives and metric collection and analysis  CoOrdinating between project stake holders – Business Client and offshore teams during all phases of project  Preparing design documentation design reviews development performing code reviews to ensure coding standards are followed testing and deployment of application enhancements  Experience in Ab Initio toolsets including the following  o Parallel and serial flow batch graphs conditional components other components like reformat normalize and join lookup and graphs processing ASCII and EBCDIC layouts  o Knowledge on Abinitio architecture including the GDE Cooperating system EME and other related items  o Fine tuning of Abinitio graphs based on performance enhancement by proper usage of memory in the components  o Well versed with various Ab Initio components such as Join Rollup Partition Departition Dedup sorted Scan Normalize DeNormalize  o Expertise knowledge improving Performance and Troubleshooting of the AbInitio graphs and monitoring ABInitio run time statistics  o Experience with advanced Abinitio metaprogramming air commands and other admin related tasks including the creation of save and performing migration from one server to the other  Establish support such as acquiring conditioned test data support from third partyteam etc for Integration Testing to simulate production environment and to ensure correctness and quality of buildparameter set up  Getting sign off from IT managers for deploying parameterset up onto production environment  Documenting implementation plan and Hour by Hour plan listing down the tasks in sequence of their execution on the date of implementation  Coordinate release activities across multiple teams          Education      Expected in   4 2008     Bachelors     Information Technology     College of Engineering Bhubaneswar      India          GPA               Accomplishments      • Tableau Desktop Specialist Certified No expiration  These are Open Badges that I have been awarded and that attest to my skills  httpswwwyouracclaimcombadges556948fdd8114f8097d245ab6c530d9clinkedinprofile   o Tableau Desktop Specialist title use their foundational knowledge of Tableau Desktop and data analytics to solve problemsDesktop Specialists can connect to prepare explore and analyze data and share their insights        Skills       ETL Tools  Ab Initio  Scripting Languages  Unix shell scripting Python SparkScala  Cloud Services Microsoft Azure Cloud Services Data Lake  DataBricks and Data Pipelines Azure Data Factory  BI Reporting Skills  Tableau Desktop Tableau Server Power BI Reporting  SAP Business Objects SAP BOUniverse Designer Tool SAP BO XI WEBI ReportingSAP BO XI Deski Reporting  Database  Oracle  Teradata Hive    Scheduler tools  Autosys ControlM  Relational Database Management  Business Intelligence Reporting  Project Management  Data Analysis and Visualization         Work History      062017   to   Current     Senior Data Engineer       Kforce Inc Williams Sonoma Inc   –   San Francisco     CA     A multitenure loyalty program across all Williams Sonoma brands – the first time that customers can earn points by purchasing across the multiple brands WSI Pottery Barn West Elm etc I work in the customer data warehouse CDW which is Teradata and work with their 3rd party marketing systems vendor There are various data points and integrations with the data warehouse which holds all customer information points transactions loyalty IDs etc   Worked with Requirements Analysts and PDMs to identify understand and document business needs for data flow Analyze requirementsUser stories at business meetings and strategize impact of requirements on different platformsapplications Worked with Project Management in creation of project estimates for each Agile story  Engaged in projects that uses Scala and Azure DatabricksDatafactoryDatalake to extract process and load Adobe Clickstream data received for Williams Sonoma ECOM website Deployed custom codes in Scala to read JSON extract CSV files from Adobe add column headers handle badmalformed records handle invalid records before finally writing cleansed data into ParquetDelta file format partitioned by date and hour  Build and review design deliverables Perform dependency analysis between new objects created in Ab Initio graphs Conduct IT and UAT testing of each of these graphs Record errors reported during Unit and Integrated testing  Prepare production Implementation Plan to deploy codes successfully to production environment  Coordinate daily standups short meetings where teammates talk through whats being worked on yesterday what was done today and if blocked Involved in variety of other scrum meetings such as planning meetings where team pulls in stories requirements grooming meeting to look at backloglist of stories and flush out timeline and work expected demo show work that was completed in sprint etc  Reporting  Visualization in Tableau and Power BI  Design and deploy rich Graphic visualizations with Drill Down and Drop down menu option Prepare Dashboards using calculations parameters Apply various reporting objects like Filters prompts Calculated fields Groups Parameters Work on the development of Dashboard reports for the Key Performance Indicators for the top management          032010   to   062017     Senior DeveloperTechnical Team Lead       Tech Mahindra Synchrony Financial Services   –   Chicago     IL     Synchrony Financial is a consumer financial services company offering consumer financing products including credit promotional financing and loyalty programs and installment lending through Synchrony Bank its wholly owned subsidiary Synchrony is the largest provider of private label credit cards in the US The company provides private label credit cards for such brands as Amazon JC Penney CheapOAir OneTravel Sams Walmart Lowe’s Guitar Center Gap BP We as Data warehouse solution providers store this credit card holder information from the instance a credit card is applied till the account is closedcharged off   Understanding the full scope of requirements from business Estimating time and effort involved from gathering project requirements till project implementation  Answering technical queries driving product initiatives and metric collection and analysis  CoOrdinating between project stake holders – Business Client and offshore teams during all phases of project  Preparing design documentation design reviews development performing code reviews to ensure coding standards are followed testing and deployment of application enhancements  Experience in Ab Initio toolsets including the following  o Parallel and serial flow batch graphs conditional components other components like reformat normalize and join lookup and graphs processing ASCII and EBCDIC layouts  o Knowledge on Abinitio architecture including the GDE Cooperating system EME and other related items  o Fine tuning of Abinitio graphs based on performance enhancement by proper usage of memory in the components  o Well versed with various Ab Initio components such as Join Rollup Partition Departition Dedup sorted Scan Normalize DeNormalize  o Expertise knowledge improving Performance and Troubleshooting of the AbInitio graphs and monitoring ABInitio run time statistics  o Experience with advanced Abinitio metaprogramming air commands and other admin related tasks including the creation of save and performing migration from one server to the other  Establish support such as acquiring conditioned test data support from third partyteam etc for Integration Testing to simulate production environment and to ensure correctness and quality of buildparameter set up  Getting sign off from IT managers for deploying parameterset up onto production environment  Documenting implementation plan and Hour by Hour plan listing down the tasks in sequence of their execution on the date of implementation  Coordinate release activities across multiple teams|none|['data warehousing', 'ETL', 'Agile', 'Scrum', 'problem solving', 'ETL', 'Unix', 'shell', 'Python', 'Azure', 'DataBricks', 'Azure', 'Data Factory', 'BI', 'Reporting Skills', 'Tableau', 'Tableau', 'Power BI', 'SAP', 'Business Objects', 'SAP', 'SAP', 'BO XI', 'WEBI', 'BO XI', 'Oracle', 'Teradata', 'Hive', 'Project Management', 'Data Analysis', 'Teradata', 'Project Management', 'Agile', 'Scala', 'Azure', 'Scala', 'JSON', 'UAT', 'testing', 'scrum', 'Tableau', 'Power BI', 'Integration Testing', 'Tableau', 'Tableau', 'Tableau', 'ETL', 'Unix', 'shell', 'Python', 'SparkScala', 'Azure', 'Data Lake', 'DataBricks', 'Azure', 'Tableau', 'Tableau', 'Power BI', 'SAP', 'Business Objects', 'SAP', 'SAP', 'BO XI', 'WEBI', 'Oracle', 'Teradata', 'Hive', 'Project Management', 'Data Analysis', 'Teradata', 'Project Management', 'Agile', 'Scala', 'Azure', 'Scala', 'JSON', 'UAT', 'scrum', 'Tableau', 'Power BI']
Data Engineer|https://www.livecareer.com/resume-search/r/data-support-services-engineer-fd6014dbd6d04903b67269cf3fc6cd59|12880653418622435105799992611550629230|Jessica    Claire                   Montgomery Street     San Francisco     CA  94105    609 Johnson Ave       49204     Tulsa     OK       H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary     Desktop and Server Support Application Server Technician Offer my Windows Server 20122008 R22003 Windows XP78 Mac OS X Linux Application Server Microsoft Office 2010 and Office Mac 2011 Active Directory and Network LANWAN switches and routers experience Desktop and server hardware and software technical support experience Strong analytical troubleshooting communication and customer service skills  Skilled Windows System Administrator offering 10 years of experience building and maintaining multiplatform technology services with a solid understanding of current Windows Mac and UNIX application systems       Highlights           Systems	Windows XP78 Desktop	Max OS X 108 109 1010	iO78 mobile and tablet  Windows 20032008 Server	Android mobile		Red Hat Enterprise Linux  Unix web applications	LANWAN  Mobile Phone	iPhoneiPad Apple	Blackberry		Android  Applications	Microsoft Office 20072010	Office Mac 2010		Final Cut 7 and 10  Microsoft Office 365	Pages OS X		Numbers OS X  Adobe Creative Suite cloud	Adobe Acrobat XXI	Adobe Lighthouse 45  Parallels	WebEx  Utilities	Symantec Antivirus	Voltage Email Encryption	Barracuda Backup Cloud  PGP DesktopServer	Symantec Ghost  Server Applications	Active Directory	PowerShell Scripting	WINS	DFS  DHCP	FilePrint Services	LDAP	Group Policy GP  Exchange 2010	NTFS security		HyperV	DNS                         Accomplishments       Requirements Analysis    Completed business requirements analysis including the evaluation of systems specifications for the Soundview Throgs Neck Community Health Center     Strategy and Planning    Developed and communicated Electronic Health Record security policies and standards to all users  Established policies and procedures for medical record documentation     IT Training    Successfully trained all employees to use Electronic Health Record and Billing systems     Network Support    Acted as first point of contact for all major technical issues including power outages system failures and disaster recovery  Oversaw infrastructure of three offices and acted as support for helpdesk technicians of Yeshiva University          Experience       Data Support Services Engineer       072013      092013     Montefiore Medical Center formally SVTNCMHC Health Care Services    –    City     STATE            Managed the daytoday IT operations for the Montefiore Medical Center  Assisted in the migration of technology services from Yeshiva Universitys servers to Montefiores servers including computers user logins data files printer settings software applications and email archives  Provided QA testing in the migration of the centers Electronic Health Record Mindlinc and Billing IMA system  Trained all staff in the use of Montefiores technical services including clerical registration billing electronic health record and emailprintingdata file usage           Technical System Support Engineer       072003      072013     Yeshiva University Health Care Services Soundview Throgs Neck Community Health Center SVTNCMHC    –    City     STATE            Desktops Mobile Phones Printers and Application Servers Provided all levels of enduser desktop server mobile phonetablet and printer technical support for 2 healthcare center locations in the Bronx  Backend technical support for all server and network services  Responsible for managing 15 application servers including 4 domain controllers  Deployed and maintained a Windows 2008R2 and 2003 Active Directory environment  Managed all aspects of server and application security using Active Directory LDAP and Linux  Planned and implemented the physical deployment and migration of Windows 7 desktops from Windows XP  Consulted daily with the executive clinical and administrative staff concerning the overall quality and possible improvement of technology systems for the medical center  Trained all staff in the use of SVTNYeshiva Universitys technical services including clerical registration billing electronic health record and emailprintingdata file usage  Provided QA support and testing of our internal and external billing system IMA  Work closely with outside vendors to design and maintain the EHR Billing and Backup applications  Implemented and maintained the centers data backup system using Barracuda Cloud Backup  Identified designed and implemented the requirements for the centers disaster recovery system  Responsible for managing and maintaining the centers audiovideo conference room system          Education       Graduate Certificate       Digital Media and Project Management       Expected in                   The New School      New York     NY     GPA        Status                  BBA       Computer Information Systems       Expected in                   Baruch College City University of New York      New York     NY     GPA        Status                  Skills      Active Directory administrative Adobe Adobe Acrobat Antivirus Apple audio Backup Billing billing system clerical Encryption Desktops DHCP disaster recovery DNS Email Ghost LAN LDAP Linux Mac managing Max Exchange Microsoft Office Office Windows 7 Windows Windows XP migration Enterprise network OS printer Printers quality QA Red Hat Servers Scripting Symantec technical support Phones Phone Unix Utilities video WAN web applications|none|['Mac', 'OS X', 'Linux', 'Microsoft Office', 'Active Directory', 'Mac', 'UNIX', 'Android', 'Linux', 'Unix', 'Android', 'Microsoft Office', 'Adobe Creative Suite', 'Active Directory', 'PowerShell', 'DHCP', 'Active Directory', 'DHCP', 'DNS', 'Linux', 'Mac', 'Exchange', 'Microsoft Office', 'Office', 'Windows 7', 'Windows', 'Windows XP', 'Unix']
Data Engineer|https://www.livecareer.com/resume-search/r/aws-data-engineer-fcef99cc1f1c4bdeb25cbb8d5b3ac32c|339690346324715490168338998099130748299|Jessica    Claire                                   609 Johnson Ave       49204     Tulsa     OK   100 Montgomery St 10th Floor    H   555 4321000    C       resumesampleexamplecom    Date of Birth         India                      single                    Summary       Dynamic and motivated IT professional with around 7 years of experience as a Big Data Engineer with expertise in designing data intensive applications using Hadoop Ecosystem  Big Data Analytical  Cloud Data engineering  Data Warehouse  Data Mart Data Visualization  Reporting  and Data Quality solutions   In  depth knowledge of Hadoop architecture and its components like YARN  HDFS Name Node Data Node Job Tracker Application Master Resource Manager  Task Tracker and Map Reduce programming paradigm  Extensive experience in Hadoop led development of enterprise level solutions utilizing Hadoop components such as Apache Spark MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper and YARN  Profound experience in performing Data Ingestion Data Processing Transformations enrichment and aggregations  Strong Knowledge on Architecture of Distributed systems and Parallel processing Indepth understanding of MapReduce programming paradigm and Spark execution framework  Experienced with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  SparkSQL  Dataframe API  Spark Streaming MLlib  Pair RDD s and worked explicitly on PySpark and Scala   Handled ingestion of data from different data sources into HDFS using Sqoop Flume and perform transformations using Hive Map Reduce and then loading data into HDFS  Managed Sqoop jobs with incremental load to populate HIVE external tables Experience in importing streaming data into HDFS using Flume sources and Flume sinks and transforming the data using Flume interceptors  Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph  DAG  of actions with control flows  Implemented the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing  Experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance   Experience with different file formats like Avro  parquet  ORC  Json and XML   Expertise in Creating Debugging Scheduling and Monitoring jobs using Airflow and Oozie  Experienced with using most common Operators in Airflow  Python Operator Bash Operator Google Cloud Storage Download Operator Google Cloud Storage Object Sensor  Handson experience in handling database issues and connections with SQL and NoSQL databases such as MongoDB  HBase  Cassandra  SQL server  and PostgreSQL   Created Java apps to handle data in MongoDB and HBase Used Phoenix to create SQL layer on HBase  Experience in designing and creating RDBMS Tables Views User Created Data Types Indexes Stored Procedures Cursors Triggers and Transactions  Expert in designing ETL data flows using creating mappingsworkflows to extract data from SQL Server and Data Migration and Transformation from OracleAccessExcel Sheets using SQL Server SSIS   Expert in designing Parallel jobs using various stages like Join Merge Lookup remove duplicates Filter Dataset Lookup file set Complex flat file Modify Aggregator XML  Handson experience with Amazon EC2 Amazon S3 Amazon RDS VPC IAM Amazon Elastic Load Balancing Auto Scaling CloudWatch SNS SES SQS Lambda EMR and other services of the AWS family  Created and configured new batch job in Denodo scheduler with email notification capabilities and Implemented Cluster setting for multiple Denodo node and created load balance for improving performance activity  Instantiated created and maintained CICD continuous integration  deployment pipelines and apply automation to environments and applications  Worked on various automation tools like GIT Terraform Ansible Experienced in fact dimensional modeling  Star schema Snowflake schema  transactional modeling and SCD Slowly changing dimension  Experienced with JSON based RESTful web services and XMLQML based SOAP web services and also worked on various applications using python integrated IDEs like Sublime Text and PyCharm   Efficient Cloud Engineer with years of experience assembling cloud infrastructure Utilizes strong managerial skills by negotiating with vendors and coordinating tasks with other IT team members Implements best practices to create cloud functions applications and databases         Skills          ·  Big Data Technologies  Hadoop MapReduce HDFS Sqoop PIG Hive HBase Oozie Flume NiFi Kafka Zookeeper Yarn Apache Spark Mahout Sparklib  ·  Databases  Oracle MySQL SQL Server MongoDB Cassandra DynamoDB PostgreSQL Teradata Cosmos  ·  Programming  Python PySpark Scala Java C C Shell script Perl script SQL  ·  Cloud Technologies  AWS Microsoft Azure  ·  Frameworks  Django REST framework MVC Hortonworks  ·  Tools  PyCharm Eclipse Visual Studio SQLPlus SQL Developer TOAD SQL Navigator Query Analyzer SQL Server Management Studio SQL Assistance Eclipse Postman  ·  Versioning tools  SVN Git GitHub    ·  Operating Systems  Windows 78XP20082012 Ubuntu Linux MacOS  ·  Network Security  Kerberos  ·  Database Modelling  Dimension Modeling ER Modeling Star Schema Modeling Snowflake Modeling  ·  Monitoring Tool  Apache Airflow  ·  Visualization Reporting  Tableau ggplot2 matplotlib SSRS and Power BI  ·  Machine Learning Techniques  Linear  Logistic Regression Classification and Regression Trees Random Forest Associative rules NLP and Clustering                      Experience       AWS Data Engineer       012022      022022     Accenture Contractor Jobs    –    Rochester     NY            Designed and setup Enterprise Data Lake to provide support for various uses cases including Analytics processing storing and Reporting of voluminous rapidly changing data  Responsible for maintaining quality reference data in source by performing operations such as cleaning transformation and ensuring Integrity in a relational environment by working closely with the stakeholders  solution architect  Designed and developed Security Framework to provide fine grained access to objects in AWS S3 using AWS Lambda DynamoDB  Set up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users  Performed end toend Architecture  implementation assessment of various AWS services like Amazon EMR Redshift S3  Implemented the machine learning algorithms using python to predict the quantity a user might want to order for a specific item so we can automatically suggest using kinesis firehose and S3 data lake  Used AWS EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service Amazon S3 and Amazon DynamoDB  Used Spark SQL for Scala  amp Python interface that automatically converts RDD case classes to schema RDD  Import the data from different sources like HDFSHBase into Spark RDD and perform computations using PySpark to generate the output response  Creating Lambda functions with Boto3 to deregister unused AMIs in all application regions to reduce the cost for EC2 resources  Importing  exporting database using SQL Server Integrations Services SSIS and Data Transformation Services DTS Packages  Coded Teradata BTEQ scripts to load transform data fix defects like SCD 2 date chaining cleaning up duplicates  Developed reusable framework to be leveraged for future migrations that automates ETL from RDBMS systems to the Data Lake utilizing Spark Data Sources and Hive data objects  Conducted Data blending Data preparation using Alteryx and SQL for Tableau consumption and publishing data sources to Tableau server  Developed Kibana Dashboards based on the Log stash data and Integrated different source and target systems into Elasticsearch for near real time log analysis of monitoring End to End transactions  Implemented AWS Step Functions to automate and orchestrate the Amazon SageMaker related tasks such as publishing data to S3 training ML model and deploying it for prediction  Integrated Apache Airflow with AWS to monitor multistage ML workflows with the tasks running on Amazon SageMaker  Environment AWS EMR S3 RDS Redshift Lambda Boto3 DynamoDB Amazon SageMaker Apache Spark HBase Apache Kafka HIVE SQOOP Map Reduce Snowflake Apache Pig Python SSRS Tableau  Assessed organization technology infrastructure and managed cloud migration process  Configured computing networking and security systems within cloud environment  Implemented cloud policies managed technology requests and maintained service availability           Data Engineer       012016      112019     Verizon    –    Beaverton     OR            Worked on Azure Data Factory to integrate data of both onprem MY SQL Cassandra and cloud Blob storage Azure SQL DB and applied transformations to load back to Azure Synapse  Managed Configured and scheduled resources across the cluster using Azure Kubernetes Service  Monitored Spark cluster using Log Analytics and Ambari Web UI  Transitioned log storage from Cassandra to Azure SQL Datawarehouse and improved the query performance  Involved in developing data ingestion pipelines on Azure HDInsight Spark cluster using Azure Data Factory and Spark SQL  Also Worked with Cosmos DB SQL API and Mongo API  Develop dashboards and visualizations to help business users analyze data as well as providing data insight to upper management with a focus on Microsoft products like SQL Server Reporting Services SSRS and Power BI  Performed the migration of large data sets to Databricks Spark create and administer cluster load data configure data pipelines loading data from ADLS Gen2 to Databricks using ADF pipelines  Created various pipelines to load the data from Azure data lake into Staging SQLDB and followed by to Azure SQL DB  Created Databrick notebooks to streamline and curate the data for various business use cases and also mounted blob storage on Databrick  Utilized Azure Logic Apps to build workflows to schedule and automate batch jobs by integrating apps ADF pipelines and other services like HTTP requests email triggers etc  Worked extensively on Azure data factory including data transformations Integration Runtimes Azure Key Vaults Triggers and migrating data factory pipelines to higher environments using ARM Templates  Ingested data in minibatches and performs RDD transformations on those minibatches of data by using Spark Streaming to perform streaming analytics in Data bricks  Environment Azure SQL DW Databrick Azure Synapse Cosmos DB ADF SSRS Power BI Azure Data lake ARM Azure HDInsight Blob storage Apache Spark  Adept in troubleshooting and identifying current issues and providing effective solutions  Managed performance monitoring and tuning while identifying and repairing issues within database realm  Identified key use cases and associated reference architectures for market segments and industry verticals  Designed surveys opinion polls and assessment tools to collect data  Tested validated and reformulated models to foster accurate prediction of outcomes  Created graphs and charts detailing data analysis results  Recommended data analysis tools to address business issues  Developed new functions and applications to conduct analyses  Cleaned and manipulated raw data  Collaborated with solution architects to define database and analytics engagement strategies for operational territories and key accounts           Big Data Engineer  Hadoop Developer       102013      122015     Two95 International Inc    –    Boca Raton     FL          AnsibleDenodoDenodoCloudWatchAvroPySparkPySparkPySparkMLlibDataframeNiFiNiFi  Interacted with business partners Business Analysts and product owner to understand requirements and build scalable distributed data solutions using Hadoop ecosystem  Developed Spark Streaming programs to process near real time data from Kafka and process data with both stateless and state full transformations  Worked with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries  Built and implemented automated procedures to split large files into smaller batches of data to facilitate FTP transfer which reduced 60 of execution time  Worked on developing ETL processes Data Stage Open Studio to load data from multiple data sources to HDFS using FLUME and SQOOP and performed structural modifications using Map Reduce HIVE  D eveloping Spark scripts UDFS using both Spark DSL and Spark SQL query for data aggregation querying and writing data back into RDBMS through Sqoop  Written multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregationAvrom multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file formats Codecs like gZip Snappy Lzo  Strong understanding of Partitioning bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance  Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders  Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake  Experience in report writing using SQL Server Reporting Services SSRS and creating various types of reports like drill down Parameterized Cascading Conditional Table Matrix Chart and Sub Reports  Used DataStax Spark connector which is used to store the data into Cassandra database or get the data from Cassandra database  Wrote oozie scripts and setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs  Worked on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform  Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard  Transformed tPySpark using AWS Glue dynamic frames with PySpark cataloged the transformed the data using Crawlers and scheduled the job and crawler using workflow feature  Worked on installing cluster commissioning  decommissioning of data node name node recovery capacity planning and slots configuration  Developed data pipeline programs with Spark Scala APIs data aggregations with Hive and formatting data JSON for visualization and generatiPySpark     Environment AWS Cassandra PySpark Apache Spark HBase Apache Kafka HIVE SQOOP FLUME Apache oozie Zookeeper ETL UDF Map Reduce Snowflake Apache Pig Python Java SSRS  Onfidential  Developed and implemented Hadoop code while observing coding standards  Optimized and tuned Hadoop environments and modified hardware to meet prescribed performance thresholds  Developed new functions and applications to conduct analyses  Created graphs and charts detailing data analysis results  Tested validated and reformulated models to foster accurate prediction of outcomes           Python Developer        092012      102013     Fiserv    –    City     STATE            AWS S3 EC2 LAMBDA EBS IAM Datadog CloudTrail CLI Ansible MySQL Python Git Jenkins DynamoDB Cloud Watch Docker Kubernetes  Leveraged open communication collective decisionmaking and thorough reviews to create performant and scalable systems  Worked with serverside and frontend technologies and leveraged common design patterns to code dynamic and userfriendly systems  Implemented new API routes architected new ORM structures and refactored code to boost application performance  Harnessed version control tools to coordinate project development and individual code submissions  Introduced cloudbased technologies into Python development to expand onpremise deployment options  Wrote clear and clean code for use in projects  Resolved customer issues by establishing workarounds and solutions to debug and create defect fixes          Education and Training       Post Graduate        Data Engineering        Expected in   022022                Purdue University      West Lafayette     IN     GPA        Status                  Post Graduate        Data Science And Business Analytics        Expected in   092021                University of Texas At Austin      Austin     TX     GPA        Status                  Bachelor of Arts       Business Administration And Management       Expected in   122009                Califonia State University       Fullerton CA          GPA        Status   |none|['Big Data', 'Hadoop', 'Big Data', 'Data Visualization', 'Hadoop', 'Hadoop', 'Hadoop', 'Spark', 'Hive', 'HBase', 'Kafka', 'programming', 'Spark', 'Spark', 'optimization', 'algorithms', 'Hadoop', 'Spark', 'Spark', 'PySpark', 'Scala', 'Hive', 'HIVE', 'Hadoop', 'Hadoop', 'Hive', 'Hive', 'Json', 'XML', 'Airflow', 'Airflow', 'Python', 'Bash', 'Google Cloud', 'Google Cloud', 'SQL', 'NoSQL', 'MongoDB', 'HBase', 'Cassandra', 'SQL server', 'PostgreSQL', 'Java', 'MongoDB', 'HBase', 'Phoenix', 'SQL', 'HBase', 'RDBMS', 'ETL', 'SQL Server', 'SQL Server', 'SSIS', 'XML', 'AWS', 'automation', 'automation', 'GIT', 'Ansible', 'modeling', 'Snowflake', 'modeling', 'JSON', 'SOAP web services', 'python', 'Sublime', 'Big Data', 'Hadoop', 'MapReduce', 'HDFS', 'Sqoop', 'PIG', 'Hive', 'HBase', 'Oozie', 'Flume', 'NiFi', 'Kafka', 'Zookeeper', 'Yarn', 'Apache', 'Spark', 'Mahout', 'Sparklib', 'Oracle', 'MySQL', 'SQL Server', 'MongoDB', 'Cassandra', 'DynamoDB', 'PostgreSQL', 'Teradata', 'Cosmos', 'Programming', 'Python', 'PySpark', 'Scala', 'Java', 'C', 'Shell', 'Perl', 'SQL', 'AWS', 'Azure', 'Django', 'REST', 'MVC', 'Hortonworks', 'PyCharm', 'Eclipse', 'Visual Studio', 'SQLPlus', 'SQL Developer', 'TOAD', 'SQL', 'Query Analyzer', 'SQL Server', 'SQL', 'Eclipse', 'Postman', 'SVN', 'Git', 'GitHub', 'Windows', 'Ubuntu Linux', 'Network Security', 'Modeling', 'Modeling', 'Star Schema', 'Modeling', 'Snowflake', 'Modeling', 'Apache', 'Airflow', 'Tableau', 'ggplot2', 'matplotlib', 'SSRS', 'Power BI', 'Machine Learning', 'NLP', 'Clustering', 'AWS', 'AWS S3', 'DynamoDB', 'HDFS', 'Hive', 'Pig', 'MapReduce', 'AWS', 'Amazon EMR', 'Redshift', 'S3', 'machine learning', 'algorithms', 'python', 'AWS EMR', 'DynamoDB', 'Spark', 'SQL', 'Scala', 'Python', 'Spark', 'PySpark', 'SQL Server', 'SSIS', 'Teradata', 'ETL', 'RDBMS', 'Spark', 'Hive', 'Data blending', 'Alteryx', 'SQL', 'Tableau', 'Tableau', 'Kibana', 'Elasticsearch', 'AWS', 'Apache', 'Airflow', 'AWS', 'AWS', 'Redshift', 'Lambda', 'Boto3', 'DynamoDB', 'Amazon SageMaker', 'Apache', 'Spark', 'HBase', 'Apache', 'Kafka', 'HIVE', 'SQOOP', 'Map Reduce', 'Snowflake', 'Apache Pig', 'Python', 'SSRS', 'Tableau', 'networking', 'Azure Data Factory', 'SQL', 'Cassandra', 'Azure', 'SQL', 'Azure Synapse', 'Azure', 'Kubernetes', 'Spark', 'Cassandra', 'Azure', 'SQL', 'Azure', 'Spark', 'Azure Data Factory', 'Spark', 'SQL', 'Cosmos', 'SQL', 'Mongo', 'SQL Server Reporting Services', 'SSRS', 'Power BI', 'Databricks', 'Spark', 'Databricks', 'ADF', 'Azure data lake', 'Azure', 'SQL', 'use cases', 'Databrick', 'Azure', 'ADF', 'Azure data factory', 'Azure', 'Spark', 'Azure', 'SQL', 'Databrick', 'Azure', 'ADF', 'SSRS', 'Power BI', 'Azure Data lake', 'Spark', 'use cases', 'data analysis', 'data analysis', 'Big Data', 'Hadoop', 'Hadoop', 'Spark', 'Kafka', 'HIVE', 'ETL', 'HIVE', 'Spark', 'Spark', 'Spark', 'SQL', 'RDBMS', 'Java', 'Hive', 'XML', 'JSON', 'CSV', 'Hive', 'Hive', 'Business Requirements', 'ETL', 'Python', 'Snowflakes', 'SnowSQL', 'SQL', 'Snowflake', 'SQL Server Reporting Services', 'SSRS', 'Spark', 'Cassandra', 'Cassandra', 'Hadoop', 'Scala', 'Kafka', 'Hive', 'HBase', 'AWS Glue', 'PySpark', 'Spark', 'Scala', 'Hive', 'JSON', 'AWS', 'Cassandra', 'PySpark', 'Apache', 'Spark', 'HBase', 'Apache', 'Kafka', 'HIVE', 'SQOOP', 'FLUME', 'Apache oozie', 'Zookeeper', 'ETL', 'Map Reduce', 'Snowflake', 'Pig', 'Python', 'Java', 'SSRS', 'Hadoop', 'Hadoop', 'data analysis', 'Python', 'AWS S3', 'Ansible', 'MySQL', 'Python', 'Git', 'Jenkins', 'DynamoDB', 'Cloud Watch', 'Docker', 'Kubernetes', 'design patterns', 'Python']
Data Engineer|https://www.livecareer.com/resume-search/r/data-science-data-engineer-intern-bd32a7569cbb4dd2a9f164b4801eef35|266612002933194965787459684177947415296|Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Summary     Highly motivated Sales Associate with extensive customer service and sales experience Outgoing sales professional with track record of driving increased sales improving buying experience and elevating company profile with target market           Skills         Machine LearningData Mining  NLP  Linear Regression neural networksdeep learning Naive Bayes SVM Logistic Regression  decision trees Kmean KNN N Grams edit distance gradient descent  Statistical Programming  Packages  R Python NumPy Matplotlib scikitlearn pandas ggplot2 Shiny dplyr caret e1071keras  Business Intelligence  Visualization  Tableau Qlik View Qlik Sense Excel OBIEE  Hadoop Ecosystem Components  Spark Hive Sqoop Flume Kafka Impala  Databases  Oracle MySql PostgreSql  Oracle ERP  Fusion Middleware  Oracle EBusiness Suite 11i  R12 Oracle SOA Oracle Service Bus  Other Languages  Tools  SQL PLSQL core java Scala RStudio Jupyter SqlDeveloper Toad Atom  Certifications  Tableau SQL  PLSQL                       Education and Training       University of Utah David Eccles School of Business    Salt Lake City     Utah      Expected in   August 2017     –      –       Master of Science        Information Systems          GPA           Information Systems Data Science and Analytics specialization   Recipient of 15000 Graduate Fellowship from David Eccles School of Business Academic Capstone Project  Big Data  Building statistical regression and classification models cleaning exploring data and developing interactive web interface using R Shiny which helps the company to classify clients loan type and predicting the amount of loan they will take in future Kaggle  House Prices Predictions  Applied different machine learning simple and advanced models on housing predictors for predicting the sales prices of houses used imputation methods for filling missing and null values in the data set Independent Study  Research  Apache Spark using Scala and Python Rajeev Gandhi Memorial College of Engineering and Technology          India                        Expected in   May 2012     –      –       Bachelor of Science        Computer Science and Engineering          GPA           Computer Science and Engineering Designed Hand Draw Shape Recognition interface which helps the user to invoke desired application just by drawing the shape linked to the application          Experience       Envestnet      Data Science  Data Engineer Intern   Secaucus     NJ                   012017      Present     Working on Big Data Ingestion using Sqoop for transferring data from multiple MySql database servers to transient storage in amazon EMR Hcatalog and using Hive to transfer data to persistent storage in amazon S3 bucket  Developing Sqoop and Hive scripts for data ingestion  Using R and spark in amazon EMR for filtering exploring analyzing providing insights on data and developing reports           First American Corporation      Oracle Technical Consultant  Data Analyst   City          India              022013      072016     Created SQL scripts for daily extracts adhoc requests  reporting and for analyzing large data sets  Designed ER diagrams conceptual models logical and physical models created database objects  Tables Indexes Sequences and Views  Developed Oracle Business Intelligence reports created and modified Oracle database objects  Tables Views and Indexes which increased the performance of Oracle Business Intelligence reports by 60 in production environment  Prepared SQL  Loader scripts for loading data from other systems into oracle ERP system worked with onsite business team in performing data fixes  Created PLSQL interfaces for doing business validation transferring data between ERP modules and loading data to base tables           CMC Limited      Data Analyst Intern   City          India              112012      012013     gt Developed SQL scripts worked on oracle 11i database and oracle reports          Skills     Academic ad Apache Big Data Business Intelligence Draw clients Data Mining Databases database EBusiness edit ERP filling drawing java Machine Learning Excel Middleware MySql NLP networks neural Oracle Oracle database PLSQL PostgreSql Programming Python reporting Research sales servers scripts SQL SQLLoader Tableau Tables Toad type validation View|none|['NLP', 'Linear Regression', 'Naive Bayes', 'SVM', 'Logistic Regression', 'decision trees', 'Kmean', 'KNN', 'N Grams', 'distance gradient descent', 'R', 'Python', 'NumPy', 'scikitlearn', 'pandas', 'ggplot2', 'Shiny', 'dplyr', 'caret', 'Tableau', 'Qlik', 'Qlik', 'Excel', 'OBIEE', 'Hadoop', 'Spark', 'Hive', 'Sqoop', 'Flume', 'Kafka', 'Impala', 'Oracle', 'MySql', 'PostgreSql', 'Oracle', 'Oracle', 'Oracle', 'Oracle', 'SQL', 'java', 'Scala', 'Jupyter', 'Tableau', 'SQL', 'PLSQL', 'Big Data', 'R', 'machine learning', 'Spark', 'Scala', 'Python', 'Big Data', 'MySql', 'Hive', 'Hive', 'R', 'spark', 'Oracle', 'SQL', 'Oracle', 'Oracle', 'Oracle', 'SQL', 'oracle', 'SQL', 'oracle', 'oracle', 'Big Data', 'Data Mining', 'java', 'Machine Learning', 'Excel', 'MySql', 'NLP', 'Oracle', 'PLSQL', 'PostgreSql', 'Programming', 'Python', 'SQL', 'Tableau']
Data Engineer|https://www.livecareer.com/resume-search/r/data-center-engineer-b2eec8f867b4482e996b65dec2238532|224493945599255295224336362821325623329|Jessica  Claire                             resumesampleexamplecom                      555 4321000                       Montgomery Street     San Francisco     CA      94105                                                                                                                                                                                                             Profile      Dedicated Data Center Engineer with excellent technical analytical and communication skills demonstrated by 5 years of experience  Innovative Fiber Optic Engineer specialize in Design and DeploymentExperience with purchasing products and vendor relationships  Multitask oriented Resourceful and Businesssavvy            Skills         Project Management  Fiber optic Design Implementation  Infrastructure Cabling Solutions  Analog Circuit Design and Deployment      PBX Telecommunications  Microsoft Office Suite Word Excel Power PointVisio  Reqlogic 8                        Education and Training       University of Phoenix               Expected in   2014     –      –       Bachelor of Science        Management          GPA                    Katharine Gibbs School    New York     NY      Expected in   2007     –      –       Associate of Applied Science        Computer Networking           GPA            Deans List Academic Achievement Award  38 GPA  Graduated Cum Laude           Accomplishments      First Technician on DCO team to achieve FOA Certified Fiber Optic Technician  and to progress to Certified Fiber Optic Design Certification   Promoted to Engineer and 2nd Shift Lead Supervisor    Successfully Trained 7 other DCO Staff Members in Fiber Optic testing and terminations   Developed Fiber Optic Strategy Deployment to help reduce installation times  This is achieved by limiting the amount of cables to be extended by increasing fiber infrastructure out to our customers and shortening the distances to within 100 FT or less Thus saving time and materials         Professional Experience       Sentinel Technologies Inc      Data Center Engineer    Lansing     MI                   072012      Current     Designed and develop Fiber Optic Infrastructure for 990000 Sq Ft Facility  Test and trouble shoot Layer 1 and 2 Local Area Network issues    Supervise 2nd shift technicians provide leadership and training for new technicians    Maintain Inventory and purchasing of all project materials    Negotiate competitive with vendors on pricing through cost evaluation of daily materials consumed   Provide estimates and cost analysis of cabling projects   Coordinate with cross functional departments and outsourced vendors to meet deadlines    Maintain Office infrastructure deployment of Internet Data and Voice Applications PBX and VOIP              Contegix      Data Center Technician    Arlington     VA                   032007      072012       Maintain Inventory and Purchase Materials for daily work and projects  Field and document network related trouble notifications while Monitoring troubleshooting and resolving Tier 1 and 2 Local Area Network problems  Install and assist with the design of Cat6 6a Copper Coax and Fiber connections for new integrated networks serving major corporations  Mounting and Installation of IBM Dell Sun Microsystem Servers Cisco Juniper Firewalls Routers Switches and DWDM Optical equipment           United States Air Force      Installation Patrolman   City     STATE                   2002      2006      Developed installation traffic management programs   Provided police services to over 10000 personnel in community   Airman of the Year Nominee   Provided security for multibillion dollar aircraft a deep spacetracking station and non nuclear munitions storage areas   Directly contributed to 48th Security Forces Squadron selection as “Outstanding Large Security Force Unit” for 2003|none|[]
