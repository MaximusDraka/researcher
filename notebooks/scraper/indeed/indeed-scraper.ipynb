{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Apr 28 11:35:04 2020\n",
    "@author: chrislovejoy\n",
    "\"\"\"\n",
    "\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys  \n",
    "sys.path.insert(1, 'C:/Users/tom/projects/skill-skeleton/utils/')\n",
    "\n",
    "import spacy_util\n",
    "\n",
    "def find_jobs_from(website: str, job_title: str, location: str, desired_characs: list, filename: str=\"results\", pages: int=1):    \n",
    "    \"\"\"\n",
    "    This function extracts all the desired characteristics of all new job postings\n",
    "    of the title and location specified and returns them in single file.\n",
    "    The arguments it takes are:\n",
    "        - Website: to specify which website to search (options: 'Indeed' or 'CWjobs')\n",
    "        - Job_title\n",
    "        - Location\n",
    "        - Desired_characs: this is a list of the job characteristics of interest,\n",
    "            from titles, companies, links and date_listed.\n",
    "        - Filename: to specify the filename and format of the output.\n",
    "            Default is .xls file called 'results.xls'\n",
    "    \"\"\"\n",
    "       \n",
    "    if website == 'Indeed':\n",
    "        start=0\n",
    "        while start < pages:\n",
    "            job_soup = load_indeed_jobs_div(job_title, location, start)                                         \n",
    "            jobs_list, num_listings = extract_job_information_indeed(job_title,job_soup, desired_characs)\n",
    "                                          \n",
    "            start += 1    \n",
    "            \n",
    "            save_jobs_to_csv(jobs_list, filename + '_' + str(start))    \n",
    "            print('{} new job postings retrieved from {}. Stored in {}.'.format(num_listings, website, filename))\n",
    "    \n",
    "    #save_jobs_to_excel(jobs_list, filename)   \n",
    "    \n",
    "\n",
    "## ======================= GENERIC FUNCTIONS ======================= ##\n",
    "\n",
    "def save_jobs_to_excel(jobs_list: list, filename: str):\n",
    "    jobs = pd.DataFrame(jobs_list)\n",
    "    jobs.to_excel(filename+ \".xlsx\",index=True, index_label=\"Id\")\n",
    "\n",
    "\n",
    "def save_jobs_to_csv(jobs_list: list, filename: str):\n",
    "    jobs = pd.DataFrame(jobs_list)\n",
    "    jobs.to_csv(filename + \".csv\",index=True, index_label=\"Id\")\n",
    "\n",
    "## ================== FUNCTIONS FOR INDEED.COM =================== ##\n",
    "\n",
    "\n",
    "def load_indeed_job_descriptions_from_list(jobs_list: list):\n",
    "    job_descs = []\n",
    "    for i in range(len(jobs_list)):\n",
    "        job_descs.append(load_indeed_job_description(jobs_list[i]))\n",
    "    return job_descs\n",
    "\n",
    "\n",
    "def load_indeed_job_description(job: str):\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(job)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    job_desc = soup.find('div', class_='jobsearch-jobDescriptionText')\n",
    "    return job_desc.text.strip()\n",
    "\n",
    "\n",
    "def load_indeed_jobs_div(job_title: str, location: str, start: int=0) -> BeautifulSoup:\n",
    "    if start == 0:\n",
    "        getVars = {'q' : job_title, 'rbl' : location, 'fromage' : 'last', 'sort' : 'date'}\n",
    "    else:\n",
    "        param_start = (start+1)*10\n",
    "        \n",
    "        getVars = {'q' : job_title, 'rbl' : location, 'fromage' : 'last', 'sort' : 'date', 'start' : param_start}\n",
    "    \n",
    "    url = ('https://www.indeed.com/jobs?' + urllib.parse.urlencode(getVars))    \n",
    "    print(url)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)    \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")    \n",
    "    job_soup = soup.find(id=\"mosaic-jobResults\")\n",
    "    driver.close()\n",
    "    \n",
    "    return job_soup\n",
    "\n",
    "\n",
    "def extract_job_information_indeed(job_title: str, job_soup: BeautifulSoup, desired_characs: list) -> tuple[dict,int]:\n",
    "    job_elems = job_soup.find_all('div', class_='cardOutline') \n",
    "         \n",
    "    cols = []\n",
    "    extracted_info = []\n",
    "    \n",
    "    cols.append('profile')\n",
    "    extracted_info.append(job_title)   \n",
    "    \n",
    "    if 'titles' in desired_characs:\n",
    "        titles = []\n",
    "        cols.append('titles')\n",
    "        for job_elem in job_elems:\n",
    "            titles.append(extract_job_title_indeed(job_elem))\n",
    "        extracted_info.append(titles)                    \n",
    "    \n",
    "    if 'companies' in desired_characs:\n",
    "        companies = []\n",
    "        cols.append('companies')\n",
    "        for job_elem in job_elems:\n",
    "            companies.append(extract_company_indeed(job_elem))\n",
    "        extracted_info.append(companies)\n",
    "    \n",
    "    if 'links' in desired_characs:\n",
    "        links = []\n",
    "        data = []\n",
    "        cols.append('links')        \n",
    "        cols.append('data')\n",
    "        driver2 = webdriver.Chrome()\n",
    "        \n",
    "        for job_elem in job_elems:\n",
    "            temp_link = extract_link_indeed(job_elem)            \n",
    "            links.append(temp_link)          \n",
    "                        \n",
    "            try:\n",
    "                driver2.get(temp_link)                   \n",
    "                soup = BeautifulSoup(driver2.page_source, \"html.parser\")\n",
    "                job_desc = soup.find('div', class_='jobsearch-JobComponent-description')         \n",
    "                data.append(job_desc.text.strip().replace('|', '-').replace(\"&nbsp;\",\" \").replace(\"\\n\",\" \"))\n",
    "            except:\n",
    "                data.append('None')\n",
    "        \n",
    "        driver2.close()            \n",
    "        extracted_info.append(links)\n",
    "        extracted_info.append(data)\n",
    "    \n",
    "    if 'date_listed' in desired_characs:\n",
    "        dates = []\n",
    "        cols.append('date_listed')\n",
    "        for job_elem in job_elems:\n",
    "            dates.append(extract_date_indeed(job_elem))\n",
    "        extracted_info.append(dates)    \n",
    "    \n",
    "    jobs_list = {}\n",
    "    \n",
    "    for j in range(len(cols)):\n",
    "        jobs_list[cols[j]] = extracted_info[j]\n",
    "    \n",
    "    num_listings = len(extracted_info[0]) \n",
    "    \n",
    "    \n",
    "    return jobs_list, num_listings\n",
    "\n",
    "\n",
    "def extract_job_title_indeed(job_elem) -> str:\n",
    "    title_elem = job_elem.find('h2', class_='jobTitle')\n",
    "    title = title_elem.text.strip()\n",
    "    return title\n",
    "\n",
    "\n",
    "def extract_company_indeed(job_elem) -> str:\n",
    "    company_elem = job_elem.find('div', class_='company_location')\n",
    "    company = company_elem.text.strip()\n",
    "    return company\n",
    "\n",
    "\n",
    "def extract_link_indeed(job_elem) -> str:\n",
    "    link = job_elem.find('a')['href']\n",
    "    link = 'http://www.indeed.com' + link\n",
    "    return link\n",
    "\n",
    "\n",
    "def extract_date_indeed(job_elem) -> str:\n",
    "    date_elem = job_elem.find('span', class_='date')\n",
    "    date = \"NA\"\n",
    "    \n",
    "    if date_elem is not None:\n",
    "        date = date_elem.text.strip()\n",
    "        \n",
    "    return date\n",
    "\n",
    "def delete_csv_files():\n",
    "    files = glob.glob('*.csv')\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=Data+Scientist&rbl=&fromage=last&sort=date\n",
      "14 new job postings retrieved from Indeed. Stored in data_scientist_jobs.\n",
      "https://www.indeed.com/jobs?q=Data+Scientist&rbl=&fromage=last&sort=date&start=20\n",
      "14 new job postings retrieved from Indeed. Stored in data_scientist_jobs.\n",
      "https://www.indeed.com/jobs?q=Data+Scientist&rbl=&fromage=last&sort=date&start=30\n",
      "14 new job postings retrieved from Indeed. Stored in data_scientist_jobs.\n",
      "https://www.indeed.com/jobs?q=Data+Scientist&rbl=&fromage=last&sort=date&start=40\n",
      "14 new job postings retrieved from Indeed. Stored in data_scientist_jobs.\n",
      "https://www.indeed.com/jobs?q=Data+Scientist&rbl=&fromage=last&sort=date&start=50\n",
      "14 new job postings retrieved from Indeed. Stored in data_scientist_jobs.\n",
      "https://www.indeed.com/jobs?q=Data+Scientist&rbl=&fromage=last&sort=date&start=60\n",
      "14 new job postings retrieved from Indeed. Stored in data_scientist_jobs.\n",
      "https://www.indeed.com/jobs?q=Data+Scientist&rbl=&fromage=last&sort=date&start=70\n",
      "14 new job postings retrieved from Indeed. Stored in data_scientist_jobs.\n",
      "['data_scientist_jobs_1.csv', 'data_scientist_jobs_2.csv', 'data_scientist_jobs_3.csv', 'data_scientist_jobs_4.csv', 'data_scientist_jobs_5.csv', 'data_scientist_jobs_6.csv', 'data_scientist_jobs_7.csv']\n"
     ]
    }
   ],
   "source": [
    "desired_characs = ['titles', 'companies', 'links', 'date_listed']\n",
    "pages = 7\n",
    "output_file = 'indeed-data.csv'\n",
    "\n",
    "#Clear folder of csv files\n",
    "delete_csv_files()\n",
    "\n",
    "\n",
    "#['Data Analyst','Data Engineer','Data Scientist','Business Analyst','Software Engineer','Machine Learning Engineer','Cloud Engineer']\n",
    "\n",
    "\n",
    "#Data Scientist\n",
    "#find_jobs_from('Indeed', 'Data Analyst', '', desired_characs, filename=\"data_analyst_jobs\", pages=pages)\n",
    "#Data Engineer\n",
    "#find_jobs_from('Indeed', 'Data Engineer', '', desired_characs, filename=\"data_engineer_jobs\", pages=pages)\n",
    "#Data Scientist\n",
    "find_jobs_from('Indeed', 'Data Scientist', '', desired_characs, filename=\"data_scientist_jobs\", pages=pages)\n",
    "\n",
    "#Out of scope - We need more data for less profiles\n",
    "#Machine Learning Engineer\n",
    "#find_jobs_from('Indeed', 'Machine Learning Engineer', '', desired_characs, filename=\"machine_learning_engineer_jobs\", pages=pages)\n",
    "#Cloud Engineer\n",
    "#find_jobs_from('Indeed', 'Cloud Engineer', '', desired_characs, filename=\"cloud_engineer_jobs\", pages=pages)\n",
    "#Business Analyst\n",
    "#find_jobs_from('Indeed', 'Business Analyst', '', desired_characs, filename=\"business_analyst_jobs\", pages=pages)\n",
    "#Software Engineer\n",
    "#find_jobs_from('Indeed', 'Software Engineer', '', desired_characs, filename=\"software_engineer_jobs\", pages=pages)\n",
    "\n",
    "\n",
    "csv_files = glob.glob('*.{}'.format('csv'))\n",
    "\n",
    "print(csv_files)  \n",
    "     \n",
    "df = pd.concat([pd.read_csv(file) for file in csv_files ], ignore_index=True)\n",
    "\n",
    "df['data'] = df['data'].apply(spacy_util.remove_punctuation)  \n",
    "\n",
    "# Specify the columns to consider when looking for duplicates\n",
    "columns = ['titles', 'companies']\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset=columns)\n",
    " \n",
    "df.to_csv(output_file,sep='|',index=False, encoding='utf-8')\n",
    "\n",
    "#NER --- df['data']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
